digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1699085896304 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1698747987232 [label=AddmmBackward0]
	1698747986896 -> 1698747987232
	1698747709200 [label="fc.bias
 (19)" fillcolor=lightblue]
	1698747709200 -> 1698747986896
	1698747986896 [label=AccumulateGrad]
	1698747986272 -> 1698747987232
	1698747986272 [label=ViewBackward0]
	1698747985792 -> 1698747986272
	1698747985792 [label=MeanBackward1]
	1698747985456 -> 1698747985792
	1698747985456 [label=ReluBackward0]
	1698747984976 -> 1698747985456
	1698747984976 [label=AddBackward0]
	1698747984496 -> 1698747984976
	1698747984496 [label=CudnnBatchNormBackward0]
	1698747983392 -> 1698747984496
	1698747983392 [label=ConvolutionBackward0]
	1698747982432 -> 1698747983392
	1698747982432 [label=ReluBackward0]
	1698747982096 -> 1698747982432
	1698747982096 [label=CudnnBatchNormBackward0]
	1698747981616 -> 1698747982096
	1698747981616 [label=ConvolutionBackward0]
	1698747984352 -> 1698747981616
	1698747984352 [label=ReluBackward0]
	1698747980176 -> 1698747984352
	1698747980176 [label=AddBackward0]
	1698747979696 -> 1698747980176
	1698747979696 [label=CudnnBatchNormBackward0]
	1698747978592 -> 1698747979696
	1698747978592 [label=ConvolutionBackward0]
	1698747977632 -> 1698747978592
	1698747977632 [label=ReluBackward0]
	1698747977296 -> 1698747977632
	1698747977296 [label=CudnnBatchNormBackward0]
	1698747976816 -> 1698747977296
	1698747976816 [label=ConvolutionBackward0]
	1698747974320 -> 1698747976816
	1698747974320 [label=ReluBackward0]
	1698747989824 -> 1698747974320
	1698747989824 [label=AddBackward0]
	1698747989344 -> 1698747989824
	1698747989344 [label=CudnnBatchNormBackward0]
	1698747989392 -> 1698747989344
	1698747989392 [label=ConvolutionBackward0]
	1698747989056 -> 1698747989392
	1698747989056 [label=ReluBackward0]
	1698747989104 -> 1698747989056
	1698747989104 [label=CudnnBatchNormBackward0]
	1698747989008 -> 1698747989104
	1698747989008 [label=ConvolutionBackward0]
	1698747989536 -> 1698747989008
	1698747989536 [label=ReluBackward0]
	1698747988624 -> 1698747989536
	1698747988624 [label=AddBackward0]
	1698747988528 -> 1698747988624
	1698747988528 [label=CudnnBatchNormBackward0]
	1698747988096 -> 1698747988528
	1698747988096 [label=ConvolutionBackward0]
	1698747987952 -> 1698747988096
	1698747987952 [label=ReluBackward0]
	1698747987760 -> 1698747987952
	1698747987760 [label=CudnnBatchNormBackward0]
	1698747987424 -> 1698747987760
	1698747987424 [label=ConvolutionBackward0]
	1698747987568 -> 1698747987424
	1698747987568 [label=ReluBackward0]
	1698747987136 -> 1698747987568
	1698747987136 [label=AddBackward0]
	1698747987040 -> 1698747987136
	1698747987040 [label=CudnnBatchNormBackward0]
	1698747987088 -> 1698747987040
	1698747987088 [label=ConvolutionBackward0]
	1698747986464 -> 1698747987088
	1698747986464 [label=ReluBackward0]
	1698747986512 -> 1698747986464
	1698747986512 [label=CudnnBatchNormBackward0]
	1698747986368 -> 1698747986512
	1698747986368 [label=ConvolutionBackward0]
	1698747986944 -> 1698747986368
	1698747986944 [label=ReluBackward0]
	1698747986032 -> 1698747986944
	1698747986032 [label=AddBackward0]
	1698747985888 -> 1698747986032
	1698747985888 [label=CudnnBatchNormBackward0]
	1698747985504 -> 1698747985888
	1698747985504 [label=ConvolutionBackward0]
	1698747985648 -> 1698747985504
	1698747985648 [label=ReluBackward0]
	1698747985216 -> 1698747985648
	1698747985216 [label=CudnnBatchNormBackward0]
	1698747985120 -> 1698747985216
	1698747985120 [label=ConvolutionBackward0]
	1698747984928 -> 1698747985120
	1698747984928 [label=ReluBackward0]
	1698747984544 -> 1698747984928
	1698747984544 [label=AddBackward0]
	1698747984784 -> 1698747984544
	1698747984784 [label=CudnnBatchNormBackward0]
	1698747984448 -> 1698747984784
	1698747984448 [label=ConvolutionBackward0]
	1698747984160 -> 1698747984448
	1698747984160 [label=ReluBackward0]
	1698747984208 -> 1698747984160
	1698747984208 [label=CudnnBatchNormBackward0]
	1698747983920 -> 1698747984208
	1698747983920 [label=ConvolutionBackward0]
	1698747984640 -> 1698747983920
	1698747984640 [label=ReluBackward0]
	1698747983728 -> 1698747984640
	1698747983728 [label=AddBackward0]
	1698747983440 -> 1698747983728
	1698747983440 [label=CudnnBatchNormBackward0]
	1698747983200 -> 1698747983440
	1698747983200 [label=ConvolutionBackward0]
	1698747983008 -> 1698747983200
	1698747983008 [label=ReluBackward0]
	1698747982624 -> 1698747983008
	1698747982624 [label=CudnnBatchNormBackward0]
	1698747982864 -> 1698747982624
	1698747982864 [label=ConvolutionBackward0]
	1698747983488 -> 1698747982864
	1698747983488 [label=MaxPool2DWithIndicesBackward0]
	1698747982144 -> 1698747983488
	1698747982144 [label=ReluBackward0]
	1698747982384 -> 1698747982144
	1698747982384 [label=CudnnBatchNormBackward0]
	1698747982288 -> 1698747982384
	1698747982288 [label=ConvolutionBackward0]
	1698747981664 -> 1698747982288
	1698747709008 [label="conv1.weight
 (64, 4, 7, 7)" fillcolor=lightblue]
	1698747709008 -> 1698747981664
	1698747981664 [label=AccumulateGrad]
	1698747982192 -> 1698747982384
	1698747684656 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1698747684656 -> 1698747982192
	1698747982192 [label=AccumulateGrad]
	1698747982768 -> 1698747982384
	1698747683888 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1698747683888 -> 1698747982768
	1698747982768 [label=AccumulateGrad]
	1698747982480 -> 1698747982864
	1698747683984 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1698747683984 -> 1698747982480
	1698747982480 [label=AccumulateGrad]
	1698747982720 -> 1698747982624
	1698747684080 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1698747684080 -> 1698747982720
	1698747982720 [label=AccumulateGrad]
	1698747982960 -> 1698747982624
	1698747684176 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1698747684176 -> 1698747982960
	1698747982960 [label=AccumulateGrad]
	1698747983248 -> 1698747983200
	1698747684464 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1698747684464 -> 1698747983248
	1698747983248 [label=AccumulateGrad]
	1698747983104 -> 1698747983440
	1698747683504 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1698747683504 -> 1698747983104
	1698747983104 [label=AccumulateGrad]
	1698747983296 -> 1698747983440
	1698747683408 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1698747683408 -> 1698747983296
	1698747983296 [label=AccumulateGrad]
	1698747983488 -> 1698747983728
	1698747983824 -> 1698747983920
	1698747682544 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1698747682544 -> 1698747983824
	1698747983824 [label=AccumulateGrad]
	1698747983968 -> 1698747984208
	1698747682736 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1698747682736 -> 1698747983968
	1698747983968 [label=AccumulateGrad]
	1698747984304 -> 1698747984208
	1698747682832 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1698747682832 -> 1698747984304
	1698747984304 [label=AccumulateGrad]
	1698747984064 -> 1698747984448
	1698747683312 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1698747683312 -> 1698747984064
	1698747984064 [label=AccumulateGrad]
	1698747984688 -> 1698747984784
	1698747683216 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1698747683216 -> 1698747984688
	1698747984688 [label=AccumulateGrad]
	1698747984592 -> 1698747984784
	1698747682256 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1698747682256 -> 1698747984592
	1698747984592 [label=AccumulateGrad]
	1698747984640 -> 1698747984544
	1698747985168 -> 1698747985120
	1698747681776 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1698747681776 -> 1698747985168
	1698747985168 [label=AccumulateGrad]
	1698747985024 -> 1698747985216
	1698747682064 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1698747682064 -> 1698747985024
	1698747985024 [label=AccumulateGrad]
	1698747985408 -> 1698747985216
	1698747681968 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1698747681968 -> 1698747985408
	1698747985408 [label=AccumulateGrad]
	1698747985552 -> 1698747985504
	1698747701328 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1698747701328 -> 1698747985552
	1698747985552 [label=AccumulateGrad]
	1698747985696 -> 1698747985888
	1698747701424 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1698747701424 -> 1698747985696
	1698747985696 [label=AccumulateGrad]
	1698747985840 -> 1698747985888
	1698747701520 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1698747701520 -> 1698747985840
	1698747985840 [label=AccumulateGrad]
	1698747986128 -> 1698747986032
	1698747986128 [label=CudnnBatchNormBackward0]
	1698747985072 -> 1698747986128
	1698747985072 [label=ConvolutionBackward0]
	1698747984928 -> 1698747985072
	1698747984880 -> 1698747985072
	1698747681872 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1698747681872 -> 1698747984880
	1698747984880 [label=AccumulateGrad]
	1698747985744 -> 1698747986128
	1698747679472 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1698747679472 -> 1698747985744
	1698747985744 [label=AccumulateGrad]
	1698747985600 -> 1698747986128
	1698747679664 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1698747679664 -> 1698747985600
	1698747985600 [label=AccumulateGrad]
	1698747986080 -> 1698747986368
	1698747701904 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1698747701904 -> 1698747986080
	1698747986080 [label=AccumulateGrad]
	1698747986608 -> 1698747986512
	1698747702000 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1698747702000 -> 1698747986608
	1698747986608 [label=AccumulateGrad]
	1698747986560 -> 1698747986512
	1698747702096 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1698747702096 -> 1698747986560
	1698747986560 [label=AccumulateGrad]
	1698747986656 -> 1698747987088
	1698747702480 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1698747702480 -> 1698747986656
	1698747986656 [label=AccumulateGrad]
	1698747986992 -> 1698747987040
	1698747702576 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1698747702576 -> 1698747986992
	1698747986992 [label=AccumulateGrad]
	1698747987184 -> 1698747987040
	1698747702672 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1698747702672 -> 1698747987184
	1698747987184 [label=AccumulateGrad]
	1698747986944 -> 1698747987136
	1698747987472 -> 1698747987424
	1698747703632 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1698747703632 -> 1698747987472
	1698747987472 [label=AccumulateGrad]
	1698747987616 -> 1698747987760
	1698747703728 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1698747703728 -> 1698747987616
	1698747987616 [label=AccumulateGrad]
	1698747988048 -> 1698747987760
	1698747703824 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1698747703824 -> 1698747988048
	1698747988048 [label=AccumulateGrad]
	1698747988144 -> 1698747988096
	1698747704208 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1698747704208 -> 1698747988144
	1698747988144 [label=AccumulateGrad]
	1698747988240 -> 1698747988528
	1698747704304 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1698747704304 -> 1698747988240
	1698747988240 [label=AccumulateGrad]
	1698747988288 -> 1698747988528
	1698747704400 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1698747704400 -> 1698747988288
	1698747988288 [label=AccumulateGrad]
	1698747988432 -> 1698747988624
	1698747988432 [label=CudnnBatchNormBackward0]
	1698747987664 -> 1698747988432
	1698747987664 [label=ConvolutionBackward0]
	1698747987568 -> 1698747987664
	1698747987328 -> 1698747987664
	1698747703056 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1698747703056 -> 1698747987328
	1698747987328 [label=AccumulateGrad]
	1698747988000 -> 1698747988432
	1698747703152 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1698747703152 -> 1698747988000
	1698747988000 [label=AccumulateGrad]
	1698747987904 -> 1698747988432
	1698747703248 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1698747703248 -> 1698747987904
	1698747987904 [label=AccumulateGrad]
	1698747988384 -> 1698747989008
	1698747704784 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1698747704784 -> 1698747988384
	1698747988384 [label=AccumulateGrad]
	1698747988912 -> 1698747989104
	1698747704880 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1698747704880 -> 1698747988912
	1698747988912 [label=AccumulateGrad]
	1698747988864 -> 1698747989104
	1698747704976 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1698747704976 -> 1698747988864
	1698747988864 [label=AccumulateGrad]
	1698747989200 -> 1698747989392
	1698747705360 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1698747705360 -> 1698747989200
	1698747989200 [label=AccumulateGrad]
	1698747989584 -> 1698747989344
	1698747705456 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1698747705456 -> 1698747989584
	1698747989584 [label=AccumulateGrad]
	1698747989440 -> 1698747989344
	1698747705552 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1698747705552 -> 1698747989440
	1698747989440 [label=AccumulateGrad]
	1698747989536 -> 1698747989824
	1698747974368 -> 1698747976816
	1698747706512 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1698747706512 -> 1698747974368
	1698747974368 [label=AccumulateGrad]
	1698747976672 -> 1698747977296
	1698747706608 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1698747706608 -> 1698747976672
	1698747976672 [label=AccumulateGrad]
	1698747977776 -> 1698747977296
	1698747706704 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1698747706704 -> 1698747977776
	1698747977776 [label=AccumulateGrad]
	1698747978256 -> 1698747978592
	1698747707088 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1698747707088 -> 1698747978256
	1698747978256 [label=AccumulateGrad]
	1698747979216 -> 1698747979696
	1698747707184 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1698747707184 -> 1698747979216
	1698747979216 [label=AccumulateGrad]
	1698747979072 -> 1698747979696
	1698747707280 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1698747707280 -> 1698747979072
	1698747979072 [label=AccumulateGrad]
	1698747979552 -> 1698747980176
	1698747979552 [label=CudnnBatchNormBackward0]
	1698747975472 -> 1698747979552
	1698747975472 [label=ConvolutionBackward0]
	1698747974320 -> 1698747975472
	1698747974944 -> 1698747975472
	1698747705936 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1698747705936 -> 1698747974944
	1698747974944 [label=AccumulateGrad]
	1698747978112 -> 1698747979552
	1698747706032 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1698747706032 -> 1698747978112
	1698747978112 [label=AccumulateGrad]
	1698747978736 -> 1698747979552
	1698747706128 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1698747706128 -> 1698747978736
	1698747978736 [label=AccumulateGrad]
	1698747980656 -> 1698747981616
	1698747707664 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1698747707664 -> 1698747980656
	1698747980656 [label=AccumulateGrad]
	1698747981472 -> 1698747982096
	1698747707760 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1698747707760 -> 1698747981472
	1698747981472 [label=AccumulateGrad]
	1698747982576 -> 1698747982096
	1698747707856 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1698747707856 -> 1698747982576
	1698747982576 [label=AccumulateGrad]
	1698747983056 -> 1698747983392
	1698747708240 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1698747708240 -> 1698747983056
	1698747983056 [label=AccumulateGrad]
	1698747984016 -> 1698747984496
	1698747708336 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1698747708336 -> 1698747984016
	1698747984016 [label=AccumulateGrad]
	1698747983872 -> 1698747984496
	1698747708432 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1698747708432 -> 1698747983872
	1698747983872 [label=AccumulateGrad]
	1698747984352 -> 1698747984976
	1698747986416 -> 1698747987232
	1698747986416 [label=TBackward0]
	1698747984832 -> 1698747986416
	1698747709104 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1698747709104 -> 1698747984832
	1698747984832 [label=AccumulateGrad]
	1698747987232 -> 1699085896304
}
