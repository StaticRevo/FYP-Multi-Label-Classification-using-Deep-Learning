digraph {
	graph [size="89.1,89.1"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1648613461264 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1648856364752 [label=AddmmBackward0]
	1648856351936 -> 1648856364752
	1648584273904 [label="29.bias
 (19)" fillcolor=lightblue]
	1648584273904 -> 1648856351936
	1648856351936 [label=AccumulateGrad]
	1648856350784 -> 1648856364752
	1648856350784 [label=NativeDropoutBackward0]
	1648856353136 -> 1648856350784
	1648856353136 [label=ViewBackward0]
	1648856353376 -> 1648856353136
	1648856353376 [label=MeanBackward1]
	1648856354576 -> 1648856353376
	1648856354576 [label=MulBackward0]
	1648856354816 -> 1648856354576
	1648856354816 [label=MulBackward0]
	1648856354480 -> 1648856354816
	1648856354480 [label=ReluBackward0]
	1648856356880 -> 1648856354480
	1648856356880 [label=AddBackward0]
	1648856359424 -> 1648856356880
	1648856359424 [label=MulBackward0]
	1648856360096 -> 1648856359424
	1648856360096 [label=DivBackward0]
	1648856358944 -> 1648856360096
	1648856358944 [label=CudnnBatchNormBackward0]
	1648856360912 -> 1648856358944
	1648856360912 [label=ConvolutionBackward0]
	1648856361632 -> 1648856360912
	1648856361632 [label=NativeDropoutBackward0]
	1648856358176 -> 1648856361632
	1648856358176 [label=ReluBackward0]
	1648856360624 -> 1648856358176
	1648856360624 [label=CudnnBatchNormBackward0]
	1648856357408 -> 1648856360624
	1648856357408 [label=ConvolutionBackward0]
	1648856356784 -> 1648856357408
	1648856356784 [label=ReluBackward0]
	1648856358752 -> 1648856356784
	1648856358752 [label=CudnnBatchNormBackward0]
	1648856351888 -> 1648856358752
	1648856351888 [label=ConvolutionBackward0]
	1648856367056 -> 1648856351888
	1648856367056 [label=ConvolutionBackward0]
	1648856366768 -> 1648856367056
	1648856366768 [label=MulBackward0]
	1648856366528 -> 1648856366768
	1648856366528 [label=ReluBackward0]
	1648856366288 -> 1648856366528
	1648856366288 [label=AddBackward0]
	1648856366240 -> 1648856366288
	1648856366240 [label=MulBackward0]
	1648856366000 -> 1648856366240
	1648856366000 [label=DivBackward0]
	1648856365664 -> 1648856366000
	1648856365664 [label=CudnnBatchNormBackward0]
	1648856365904 -> 1648856365664
	1648856365904 [label=ConvolutionBackward0]
	1648856365472 -> 1648856365904
	1648856365472 [label=NativeDropoutBackward0]
	1648856365232 -> 1648856365472
	1648856365232 [label=ReluBackward0]
	1648856365184 -> 1648856365232
	1648856365184 [label=CudnnBatchNormBackward0]
	1648856364992 -> 1648856365184
	1648856364992 [label=ConvolutionBackward0]
	1648856366432 -> 1648856364992
	1648856366432 [label=ConvolutionBackward0]
	1648856361488 -> 1648856366432
	1648856361488 [label=CatBackward0]
	1648856364608 -> 1648856361488
	1648856364608 [label=ConvolutionBackward0]
	1648856364128 -> 1648856364608
	1648856364128 [label=ReluBackward0]
	1648856363600 -> 1648856364128
	1648856363600 [label=CudnnBatchNormBackward0]
	1648856359520 -> 1648856363600
	1648856359520 [label=ConvolutionBackward0]
	1648856352176 -> 1648856359520
	1648856352176 [label=ConvolutionBackward0]
	1648856352704 -> 1648856352176
	1648856352704 [label=MulBackward0]
	1648856352080 -> 1648856352704
	1648856352080 [label=ReluBackward0]
	1648856353184 -> 1648856352080
	1648856353184 [label=AddBackward0]
	1648856352656 -> 1648856353184
	1648856352656 [label=MulBackward0]
	1648856352800 -> 1648856352656
	1648856352800 [label=DivBackward0]
	1648856351072 -> 1648856352800
	1648856351072 [label=CudnnBatchNormBackward0]
	1648856351408 -> 1648856351072
	1648856351408 [label=ConvolutionBackward0]
	1648856354144 -> 1648856351408
	1648856354144 [label=NativeDropoutBackward0]
	1648856353664 -> 1648856354144
	1648856353664 [label=ReluBackward0]
	1648856353952 -> 1648856353664
	1648856353952 [label=CudnnBatchNormBackward0]
	1648856353712 -> 1648856353952
	1648856353712 [label=ConvolutionBackward0]
	1648856352464 -> 1648856353712
	1648856352464 [label=ConvolutionBackward0]
	1648856355152 -> 1648856352464
	1648856355152 [label=CatBackward0]
	1648856355632 -> 1648856355152
	1648856355632 [label=ConvolutionBackward0]
	1648856356976 -> 1648856355632
	1648856356976 [label=ReluBackward0]
	1648856356448 -> 1648856356976
	1648856356448 [label=CudnnBatchNormBackward0]
	1648856356016 -> 1648856356448
	1648856356016 [label=ConvolutionBackward0]
	1648856357312 -> 1648856356016
	1648856357312 [label=ConvolutionBackward0]
	1648856357888 -> 1648856357312
	1648856357888 [label=MulBackward0]
	1648856356496 -> 1648856357888
	1648856356496 [label=MulBackward0]
	1648856355008 -> 1648856356496
	1648856355008 [label=MulBackward0]
	1648856358512 -> 1648856355008
	1648856358512 [label=ReluBackward0]
	1648856358608 -> 1648856358512
	1648856358608 [label=AddBackward0]
	1648856358464 -> 1648856358608
	1648856358464 [label=MulBackward0]
	1648856359376 -> 1648856358464
	1648856359376 [label=DivBackward0]
	1648856359856 -> 1648856359376
	1648856359856 [label=CudnnBatchNormBackward0]
	1648856360144 -> 1648856359856
	1648856360144 [label=ConvolutionBackward0]
	1648856359136 -> 1648856360144
	1648856359136 [label=NativeDropoutBackward0]
	1648856360336 -> 1648856359136
	1648856360336 [label=ReluBackward0]
	1648856359664 -> 1648856360336
	1648856359664 [label=CudnnBatchNormBackward0]
	1648856352416 -> 1648856359664
	1648856352416 [label=ConvolutionBackward0]
	1648856359040 -> 1648856352416
	1648856359040 [label=ReluBackward0]
	1648856358080 -> 1648856359040
	1648856358080 [label=CudnnBatchNormBackward0]
	1648856361872 -> 1648856358080
	1648856361872 [label=ConvolutionBackward0]
	1648856362160 -> 1648856361872
	1648856362160 [label=ConvolutionBackward0]
	1648856361392 -> 1648856362160
	1648856361392 [label=ReluBackward0]
	1648856362688 -> 1648856361392
	1648856362688 [label=CudnnBatchNormBackward0]
	1648856360816 -> 1648856362688
	1648856360816 [label=ConvolutionBackward0]
	1648856362928 -> 1648856360816
	1648553182416 [label="0.weight
 (16, 12, 1, 1)" fillcolor=lightblue]
	1648553182416 -> 1648856362928
	1648856362928 [label=AccumulateGrad]
	1648856363648 -> 1648856362688
	1648553183184 [label="1.weight
 (16)" fillcolor=lightblue]
	1648553183184 -> 1648856363648
	1648856363648 [label=AccumulateGrad]
	1648856362784 -> 1648856362688
	1648553182128 [label="1.bias
 (16)" fillcolor=lightblue]
	1648553182128 -> 1648856362784
	1648856362784 [label=AccumulateGrad]
	1648856356064 -> 1648856362160
	1648553182704 [label="3.depthwise.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	1648553182704 -> 1648856356064
	1648856356064 [label=AccumulateGrad]
	1648856362592 -> 1648856361872
	1648553182800 [label="3.pointwise.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	1648553182800 -> 1648856362592
	1648856362592 [label=AccumulateGrad]
	1648856360048 -> 1648856358080
	1648553182608 [label="4.weight
 (32)" fillcolor=lightblue]
	1648553182608 -> 1648856360048
	1648856360048 [label=AccumulateGrad]
	1648856360768 -> 1648856358080
	1648553182896 [label="4.bias
 (32)" fillcolor=lightblue]
	1648553182896 -> 1648856360768
	1648856360768 [label=AccumulateGrad]
	1648856361200 -> 1648856352416
	1648553181264 [label="6.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1648553181264 -> 1648856361200
	1648856361200 [label=AccumulateGrad]
	1648856360672 -> 1648856359664
	1648553181936 [label="6.bn1.weight
 (32)" fillcolor=lightblue]
	1648553181936 -> 1648856360672
	1648856360672 [label=AccumulateGrad]
	1648856357696 -> 1648856359664
	1648553181360 [label="6.bn1.bias
 (32)" fillcolor=lightblue]
	1648553181360 -> 1648856357696
	1648856357696 [label=AccumulateGrad]
	1648856358368 -> 1648856360144
	1648553181744 [label="6.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1648553181744 -> 1648856358368
	1648856358368 [label=AccumulateGrad]
	1648856359808 -> 1648856359856
	1648553181840 [label="6.bn2.weight
 (32)" fillcolor=lightblue]
	1648553181840 -> 1648856359808
	1648856359808 [label=AccumulateGrad]
	1648856358272 -> 1648856359856
	1648734338992 [label="6.bn2.bias
 (32)" fillcolor=lightblue]
	1648734338992 -> 1648856358272
	1648856358272 [label=AccumulateGrad]
	1648856359040 -> 1648856358608
	1648856358800 -> 1648856355008
	1648856358800 [label=ViewBackward0]
	1648856358656 -> 1648856358800
	1648856358656 [label=SigmoidBackward0]
	1648856357456 -> 1648856358656
	1648856357456 [label=AddmmBackward0]
	1648856360432 -> 1648856357456
	1648734337360 [label="7.fc2.bias
 (32)" fillcolor=lightblue]
	1648734337360 -> 1648856360432
	1648856360432 [label=AccumulateGrad]
	1648856359616 -> 1648856357456
	1648856359616 [label=ReluBackward0]
	1648856359952 -> 1648856359616
	1648856359952 [label=AddmmBackward0]
	1648856360960 -> 1648856359952
	1648734338512 [label="7.fc1.bias
 (2)" fillcolor=lightblue]
	1648734338512 -> 1648856360960
	1648856360960 [label=AccumulateGrad]
	1648856359760 -> 1648856359952
	1648856359760 [label=MeanBackward1]
	1648856361920 -> 1648856359760
	1648856361920 [label=ViewBackward0]
	1648856358512 -> 1648856361920
	1648856359472 -> 1648856359952
	1648856359472 [label=TBackward0]
	1648856358992 -> 1648856359472
	1648734338608 [label="7.fc1.weight
 (2, 32)" fillcolor=lightblue]
	1648734338608 -> 1648856358992
	1648856358992 [label=AccumulateGrad]
	1648856355104 -> 1648856357456
	1648856355104 [label=TBackward0]
	1648856363792 -> 1648856355104
	1648734337744 [label="7.fc2.weight
 (32, 2)" fillcolor=lightblue]
	1648734337744 -> 1648856363792
	1648856363792 [label=AccumulateGrad]
	1648856357264 -> 1648856356496
	1648856357264 [label=SigmoidBackward0]
	1648856360192 -> 1648856357264
	1648856360192 [label=ConvolutionBackward0]
	1648856362736 -> 1648856360192
	1648856362736 [label=SplitWithSizesBackward0]
	1648856361776 -> 1648856362736
	1648856361776 [label=ReluBackward0]
	1648856362208 -> 1648856361776
	1648856362208 [label=CudnnBatchNormBackward0]
	1648856360720 -> 1648856362208
	1648856360720 [label=ConvolutionBackward0]
	1648856364224 -> 1648856360720
	1648856364224 [label=CatBackward0]
	1648856364656 -> 1648856364224
	1648856364656 [label=AdaptiveAvgPool2DBackward0]
	1648856355008 -> 1648856364656
	1648856363360 -> 1648856364224
	1648856363360 [label=PermuteBackward0]
	1648856363456 -> 1648856363360
	1648856363456 [label=AdaptiveAvgPool2DBackward0]
	1648856355008 -> 1648856363456
	1648856364320 -> 1648856360720
	1648734335824 [label="8.conv1.weight
 (8, 32, 1, 1)" fillcolor=lightblue]
	1648734335824 -> 1648856364320
	1648856364320 [label=AccumulateGrad]
	1648856363696 -> 1648856362208
	1648734338224 [label="8.bn1.weight
 (8)" fillcolor=lightblue]
	1648734338224 -> 1648856363696
	1648856363696 [label=AccumulateGrad]
	1648856362016 -> 1648856362208
	1648734336016 [label="8.bn1.bias
 (8)" fillcolor=lightblue]
	1648734336016 -> 1648856362016
	1648856362016 [label=AccumulateGrad]
	1648856358320 -> 1648856360192
	1648734338416 [label="8.conv_h.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1648734338416 -> 1648856358320
	1648856358320 [label=AccumulateGrad]
	1648856355920 -> 1648856357888
	1648856355920 [label=PermuteBackward0]
	1648856357744 -> 1648856355920
	1648856357744 [label=SigmoidBackward0]
	1648856361248 -> 1648856357744
	1648856361248 [label=ConvolutionBackward0]
	1648856362736 -> 1648856361248
	1648856362352 -> 1648856361248
	1648734338128 [label="8.conv_w.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1648734338128 -> 1648856362352
	1648856362352 [label=AccumulateGrad]
	1648856354048 -> 1648856357312
	1648584266032 [label="9.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	1648584266032 -> 1648856354048
	1648856354048 [label=AccumulateGrad]
	1648856356304 -> 1648856356016
	1648584266128 [label="9.pointwise.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	1648584266128 -> 1648856356304
	1648856356304 [label=AccumulateGrad]
	1648856356928 -> 1648856356448
	1648734338320 [label="10.weight
 (64)" fillcolor=lightblue]
	1648734338320 -> 1648856356928
	1648856356928 [label=AccumulateGrad]
	1648856357120 -> 1648856356448
	1648584266224 [label="10.bias
 (64)" fillcolor=lightblue]
	1648584266224 -> 1648856357120
	1648856357120 [label=AccumulateGrad]
	1648856357024 -> 1648856355632
	1648584266800 [label="12.conv_dil1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1648584266800 -> 1648856357024
	1648856357024 [label=AccumulateGrad]
	1648856354624 -> 1648856355632
	1648584266896 [label="12.conv_dil1.bias
 (64)" fillcolor=lightblue]
	1648584266896 -> 1648856354624
	1648856354624 [label=AccumulateGrad]
	1648856354000 -> 1648856355152
	1648856354000 [label=ConvolutionBackward0]
	1648856356976 -> 1648856354000
	1648856356160 -> 1648856354000
	1648584266992 [label="12.conv_dil2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1648584266992 -> 1648856356160
	1648856356160 [label=AccumulateGrad]
	1648856357360 -> 1648856354000
	1648584267088 [label="12.conv_dil2.bias
 (64)" fillcolor=lightblue]
	1648584267088 -> 1648856357360
	1648856357360 [label=AccumulateGrad]
	1648856355824 -> 1648856355152
	1648856355824 [label=ConvolutionBackward0]
	1648856356976 -> 1648856355824
	1648856357936 -> 1648856355824
	1648584267184 [label="12.conv_dil3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1648584267184 -> 1648856357936
	1648856357936 [label=AccumulateGrad]
	1648856356112 -> 1648856355824
	1648584267280 [label="12.conv_dil3.bias
 (64)" fillcolor=lightblue]
	1648584267280 -> 1648856356112
	1648856356112 [label=AccumulateGrad]
	1648856356256 -> 1648856352464
	1648584267376 [label="12.fuse.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	1648584267376 -> 1648856356256
	1648856356256 [label=AccumulateGrad]
	1648856354672 -> 1648856352464
	1648584267472 [label="12.fuse.bias
 (64)" fillcolor=lightblue]
	1648584267472 -> 1648856354672
	1648856354672 [label=AccumulateGrad]
	1648856355776 -> 1648856353712
	1648584267568 [label="13.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1648584267568 -> 1648856355776
	1648856355776 [label=AccumulateGrad]
	1648856355056 -> 1648856353952
	1648584266704 [label="13.bn1.weight
 (64)" fillcolor=lightblue]
	1648584266704 -> 1648856355056
	1648856355056 [label=AccumulateGrad]
	1648856351984 -> 1648856353952
	1648584267664 [label="13.bn1.bias
 (64)" fillcolor=lightblue]
	1648584267664 -> 1648856351984
	1648856351984 [label=AccumulateGrad]
	1648856353808 -> 1648856351408
	1648584268144 [label="13.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1648584268144 -> 1648856353808
	1648856353808 [label=AccumulateGrad]
	1648856353472 -> 1648856351072
	1648584268048 [label="13.bn2.weight
 (64)" fillcolor=lightblue]
	1648584268048 -> 1648856353472
	1648856353472 [label=AccumulateGrad]
	1648856352368 -> 1648856351072
	1648584268240 [label="13.bn2.bias
 (64)" fillcolor=lightblue]
	1648584268240 -> 1648856352368
	1648856352368 [label=AccumulateGrad]
	1648856352464 -> 1648856353184
	1648856351456 -> 1648856352704
	1648856351456 [label=UnsqueezeBackward0]
	1648856353328 -> 1648856351456
	1648856353328 [label=UnsqueezeBackward0]
	1648856354384 -> 1648856353328
	1648856354384 [label=SigmoidBackward0]
	1648856355200 -> 1648856354384
	1648856355200 [label=SqueezeBackward1]
	1648856352944 -> 1648856355200
	1648856352944 [label=ConvolutionBackward0]
	1648856354288 -> 1648856352944
	1648856354288 [label=UnsqueezeBackward0]
	1648856361680 -> 1648856354288
	1648856361680 [label=SqueezeBackward1]
	1648856358224 -> 1648856361680
	1648856358224 [label=SqueezeBackward1]
	1648856364368 -> 1648856358224
	1648856364368 [label=MeanBackward1]
	1648856352080 -> 1648856364368
	1648856354864 -> 1648856352944
	1648584268720 [label="14.conv.weight
 (1, 1, 3)" fillcolor=lightblue]
	1648584268720 -> 1648856354864
	1648856354864 [label=AccumulateGrad]
	1648856351360 -> 1648856352176
	1648584268816 [label="15.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	1648584268816 -> 1648856351360
	1648856351360 [label=AccumulateGrad]
	1648856362400 -> 1648856359520
	1648584268912 [label="15.pointwise.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1648584268912 -> 1648856362400
	1648856362400 [label=AccumulateGrad]
	1648856351024 -> 1648856363600
	1648584268624 [label="16.weight
 (128)" fillcolor=lightblue]
	1648584268624 -> 1648856351024
	1648856351024 [label=AccumulateGrad]
	1648856364032 -> 1648856363600
	1648584269008 [label="16.bias
 (128)" fillcolor=lightblue]
	1648584269008 -> 1648856364032
	1648856364032 [label=AccumulateGrad]
	1648856362112 -> 1648856364608
	1648584269488 [label="18.conv_dil1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1648584269488 -> 1648856362112
	1648856362112 [label=AccumulateGrad]
	1648856364560 -> 1648856364608
	1648584269584 [label="18.conv_dil1.bias
 (128)" fillcolor=lightblue]
	1648584269584 -> 1648856364560
	1648856364560 [label=AccumulateGrad]
	1648856363264 -> 1648856361488
	1648856363264 [label=ConvolutionBackward0]
	1648856364128 -> 1648856363264
	1648856364272 -> 1648856363264
	1648584269680 [label="18.conv_dil2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1648584269680 -> 1648856364272
	1648856364272 [label=AccumulateGrad]
	1648856363168 -> 1648856363264
	1648584269776 [label="18.conv_dil2.bias
 (128)" fillcolor=lightblue]
	1648584269776 -> 1648856363168
	1648856363168 [label=AccumulateGrad]
	1648856363024 -> 1648856361488
	1648856363024 [label=ConvolutionBackward0]
	1648856364128 -> 1648856363024
	1648856351264 -> 1648856363024
	1648584269872 [label="18.conv_dil3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1648584269872 -> 1648856351264
	1648856351264 [label=AccumulateGrad]
	1648856351312 -> 1648856363024
	1648584269968 [label="18.conv_dil3.bias
 (128)" fillcolor=lightblue]
	1648584269968 -> 1648856351312
	1648856351312 [label=AccumulateGrad]
	1648856364848 -> 1648856366432
	1648584270064 [label="18.fuse.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	1648584270064 -> 1648856364848
	1648856364848 [label=AccumulateGrad]
	1648856364800 -> 1648856366432
	1648584270160 [label="18.fuse.bias
 (128)" fillcolor=lightblue]
	1648584270160 -> 1648856364800
	1648856364800 [label=AccumulateGrad]
	1648856362256 -> 1648856364992
	1648584270256 [label="19.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1648584270256 -> 1648856362256
	1648856362256 [label=AccumulateGrad]
	1648856365280 -> 1648856365184
	1648584269392 [label="19.bn1.weight
 (128)" fillcolor=lightblue]
	1648584269392 -> 1648856365280
	1648856365280 [label=AccumulateGrad]
	1648856365328 -> 1648856365184
	1648584270352 [label="19.bn1.bias
 (128)" fillcolor=lightblue]
	1648584270352 -> 1648856365328
	1648856365328 [label=AccumulateGrad]
	1648856365520 -> 1648856365904
	1648584270832 [label="19.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1648584270832 -> 1648856365520
	1648856365520 [label=AccumulateGrad]
	1648856365760 -> 1648856365664
	1648584270736 [label="19.bn2.weight
 (128)" fillcolor=lightblue]
	1648584270736 -> 1648856365760
	1648856365760 [label=AccumulateGrad]
	1648856366336 -> 1648856365664
	1648584270928 [label="19.bn2.bias
 (128)" fillcolor=lightblue]
	1648584270928 -> 1648856366336
	1648856366336 [label=AccumulateGrad]
	1648856366432 -> 1648856366288
	1648856366576 -> 1648856366768
	1648856366576 [label=SigmoidBackward0]
	1648856366048 -> 1648856366576
	1648856366048 [label=ConvolutionBackward0]
	1648856365712 -> 1648856366048
	1648856365712 [label=NativeDropoutBackward0]
	1648856365376 -> 1648856365712
	1648856365376 [label=ReluBackward0]
	1648856363744 -> 1648856365376
	1648856363744 [label=ConvolutionBackward0]
	1648856363840 -> 1648856363744
	1648856363840 [label=MeanBackward1]
	1648856366528 -> 1648856363840
	1648856364416 -> 1648856363744
	1648584271408 [label="20.fc1.weight
 (8, 128, 1, 1)" fillcolor=lightblue]
	1648584271408 -> 1648856364416
	1648856364416 [label=AccumulateGrad]
	1648856365856 -> 1648856366048
	1648584271504 [label="20.fc2.weight
 (128, 8, 1, 1)" fillcolor=lightblue]
	1648584271504 -> 1648856365856
	1648856365856 [label=AccumulateGrad]
	1648856366960 -> 1648856367056
	1648584271600 [label="21.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	1648584271600 -> 1648856366960
	1648856366960 [label=AccumulateGrad]
	1648856353088 -> 1648856351888
	1648584271696 [label="21.pointwise.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1648584271696 -> 1648856353088
	1648856353088 [label=AccumulateGrad]
	1648856358128 -> 1648856358752
	1648584271312 [label="22.weight
 (256)" fillcolor=lightblue]
	1648584271312 -> 1648856358128
	1648856358128 [label=AccumulateGrad]
	1648856354960 -> 1648856358752
	1648584271792 [label="22.bias
 (256)" fillcolor=lightblue]
	1648584271792 -> 1648856354960
	1648856354960 [label=AccumulateGrad]
	1648856353040 -> 1648856357408
	1648584272272 [label="24.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1648584272272 -> 1648856353040
	1648856353040 [label=AccumulateGrad]
	1648856361536 -> 1648856360624
	1648584272176 [label="24.bn1.weight
 (256)" fillcolor=lightblue]
	1648584272176 -> 1648856361536
	1648856361536 [label=AccumulateGrad]
	1648856364080 -> 1648856360624
	1648584272368 [label="24.bn1.bias
 (256)" fillcolor=lightblue]
	1648584272368 -> 1648856364080
	1648856364080 [label=AccumulateGrad]
	1648856363984 -> 1648856360912
	1648584272848 [label="24.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1648584272848 -> 1648856363984
	1648856363984 [label=AccumulateGrad]
	1648856361344 -> 1648856358944
	1648584272752 [label="24.bn2.weight
 (256)" fillcolor=lightblue]
	1648584272752 -> 1648856361344
	1648856361344 [label=AccumulateGrad]
	1648856356736 -> 1648856358944
	1648584272944 [label="24.bn2.bias
 (256)" fillcolor=lightblue]
	1648584272944 -> 1648856356736
	1648856356736 [label=AccumulateGrad]
	1648856356784 -> 1648856356880
	1648856357072 -> 1648856354816
	1648856357072 [label=ViewBackward0]
	1648856360288 -> 1648856357072
	1648856360288 [label=SigmoidBackward0]
	1648856361584 -> 1648856360288
	1648856361584 [label=AddmmBackward0]
	1648856359184 -> 1648856361584
	1648584273616 [label="25.channel_att.fc2.bias
 (256)" fillcolor=lightblue]
	1648584273616 -> 1648856359184
	1648856359184 [label=AccumulateGrad]
	1648856361728 -> 1648856361584
	1648856361728 [label=ReluBackward0]
	1648856362976 -> 1648856361728
	1648856362976 [label=AddmmBackward0]
	1648856352128 -> 1648856362976
	1648584273424 [label="25.channel_att.fc1.bias
 (16)" fillcolor=lightblue]
	1648584273424 -> 1648856352128
	1648856352128 [label=AccumulateGrad]
	1648856356400 -> 1648856362976
	1648856356400 [label=MeanBackward1]
	1648856366192 -> 1648856356400
	1648856366192 [label=ViewBackward0]
	1648856354480 -> 1648856366192
	1648856356640 -> 1648856362976
	1648856356640 [label=TBackward0]
	1648856365808 -> 1648856356640
	1648584273328 [label="25.channel_att.fc1.weight
 (16, 256)" fillcolor=lightblue]
	1648584273328 -> 1648856365808
	1648856365808 [label=AccumulateGrad]
	1648856357600 -> 1648856361584
	1648856357600 [label=TBackward0]
	1648856365136 -> 1648856357600
	1648584273520 [label="25.channel_att.fc2.weight
 (256, 16)" fillcolor=lightblue]
	1648584273520 -> 1648856365136
	1648856365136 [label=AccumulateGrad]
	1648856355344 -> 1648856354576
	1648856355344 [label=SigmoidBackward0]
	1648856361296 -> 1648856355344
	1648856361296 [label=ConvolutionBackward0]
	1648856366384 -> 1648856361296
	1648856366384 [label=CatBackward0]
	1648856355488 -> 1648856366384
	1648856355488 [label=MeanBackward1]
	1648856354816 -> 1648856355488
	1648856364944 -> 1648856366384
	1648856364944 [label=MaxBackward0]
	1648856354816 -> 1648856364944
	1648856357216 -> 1648856361296
	1648584273808 [label="25.spatial_att.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	1648584273808 -> 1648856357216
	1648856357216 [label=AccumulateGrad]
	1648856353232 -> 1648856364752
	1648856353232 [label=TBackward0]
	1648856351120 -> 1648856353232
	1648584273712 [label="29.weight
 (19, 256)" fillcolor=lightblue]
	1648584273712 -> 1648856351120
	1648856351120 [label=AccumulateGrad]
	1648856364752 -> 1648613461264
}
