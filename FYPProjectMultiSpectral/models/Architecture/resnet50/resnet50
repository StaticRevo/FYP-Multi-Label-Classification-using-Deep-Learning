digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2123533007088 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2123481527680 [label=AddmmBackward0]
	2123481527344 -> 2123481527680
	2123452333616 [label="fc.bias
 (19)" fillcolor=lightblue]
	2123452333616 -> 2123481527344
	2123481527344 [label=AccumulateGrad]
	2123481526720 -> 2123481527680
	2123481526720 [label=ViewBackward0]
	2123481526240 -> 2123481526720
	2123481526240 [label=MeanBackward1]
	2123481525904 -> 2123481526240
	2123481525904 [label=ReluBackward0]
	2123481525424 -> 2123481525904
	2123481525424 [label=AddBackward0]
	2123481524944 -> 2123481525424
	2123481524944 [label=CudnnBatchNormBackward0]
	2123481523840 -> 2123481524944
	2123481523840 [label=ConvolutionBackward0]
	2123481522880 -> 2123481523840
	2123481522880 [label=ReluBackward0]
	2123481522544 -> 2123481522880
	2123481522544 [label=CudnnBatchNormBackward0]
	2123481517744 -> 2123481522544
	2123481517744 [label=ConvolutionBackward0]
	2123481515152 -> 2123481517744
	2123481515152 [label=ReluBackward0]
	2123481530176 -> 2123481515152
	2123481530176 [label=CudnnBatchNormBackward0]
	2123481529984 -> 2123481530176
	2123481529984 [label=ConvolutionBackward0]
	2123481524800 -> 2123481529984
	2123481524800 [label=ReluBackward0]
	2123481529696 -> 2123481524800
	2123481529696 [label=AddBackward0]
	2123481529504 -> 2123481529696
	2123481529504 [label=CudnnBatchNormBackward0]
	2123481529552 -> 2123481529504
	2123481529552 [label=ConvolutionBackward0]
	2123481529168 -> 2123481529552
	2123481529168 [label=ReluBackward0]
	2123481528928 -> 2123481529168
	2123481528928 [label=CudnnBatchNormBackward0]
	2123481528880 -> 2123481528928
	2123481528880 [label=ConvolutionBackward0]
	2123481528544 -> 2123481528880
	2123481528544 [label=ReluBackward0]
	2123481528592 -> 2123481528544
	2123481528592 [label=CudnnBatchNormBackward0]
	2123481528496 -> 2123481528592
	2123481528496 [label=ConvolutionBackward0]
	2123481529648 -> 2123481528496
	2123481529648 [label=ReluBackward0]
	2123481528112 -> 2123481529648
	2123481528112 [label=AddBackward0]
	2123481528016 -> 2123481528112
	2123481528016 [label=CudnnBatchNormBackward0]
	2123481527584 -> 2123481528016
	2123481527584 [label=ConvolutionBackward0]
	2123481527440 -> 2123481527584
	2123481527440 [label=ReluBackward0]
	2123481527248 -> 2123481527440
	2123481527248 [label=CudnnBatchNormBackward0]
	2123481526912 -> 2123481527248
	2123481526912 [label=ConvolutionBackward0]
	2123481527056 -> 2123481526912
	2123481527056 [label=ReluBackward0]
	2123481526624 -> 2123481527056
	2123481526624 [label=CudnnBatchNormBackward0]
	2123481526528 -> 2123481526624
	2123481526528 [label=ConvolutionBackward0]
	2123481526336 -> 2123481526528
	2123481526336 [label=ReluBackward0]
	2123481525952 -> 2123481526336
	2123481525952 [label=AddBackward0]
	2123481526192 -> 2123481525952
	2123481526192 [label=CudnnBatchNormBackward0]
	2123481525856 -> 2123481526192
	2123481525856 [label=ConvolutionBackward0]
	2123481525568 -> 2123481525856
	2123481525568 [label=ReluBackward0]
	2123481525616 -> 2123481525568
	2123481525616 [label=CudnnBatchNormBackward0]
	2123481525328 -> 2123481525616
	2123481525328 [label=ConvolutionBackward0]
	2123481525232 -> 2123481525328
	2123481525232 [label=ReluBackward0]
	2123481524896 -> 2123481525232
	2123481524896 [label=CudnnBatchNormBackward0]
	2123481524704 -> 2123481524896
	2123481524704 [label=ConvolutionBackward0]
	2123481526048 -> 2123481524704
	2123481526048 [label=ReluBackward0]
	2123481524416 -> 2123481526048
	2123481524416 [label=AddBackward0]
	2123481524224 -> 2123481524416
	2123481524224 [label=CudnnBatchNormBackward0]
	2123481524272 -> 2123481524224
	2123481524272 [label=ConvolutionBackward0]
	2123481523888 -> 2123481524272
	2123481523888 [label=ReluBackward0]
	2123481523648 -> 2123481523888
	2123481523648 [label=CudnnBatchNormBackward0]
	2123481523600 -> 2123481523648
	2123481523600 [label=ConvolutionBackward0]
	2123481523264 -> 2123481523600
	2123481523264 [label=ReluBackward0]
	2123481523312 -> 2123481523264
	2123481523312 [label=CudnnBatchNormBackward0]
	2123481523216 -> 2123481523312
	2123481523216 [label=ConvolutionBackward0]
	2123481524368 -> 2123481523216
	2123481524368 [label=ReluBackward0]
	2123481522832 -> 2123481524368
	2123481522832 [label=AddBackward0]
	2123481522736 -> 2123481522832
	2123481522736 [label=CudnnBatchNormBackward0]
	2123481517408 -> 2123481522736
	2123481517408 [label=ConvolutionBackward0]
	2123481515104 -> 2123481517408
	2123481515104 [label=ReluBackward0]
	2123481522112 -> 2123481515104
	2123481522112 [label=CudnnBatchNormBackward0]
	2123481518560 -> 2123481522112
	2123481518560 [label=ConvolutionBackward0]
	2123481518512 -> 2123481518560
	2123481518512 [label=ReluBackward0]
	2123481517216 -> 2123481518512
	2123481517216 [label=CudnnBatchNormBackward0]
	2123481520816 -> 2123481517216
	2123481520816 [label=ConvolutionBackward0]
	2123481522640 -> 2123481520816
	2123481522640 [label=ReluBackward0]
	2123481517456 -> 2123481522640
	2123481517456 [label=AddBackward0]
	2123481521536 -> 2123481517456
	2123481521536 [label=CudnnBatchNormBackward0]
	2123481520624 -> 2123481521536
	2123481520624 [label=ConvolutionBackward0]
	2123481521056 -> 2123481520624
	2123481521056 [label=ReluBackward0]
	2123481514192 -> 2123481521056
	2123481514192 [label=CudnnBatchNormBackward0]
	2123481517792 -> 2123481514192
	2123481517792 [label=ConvolutionBackward0]
	2123481520576 -> 2123481517792
	2123481520576 [label=ReluBackward0]
	2123481518992 -> 2123481520576
	2123481518992 [label=CudnnBatchNormBackward0]
	2123481519664 -> 2123481518992
	2123481519664 [label=ConvolutionBackward0]
	2123481519280 -> 2123481519664
	2123481519280 [label=ReluBackward0]
	2123481516736 -> 2123481519280
	2123481516736 [label=AddBackward0]
	2123481514528 -> 2123481516736
	2123481514528 [label=CudnnBatchNormBackward0]
	2123481522400 -> 2123481514528
	2123481522400 [label=ConvolutionBackward0]
	2123481522352 -> 2123481522400
	2123481522352 [label=ReluBackward0]
	2123481514096 -> 2123481522352
	2123481514096 [label=CudnnBatchNormBackward0]
	2123481522208 -> 2123481514096
	2123481522208 [label=ConvolutionBackward0]
	2123481521104 -> 2123481522208
	2123481521104 [label=ReluBackward0]
	2123481520864 -> 2123481521104
	2123481520864 [label=CudnnBatchNormBackward0]
	2123481521392 -> 2123481520864
	2123481521392 [label=ConvolutionBackward0]
	2123481519856 -> 2123481521392
	2123481519856 [label=ReluBackward0]
	2123481518272 -> 2123481519856
	2123481518272 [label=AddBackward0]
	2123481520480 -> 2123481518272
	2123481520480 [label=CudnnBatchNormBackward0]
	2123481515488 -> 2123481520480
	2123481515488 [label=ConvolutionBackward0]
	2123481520960 -> 2123481515488
	2123481520960 [label=ReluBackward0]
	2123481517936 -> 2123481520960
	2123481517936 [label=CudnnBatchNormBackward0]
	2123481519088 -> 2123481517936
	2123481519088 [label=ConvolutionBackward0]
	2123481518128 -> 2123481519088
	2123481518128 [label=ReluBackward0]
	2123481522064 -> 2123481518128
	2123481522064 [label=CudnnBatchNormBackward0]
	2123481515680 -> 2123481522064
	2123481515680 [label=ConvolutionBackward0]
	2123522536640 -> 2123481515680
	2123522536640 [label=ReluBackward0]
	2123522532560 -> 2123522536640
	2123522532560 [label=AddBackward0]
	2123522523488 -> 2123522532560
	2123522523488 [label=CudnnBatchNormBackward0]
	2123522533760 -> 2123522523488
	2123522533760 [label=ConvolutionBackward0]
	2123522530400 -> 2123522533760
	2123522530400 [label=ReluBackward0]
	2123522525360 -> 2123522530400
	2123522525360 [label=CudnnBatchNormBackward0]
	2123502165952 -> 2123522525360
	2123502165952 [label=ConvolutionBackward0]
	2123533327792 -> 2123502165952
	2123533327792 [label=ReluBackward0]
	2123533326688 -> 2123533327792
	2123533326688 [label=CudnnBatchNormBackward0]
	2123533326208 -> 2123533326688
	2123533326208 [label=ConvolutionBackward0]
	2123522523440 -> 2123533326208
	2123522523440 [label=ReluBackward0]
	2123533324768 -> 2123522523440
	2123533324768 [label=AddBackward0]
	2123533324288 -> 2123533324768
	2123533324288 [label=CudnnBatchNormBackward0]
	2123533323952 -> 2123533324288
	2123533323952 [label=ConvolutionBackward0]
	2123533322992 -> 2123533323952
	2123533322992 [label=ReluBackward0]
	2123533321888 -> 2123533322992
	2123533321888 [label=CudnnBatchNormBackward0]
	2123533321408 -> 2123533321888
	2123533321408 [label=ConvolutionBackward0]
	2123533320448 -> 2123533321408
	2123533320448 [label=ReluBackward0]
	2123533328176 -> 2123533320448
	2123533328176 [label=CudnnBatchNormBackward0]
	2123533327840 -> 2123533328176
	2123533327840 [label=ConvolutionBackward0]
	2123533324912 -> 2123533327840
	2123533324912 [label=ReluBackward0]
	2123533327696 -> 2123533324912
	2123533327696 [label=AddBackward0]
	2123533327360 -> 2123533327696
	2123533327360 [label=CudnnBatchNormBackward0]
	2123533327408 -> 2123533327360
	2123533327408 [label=ConvolutionBackward0]
	2123533327072 -> 2123533327408
	2123533327072 [label=ReluBackward0]
	2123533327120 -> 2123533327072
	2123533327120 [label=CudnnBatchNormBackward0]
	2123533327024 -> 2123533327120
	2123533327024 [label=ConvolutionBackward0]
	2123533326400 -> 2123533327024
	2123533326400 [label=ReluBackward0]
	2123533326448 -> 2123533326400
	2123533326448 [label=CudnnBatchNormBackward0]
	2123533326304 -> 2123533326448
	2123533326304 [label=ConvolutionBackward0]
	2123533327552 -> 2123533326304
	2123533327552 [label=ReluBackward0]
	2123533325968 -> 2123533327552
	2123533325968 [label=AddBackward0]
	2123533325824 -> 2123533325968
	2123533325824 [label=CudnnBatchNormBackward0]
	2123533325440 -> 2123533325824
	2123533325440 [label=ConvolutionBackward0]
	2123533325584 -> 2123533325440
	2123533325584 [label=ReluBackward0]
	2123533325152 -> 2123533325584
	2123533325152 [label=CudnnBatchNormBackward0]
	2123533325056 -> 2123533325152
	2123533325056 [label=ConvolutionBackward0]
	2123533324864 -> 2123533325056
	2123533324864 [label=ReluBackward0]
	2123533324480 -> 2123533324864
	2123533324480 [label=CudnnBatchNormBackward0]
	2123533324720 -> 2123533324480
	2123533324720 [label=ConvolutionBackward0]
	2123533324336 -> 2123533324720
	2123533324336 [label=ReluBackward0]
	2123533324096 -> 2123533324336
	2123533324096 [label=AddBackward0]
	2123533324048 -> 2123533324096
	2123533324048 [label=CudnnBatchNormBackward0]
	2123533323856 -> 2123533324048
	2123533323856 [label=ConvolutionBackward0]
	2123533323760 -> 2123533323856
	2123533323760 [label=ReluBackward0]
	2123533323424 -> 2123533323760
	2123533323424 [label=CudnnBatchNormBackward0]
	2123533323232 -> 2123533323424
	2123533323232 [label=ConvolutionBackward0]
	2123533323088 -> 2123533323232
	2123533323088 [label=ReluBackward0]
	2123533322896 -> 2123533323088
	2123533322896 [label=CudnnBatchNormBackward0]
	2123533322560 -> 2123533322896
	2123533322560 [label=ConvolutionBackward0]
	2123533324240 -> 2123533322560
	2123533324240 [label=ReluBackward0]
	2123533322416 -> 2123533324240
	2123533322416 [label=AddBackward0]
	2123533322080 -> 2123533322416
	2123533322080 [label=CudnnBatchNormBackward0]
	2123533322128 -> 2123533322080
	2123533322128 [label=ConvolutionBackward0]
	2123533321792 -> 2123533322128
	2123533321792 [label=ReluBackward0]
	2123533321840 -> 2123533321792
	2123533321840 [label=CudnnBatchNormBackward0]
	2123533321744 -> 2123533321840
	2123533321744 [label=ConvolutionBackward0]
	2123533321120 -> 2123533321744
	2123533321120 [label=ReluBackward0]
	2123533321168 -> 2123533321120
	2123533321168 [label=CudnnBatchNormBackward0]
	2123533321024 -> 2123533321168
	2123533321024 [label=ConvolutionBackward0]
	2123533322272 -> 2123533321024
	2123533322272 [label=ReluBackward0]
	2123533320688 -> 2123533322272
	2123533320688 [label=AddBackward0]
	2123533320544 -> 2123533320688
	2123533320544 [label=CudnnBatchNormBackward0]
	2123533320256 -> 2123533320544
	2123533320256 [label=ConvolutionBackward0]
	2123533328704 -> 2123533320256
	2123533328704 [label=ReluBackward0]
	2123533329040 -> 2123533328704
	2123533329040 [label=CudnnBatchNormBackward0]
	2123533329136 -> 2123533329040
	2123533329136 [label=ConvolutionBackward0]
	2123533329328 -> 2123533329136
	2123533329328 [label=ReluBackward0]
	2123533329472 -> 2123533329328
	2123533329472 [label=CudnnBatchNormBackward0]
	2123533329568 -> 2123533329472
	2123533329568 [label=ConvolutionBackward0]
	2123533329760 -> 2123533329568
	2123533329760 [label=MaxPool2DWithIndicesBackward0]
	2123533329904 -> 2123533329760
	2123533329904 [label=ReluBackward0]
	2123533330000 -> 2123533329904
	2123533330000 [label=CudnnBatchNormBackward0]
	2123533330096 -> 2123533330000
	2123533330096 [label=ConvolutionBackward0]
	2123533330288 -> 2123533330096
	2123452333424 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2123452333424 -> 2123533330288
	2123533330288 [label=AccumulateGrad]
	2123533330048 -> 2123533330000
	2123456565360 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2123456565360 -> 2123533330048
	2123533330048 [label=AccumulateGrad]
	2123533329808 -> 2123533330000
	2123456565072 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2123456565072 -> 2123533329808
	2123533329808 [label=AccumulateGrad]
	2123533329712 -> 2123533329568
	2123456564976 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2123456564976 -> 2123533329712
	2123533329712 [label=AccumulateGrad]
	2123533329520 -> 2123533329472
	2123456564880 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2123456564880 -> 2123533329520
	2123533329520 [label=AccumulateGrad]
	2123533329376 -> 2123533329472
	2123456564112 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2123456564112 -> 2123533329376
	2123533329376 [label=AccumulateGrad]
	2123533329280 -> 2123533329136
	2123456564208 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2123456564208 -> 2123533329280
	2123533329280 [label=AccumulateGrad]
	2123533329088 -> 2123533329040
	2123456564304 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2123456564304 -> 2123533329088
	2123533329088 [label=AccumulateGrad]
	2123533328944 -> 2123533329040
	2123456564400 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2123456564400 -> 2123533328944
	2123533328944 [label=AccumulateGrad]
	2123533328848 -> 2123533320256
	2123456564688 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2123456564688 -> 2123533328848
	2123533328848 [label=AccumulateGrad]
	2123533320352 -> 2123533320544
	2123456563728 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2123456563728 -> 2123533320352
	2123533320352 [label=AccumulateGrad]
	2123533320496 -> 2123533320544
	2123456563632 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2123456563632 -> 2123533320496
	2123533320496 [label=AccumulateGrad]
	2123533320784 -> 2123533320688
	2123533320784 [label=CudnnBatchNormBackward0]
	2123533329232 -> 2123533320784
	2123533329232 [label=ConvolutionBackward0]
	2123533329760 -> 2123533329232
	2123533329616 -> 2123533329232
	2123456565552 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2123456565552 -> 2123533329616
	2123533329616 [label=AccumulateGrad]
	2123533320304 -> 2123533320784
	2123456565648 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2123456565648 -> 2123533320304
	2123533320304 [label=AccumulateGrad]
	2123533320400 -> 2123533320784
	2123456565744 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2123456565744 -> 2123533320400
	2123533320400 [label=AccumulateGrad]
	2123533320736 -> 2123533321024
	2123456561520 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2123456561520 -> 2123533320736
	2123533320736 [label=AccumulateGrad]
	2123533321264 -> 2123533321168
	2123456562384 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2123456562384 -> 2123533321264
	2123533321264 [label=AccumulateGrad]
	2123533321216 -> 2123533321168
	2123456562768 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2123456562768 -> 2123533321216
	2123533321216 [label=AccumulateGrad]
	2123533321312 -> 2123533321744
	2123456563536 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2123456563536 -> 2123533321312
	2123533321312 [label=AccumulateGrad]
	2123533321648 -> 2123533321840
	2123456563440 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2123456563440 -> 2123533321648
	2123533321648 [label=AccumulateGrad]
	2123533321600 -> 2123533321840
	2123452617584 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2123452617584 -> 2123533321600
	2123533321600 [label=AccumulateGrad]
	2123533321936 -> 2123533322128
	2123452617968 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2123452617968 -> 2123533321936
	2123533321936 [label=AccumulateGrad]
	2123533322320 -> 2123533322080
	2123452618064 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2123452618064 -> 2123533322320
	2123533322320 [label=AccumulateGrad]
	2123533322176 -> 2123533322080
	2123452618160 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2123452618160 -> 2123533322176
	2123533322176 [label=AccumulateGrad]
	2123533322272 -> 2123533322416
	2123533322704 -> 2123533322560
	2123452618544 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2123452618544 -> 2123533322704
	2123533322704 [label=AccumulateGrad]
	2123533322752 -> 2123533322896
	2123452618640 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2123452618640 -> 2123533322752
	2123533322752 [label=AccumulateGrad]
	2123533323184 -> 2123533322896
	2123452618736 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2123452618736 -> 2123533323184
	2123533323184 [label=AccumulateGrad]
	2123533323280 -> 2123533323232
	2123452619120 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2123452619120 -> 2123533323280
	2123533323280 [label=AccumulateGrad]
	2123533323376 -> 2123533323424
	2123452619216 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2123452619216 -> 2123533323376
	2123533323376 [label=AccumulateGrad]
	2123533323568 -> 2123533323424
	2123452619312 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2123452619312 -> 2123533323568
	2123533323568 [label=AccumulateGrad]
	2123533323616 -> 2123533323856
	2123452619696 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2123452619696 -> 2123533323616
	2123533323616 [label=AccumulateGrad]
	2123533323904 -> 2123533324048
	2123452619792 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2123452619792 -> 2123533323904
	2123533323904 [label=AccumulateGrad]
	2123533324144 -> 2123533324048
	2123452619888 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2123452619888 -> 2123533324144
	2123533324144 [label=AccumulateGrad]
	2123533324240 -> 2123533324096
	2123533324384 -> 2123533324720
	2123452620848 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2123452620848 -> 2123533324384
	2123533324384 [label=AccumulateGrad]
	2123533324576 -> 2123533324480
	2123452620944 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2123452620944 -> 2123533324576
	2123533324576 [label=AccumulateGrad]
	2123533324816 -> 2123533324480
	2123452621040 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2123452621040 -> 2123533324816
	2123533324816 [label=AccumulateGrad]
	2123533325104 -> 2123533325056
	2123452621424 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2123452621424 -> 2123533325104
	2123533325104 [label=AccumulateGrad]
	2123533324960 -> 2123533325152
	2123452621520 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2123452621520 -> 2123533324960
	2123533324960 [label=AccumulateGrad]
	2123533325344 -> 2123533325152
	2123452621616 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2123452621616 -> 2123533325344
	2123533325344 [label=AccumulateGrad]
	2123533325488 -> 2123533325440
	2123452622000 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2123452622000 -> 2123533325488
	2123533325488 [label=AccumulateGrad]
	2123533325632 -> 2123533325824
	2123452622096 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2123452622096 -> 2123533325632
	2123533325632 [label=AccumulateGrad]
	2123533325776 -> 2123533325824
	2123452622192 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2123452622192 -> 2123533325776
	2123533325776 [label=AccumulateGrad]
	2123533326064 -> 2123533325968
	2123533326064 [label=CudnnBatchNormBackward0]
	2123533325008 -> 2123533326064
	2123533325008 [label=ConvolutionBackward0]
	2123533324336 -> 2123533325008
	2123533324528 -> 2123533325008
	2123452620272 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2123452620272 -> 2123533324528
	2123533324528 [label=AccumulateGrad]
	2123533325680 -> 2123533326064
	2123452620368 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2123452620368 -> 2123533325680
	2123533325680 [label=AccumulateGrad]
	2123533325536 -> 2123533326064
	2123452620464 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2123452620464 -> 2123533325536
	2123533325536 [label=AccumulateGrad]
	2123533326016 -> 2123533326304
	2123452622576 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2123452622576 -> 2123533326016
	2123533326016 [label=AccumulateGrad]
	2123533326544 -> 2123533326448
	2123452622672 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2123452622672 -> 2123533326544
	2123533326544 [label=AccumulateGrad]
	2123533326496 -> 2123533326448
	2123452622768 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2123452622768 -> 2123533326496
	2123533326496 [label=AccumulateGrad]
	2123533326592 -> 2123533327024
	2123452623152 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2123452623152 -> 2123533326592
	2123533326592 [label=AccumulateGrad]
	2123533326928 -> 2123533327120
	2123452623248 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2123452623248 -> 2123533326928
	2123533326928 [label=AccumulateGrad]
	2123533326880 -> 2123533327120
	2123452623344 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2123452623344 -> 2123533326880
	2123533326880 [label=AccumulateGrad]
	2123533327216 -> 2123533327408
	2123452623728 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2123452623728 -> 2123533327216
	2123533327216 [label=AccumulateGrad]
	2123533327600 -> 2123533327360
	2123452623824 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2123452623824 -> 2123533327600
	2123533327600 [label=AccumulateGrad]
	2123533327456 -> 2123533327360
	2123452623920 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2123452623920 -> 2123533327456
	2123533327456 [label=AccumulateGrad]
	2123533327552 -> 2123533327696
	2123533327984 -> 2123533327840
	2123452624304 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2123452624304 -> 2123533327984
	2123533327984 [label=AccumulateGrad]
	2123533328032 -> 2123533328176
	2123452624400 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2123452624400 -> 2123533328032
	2123533328032 [label=AccumulateGrad]
	2123533320592 -> 2123533328176
	2123452624496 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2123452624496 -> 2123533320592
	2123533320592 [label=AccumulateGrad]
	2123533321072 -> 2123533321408
	2123452624880 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2123452624880 -> 2123533321072
	2123533321072 [label=AccumulateGrad]
	2123533322032 -> 2123533321888
	2123452624976 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2123452624976 -> 2123533322032
	2123533322032 [label=AccumulateGrad]
	2123533322368 -> 2123533321888
	2123452625072 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2123452625072 -> 2123533322368
	2123533322368 [label=AccumulateGrad]
	2123533322848 -> 2123533323952
	2123452625456 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2123452625456 -> 2123533322848
	2123533322848 [label=AccumulateGrad]
	2123533323808 -> 2123533324288
	2123452625552 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2123452625552 -> 2123533323808
	2123533323808 [label=AccumulateGrad]
	2123533324432 -> 2123533324288
	2123452625648 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2123452625648 -> 2123533324432
	2123533324432 [label=AccumulateGrad]
	2123533324912 -> 2123533324768
	2123533325248 -> 2123533326208
	2123452626032 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2123452626032 -> 2123533325248
	2123533325248 [label=AccumulateGrad]
	2123533326832 -> 2123533326688
	2123452626128 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2123452626128 -> 2123533326832
	2123533326832 [label=AccumulateGrad]
	2123533327168 -> 2123533326688
	2123452626224 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2123452626224 -> 2123533327168
	2123533327168 [label=AccumulateGrad]
	2123533327648 -> 2123502165952
	2123452626608 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2123452626608 -> 2123533327648
	2123533327648 [label=AccumulateGrad]
	2123522527808 -> 2123522525360
	2123452626704 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2123452626704 -> 2123522527808
	2123522527808 [label=AccumulateGrad]
	2123522526128 -> 2123522525360
	2123452626800 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2123452626800 -> 2123522526128
	2123522526128 [label=AccumulateGrad]
	2123522525792 -> 2123522533760
	2123452627184 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2123452627184 -> 2123522525792
	2123522525792 [label=AccumulateGrad]
	2123522529680 -> 2123522523488
	2123452627280 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2123452627280 -> 2123522529680
	2123522529680 [label=AccumulateGrad]
	2123522533136 -> 2123522523488
	2123452627376 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2123452627376 -> 2123522533136
	2123522533136 [label=AccumulateGrad]
	2123522523440 -> 2123522532560
	2123522525072 -> 2123481515680
	2123452628336 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2123452628336 -> 2123522525072
	2123522525072 [label=AccumulateGrad]
	2123481517168 -> 2123481522064
	2123452628432 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2123452628432 -> 2123481517168
	2123481517168 [label=AccumulateGrad]
	2123481515824 -> 2123481522064
	2123452628528 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2123452628528 -> 2123481515824
	2123481515824 [label=AccumulateGrad]
	2123481518080 -> 2123481519088
	2123452628912 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2123452628912 -> 2123481518080
	2123481518080 [label=AccumulateGrad]
	2123481516400 -> 2123481517936
	2123452317776 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2123452317776 -> 2123481516400
	2123481516400 [label=AccumulateGrad]
	2123481519136 -> 2123481517936
	2123452317872 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2123452317872 -> 2123481519136
	2123481519136 [label=AccumulateGrad]
	2123481520528 -> 2123481515488
	2123452318256 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2123452318256 -> 2123481520528
	2123481520528 [label=AccumulateGrad]
	2123481515968 -> 2123481520480
	2123452318352 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2123452318352 -> 2123481515968
	2123481515968 [label=AccumulateGrad]
	2123481519616 -> 2123481520480
	2123452318448 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2123452318448 -> 2123481519616
	2123481519616 [label=AccumulateGrad]
	2123481518368 -> 2123481518272
	2123481518368 [label=CudnnBatchNormBackward0]
	2123481516016 -> 2123481518368
	2123481516016 [label=ConvolutionBackward0]
	2123522536640 -> 2123481516016
	2123481518608 -> 2123481516016
	2123452627760 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2123452627760 -> 2123481518608
	2123481518608 [label=AccumulateGrad]
	2123481521632 -> 2123481518368
	2123452627856 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2123452627856 -> 2123481521632
	2123481521632 [label=AccumulateGrad]
	2123481517840 -> 2123481518368
	2123452627952 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2123452627952 -> 2123481517840
	2123481517840 [label=AccumulateGrad]
	2123481515296 -> 2123481521392
	2123452318832 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2123452318832 -> 2123481515296
	2123481515296 [label=AccumulateGrad]
	2123481521968 -> 2123481520864
	2123452318928 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2123452318928 -> 2123481521968
	2123481521968 [label=AccumulateGrad]
	2123481519376 -> 2123481520864
	2123452319024 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2123452319024 -> 2123481519376
	2123481519376 [label=AccumulateGrad]
	2123481521344 -> 2123481522208
	2123452319408 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2123452319408 -> 2123481521344
	2123481521344 [label=AccumulateGrad]
	2123481521296 -> 2123481514096
	2123452319504 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2123452319504 -> 2123481521296
	2123481521296 [label=AccumulateGrad]
	2123481522160 -> 2123481514096
	2123452319600 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2123452319600 -> 2123481522160
	2123481522160 [label=AccumulateGrad]
	2123481522448 -> 2123481522400
	2123452319984 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2123452319984 -> 2123481522448
	2123481522448 [label=AccumulateGrad]
	2123481516592 -> 2123481514528
	2123452320080 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2123452320080 -> 2123481516592
	2123481516592 [label=AccumulateGrad]
	2123481519040 -> 2123481514528
	2123452320176 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2123452320176 -> 2123481519040
	2123481519040 [label=AccumulateGrad]
	2123481519856 -> 2123481516736
	2123481519904 -> 2123481519664
	2123452320560 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2123452320560 -> 2123481519904
	2123481519904 [label=AccumulateGrad]
	2123481518416 -> 2123481518992
	2123452320656 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2123452320656 -> 2123481518416
	2123481518416 [label=AccumulateGrad]
	2123481519808 -> 2123481518992
	2123452320752 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2123452320752 -> 2123481519808
	2123481519808 [label=AccumulateGrad]
	2123481516304 -> 2123481517792
	2123452321136 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2123452321136 -> 2123481516304
	2123481516304 [label=AccumulateGrad]
	2123481514912 -> 2123481514192
	2123452321232 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2123452321232 -> 2123481514912
	2123481514912 [label=AccumulateGrad]
	2123481514864 -> 2123481514192
	2123452321328 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2123452321328 -> 2123481514864
	2123481514864 [label=AccumulateGrad]
	2123481520672 -> 2123481520624
	2123452321712 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2123452321712 -> 2123481520672
	2123481520672 [label=AccumulateGrad]
	2123481517264 -> 2123481521536
	2123452321808 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2123452321808 -> 2123481517264
	2123481517264 [label=AccumulateGrad]
	2123481516208 -> 2123481521536
	2123452321904 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2123452321904 -> 2123481516208
	2123481516208 [label=AccumulateGrad]
	2123481519280 -> 2123481517456
	2123481520384 -> 2123481520816
	2123452322288 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2123452322288 -> 2123481520384
	2123481520384 [label=AccumulateGrad]
	2123481519952 -> 2123481517216
	2123452322384 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2123452322384 -> 2123481519952
	2123481519952 [label=AccumulateGrad]
	2123481521920 -> 2123481517216
	2123452322480 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2123452322480 -> 2123481521920
	2123481521920 [label=AccumulateGrad]
	2123481521728 -> 2123481518560
	2123452322864 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2123452322864 -> 2123481521728
	2123481521728 [label=AccumulateGrad]
	2123481518896 -> 2123481522112
	2123452322960 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2123452322960 -> 2123481518896
	2123481518896 [label=AccumulateGrad]
	2123481515632 -> 2123481522112
	2123452323056 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2123452323056 -> 2123481515632
	2123481515632 [label=AccumulateGrad]
	2123481518320 -> 2123481517408
	2123452323440 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2123452323440 -> 2123481518320
	2123481518320 [label=AccumulateGrad]
	2123481514720 -> 2123481522736
	2123452323536 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2123452323536 -> 2123481514720
	2123481514720 [label=AccumulateGrad]
	2123481522496 -> 2123481522736
	2123452323632 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2123452323632 -> 2123481522496
	2123481522496 [label=AccumulateGrad]
	2123481522640 -> 2123481522832
	2123481522592 -> 2123481523216
	2123452324016 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2123452324016 -> 2123481522592
	2123481522592 [label=AccumulateGrad]
	2123481523120 -> 2123481523312
	2123452324112 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2123452324112 -> 2123481523120
	2123481523120 [label=AccumulateGrad]
	2123481523072 -> 2123481523312
	2123452324208 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2123452324208 -> 2123481523072
	2123481523072 [label=AccumulateGrad]
	2123481523408 -> 2123481523600
	2123452324592 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2123452324592 -> 2123481523408
	2123481523408 [label=AccumulateGrad]
	2123481523792 -> 2123481523648
	2123452324688 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2123452324688 -> 2123481523792
	2123481523792 [label=AccumulateGrad]
	2123481523744 -> 2123481523648
	2123452324784 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2123452324784 -> 2123481523744
	2123481523744 [label=AccumulateGrad]
	2123481523936 -> 2123481524272
	2123452325168 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2123452325168 -> 2123481523936
	2123481523936 [label=AccumulateGrad]
	2123481524128 -> 2123481524224
	2123452325264 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2123452325264 -> 2123481524128
	2123481524128 [label=AccumulateGrad]
	2123481524032 -> 2123481524224
	2123452325360 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2123452325360 -> 2123481524032
	2123481524032 [label=AccumulateGrad]
	2123481524368 -> 2123481524416
	2123481524560 -> 2123481524704
	2123452325744 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2123452325744 -> 2123481524560
	2123481524560 [label=AccumulateGrad]
	2123481524848 -> 2123481524896
	2123452325840 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2123452325840 -> 2123481524848
	2123481524848 [label=AccumulateGrad]
	2123481525040 -> 2123481524896
	2123452325936 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2123452325936 -> 2123481525040
	2123481525040 [label=AccumulateGrad]
	2123481525088 -> 2123481525328
	2123452326320 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2123452326320 -> 2123481525088
	2123481525088 [label=AccumulateGrad]
	2123481525376 -> 2123481525616
	2123452326416 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2123452326416 -> 2123481525376
	2123481525376 [label=AccumulateGrad]
	2123481525712 -> 2123481525616
	2123452326512 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2123452326512 -> 2123481525712
	2123481525712 [label=AccumulateGrad]
	2123481525472 -> 2123481525856
	2123452326896 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2123452326896 -> 2123481525472
	2123481525472 [label=AccumulateGrad]
	2123481526096 -> 2123481526192
	2123452326992 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2123452326992 -> 2123481526096
	2123481526096 [label=AccumulateGrad]
	2123481526000 -> 2123481526192
	2123452327088 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2123452327088 -> 2123481526000
	2123481526000 [label=AccumulateGrad]
	2123481526048 -> 2123481525952
	2123481526576 -> 2123481526528
	2123452328048 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2123452328048 -> 2123481526576
	2123481526576 [label=AccumulateGrad]
	2123481526432 -> 2123481526624
	2123452328144 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2123452328144 -> 2123481526432
	2123481526432 [label=AccumulateGrad]
	2123481526816 -> 2123481526624
	2123452328240 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2123452328240 -> 2123481526816
	2123481526816 [label=AccumulateGrad]
	2123481526960 -> 2123481526912
	2123452328624 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2123452328624 -> 2123481526960
	2123481526960 [label=AccumulateGrad]
	2123481527104 -> 2123481527248
	2123452328720 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2123452328720 -> 2123481527104
	2123481527104 [label=AccumulateGrad]
	2123481527536 -> 2123481527248
	2123452328816 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2123452328816 -> 2123481527536
	2123481527536 [label=AccumulateGrad]
	2123481527632 -> 2123481527584
	2123452329200 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2123452329200 -> 2123481527632
	2123481527632 [label=AccumulateGrad]
	2123481527728 -> 2123481528016
	2123452329296 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2123452329296 -> 2123481527728
	2123481527728 [label=AccumulateGrad]
	2123481527776 -> 2123481528016
	2123452329392 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2123452329392 -> 2123481527776
	2123481527776 [label=AccumulateGrad]
	2123481527920 -> 2123481528112
	2123481527920 [label=CudnnBatchNormBackward0]
	2123481527152 -> 2123481527920
	2123481527152 [label=ConvolutionBackward0]
	2123481526336 -> 2123481527152
	2123481526672 -> 2123481527152
	2123452327472 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2123452327472 -> 2123481526672
	2123481526672 [label=AccumulateGrad]
	2123481527488 -> 2123481527920
	2123452327568 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2123452327568 -> 2123481527488
	2123481527488 [label=AccumulateGrad]
	2123481527392 -> 2123481527920
	2123452327664 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2123452327664 -> 2123481527392
	2123481527392 [label=AccumulateGrad]
	2123481527872 -> 2123481528496
	2123452329776 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2123452329776 -> 2123481527872
	2123481527872 [label=AccumulateGrad]
	2123481528400 -> 2123481528592
	2123452329872 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2123452329872 -> 2123481528400
	2123481528400 [label=AccumulateGrad]
	2123481528352 -> 2123481528592
	2123452329968 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2123452329968 -> 2123481528352
	2123481528352 [label=AccumulateGrad]
	2123481528688 -> 2123481528880
	2123452330352 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2123452330352 -> 2123481528688
	2123481528688 [label=AccumulateGrad]
	2123481529072 -> 2123481528928
	2123452330448 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2123452330448 -> 2123481529072
	2123481529072 [label=AccumulateGrad]
	2123481529024 -> 2123481528928
	2123452330544 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2123452330544 -> 2123481529024
	2123481529024 [label=AccumulateGrad]
	2123481529216 -> 2123481529552
	2123452330928 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2123452330928 -> 2123481529216
	2123481529216 [label=AccumulateGrad]
	2123481529408 -> 2123481529504
	2123452331024 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2123452331024 -> 2123481529408
	2123481529408 [label=AccumulateGrad]
	2123481529312 -> 2123481529504
	2123452331120 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2123452331120 -> 2123481529312
	2123481529312 [label=AccumulateGrad]
	2123481529648 -> 2123481529696
	2123481529840 -> 2123481529984
	2123452331504 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2123452331504 -> 2123481529840
	2123481529840 [label=AccumulateGrad]
	2123481530128 -> 2123481530176
	2123452331600 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2123452331600 -> 2123481530128
	2123481530128 [label=AccumulateGrad]
	2123481530272 -> 2123481530176
	2123452331696 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2123452331696 -> 2123481530272
	2123481530272 [label=AccumulateGrad]
	2123481521584 -> 2123481517744
	2123452332080 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2123452332080 -> 2123481521584
	2123481521584 [label=AccumulateGrad]
	2123481520240 -> 2123481522544
	2123452332176 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2123452332176 -> 2123481520240
	2123481520240 [label=AccumulateGrad]
	2123481523024 -> 2123481522544
	2123452332272 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2123452332272 -> 2123481523024
	2123481523024 [label=AccumulateGrad]
	2123481523504 -> 2123481523840
	2123452332656 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2123452332656 -> 2123481523504
	2123481523504 [label=AccumulateGrad]
	2123481524464 -> 2123481524944
	2123452332752 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2123452332752 -> 2123481524464
	2123481524464 [label=AccumulateGrad]
	2123481524320 -> 2123481524944
	2123452332848 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2123452332848 -> 2123481524320
	2123481524320 [label=AccumulateGrad]
	2123481524800 -> 2123481525424
	2123481526864 -> 2123481527680
	2123481526864 [label=TBackward0]
	2123481525280 -> 2123481526864
	2123452333520 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2123452333520 -> 2123481525280
	2123481525280 [label=AccumulateGrad]
	2123481527680 -> 2123533007088
}
