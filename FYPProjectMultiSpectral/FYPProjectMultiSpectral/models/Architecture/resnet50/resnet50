digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2696151601072 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2696144533200 [label=AddmmBackward0]
	2696144532096 -> 2696144533200
	2696152240720 [label="fc.bias
 (19)" fillcolor=lightblue]
	2696152240720 -> 2696144532096
	2696144532096 [label=AccumulateGrad]
	2696144532240 -> 2696144533200
	2696144532240 [label=ViewBackward0]
	2696144531760 -> 2696144532240
	2696144531760 [label=MeanBackward1]
	2696144530656 -> 2696144531760
	2696144530656 [label=ReluBackward0]
	2696144530176 -> 2696144530656
	2696144530176 [label=AddBackward0]
	2696144529696 -> 2696144530176
	2696144529696 [label=CudnnBatchNormBackward0]
	2696144529360 -> 2696144529696
	2696144529360 [label=ConvolutionBackward0]
	2696144528400 -> 2696144529360
	2696144528400 [label=ReluBackward0]
	2696144527296 -> 2696144528400
	2696144527296 [label=CudnnBatchNormBackward0]
	2696144526816 -> 2696144527296
	2696144526816 [label=ConvolutionBackward0]
	2696144524464 -> 2696144526816
	2696144524464 [label=ReluBackward0]
	2696144535360 -> 2696144524464
	2696144535360 [label=CudnnBatchNormBackward0]
	2696144535264 -> 2696144535360
	2696144535264 [label=ConvolutionBackward0]
	2696144530320 -> 2696144535264
	2696144530320 [label=ReluBackward0]
	2696144534880 -> 2696144530320
	2696144534880 [label=AddBackward0]
	2696144534784 -> 2696144534880
	2696144534784 [label=CudnnBatchNormBackward0]
	2696144534832 -> 2696144534784
	2696144534832 [label=ConvolutionBackward0]
	2696144534208 -> 2696144534832
	2696144534208 [label=ReluBackward0]
	2696144534256 -> 2696144534208
	2696144534256 [label=CudnnBatchNormBackward0]
	2696144534112 -> 2696144534256
	2696144534112 [label=ConvolutionBackward0]
	2696144533824 -> 2696144534112
	2696144533824 [label=ReluBackward0]
	2696144533872 -> 2696144533824
	2696144533872 [label=CudnnBatchNormBackward0]
	2696144533584 -> 2696144533872
	2696144533584 [label=ConvolutionBackward0]
	2696144534688 -> 2696144533584
	2696144534688 [label=ReluBackward0]
	2696144533392 -> 2696144534688
	2696144533392 [label=AddBackward0]
	2696144533104 -> 2696144533392
	2696144533104 [label=CudnnBatchNormBackward0]
	2696144532864 -> 2696144533104
	2696144532864 [label=ConvolutionBackward0]
	2696144532672 -> 2696144532864
	2696144532672 [label=ReluBackward0]
	2696144532288 -> 2696144532672
	2696144532288 [label=CudnnBatchNormBackward0]
	2696144532528 -> 2696144532288
	2696144532528 [label=ConvolutionBackward0]
	2696144532144 -> 2696144532528
	2696144532144 [label=ReluBackward0]
	2696144531904 -> 2696144532144
	2696144531904 [label=CudnnBatchNormBackward0]
	2696144531856 -> 2696144531904
	2696144531856 [label=ConvolutionBackward0]
	2696144531520 -> 2696144531856
	2696144531520 [label=ReluBackward0]
	2696144531568 -> 2696144531520
	2696144531568 [label=AddBackward0]
	2696144531472 -> 2696144531568
	2696144531472 [label=CudnnBatchNormBackward0]
	2696144531040 -> 2696144531472
	2696144531040 [label=ConvolutionBackward0]
	2696144530896 -> 2696144531040
	2696144530896 [label=ReluBackward0]
	2696144530704 -> 2696144530896
	2696144530704 [label=CudnnBatchNormBackward0]
	2696144530368 -> 2696144530704
	2696144530368 [label=ConvolutionBackward0]
	2696144530512 -> 2696144530368
	2696144530512 [label=ReluBackward0]
	2696144530080 -> 2696144530512
	2696144530080 [label=CudnnBatchNormBackward0]
	2696144529984 -> 2696144530080
	2696144529984 [label=ConvolutionBackward0]
	2696144531376 -> 2696144529984
	2696144531376 [label=ReluBackward0]
	2696144529600 -> 2696144531376
	2696144529600 [label=AddBackward0]
	2696144529504 -> 2696144529600
	2696144529504 [label=CudnnBatchNormBackward0]
	2696144529552 -> 2696144529504
	2696144529552 [label=ConvolutionBackward0]
	2696144528928 -> 2696144529552
	2696144528928 [label=ReluBackward0]
	2696144528976 -> 2696144528928
	2696144528976 [label=CudnnBatchNormBackward0]
	2696144528832 -> 2696144528976
	2696144528832 [label=ConvolutionBackward0]
	2696144528544 -> 2696144528832
	2696144528544 [label=ReluBackward0]
	2696144528592 -> 2696144528544
	2696144528592 [label=CudnnBatchNormBackward0]
	2696144528304 -> 2696144528592
	2696144528304 [label=ConvolutionBackward0]
	2696144529408 -> 2696144528304
	2696144529408 [label=ReluBackward0]
	2696144528112 -> 2696144529408
	2696144528112 [label=AddBackward0]
	2696144527824 -> 2696144528112
	2696144527824 [label=CudnnBatchNormBackward0]
	2696144527584 -> 2696144527824
	2696144527584 [label=ConvolutionBackward0]
	2696144527392 -> 2696144527584
	2696144527392 [label=ReluBackward0]
	2696144527008 -> 2696144527392
	2696144527008 [label=CudnnBatchNormBackward0]
	2696144527248 -> 2696144527008
	2696144527248 [label=ConvolutionBackward0]
	2696144526864 -> 2696144527248
	2696144526864 [label=ReluBackward0]
	2696144526768 -> 2696144526864
	2696144526768 [label=CudnnBatchNormBackward0]
	2696144523984 -> 2696144526768
	2696144523984 [label=ConvolutionBackward0]
	2696144527872 -> 2696144523984
	2696144527872 [label=ReluBackward0]
	2696144523504 -> 2696144527872
	2696144523504 [label=AddBackward0]
	2696144522736 -> 2696144523504
	2696144522736 [label=CudnnBatchNormBackward0]
	2696144524320 -> 2696144522736
	2696144524320 [label=ConvolutionBackward0]
	2696144525472 -> 2696144524320
	2696144525472 [label=ReluBackward0]
	2696144524128 -> 2696144525472
	2696144524128 [label=CudnnBatchNormBackward0]
	2696144526144 -> 2696144524128
	2696144526144 [label=ConvolutionBackward0]
	2696144519424 -> 2696144526144
	2696144519424 [label=ReluBackward0]
	2696144524752 -> 2696144519424
	2696144524752 [label=CudnnBatchNormBackward0]
	2696144521968 -> 2696144524752
	2696144521968 [label=ConvolutionBackward0]
	2696144523408 -> 2696144521968
	2696144523408 [label=ReluBackward0]
	2696144523888 -> 2696144523408
	2696144523888 [label=AddBackward0]
	2696144525904 -> 2696144523888
	2696144525904 [label=CudnnBatchNormBackward0]
	2696144524272 -> 2696144525904
	2696144524272 [label=ConvolutionBackward0]
	2696144524368 -> 2696144524272
	2696144524368 [label=ReluBackward0]
	2696144525616 -> 2696144524368
	2696144525616 [label=CudnnBatchNormBackward0]
	2696144521728 -> 2696144525616
	2696144521728 [label=ConvolutionBackward0]
	2696144522448 -> 2696144521728
	2696144522448 [label=ReluBackward0]
	2696144520864 -> 2696144522448
	2696144520864 [label=CudnnBatchNormBackward0]
	2696144526480 -> 2696144520864
	2696144526480 [label=ConvolutionBackward0]
	2696144521248 -> 2696144526480
	2696144521248 [label=ReluBackward0]
	2696144523792 -> 2696144521248
	2696144523792 [label=AddBackward0]
	2696144526672 -> 2696144523792
	2696144526672 [label=CudnnBatchNormBackward0]
	2696144524800 -> 2696144526672
	2696144524800 [label=ConvolutionBackward0]
	2696144521680 -> 2696144524800
	2696144521680 [label=ReluBackward0]
	2696144520624 -> 2696144521680
	2696144520624 [label=CudnnBatchNormBackward0]
	2696144521104 -> 2696144520624
	2696144521104 [label=ConvolutionBackward0]
	2696136374256 -> 2696144521104
	2696136374256 [label=ReluBackward0]
	2696136375408 -> 2696136374256
	2696136375408 [label=CudnnBatchNormBackward0]
	2696136374208 -> 2696136375408
	2696136374208 [label=ConvolutionBackward0]
	2696136374304 -> 2696136374208
	2696136374304 [label=ReluBackward0]
	2696136375456 -> 2696136374304
	2696136375456 [label=AddBackward0]
	2696136374688 -> 2696136375456
	2696136374688 [label=CudnnBatchNormBackward0]
	2696136375840 -> 2696136374688
	2696136375840 [label=ConvolutionBackward0]
	2696136375888 -> 2696136375840
	2696136375888 [label=ReluBackward0]
	2696136376080 -> 2696136375888
	2696136376080 [label=CudnnBatchNormBackward0]
	2696136373632 -> 2696136376080
	2696136373632 [label=ConvolutionBackward0]
	2696136372912 -> 2696136373632
	2696136372912 [label=ReluBackward0]
	2696144436960 -> 2696136372912
	2696144436960 [label=CudnnBatchNormBackward0]
	2696151669760 -> 2696144436960
	2696151669760 [label=ConvolutionBackward0]
	2696136374112 -> 2696151669760
	2696136374112 [label=ReluBackward0]
	2696151668320 -> 2696136374112
	2696151668320 [label=AddBackward0]
	2696151667840 -> 2696151668320
	2696151667840 [label=CudnnBatchNormBackward0]
	2696151667504 -> 2696151667840
	2696151667504 [label=ConvolutionBackward0]
	2696151666544 -> 2696151667504
	2696151666544 [label=ReluBackward0]
	2696151665440 -> 2696151666544
	2696151665440 [label=CudnnBatchNormBackward0]
	2696151664960 -> 2696151665440
	2696151664960 [label=ConvolutionBackward0]
	2696151664000 -> 2696151664960
	2696151664000 [label=ReluBackward0]
	2696151663664 -> 2696151664000
	2696151663664 [label=CudnnBatchNormBackward0]
	2696151663184 -> 2696151663664
	2696151663184 [label=ConvolutionBackward0]
	2696151668464 -> 2696151663184
	2696151668464 [label=ReluBackward0]
	2696151670048 -> 2696151668464
	2696151670048 [label=AddBackward0]
	2696151670000 -> 2696151670048
	2696151670000 [label=CudnnBatchNormBackward0]
	2696151669808 -> 2696151670000
	2696151669808 [label=ConvolutionBackward0]
	2696151669712 -> 2696151669808
	2696151669712 [label=ReluBackward0]
	2696151669376 -> 2696151669712
	2696151669376 [label=CudnnBatchNormBackward0]
	2696151669184 -> 2696151669376
	2696151669184 [label=ConvolutionBackward0]
	2696151669040 -> 2696151669184
	2696151669040 [label=ReluBackward0]
	2696151668848 -> 2696151669040
	2696151668848 [label=CudnnBatchNormBackward0]
	2696151668512 -> 2696151668848
	2696151668512 [label=ConvolutionBackward0]
	2696151670192 -> 2696151668512
	2696151670192 [label=ReluBackward0]
	2696151668368 -> 2696151670192
	2696151668368 [label=AddBackward0]
	2696151668032 -> 2696151668368
	2696151668032 [label=CudnnBatchNormBackward0]
	2696151668080 -> 2696151668032
	2696151668080 [label=ConvolutionBackward0]
	2696151667744 -> 2696151668080
	2696151667744 [label=ReluBackward0]
	2696151667792 -> 2696151667744
	2696151667792 [label=CudnnBatchNormBackward0]
	2696151667696 -> 2696151667792
	2696151667696 [label=ConvolutionBackward0]
	2696151667072 -> 2696151667696
	2696151667072 [label=ReluBackward0]
	2696151667120 -> 2696151667072
	2696151667120 [label=CudnnBatchNormBackward0]
	2696151666976 -> 2696151667120
	2696151666976 [label=ConvolutionBackward0]
	2696151666688 -> 2696151666976
	2696151666688 [label=ReluBackward0]
	2696151666736 -> 2696151666688
	2696151666736 [label=AddBackward0]
	2696151666448 -> 2696151666736
	2696151666448 [label=CudnnBatchNormBackward0]
	2696151666208 -> 2696151666448
	2696151666208 [label=ConvolutionBackward0]
	2696151666016 -> 2696151666208
	2696151666016 [label=ReluBackward0]
	2696151665632 -> 2696151666016
	2696151665632 [label=CudnnBatchNormBackward0]
	2696151665872 -> 2696151665632
	2696151665872 [label=ConvolutionBackward0]
	2696151665488 -> 2696151665872
	2696151665488 [label=ReluBackward0]
	2696151665248 -> 2696151665488
	2696151665248 [label=CudnnBatchNormBackward0]
	2696151665200 -> 2696151665248
	2696151665200 [label=ConvolutionBackward0]
	2696151666496 -> 2696151665200
	2696151666496 [label=ReluBackward0]
	2696151664768 -> 2696151666496
	2696151664768 [label=AddBackward0]
	2696151664720 -> 2696151664768
	2696151664720 [label=CudnnBatchNormBackward0]
	2696151664528 -> 2696151664720
	2696151664528 [label=ConvolutionBackward0]
	2696151664432 -> 2696151664528
	2696151664432 [label=ReluBackward0]
	2696151664096 -> 2696151664432
	2696151664096 [label=CudnnBatchNormBackward0]
	2696151663904 -> 2696151664096
	2696151663904 [label=ConvolutionBackward0]
	2696151663760 -> 2696151663904
	2696151663760 [label=ReluBackward0]
	2696151663568 -> 2696151663760
	2696151663568 [label=CudnnBatchNormBackward0]
	2696151663232 -> 2696151663568
	2696151663232 [label=ConvolutionBackward0]
	2696151664912 -> 2696151663232
	2696151664912 [label=ReluBackward0]
	2696151663088 -> 2696151664912
	2696151663088 [label=AddBackward0]
	2696151662752 -> 2696151663088
	2696151662752 [label=CudnnBatchNormBackward0]
	2696151662800 -> 2696151662752
	2696151662800 [label=ConvolutionBackward0]
	2696151670816 -> 2696151662800
	2696151670816 [label=ReluBackward0]
	2696151671152 -> 2696151670816
	2696151671152 [label=CudnnBatchNormBackward0]
	2696151671248 -> 2696151671152
	2696151671248 [label=ConvolutionBackward0]
	2696151671440 -> 2696151671248
	2696151671440 [label=ReluBackward0]
	2696151671584 -> 2696151671440
	2696151671584 [label=CudnnBatchNormBackward0]
	2696151671680 -> 2696151671584
	2696151671680 [label=ConvolutionBackward0]
	2696151671872 -> 2696151671680
	2696151671872 [label=MaxPool2DWithIndicesBackward0]
	2696151672016 -> 2696151671872
	2696151672016 [label=ReluBackward0]
	2696151672112 -> 2696151672016
	2696151672112 [label=CudnnBatchNormBackward0]
	2696151672208 -> 2696151672112
	2696151672208 [label=ConvolutionBackward0]
	2696151672400 -> 2696151672208
	2696152240528 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2696152240528 -> 2696151672400
	2696151672400 [label=AccumulateGrad]
	2696151672160 -> 2696151672112
	2696122129808 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2696122129808 -> 2696151672160
	2696151672160 [label=AccumulateGrad]
	2696151671920 -> 2696151672112
	2696122129520 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2696122129520 -> 2696151671920
	2696151671920 [label=AccumulateGrad]
	2696151671824 -> 2696151671680
	2696122129424 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2696122129424 -> 2696151671824
	2696151671824 [label=AccumulateGrad]
	2696151671632 -> 2696151671584
	2696122129328 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2696122129328 -> 2696151671632
	2696151671632 [label=AccumulateGrad]
	2696151671488 -> 2696151671584
	2696122128560 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2696122128560 -> 2696151671488
	2696151671488 [label=AccumulateGrad]
	2696151671392 -> 2696151671248
	2696122128656 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2696122128656 -> 2696151671392
	2696151671392 [label=AccumulateGrad]
	2696151671200 -> 2696151671152
	2696122128752 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2696122128752 -> 2696151671200
	2696151671200 [label=AccumulateGrad]
	2696151671056 -> 2696151671152
	2696122128848 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2696122128848 -> 2696151671056
	2696151671056 [label=AccumulateGrad]
	2696151670960 -> 2696151662800
	2696122129136 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2696122129136 -> 2696151670960
	2696151670960 [label=AccumulateGrad]
	2696151662992 -> 2696151662752
	2696122128176 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2696122128176 -> 2696151662992
	2696151662992 [label=AccumulateGrad]
	2696151662848 -> 2696151662752
	2696122128080 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2696122128080 -> 2696151662848
	2696151662848 [label=AccumulateGrad]
	2696151662944 -> 2696151663088
	2696151662944 [label=CudnnBatchNormBackward0]
	2696151671344 -> 2696151662944
	2696151671344 [label=ConvolutionBackward0]
	2696151671872 -> 2696151671344
	2696151671728 -> 2696151671344
	2696122130000 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2696122130000 -> 2696151671728
	2696151671728 [label=AccumulateGrad]
	2696151662656 -> 2696151662944
	2696122130096 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2696122130096 -> 2696151662656
	2696151662656 [label=AccumulateGrad]
	2696151662896 -> 2696151662944
	2696122130192 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2696122130192 -> 2696151662896
	2696151662896 [label=AccumulateGrad]
	2696151663376 -> 2696151663232
	2696122125392 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2696122125392 -> 2696151663376
	2696151663376 [label=AccumulateGrad]
	2696151663424 -> 2696151663568
	2696122125584 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2696122125584 -> 2696151663424
	2696151663424 [label=AccumulateGrad]
	2696151663856 -> 2696151663568
	2696122127216 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2696122127216 -> 2696151663856
	2696151663856 [label=AccumulateGrad]
	2696151663952 -> 2696151663904
	2696122127984 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2696122127984 -> 2696151663952
	2696151663952 [label=AccumulateGrad]
	2696151664048 -> 2696151664096
	2696122127888 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2696122127888 -> 2696151664048
	2696151664048 [label=AccumulateGrad]
	2696151664240 -> 2696151664096
	2696101406544 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2696101406544 -> 2696151664240
	2696151664240 [label=AccumulateGrad]
	2696151664288 -> 2696151664528
	2696101406928 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2696101406928 -> 2696151664288
	2696151664288 [label=AccumulateGrad]
	2696151664576 -> 2696151664720
	2696101407024 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2696101407024 -> 2696151664576
	2696151664576 [label=AccumulateGrad]
	2696151664816 -> 2696151664720
	2696101407120 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2696101407120 -> 2696151664816
	2696151664816 [label=AccumulateGrad]
	2696151664912 -> 2696151664768
	2696151664864 -> 2696151665200
	2696101407504 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2696101407504 -> 2696151664864
	2696151664864 [label=AccumulateGrad]
	2696151665392 -> 2696151665248
	2696101407600 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2696101407600 -> 2696151665392
	2696151665392 [label=AccumulateGrad]
	2696151665344 -> 2696151665248
	2696101407696 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2696101407696 -> 2696151665344
	2696151665344 [label=AccumulateGrad]
	2696151665536 -> 2696151665872
	2696101408080 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2696101408080 -> 2696151665536
	2696151665536 [label=AccumulateGrad]
	2696151665728 -> 2696151665632
	2696101408176 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2696101408176 -> 2696151665728
	2696151665728 [label=AccumulateGrad]
	2696151665968 -> 2696151665632
	2696101408272 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2696101408272 -> 2696151665968
	2696151665968 [label=AccumulateGrad]
	2696151666256 -> 2696151666208
	2696101408656 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2696101408656 -> 2696151666256
	2696151666256 [label=AccumulateGrad]
	2696151666112 -> 2696151666448
	2696101408752 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2696101408752 -> 2696151666112
	2696151666112 [label=AccumulateGrad]
	2696151666304 -> 2696151666448
	2696101408848 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2696101408848 -> 2696151666304
	2696151666304 [label=AccumulateGrad]
	2696151666496 -> 2696151666736
	2696151666592 -> 2696151666976
	2696101409808 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2696101409808 -> 2696151666592
	2696151666592 [label=AccumulateGrad]
	2696151667216 -> 2696151667120
	2696101409904 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2696101409904 -> 2696151667216
	2696151667216 [label=AccumulateGrad]
	2696151667168 -> 2696151667120
	2696101410000 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2696101410000 -> 2696151667168
	2696151667168 [label=AccumulateGrad]
	2696151667264 -> 2696151667696
	2696101410384 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2696101410384 -> 2696151667264
	2696151667264 [label=AccumulateGrad]
	2696151667600 -> 2696151667792
	2696101410480 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2696101410480 -> 2696151667600
	2696151667600 [label=AccumulateGrad]
	2696151667552 -> 2696151667792
	2696101410576 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2696101410576 -> 2696151667552
	2696151667552 [label=AccumulateGrad]
	2696151667888 -> 2696151668080
	2696101410960 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2696101410960 -> 2696151667888
	2696151667888 [label=AccumulateGrad]
	2696151668272 -> 2696151668032
	2696101411056 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2696101411056 -> 2696151668272
	2696151668272 [label=AccumulateGrad]
	2696151668128 -> 2696151668032
	2696101411152 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2696101411152 -> 2696151668128
	2696151668128 [label=AccumulateGrad]
	2696151668224 -> 2696151668368
	2696151668224 [label=CudnnBatchNormBackward0]
	2696151667408 -> 2696151668224
	2696151667408 [label=ConvolutionBackward0]
	2696151666688 -> 2696151667408
	2696151666928 -> 2696151667408
	2696101409232 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2696101409232 -> 2696151666928
	2696151666928 [label=AccumulateGrad]
	2696151667936 -> 2696151668224
	2696101409328 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2696101409328 -> 2696151667936
	2696151667936 [label=AccumulateGrad]
	2696151668176 -> 2696151668224
	2696101409424 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2696101409424 -> 2696151668176
	2696151668176 [label=AccumulateGrad]
	2696151668656 -> 2696151668512
	2696101411536 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2696101411536 -> 2696151668656
	2696151668656 [label=AccumulateGrad]
	2696151668704 -> 2696151668848
	2696101411632 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2696101411632 -> 2696151668704
	2696151668704 [label=AccumulateGrad]
	2696151669136 -> 2696151668848
	2696101411728 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2696101411728 -> 2696151669136
	2696151669136 [label=AccumulateGrad]
	2696151669232 -> 2696151669184
	2696101412112 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2696101412112 -> 2696151669232
	2696151669232 [label=AccumulateGrad]
	2696151669328 -> 2696151669376
	2696101412208 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2696101412208 -> 2696151669328
	2696151669328 [label=AccumulateGrad]
	2696151669520 -> 2696151669376
	2696101412304 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2696101412304 -> 2696151669520
	2696151669520 [label=AccumulateGrad]
	2696151669568 -> 2696151669808
	2696101412688 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2696101412688 -> 2696151669568
	2696151669568 [label=AccumulateGrad]
	2696151669856 -> 2696151670000
	2696101412784 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2696101412784 -> 2696151669856
	2696151669856 [label=AccumulateGrad]
	2696151670096 -> 2696151670000
	2696101118032 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2696101118032 -> 2696151670096
	2696151670096 [label=AccumulateGrad]
	2696151670192 -> 2696151670048
	2696151670144 -> 2696151663184
	2696101118416 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2696101118416 -> 2696151670144
	2696151670144 [label=AccumulateGrad]
	2696151663040 -> 2696151663664
	2696101118512 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2696101118512 -> 2696151663040
	2696151663040 [label=AccumulateGrad]
	2696151664144 -> 2696151663664
	2696101118608 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2696101118608 -> 2696151664144
	2696151664144 [label=AccumulateGrad]
	2696151664624 -> 2696151664960
	2696101118992 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2696101118992 -> 2696151664624
	2696151664624 [label=AccumulateGrad]
	2696151665584 -> 2696151665440
	2696101119088 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2696101119088 -> 2696151665584
	2696151665584 [label=AccumulateGrad]
	2696151665920 -> 2696151665440
	2696101119184 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2696101119184 -> 2696151665920
	2696151665920 [label=AccumulateGrad]
	2696151666400 -> 2696151667504
	2696101119568 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2696101119568 -> 2696151666400
	2696151666400 [label=AccumulateGrad]
	2696151667360 -> 2696151667840
	2696101119664 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2696101119664 -> 2696151667360
	2696151667360 [label=AccumulateGrad]
	2696151667984 -> 2696151667840
	2696101119760 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2696101119760 -> 2696151667984
	2696151667984 [label=AccumulateGrad]
	2696151668464 -> 2696151668320
	2696151668800 -> 2696151669760
	2696101120144 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2696101120144 -> 2696151668800
	2696151668800 [label=AccumulateGrad]
	2696151670240 -> 2696144436960
	2696101120240 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2696101120240 -> 2696151670240
	2696151670240 [label=AccumulateGrad]
	2696151670384 -> 2696144436960
	2696101120336 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2696101120336 -> 2696151670384
	2696151670384 [label=AccumulateGrad]
	2696136374544 -> 2696136373632
	2696101120720 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2696101120720 -> 2696136374544
	2696136374544 [label=AccumulateGrad]
	2696136374976 -> 2696136376080
	2696101120816 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2696101120816 -> 2696136374976
	2696136374976 [label=AccumulateGrad]
	2696136375360 -> 2696136376080
	2696101120912 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2696101120912 -> 2696136375360
	2696136375360 [label=AccumulateGrad]
	2696136373728 -> 2696136375840
	2696101121296 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2696101121296 -> 2696136373728
	2696136373728 [label=AccumulateGrad]
	2696136375792 -> 2696136374688
	2696101121392 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2696101121392 -> 2696136375792
	2696136375792 [label=AccumulateGrad]
	2696136372864 -> 2696136374688
	2696101121488 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2696101121488 -> 2696136372864
	2696136372864 [label=AccumulateGrad]
	2696136374112 -> 2696136375456
	2696136375168 -> 2696136374208
	2696101122448 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2696101122448 -> 2696136375168
	2696136375168 [label=AccumulateGrad]
	2696136374736 -> 2696136375408
	2696101122544 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2696101122544 -> 2696136374736
	2696136374736 [label=AccumulateGrad]
	2696136375072 -> 2696136375408
	2696101122640 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2696101122640 -> 2696136375072
	2696136375072 [label=AccumulateGrad]
	2696136374160 -> 2696144521104
	2696101123024 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2696101123024 -> 2696136374160
	2696136374160 [label=AccumulateGrad]
	2696144519232 -> 2696144520624
	2696101123120 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2696101123120 -> 2696144519232
	2696144519232 [label=AccumulateGrad]
	2696144524224 -> 2696144520624
	2696101123216 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2696101123216 -> 2696144524224
	2696144524224 [label=AccumulateGrad]
	2696144522592 -> 2696144524800
	2696101123600 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2696101123600 -> 2696144522592
	2696144522592 [label=AccumulateGrad]
	2696144525136 -> 2696144526672
	2696101123696 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2696101123696 -> 2696144525136
	2696144525136 [label=AccumulateGrad]
	2696144522304 -> 2696144526672
	2696101123792 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2696101123792 -> 2696144522304
	2696144522304 [label=AccumulateGrad]
	2696144523312 -> 2696144523792
	2696144523312 [label=CudnnBatchNormBackward0]
	2696144522784 -> 2696144523312
	2696144522784 [label=ConvolutionBackward0]
	2696136374304 -> 2696144522784
	2696136372960 -> 2696144522784
	2696101121872 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2696101121872 -> 2696136372960
	2696136372960 [label=AccumulateGrad]
	2696144521152 -> 2696144523312
	2696101121968 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2696101121968 -> 2696144521152
	2696144521152 [label=AccumulateGrad]
	2696144523216 -> 2696144523312
	2696101122064 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2696101122064 -> 2696144523216
	2696144523216 [label=AccumulateGrad]
	2696144522928 -> 2696144526480
	2696101124176 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2696101124176 -> 2696144522928
	2696144522928 [label=AccumulateGrad]
	2696144524848 -> 2696144520864
	2696101124272 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2696101124272 -> 2696144524848
	2696144524848 [label=AccumulateGrad]
	2696144525856 -> 2696144520864
	2696101124368 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2696101124368 -> 2696144525856
	2696144525856 [label=AccumulateGrad]
	2696144523456 -> 2696144521728
	2696101124752 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2696101124752 -> 2696144523456
	2696144523456 [label=AccumulateGrad]
	2696144526528 -> 2696144525616
	2696101124848 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2696101124848 -> 2696144526528
	2696144526528 [label=AccumulateGrad]
	2696144526720 -> 2696144525616
	2696101124944 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2696101124944 -> 2696144526720
	2696144526720 [label=AccumulateGrad]
	2696144520960 -> 2696144524272
	2696101125328 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2696101125328 -> 2696144520960
	2696144520960 [label=AccumulateGrad]
	2696144522064 -> 2696144525904
	2696101125424 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2696101125424 -> 2696144522064
	2696144522064 [label=AccumulateGrad]
	2696144521872 -> 2696144525904
	2696101125520 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2696101125520 -> 2696144521872
	2696144521872 [label=AccumulateGrad]
	2696144521248 -> 2696144523888
	2696144526384 -> 2696144521968
	2696101125904 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2696101125904 -> 2696144526384
	2696144526384 [label=AccumulateGrad]
	2696144519712 -> 2696144524752
	2696101126000 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2696101126000 -> 2696144519712
	2696144519712 [label=AccumulateGrad]
	2696144525232 -> 2696144524752
	2696101126096 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2696101126096 -> 2696144525232
	2696144525232 [label=AccumulateGrad]
	2696144521488 -> 2696144526144
	2696101126480 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2696101126480 -> 2696144521488
	2696144521488 [label=AccumulateGrad]
	2696144525520 -> 2696144524128
	2696101126576 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2696101126576 -> 2696144525520
	2696144525520 [label=AccumulateGrad]
	2696144525088 -> 2696144524128
	2696101126672 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2696101126672 -> 2696144525088
	2696144525088 [label=AccumulateGrad]
	2696144522400 -> 2696144524320
	2696101127056 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2696101127056 -> 2696144522400
	2696144522400 [label=AccumulateGrad]
	2696144523600 -> 2696144522736
	2696101127152 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2696101127152 -> 2696144523600
	2696144523600 [label=AccumulateGrad]
	2696144525280 -> 2696144522736
	2696101127248 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2696101127248 -> 2696144525280
	2696144525280 [label=AccumulateGrad]
	2696144523408 -> 2696144523504
	2696144519808 -> 2696144523984
	2696101127632 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2696101127632 -> 2696144519808
	2696144519808 [label=AccumulateGrad]
	2696144519472 -> 2696144526768
	2696101127728 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2696101127728 -> 2696144519472
	2696144519472 [label=AccumulateGrad]
	2696144520816 -> 2696144526768
	2696101127824 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2696101127824 -> 2696144520816
	2696144520816 [label=AccumulateGrad]
	2696144526912 -> 2696144527248
	2696101128208 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2696101128208 -> 2696144526912
	2696144526912 [label=AccumulateGrad]
	2696144527104 -> 2696144527008
	2696101128304 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2696101128304 -> 2696144527104
	2696144527104 [label=AccumulateGrad]
	2696144527344 -> 2696144527008
	2696101128400 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2696101128400 -> 2696144527344
	2696144527344 [label=AccumulateGrad]
	2696144527632 -> 2696144527584
	2696101128784 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2696101128784 -> 2696144527632
	2696144527632 [label=AccumulateGrad]
	2696144527488 -> 2696144527824
	2696101128880 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2696101128880 -> 2696144527488
	2696144527488 [label=AccumulateGrad]
	2696144527680 -> 2696144527824
	2696101128976 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2696101128976 -> 2696144527680
	2696144527680 [label=AccumulateGrad]
	2696144527872 -> 2696144528112
	2696144528208 -> 2696144528304
	2696101129360 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2696101129360 -> 2696144528208
	2696144528208 [label=AccumulateGrad]
	2696144528352 -> 2696144528592
	2696101129456 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2696101129456 -> 2696144528352
	2696144528352 [label=AccumulateGrad]
	2696144528688 -> 2696144528592
	2696101129552 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2696101129552 -> 2696144528688
	2696144528688 [label=AccumulateGrad]
	2696144528448 -> 2696144528832
	2696101129936 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2696101129936 -> 2696144528448
	2696144528448 [label=AccumulateGrad]
	2696144529072 -> 2696144528976
	2696101130032 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2696101130032 -> 2696144529072
	2696144529072 [label=AccumulateGrad]
	2696144529024 -> 2696144528976
	2696101130128 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2696101130128 -> 2696144529024
	2696144529024 [label=AccumulateGrad]
	2696144529120 -> 2696144529552
	2696101130512 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2696101130512 -> 2696144529120
	2696144529120 [label=AccumulateGrad]
	2696144529456 -> 2696144529504
	2696101130608 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2696101130608 -> 2696144529456
	2696144529456 [label=AccumulateGrad]
	2696144529648 -> 2696144529504
	2696101130704 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2696101130704 -> 2696144529648
	2696144529648 [label=AccumulateGrad]
	2696144529408 -> 2696144529600
	2696144529792 -> 2696144529984
	2696101131088 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2696101131088 -> 2696144529792
	2696144529792 [label=AccumulateGrad]
	2696144529888 -> 2696144530080
	2696101131184 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2696101131184 -> 2696144529888
	2696144529888 [label=AccumulateGrad]
	2696144530272 -> 2696144530080
	2696101131280 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2696101131280 -> 2696144530272
	2696144530272 [label=AccumulateGrad]
	2696144530416 -> 2696144530368
	2696101131664 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2696101131664 -> 2696144530416
	2696144530416 [label=AccumulateGrad]
	2696144530560 -> 2696144530704
	2696101131760 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2696101131760 -> 2696144530560
	2696144530560 [label=AccumulateGrad]
	2696144530992 -> 2696144530704
	2696101131856 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2696101131856 -> 2696144530992
	2696144530992 [label=AccumulateGrad]
	2696144531088 -> 2696144531040
	2696101132240 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2696101132240 -> 2696144531088
	2696144531088 [label=AccumulateGrad]
	2696144531184 -> 2696144531472
	2696101132336 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2696101132336 -> 2696144531184
	2696144531184 [label=AccumulateGrad]
	2696144531232 -> 2696144531472
	2696101132432 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2696101132432 -> 2696144531232
	2696144531232 [label=AccumulateGrad]
	2696144531376 -> 2696144531568
	2696144531664 -> 2696144531856
	2696101133392 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2696101133392 -> 2696144531664
	2696144531664 [label=AccumulateGrad]
	2696144532048 -> 2696144531904
	2696101133488 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2696101133488 -> 2696144532048
	2696144532048 [label=AccumulateGrad]
	2696144532000 -> 2696144531904
	2696101133584 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2696101133584 -> 2696144532000
	2696144532000 [label=AccumulateGrad]
	2696144532192 -> 2696144532528
	2696101133968 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2696101133968 -> 2696144532192
	2696144532192 [label=AccumulateGrad]
	2696144532384 -> 2696144532288
	2696101134064 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2696101134064 -> 2696144532384
	2696144532384 [label=AccumulateGrad]
	2696144532624 -> 2696144532288
	2696101134160 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2696101134160 -> 2696144532624
	2696144532624 [label=AccumulateGrad]
	2696144532912 -> 2696144532864
	2696152236304 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2696152236304 -> 2696144532912
	2696144532912 [label=AccumulateGrad]
	2696144532768 -> 2696144533104
	2696152236400 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2696152236400 -> 2696144532768
	2696144532768 [label=AccumulateGrad]
	2696144532960 -> 2696144533104
	2696152236496 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2696152236496 -> 2696144532960
	2696144532960 [label=AccumulateGrad]
	2696144533152 -> 2696144533392
	2696144533152 [label=CudnnBatchNormBackward0]
	2696144532432 -> 2696144533152
	2696144532432 [label=ConvolutionBackward0]
	2696144531520 -> 2696144532432
	2696144531952 -> 2696144532432
	2696101132816 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2696101132816 -> 2696144531952
	2696144531952 [label=AccumulateGrad]
	2696144532816 -> 2696144533152
	2696101132912 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2696101132912 -> 2696144532816
	2696144532816 [label=AccumulateGrad]
	2696144533008 -> 2696144533152
	2696101133008 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2696101133008 -> 2696144533008
	2696144533008 [label=AccumulateGrad]
	2696144533488 -> 2696144533584
	2696152236880 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2696152236880 -> 2696144533488
	2696144533488 [label=AccumulateGrad]
	2696144533632 -> 2696144533872
	2696152236976 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2696152236976 -> 2696144533632
	2696144533632 [label=AccumulateGrad]
	2696144533968 -> 2696144533872
	2696152237072 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2696152237072 -> 2696144533968
	2696144533968 [label=AccumulateGrad]
	2696144533728 -> 2696144534112
	2696152237456 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2696152237456 -> 2696144533728
	2696144533728 [label=AccumulateGrad]
	2696144534352 -> 2696144534256
	2696152237552 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2696152237552 -> 2696144534352
	2696144534352 [label=AccumulateGrad]
	2696144534304 -> 2696144534256
	2696152237648 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2696152237648 -> 2696144534304
	2696144534304 [label=AccumulateGrad]
	2696144534400 -> 2696144534832
	2696152238032 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2696152238032 -> 2696144534400
	2696144534400 [label=AccumulateGrad]
	2696144534736 -> 2696144534784
	2696152238128 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2696152238128 -> 2696144534736
	2696144534736 [label=AccumulateGrad]
	2696144534928 -> 2696144534784
	2696152238224 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2696152238224 -> 2696144534928
	2696144534928 [label=AccumulateGrad]
	2696144534688 -> 2696144534880
	2696144535072 -> 2696144535264
	2696152238608 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2696152238608 -> 2696144535072
	2696144535072 [label=AccumulateGrad]
	2696144535168 -> 2696144535360
	2696152238704 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2696152238704 -> 2696144535168
	2696144535168 [label=AccumulateGrad]
	2696144524896 -> 2696144535360
	2696152238800 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2696152238800 -> 2696144524896
	2696144524896 [label=AccumulateGrad]
	2696144521200 -> 2696144526816
	2696152239184 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2696152239184 -> 2696144521200
	2696144521200 [label=AccumulateGrad]
	2696144527440 -> 2696144527296
	2696152239280 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2696152239280 -> 2696144527440
	2696144527440 [label=AccumulateGrad]
	2696144527776 -> 2696144527296
	2696152239376 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2696152239376 -> 2696144527776
	2696144527776 [label=AccumulateGrad]
	2696144528256 -> 2696144529360
	2696152239760 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2696152239760 -> 2696144528256
	2696144528256 [label=AccumulateGrad]
	2696144529216 -> 2696144529696
	2696152239856 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2696152239856 -> 2696144529216
	2696144529216 [label=AccumulateGrad]
	2696144529840 -> 2696144529696
	2696152239952 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2696152239952 -> 2696144529840
	2696144529840 [label=AccumulateGrad]
	2696144530320 -> 2696144530176
	2696144531616 -> 2696144533200
	2696144531616 [label=TBackward0]
	2696144530800 -> 2696144531616
	2696152240624 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2696152240624 -> 2696144530800
	2696144530800 [label=AccumulateGrad]
	2696144533200 -> 2696151601072
}
