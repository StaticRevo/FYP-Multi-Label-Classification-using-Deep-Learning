digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2351601190864 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2351593446688 [label=AddmmBackward0]
	2351593450720 -> 2351593446688
	2351601626640 [label="fc.bias
 (19)" fillcolor=lightblue]
	2351601626640 -> 2351593450720
	2351593450720 [label=AccumulateGrad]
	2351593446640 -> 2351593446688
	2351593446640 [label=ViewBackward0]
	2351593442608 -> 2351593446640
	2351593442608 [label=MeanBackward1]
	2351593451776 -> 2351593442608
	2351593451776 [label=ReluBackward0]
	2351593454128 -> 2351593451776
	2351593454128 [label=AddBackward0]
	2351593451728 -> 2351593454128
	2351593451728 [label=CudnnBatchNormBackward0]
	2351593444192 -> 2351593451728
	2351593444192 [label=ConvolutionBackward0]
	2351593453744 -> 2351593444192
	2351593453744 [label=ReluBackward0]
	2351593455088 -> 2351593453744
	2351593455088 [label=CudnnBatchNormBackward0]
	2351593450576 -> 2351593455088
	2351593450576 [label=ConvolutionBackward0]
	2351593444672 -> 2351593450576
	2351593444672 [label=ReluBackward0]
	2351593449712 -> 2351593444672
	2351593449712 [label=AddBackward0]
	2351593450192 -> 2351593449712
	2351593450192 [label=CudnnBatchNormBackward0]
	2351593453120 -> 2351593450192
	2351593453120 [label=ConvolutionBackward0]
	2351593439344 -> 2351593453120
	2351593439344 [label=ReluBackward0]
	2351593451872 -> 2351593439344
	2351593451872 [label=CudnnBatchNormBackward0]
	2351593442656 -> 2351593451872
	2351593442656 [label=ConvolutionBackward0]
	2351593447456 -> 2351593442656
	2351593447456 [label=ReluBackward0]
	2351596727088 -> 2351593447456
	2351596727088 [label=AddBackward0]
	2351596729968 -> 2351596727088
	2351596729968 [label=CudnnBatchNormBackward0]
	2351596728576 -> 2351596729968
	2351596728576 [label=ConvolutionBackward0]
	2351596729536 -> 2351596728576
	2351596729536 [label=ReluBackward0]
	2351596732272 -> 2351596729536
	2351596732272 [label=CudnnBatchNormBackward0]
	2351596728240 -> 2351596732272
	2351596728240 [label=ConvolutionBackward0]
	2351596726752 -> 2351596728240
	2351596726752 [label=ReluBackward0]
	2351596727760 -> 2351596726752
	2351596727760 [label=AddBackward0]
	2351596729104 -> 2351596727760
	2351596729104 [label=CudnnBatchNormBackward0]
	2351596726944 -> 2351596729104
	2351596726944 [label=ConvolutionBackward0]
	2351596727568 -> 2351596726944
	2351596727568 [label=ReluBackward0]
	2351596727664 -> 2351596727568
	2351596727664 [label=CudnnBatchNormBackward0]
	2351596728096 -> 2351596727664
	2351596728096 [label=ConvolutionBackward0]
	2351596727952 -> 2351596728096
	2351596727952 [label=ReluBackward0]
	2351596727616 -> 2351596727952
	2351596727616 [label=AddBackward0]
	2351596730304 -> 2351596727616
	2351596730304 [label=CudnnBatchNormBackward0]
	2351596731744 -> 2351596730304
	2351596731744 [label=ConvolutionBackward0]
	2351596730064 -> 2351596731744
	2351596730064 [label=ReluBackward0]
	2351596727328 -> 2351596730064
	2351596727328 [label=CudnnBatchNormBackward0]
	2351596732176 -> 2351596727328
	2351596732176 [label=ConvolutionBackward0]
	2351596730160 -> 2351596732176
	2351596730160 [label=ReluBackward0]
	2351596728912 -> 2351596730160
	2351596728912 [label=AddBackward0]
	2351596731216 -> 2351596728912
	2351596731216 [label=CudnnBatchNormBackward0]
	2351596730736 -> 2351596731216
	2351596730736 [label=ConvolutionBackward0]
	2351596731168 -> 2351596730736
	2351596731168 [label=ReluBackward0]
	2351596730208 -> 2351596731168
	2351596730208 [label=CudnnBatchNormBackward0]
	2351596728864 -> 2351596730208
	2351596728864 [label=ConvolutionBackward0]
	2351596732224 -> 2351596728864
	2351596732224 [label=ReluBackward0]
	2351596727232 -> 2351596732224
	2351596727232 [label=AddBackward0]
	2351593380096 -> 2351596727232
	2351593380096 [label=CudnnBatchNormBackward0]
	2351593380912 -> 2351593380096
	2351593380912 [label=ConvolutionBackward0]
	2351593384032 -> 2351593380912
	2351593384032 [label=ReluBackward0]
	2351593381344 -> 2351593384032
	2351593381344 [label=CudnnBatchNormBackward0]
	2351593380960 -> 2351593381344
	2351593380960 [label=ConvolutionBackward0]
	2351593380000 -> 2351593380960
	2351593380000 [label=ReluBackward0]
	2351593383264 -> 2351593380000
	2351593383264 [label=AddBackward0]
	2351593381440 -> 2351593383264
	2351593381440 [label=CudnnBatchNormBackward0]
	2351593379088 -> 2351593381440
	2351593379088 [label=ConvolutionBackward0]
	2351593384704 -> 2351593379088
	2351593384704 [label=ReluBackward0]
	2351593382928 -> 2351593384704
	2351593382928 [label=CudnnBatchNormBackward0]
	2351593382784 -> 2351593382928
	2351593382784 [label=ConvolutionBackward0]
	2351593374048 -> 2351593382784
	2351593374048 [label=MaxPool2DWithIndicesBackward0]
	2351593383888 -> 2351593374048
	2351593383888 [label=ReluBackward0]
	2351593384512 -> 2351593383888
	2351593384512 [label=CudnnBatchNormBackward0]
	2351593383360 -> 2351593384512
	2351593383360 [label=ConvolutionBackward0]
	2351593383648 -> 2351593383360
	2351601626256 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2351601626256 -> 2351593383648
	2351593383648 [label=AccumulateGrad]
	2351593384560 -> 2351593384512
	2351586999952 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2351586999952 -> 2351593384560
	2351593384560 [label=AccumulateGrad]
	2351593380288 -> 2351593384512
	2351586998224 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2351586998224 -> 2351593380288
	2351593380288 [label=AccumulateGrad]
	2351593383840 -> 2351593382784
	2351587000240 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2351587000240 -> 2351593383840
	2351593383840 [label=AccumulateGrad]
	2351593378608 -> 2351593382928
	2351555955600 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2351555955600 -> 2351593378608
	2351593378608 [label=AccumulateGrad]
	2351593379424 -> 2351593382928
	2351555955504 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2351555955504 -> 2351593379424
	2351593379424 [label=AccumulateGrad]
	2351593382016 -> 2351593379088
	2351555954640 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2351555954640 -> 2351593382016
	2351593382016 [label=AccumulateGrad]
	2351593378560 -> 2351593381440
	2351555954832 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2351555954832 -> 2351593378560
	2351593378560 [label=AccumulateGrad]
	2351593380192 -> 2351593381440
	2351555954928 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2351555954928 -> 2351593380192
	2351593380192 [label=AccumulateGrad]
	2351593374048 -> 2351593383264
	2351593379616 -> 2351593380960
	2351555955408 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2351555955408 -> 2351593379616
	2351593379616 [label=AccumulateGrad]
	2351593384128 -> 2351593381344
	2351555955312 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2351555955312 -> 2351593384128
	2351593384128 [label=AccumulateGrad]
	2351593384320 -> 2351593381344
	2351555954352 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2351555954352 -> 2351593384320
	2351593384320 [label=AccumulateGrad]
	2351593381152 -> 2351593380912
	2351555953296 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2351555953296 -> 2351593381152
	2351593381152 [label=AccumulateGrad]
	2351593381872 -> 2351593380096
	2351555953392 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2351555953392 -> 2351593381872
	2351593381872 [label=AccumulateGrad]
	2351593381392 -> 2351593380096
	2351555953584 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2351555953584 -> 2351593381392
	2351593381392 [label=AccumulateGrad]
	2351593380000 -> 2351596727232
	2351596726848 -> 2351596728864
	2351555952912 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2351555952912 -> 2351596726848
	2351596726848 [label=AccumulateGrad]
	2351596727712 -> 2351596730208
	2351555952816 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2351555952816 -> 2351596727712
	2351596727712 [label=AccumulateGrad]
	2351596730784 -> 2351596730208
	2351601618480 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2351601618480 -> 2351596730784
	2351596730784 [label=AccumulateGrad]
	2351596731456 -> 2351596730736
	2351601618864 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2351601618864 -> 2351596731456
	2351596731456 [label=AccumulateGrad]
	2351596728048 -> 2351596731216
	2351601618960 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2351601618960 -> 2351596728048
	2351596728048 [label=AccumulateGrad]
	2351596729392 -> 2351596731216
	2351601619056 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2351601619056 -> 2351596729392
	2351596729392 [label=AccumulateGrad]
	2351596731552 -> 2351596728912
	2351596731552 [label=CudnnBatchNormBackward0]
	2351596727280 -> 2351596731552
	2351596727280 [label=ConvolutionBackward0]
	2351596732224 -> 2351596727280
	2351596731504 -> 2351596727280
	2351555953968 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2351555953968 -> 2351596731504
	2351596731504 [label=AccumulateGrad]
	2351596728768 -> 2351596731552
	2351555954160 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2351555954160 -> 2351596728768
	2351596728768 [label=AccumulateGrad]
	2351596728528 -> 2351596731552
	2351555954064 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2351555954064 -> 2351596728528
	2351596728528 [label=AccumulateGrad]
	2351596729296 -> 2351596732176
	2351601619440 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2351601619440 -> 2351596729296
	2351596729296 [label=AccumulateGrad]
	2351596730496 -> 2351596727328
	2351601619536 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2351601619536 -> 2351596730496
	2351596730496 [label=AccumulateGrad]
	2351596727424 -> 2351596727328
	2351601619632 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2351601619632 -> 2351596727424
	2351596727424 [label=AccumulateGrad]
	2351596728384 -> 2351596731744
	2351601620016 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2351601620016 -> 2351596728384
	2351596728384 [label=AccumulateGrad]
	2351596731312 -> 2351596730304
	2351601620112 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2351601620112 -> 2351596731312
	2351596731312 [label=AccumulateGrad]
	2351596729056 -> 2351596730304
	2351601620208 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2351601620208 -> 2351596729056
	2351596729056 [label=AccumulateGrad]
	2351596730160 -> 2351596727616
	2351596729440 -> 2351596728096
	2351601621168 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2351601621168 -> 2351596729440
	2351596729440 [label=AccumulateGrad]
	2351596728960 -> 2351596727664
	2351601621264 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2351601621264 -> 2351596728960
	2351596728960 [label=AccumulateGrad]
	2351596730544 -> 2351596727664
	2351601621360 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2351601621360 -> 2351596730544
	2351596730544 [label=AccumulateGrad]
	2351596728816 -> 2351596726944
	2351601621744 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2351601621744 -> 2351596728816
	2351596728816 [label=AccumulateGrad]
	2351596731792 -> 2351596729104
	2351601621840 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2351601621840 -> 2351596731792
	2351596731792 [label=AccumulateGrad]
	2351596729488 -> 2351596729104
	2351601621936 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2351601621936 -> 2351596729488
	2351596729488 [label=AccumulateGrad]
	2351596730016 -> 2351596727760
	2351596730016 [label=CudnnBatchNormBackward0]
	2351596729632 -> 2351596730016
	2351596729632 [label=ConvolutionBackward0]
	2351596727952 -> 2351596729632
	2351596727472 -> 2351596729632
	2351601620592 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2351601620592 -> 2351596727472
	2351596727472 [label=AccumulateGrad]
	2351596730592 -> 2351596730016
	2351601620688 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2351601620688 -> 2351596730592
	2351596730592 [label=AccumulateGrad]
	2351596730832 -> 2351596730016
	2351601620784 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2351601620784 -> 2351596730832
	2351596730832 [label=AccumulateGrad]
	2351596731696 -> 2351596728240
	2351601622320 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2351601622320 -> 2351596731696
	2351596731696 [label=AccumulateGrad]
	2351596726992 -> 2351596732272
	2351601622416 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2351601622416 -> 2351596726992
	2351596726992 [label=AccumulateGrad]
	2351596731120 -> 2351596732272
	2351601622512 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2351601622512 -> 2351596731120
	2351596731120 [label=AccumulateGrad]
	2351596731072 -> 2351596728576
	2351601622896 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2351601622896 -> 2351596731072
	2351596731072 [label=AccumulateGrad]
	2351596731936 -> 2351596729968
	2351601622992 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2351601622992 -> 2351596731936
	2351596731936 [label=AccumulateGrad]
	2351596729872 -> 2351596729968
	2351601623088 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2351601623088 -> 2351596729872
	2351596729872 [label=AccumulateGrad]
	2351596726752 -> 2351596727088
	2351593453408 -> 2351593442656
	2351601624048 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2351601624048 -> 2351593453408
	2351593453408 [label=AccumulateGrad]
	2351593443472 -> 2351593451872
	2351601624144 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2351601624144 -> 2351593443472
	2351593443472 [label=AccumulateGrad]
	2351593453168 -> 2351593451872
	2351601624240 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2351601624240 -> 2351593453168
	2351593453168 [label=AccumulateGrad]
	2351593454416 -> 2351593453120
	2351601624624 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2351601624624 -> 2351593454416
	2351593454416 [label=AccumulateGrad]
	2351593449952 -> 2351593450192
	2351601624720 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2351601624720 -> 2351593449952
	2351593449952 [label=AccumulateGrad]
	2351593447072 -> 2351593450192
	2351601624816 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2351601624816 -> 2351593447072
	2351593447072 [label=AccumulateGrad]
	2351593444000 -> 2351593449712
	2351593444000 [label=CudnnBatchNormBackward0]
	2351593454512 -> 2351593444000
	2351593454512 [label=ConvolutionBackward0]
	2351593447456 -> 2351593454512
	2351596726896 -> 2351593454512
	2351601623472 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2351601623472 -> 2351596726896
	2351596726896 [label=AccumulateGrad]
	2351593447408 -> 2351593444000
	2351601623568 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2351601623568 -> 2351593447408
	2351593447408 [label=AccumulateGrad]
	2351593452112 -> 2351593444000
	2351601623664 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2351601623664 -> 2351593452112
	2351593452112 [label=AccumulateGrad]
	2351593449904 -> 2351593450576
	2351601625200 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2351601625200 -> 2351593449904
	2351593449904 [label=AccumulateGrad]
	2351593445536 -> 2351593455088
	2351601625296 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2351601625296 -> 2351593445536
	2351593445536 [label=AccumulateGrad]
	2351593451632 -> 2351593455088
	2351601625392 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2351601625392 -> 2351593451632
	2351593451632 [label=AccumulateGrad]
	2351593446784 -> 2351593444192
	2351601625776 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2351601625776 -> 2351593446784
	2351593446784 [label=AccumulateGrad]
	2351593444576 -> 2351593451728
	2351601625872 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2351601625872 -> 2351593444576
	2351593444576 [label=AccumulateGrad]
	2351593441600 -> 2351593451728
	2351601625968 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2351601625968 -> 2351593441600
	2351593441600 [label=AccumulateGrad]
	2351593444672 -> 2351593454128
	2351593448752 -> 2351593446688
	2351593448752 [label=TBackward0]
	2351593443376 -> 2351593448752
	2351601626544 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2351601626544 -> 2351593443376
	2351593443376 [label=AccumulateGrad]
	2351593446688 -> 2351601190864
}
