digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2273394915088 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2273419966112 [label=AddmmBackward0]
	2273419965008 -> 2273419966112
	2273387657648 [label="fc.bias
 (19)" fillcolor=lightblue]
	2273387657648 -> 2273419965008
	2273419965008 [label=AccumulateGrad]
	2273419965152 -> 2273419966112
	2273419965152 [label=ViewBackward0]
	2273419964672 -> 2273419965152
	2273419964672 [label=MeanBackward1]
	2273419963568 -> 2273419964672
	2273419963568 [label=ReluBackward0]
	2273419963088 -> 2273419963568
	2273419963088 [label=AddBackward0]
	2273419962608 -> 2273419963088
	2273419962608 [label=CudnnBatchNormBackward0]
	2273419962272 -> 2273419962608
	2273419962272 [label=ConvolutionBackward0]
	2273419961312 -> 2273419962272
	2273419961312 [label=ReluBackward0]
	2273419960208 -> 2273419961312
	2273419960208 [label=CudnnBatchNormBackward0]
	2273419957424 -> 2273419960208
	2273419957424 [label=ConvolutionBackward0]
	2273419952288 -> 2273419957424
	2273419952288 [label=ReluBackward0]
	2273419968416 -> 2273419952288
	2273419968416 [label=CudnnBatchNormBackward0]
	2273419968080 -> 2273419968416
	2273419968080 [label=ConvolutionBackward0]
	2273419963232 -> 2273419968080
	2273419963232 [label=ReluBackward0]
	2273419967936 -> 2273419963232
	2273419967936 [label=AddBackward0]
	2273419967600 -> 2273419967936
	2273419967600 [label=CudnnBatchNormBackward0]
	2273419967648 -> 2273419967600
	2273419967648 [label=ConvolutionBackward0]
	2273419967312 -> 2273419967648
	2273419967312 [label=ReluBackward0]
	2273419967360 -> 2273419967312
	2273419967360 [label=CudnnBatchNormBackward0]
	2273419967264 -> 2273419967360
	2273419967264 [label=ConvolutionBackward0]
	2273419966640 -> 2273419967264
	2273419966640 [label=ReluBackward0]
	2273419966688 -> 2273419966640
	2273419966688 [label=CudnnBatchNormBackward0]
	2273419966544 -> 2273419966688
	2273419966544 [label=ConvolutionBackward0]
	2273419967792 -> 2273419966544
	2273419967792 [label=ReluBackward0]
	2273419966208 -> 2273419967792
	2273419966208 [label=AddBackward0]
	2273419966064 -> 2273419966208
	2273419966064 [label=CudnnBatchNormBackward0]
	2273419965680 -> 2273419966064
	2273419965680 [label=ConvolutionBackward0]
	2273419965824 -> 2273419965680
	2273419965824 [label=ReluBackward0]
	2273419965392 -> 2273419965824
	2273419965392 [label=CudnnBatchNormBackward0]
	2273419965296 -> 2273419965392
	2273419965296 [label=ConvolutionBackward0]
	2273419965104 -> 2273419965296
	2273419965104 [label=ReluBackward0]
	2273419964720 -> 2273419965104
	2273419964720 [label=CudnnBatchNormBackward0]
	2273419964960 -> 2273419964720
	2273419964960 [label=ConvolutionBackward0]
	2273419964576 -> 2273419964960
	2273419964576 [label=ReluBackward0]
	2273419964336 -> 2273419964576
	2273419964336 [label=AddBackward0]
	2273419964288 -> 2273419964336
	2273419964288 [label=CudnnBatchNormBackward0]
	2273419964096 -> 2273419964288
	2273419964096 [label=ConvolutionBackward0]
	2273419964000 -> 2273419964096
	2273419964000 [label=ReluBackward0]
	2273419963664 -> 2273419964000
	2273419963664 [label=CudnnBatchNormBackward0]
	2273419963472 -> 2273419963664
	2273419963472 [label=ConvolutionBackward0]
	2273419963328 -> 2273419963472
	2273419963328 [label=ReluBackward0]
	2273419963136 -> 2273419963328
	2273419963136 [label=CudnnBatchNormBackward0]
	2273419962800 -> 2273419963136
	2273419962800 [label=ConvolutionBackward0]
	2273419964480 -> 2273419962800
	2273419964480 [label=ReluBackward0]
	2273419962656 -> 2273419964480
	2273419962656 [label=AddBackward0]
	2273419962320 -> 2273419962656
	2273419962320 [label=CudnnBatchNormBackward0]
	2273419962368 -> 2273419962320
	2273419962368 [label=ConvolutionBackward0]
	2273419962032 -> 2273419962368
	2273419962032 [label=ReluBackward0]
	2273419962080 -> 2273419962032
	2273419962080 [label=CudnnBatchNormBackward0]
	2273419961984 -> 2273419962080
	2273419961984 [label=ConvolutionBackward0]
	2273419961360 -> 2273419961984
	2273419961360 [label=ReluBackward0]
	2273419961408 -> 2273419961360
	2273419961408 [label=CudnnBatchNormBackward0]
	2273419961264 -> 2273419961408
	2273419961264 [label=ConvolutionBackward0]
	2273419962512 -> 2273419961264
	2273419962512 [label=ReluBackward0]
	2273419960928 -> 2273419962512
	2273419960928 [label=AddBackward0]
	2273419960784 -> 2273419960928
	2273419960784 [label=CudnnBatchNormBackward0]
	2273419960400 -> 2273419960784
	2273419960400 [label=ConvolutionBackward0]
	2273419960544 -> 2273419960400
	2273419960544 [label=ReluBackward0]
	2273419960112 -> 2273419960544
	2273419960112 [label=CudnnBatchNormBackward0]
	2273419960016 -> 2273419960112
	2273419960016 [label=ConvolutionBackward0]
	2273419954544 -> 2273419960016
	2273419954544 [label=ReluBackward0]
	2273419959392 -> 2273419954544
	2273419959392 [label=CudnnBatchNormBackward0]
	2273419953584 -> 2273419959392
	2273419953584 [label=ConvolutionBackward0]
	2273419961024 -> 2273419953584
	2273419961024 [label=ReluBackward0]
	2273419956032 -> 2273419961024
	2273419956032 [label=AddBackward0]
	2273419954640 -> 2273419956032
	2273419954640 [label=CudnnBatchNormBackward0]
	2273419955264 -> 2273419954640
	2273419955264 [label=ConvolutionBackward0]
	2273419956368 -> 2273419955264
	2273419956368 [label=ReluBackward0]
	2273419958960 -> 2273419956368
	2273419958960 [label=CudnnBatchNormBackward0]
	2273419953920 -> 2273419958960
	2273419953920 [label=ConvolutionBackward0]
	2273419957904 -> 2273419953920
	2273419957904 [label=ReluBackward0]
	2273419959200 -> 2273419957904
	2273419959200 [label=CudnnBatchNormBackward0]
	2273419958000 -> 2273419959200
	2273419958000 [label=ConvolutionBackward0]
	2273419958912 -> 2273419958000
	2273419958912 [label=ReluBackward0]
	2273419958096 -> 2273419958912
	2273419958096 [label=AddBackward0]
	2273419958528 -> 2273419958096
	2273419958528 [label=CudnnBatchNormBackward0]
	2273419957136 -> 2273419958528
	2273419957136 [label=ConvolutionBackward0]
	2273419957232 -> 2273419957136
	2273419957232 [label=ReluBackward0]
	2273419959728 -> 2273419957232
	2273419959728 [label=CudnnBatchNormBackward0]
	2273419959680 -> 2273419959728
	2273419959680 [label=ConvolutionBackward0]
	2273419954352 -> 2273419959680
	2273419954352 [label=ReluBackward0]
	2273419953248 -> 2273419954352
	2273419953248 [label=CudnnBatchNormBackward0]
	2273419952816 -> 2273419953248
	2273419952816 [label=ConvolutionBackward0]
	2273419953488 -> 2273419952816
	2273419953488 [label=ReluBackward0]
	2273419958384 -> 2273419953488
	2273419958384 [label=AddBackward0]
	2273419956944 -> 2273419958384
	2273419956944 [label=CudnnBatchNormBackward0]
	2273419959920 -> 2273419956944
	2273419959920 [label=ConvolutionBackward0]
	2273419952672 -> 2273419959920
	2273419952672 [label=ReluBackward0]
	2273419959104 -> 2273419952672
	2273419959104 [label=CudnnBatchNormBackward0]
	2273419955552 -> 2273419959104
	2273419955552 [label=ConvolutionBackward0]
	2273419955168 -> 2273419955552
	2273419955168 [label=ReluBackward0]
	2273419959776 -> 2273419955168
	2273419959776 [label=CudnnBatchNormBackward0]
	2273419957040 -> 2273419959776
	2273419957040 [label=ConvolutionBackward0]
	2273419953440 -> 2273419957040
	2273419953440 [label=ReluBackward0]
	2273420114576 -> 2273419953440
	2273420114576 [label=AddBackward0]
	2273420110976 -> 2273420114576
	2273420110976 [label=CudnnBatchNormBackward0]
	2273420113232 -> 2273420110976
	2273420113232 [label=ConvolutionBackward0]
	2273420102000 -> 2273420113232
	2273420102000 [label=ReluBackward0]
	2273420100512 -> 2273420102000
	2273420100512 [label=CudnnBatchNormBackward0]
	2273420112992 -> 2273420100512
	2273420112992 [label=ConvolutionBackward0]
	2273420105792 -> 2273420112992
	2273420105792 [label=ReluBackward0]
	2273088796128 -> 2273420105792
	2273088796128 [label=CudnnBatchNormBackward0]
	2273420115344 -> 2273088796128
	2273420115344 [label=ConvolutionBackward0]
	2273420115440 -> 2273420115344
	2273420115440 [label=ReluBackward0]
	2273395234976 -> 2273420115440
	2273395234976 [label=AddBackward0]
	2273395234496 -> 2273395234976
	2273395234496 [label=CudnnBatchNormBackward0]
	2273395233392 -> 2273395234496
	2273395233392 [label=ConvolutionBackward0]
	2273395232432 -> 2273395233392
	2273395232432 [label=ReluBackward0]
	2273395232096 -> 2273395232432
	2273395232096 [label=CudnnBatchNormBackward0]
	2273395231616 -> 2273395232096
	2273395231616 [label=ConvolutionBackward0]
	2273395230656 -> 2273395231616
	2273395230656 [label=ReluBackward0]
	2273395229552 -> 2273395230656
	2273395229552 [label=CudnnBatchNormBackward0]
	2273395229072 -> 2273395229552
	2273395229072 [label=ConvolutionBackward0]
	2273395234352 -> 2273395229072
	2273395234352 [label=ReluBackward0]
	2273395235984 -> 2273395234352
	2273395235984 [label=AddBackward0]
	2273395236224 -> 2273395235984
	2273395236224 [label=CudnnBatchNormBackward0]
	2273395235888 -> 2273395236224
	2273395235888 [label=ConvolutionBackward0]
	2273395235600 -> 2273395235888
	2273395235600 [label=ReluBackward0]
	2273395235648 -> 2273395235600
	2273395235648 [label=CudnnBatchNormBackward0]
	2273395235360 -> 2273395235648
	2273395235360 [label=ConvolutionBackward0]
	2273395235264 -> 2273395235360
	2273395235264 [label=ReluBackward0]
	2273395234928 -> 2273395235264
	2273395234928 [label=CudnnBatchNormBackward0]
	2273395234736 -> 2273395234928
	2273395234736 [label=ConvolutionBackward0]
	2273395236080 -> 2273395234736
	2273395236080 [label=ReluBackward0]
	2273395234448 -> 2273395236080
	2273395234448 [label=AddBackward0]
	2273395234256 -> 2273395234448
	2273395234256 [label=CudnnBatchNormBackward0]
	2273395234304 -> 2273395234256
	2273395234304 [label=ConvolutionBackward0]
	2273395233920 -> 2273395234304
	2273395233920 [label=ReluBackward0]
	2273395233680 -> 2273395233920
	2273395233680 [label=CudnnBatchNormBackward0]
	2273395233632 -> 2273395233680
	2273395233632 [label=ConvolutionBackward0]
	2273395233296 -> 2273395233632
	2273395233296 [label=ReluBackward0]
	2273395233344 -> 2273395233296
	2273395233344 [label=CudnnBatchNormBackward0]
	2273395233248 -> 2273395233344
	2273395233248 [label=ConvolutionBackward0]
	2273395232624 -> 2273395233248
	2273395232624 [label=ReluBackward0]
	2273395232672 -> 2273395232624
	2273395232672 [label=AddBackward0]
	2273395232528 -> 2273395232672
	2273395232528 [label=CudnnBatchNormBackward0]
	2273395232144 -> 2273395232528
	2273395232144 [label=ConvolutionBackward0]
	2273395232288 -> 2273395232144
	2273395232288 [label=ReluBackward0]
	2273395231856 -> 2273395232288
	2273395231856 [label=CudnnBatchNormBackward0]
	2273395231760 -> 2273395231856
	2273395231760 [label=ConvolutionBackward0]
	2273395231568 -> 2273395231760
	2273395231568 [label=ReluBackward0]
	2273395231184 -> 2273395231568
	2273395231184 [label=CudnnBatchNormBackward0]
	2273395231424 -> 2273395231184
	2273395231424 [label=ConvolutionBackward0]
	2273395232768 -> 2273395231424
	2273395232768 [label=ReluBackward0]
	2273395230704 -> 2273395232768
	2273395230704 [label=AddBackward0]
	2273395230944 -> 2273395230704
	2273395230944 [label=CudnnBatchNormBackward0]
	2273395230608 -> 2273395230944
	2273395230608 [label=ConvolutionBackward0]
	2273395230320 -> 2273395230608
	2273395230320 [label=ReluBackward0]
	2273395230368 -> 2273395230320
	2273395230368 [label=CudnnBatchNormBackward0]
	2273395230080 -> 2273395230368
	2273395230080 [label=ConvolutionBackward0]
	2273395229984 -> 2273395230080
	2273395229984 [label=ReluBackward0]
	2273395229648 -> 2273395229984
	2273395229648 [label=CudnnBatchNormBackward0]
	2273395229456 -> 2273395229648
	2273395229456 [label=ConvolutionBackward0]
	2273395230800 -> 2273395229456
	2273395230800 [label=ReluBackward0]
	2273395229168 -> 2273395230800
	2273395229168 [label=AddBackward0]
	2273395228976 -> 2273395229168
	2273395228976 [label=CudnnBatchNormBackward0]
	2273395229024 -> 2273395228976
	2273395229024 [label=ConvolutionBackward0]
	2273395236848 -> 2273395229024
	2273395236848 [label=ReluBackward0]
	2273395237184 -> 2273395236848
	2273395237184 [label=CudnnBatchNormBackward0]
	2273395237280 -> 2273395237184
	2273395237280 [label=ConvolutionBackward0]
	2273395237472 -> 2273395237280
	2273395237472 [label=ReluBackward0]
	2273395237616 -> 2273395237472
	2273395237616 [label=CudnnBatchNormBackward0]
	2273395237712 -> 2273395237616
	2273395237712 [label=ConvolutionBackward0]
	2273395237904 -> 2273395237712
	2273395237904 [label=MaxPool2DWithIndicesBackward0]
	2273395238048 -> 2273395237904
	2273395238048 [label=ReluBackward0]
	2273395238144 -> 2273395238048
	2273395238144 [label=CudnnBatchNormBackward0]
	2273395238240 -> 2273395238144
	2273395238240 [label=ConvolutionBackward0]
	2273395238432 -> 2273395238240
	2273387657456 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2273387657456 -> 2273395238432
	2273395238432 [label=AccumulateGrad]
	2273395238192 -> 2273395238144
	2273324747760 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2273324747760 -> 2273395238192
	2273395238192 [label=AccumulateGrad]
	2273395237952 -> 2273395238144
	2273324747472 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2273324747472 -> 2273395237952
	2273395237952 [label=AccumulateGrad]
	2273395237856 -> 2273395237712
	2273324747376 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2273324747376 -> 2273395237856
	2273395237856 [label=AccumulateGrad]
	2273395237664 -> 2273395237616
	2273324747280 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2273324747280 -> 2273395237664
	2273395237664 [label=AccumulateGrad]
	2273395237520 -> 2273395237616
	2273324746512 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2273324746512 -> 2273395237520
	2273395237520 [label=AccumulateGrad]
	2273395237424 -> 2273395237280
	2273324746608 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2273324746608 -> 2273395237424
	2273395237424 [label=AccumulateGrad]
	2273395237232 -> 2273395237184
	2273324746704 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2273324746704 -> 2273395237232
	2273395237232 [label=AccumulateGrad]
	2273395237088 -> 2273395237184
	2273324746800 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2273324746800 -> 2273395237088
	2273395237088 [label=AccumulateGrad]
	2273395236992 -> 2273395229024
	2273324747088 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2273324747088 -> 2273395236992
	2273395236992 [label=AccumulateGrad]
	2273395228880 -> 2273395228976
	2273324746128 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2273324746128 -> 2273395228880
	2273395228880 [label=AccumulateGrad]
	2273395228784 -> 2273395228976
	2273324746032 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2273324746032 -> 2273395228784
	2273395228784 [label=AccumulateGrad]
	2273395229120 -> 2273395229168
	2273395229120 [label=CudnnBatchNormBackward0]
	2273395237376 -> 2273395229120
	2273395237376 [label=ConvolutionBackward0]
	2273395237904 -> 2273395237376
	2273395237760 -> 2273395237376
	2273324747952 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2273324747952 -> 2273395237760
	2273395237760 [label=AccumulateGrad]
	2273395228928 -> 2273395229120
	2273324748048 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2273324748048 -> 2273395228928
	2273395228928 [label=AccumulateGrad]
	2273395228832 -> 2273395229120
	2273324748144 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2273324748144 -> 2273395228832
	2273395228832 [label=AccumulateGrad]
	2273395229312 -> 2273395229456
	2273324745168 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2273324745168 -> 2273395229312
	2273395229312 [label=AccumulateGrad]
	2273395229600 -> 2273395229648
	2273324745072 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2273324745072 -> 2273395229600
	2273395229600 [label=AccumulateGrad]
	2273395229792 -> 2273395229648
	2273324744880 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2273324744880 -> 2273395229792
	2273395229792 [label=AccumulateGrad]
	2273395229840 -> 2273395230080
	2273324745936 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2273324745936 -> 2273395229840
	2273395229840 [label=AccumulateGrad]
	2273395230128 -> 2273395230368
	2273324745840 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2273324745840 -> 2273395230128
	2273395230128 [label=AccumulateGrad]
	2273395230464 -> 2273395230368
	2273387974384 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2273387974384 -> 2273395230464
	2273395230464 [label=AccumulateGrad]
	2273395230224 -> 2273395230608
	2273387974768 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2273387974768 -> 2273395230224
	2273395230224 [label=AccumulateGrad]
	2273395230848 -> 2273395230944
	2273387974864 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2273387974864 -> 2273395230848
	2273395230848 [label=AccumulateGrad]
	2273395230752 -> 2273395230944
	2273387974960 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2273387974960 -> 2273395230752
	2273395230752 [label=AccumulateGrad]
	2273395230800 -> 2273395230704
	2273395231040 -> 2273395231424
	2273387975344 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2273387975344 -> 2273395231040
	2273395231040 [label=AccumulateGrad]
	2273395231280 -> 2273395231184
	2273387975440 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2273387975440 -> 2273395231280
	2273395231280 [label=AccumulateGrad]
	2273395231520 -> 2273395231184
	2273387975536 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2273387975536 -> 2273395231520
	2273395231520 [label=AccumulateGrad]
	2273395231808 -> 2273395231760
	2273387975920 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2273387975920 -> 2273395231808
	2273395231808 [label=AccumulateGrad]
	2273395231664 -> 2273395231856
	2273387976016 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2273387976016 -> 2273395231664
	2273395231664 [label=AccumulateGrad]
	2273395232048 -> 2273395231856
	2273387976112 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2273387976112 -> 2273395232048
	2273395232048 [label=AccumulateGrad]
	2273395232192 -> 2273395232144
	2273387976496 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2273387976496 -> 2273395232192
	2273395232192 [label=AccumulateGrad]
	2273395232336 -> 2273395232528
	2273387976592 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2273387976592 -> 2273395232336
	2273395232336 [label=AccumulateGrad]
	2273395232480 -> 2273395232528
	2273387976688 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2273387976688 -> 2273395232480
	2273395232480 [label=AccumulateGrad]
	2273395232768 -> 2273395232672
	2273395232816 -> 2273395233248
	2273387977648 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2273387977648 -> 2273395232816
	2273395232816 [label=AccumulateGrad]
	2273395233152 -> 2273395233344
	2273387977744 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2273387977744 -> 2273395233152
	2273395233152 [label=AccumulateGrad]
	2273395233104 -> 2273395233344
	2273387977840 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2273387977840 -> 2273395233104
	2273395233104 [label=AccumulateGrad]
	2273395233440 -> 2273395233632
	2273387978224 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2273387978224 -> 2273395233440
	2273395233440 [label=AccumulateGrad]
	2273395233824 -> 2273395233680
	2273387978320 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2273387978320 -> 2273395233824
	2273395233824 [label=AccumulateGrad]
	2273395233776 -> 2273395233680
	2273387978416 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2273387978416 -> 2273395233776
	2273395233776 [label=AccumulateGrad]
	2273395233968 -> 2273395234304
	2273387978800 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2273387978800 -> 2273395233968
	2273395233968 [label=AccumulateGrad]
	2273395234160 -> 2273395234256
	2273387978896 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2273387978896 -> 2273395234160
	2273395234160 [label=AccumulateGrad]
	2273395234064 -> 2273395234256
	2273387978992 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2273387978992 -> 2273395234064
	2273395234064 [label=AccumulateGrad]
	2273395234400 -> 2273395234448
	2273395234400 [label=CudnnBatchNormBackward0]
	2273395233488 -> 2273395234400
	2273395233488 [label=ConvolutionBackward0]
	2273395232624 -> 2273395233488
	2273395233008 -> 2273395233488
	2273387977072 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2273387977072 -> 2273395233008
	2273395233008 [label=AccumulateGrad]
	2273395234208 -> 2273395234400
	2273387977168 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2273387977168 -> 2273395234208
	2273395234208 [label=AccumulateGrad]
	2273395234112 -> 2273395234400
	2273387977264 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2273387977264 -> 2273395234112
	2273395234112 [label=AccumulateGrad]
	2273395234592 -> 2273395234736
	2273387979376 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2273387979376 -> 2273395234592
	2273395234592 [label=AccumulateGrad]
	2273395234880 -> 2273395234928
	2273387979472 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2273387979472 -> 2273395234880
	2273395234880 [label=AccumulateGrad]
	2273395235072 -> 2273395234928
	2273387979568 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2273387979568 -> 2273395235072
	2273395235072 [label=AccumulateGrad]
	2273395235120 -> 2273395235360
	2273387979952 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2273387979952 -> 2273395235120
	2273395235120 [label=AccumulateGrad]
	2273395235408 -> 2273395235648
	2273387980048 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2273387980048 -> 2273395235408
	2273395235408 [label=AccumulateGrad]
	2273395235744 -> 2273395235648
	2273387980144 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2273387980144 -> 2273395235744
	2273395235744 [label=AccumulateGrad]
	2273395235504 -> 2273395235888
	2273387980528 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2273387980528 -> 2273395235504
	2273395235504 [label=AccumulateGrad]
	2273395236128 -> 2273395236224
	2273387980624 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2273387980624 -> 2273395236128
	2273395236128 [label=AccumulateGrad]
	2273395236032 -> 2273395236224
	2273387980720 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2273387980720 -> 2273395236032
	2273395236032 [label=AccumulateGrad]
	2273395236080 -> 2273395235984
	2273395236320 -> 2273395229072
	2273387981104 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2273387981104 -> 2273395236320
	2273395236320 [label=AccumulateGrad]
	2273395229696 -> 2273395229552
	2273387981200 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2273387981200 -> 2273395229696
	2273395229696 [label=AccumulateGrad]
	2273395230032 -> 2273395229552
	2273387981296 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2273387981296 -> 2273395230032
	2273395230032 [label=AccumulateGrad]
	2273395230512 -> 2273395231616
	2273387981680 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2273387981680 -> 2273395230512
	2273395230512 [label=AccumulateGrad]
	2273395231472 -> 2273395232096
	2273387981776 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2273387981776 -> 2273395231472
	2273395231472 [label=AccumulateGrad]
	2273395232576 -> 2273395232096
	2273387981872 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2273387981872 -> 2273395232576
	2273395232576 [label=AccumulateGrad]
	2273395233056 -> 2273395233392
	2273387982256 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2273387982256 -> 2273395233056
	2273395233056 [label=AccumulateGrad]
	2273395234016 -> 2273395234496
	2273387982352 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2273387982352 -> 2273395234016
	2273395234016 [label=AccumulateGrad]
	2273395233872 -> 2273395234496
	2273387982448 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2273387982448 -> 2273395233872
	2273395233872 [label=AccumulateGrad]
	2273395234352 -> 2273395234976
	2273395235456 -> 2273420115344
	2273387982832 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2273387982832 -> 2273395235456
	2273395235456 [label=AccumulateGrad]
	2273395236272 -> 2273088796128
	2273387982928 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2273387982928 -> 2273395236272
	2273395236272 [label=AccumulateGrad]
	2273395236416 -> 2273088796128
	2273387983024 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2273387983024 -> 2273395236416
	2273395236416 [label=AccumulateGrad]
	2273420114816 -> 2273420112992
	2273387983408 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2273387983408 -> 2273420114816
	2273420114816 [label=AccumulateGrad]
	2273420115728 -> 2273420100512
	2273387983504 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2273387983504 -> 2273420115728
	2273420115728 [label=AccumulateGrad]
	2273420114288 -> 2273420100512
	2273387983600 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2273387983600 -> 2273420114288
	2273420114288 [label=AccumulateGrad]
	2273420115536 -> 2273420113232
	2273387983984 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2273387983984 -> 2273420115536
	2273420115536 [label=AccumulateGrad]
	2273420113664 -> 2273420110976
	2273387984080 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2273387984080 -> 2273420113664
	2273420113664 [label=AccumulateGrad]
	2273420101904 -> 2273420110976
	2273387984176 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2273387984176 -> 2273420101904
	2273420101904 [label=AccumulateGrad]
	2273420115440 -> 2273420114576
	2273419952960 -> 2273419957040
	2273387985136 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2273387985136 -> 2273419952960
	2273419952960 [label=AccumulateGrad]
	2273419955744 -> 2273419959776
	2273387985232 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2273387985232 -> 2273419955744
	2273419955744 [label=AccumulateGrad]
	2273419953632 -> 2273419959776
	2273387985328 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2273387985328 -> 2273419953632
	2273419953632 [label=AccumulateGrad]
	2273419954064 -> 2273419955552
	2273387985712 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2273387985712 -> 2273419954064
	2273419954064 [label=AccumulateGrad]
	2273419956080 -> 2273419959104
	2273387985808 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2273387985808 -> 2273419956080
	2273419956080 [label=AccumulateGrad]
	2273419954592 -> 2273419959104
	2273387985904 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2273387985904 -> 2273419954592
	2273419954592 [label=AccumulateGrad]
	2273419954928 -> 2273419959920
	2273387986288 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2273387986288 -> 2273419954928
	2273419954928 [label=AccumulateGrad]
	2273419953008 -> 2273419956944
	2273387986384 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2273387986384 -> 2273419953008
	2273419953008 [label=AccumulateGrad]
	2273419953872 -> 2273419956944
	2273387986480 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2273387986480 -> 2273419953872
	2273419953872 [label=AccumulateGrad]
	2273419959152 -> 2273419958384
	2273419959152 [label=CudnnBatchNormBackward0]
	2273419954880 -> 2273419959152
	2273419954880 [label=ConvolutionBackward0]
	2273419953440 -> 2273419954880
	2273419959632 -> 2273419954880
	2273387984560 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2273387984560 -> 2273419959632
	2273419959632 [label=AccumulateGrad]
	2273419956848 -> 2273419959152
	2273387984656 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2273387984656 -> 2273419956848
	2273419956848 [label=AccumulateGrad]
	2273419957088 -> 2273419959152
	2273387984752 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2273387984752 -> 2273419957088
	2273419957088 [label=AccumulateGrad]
	2273419956464 -> 2273419952816
	2273387986864 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2273387986864 -> 2273419956464
	2273419956464 [label=AccumulateGrad]
	2273419958768 -> 2273419953248
	2273387642960 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2273387642960 -> 2273419958768
	2273419958768 [label=AccumulateGrad]
	2273419959008 -> 2273419953248
	2273387643056 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2273387643056 -> 2273419959008
	2273419959008 [label=AccumulateGrad]
	2273419953968 -> 2273419959680
	2273387643440 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2273387643440 -> 2273419953968
	2273419953968 [label=AccumulateGrad]
	2273419955072 -> 2273419959728
	2273387643536 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2273387643536 -> 2273419955072
	2273419955072 [label=AccumulateGrad]
	2273419952240 -> 2273419959728
	2273387643632 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2273387643632 -> 2273419952240
	2273419952240 [label=AccumulateGrad]
	2273419956608 -> 2273419957136
	2273387644016 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2273387644016 -> 2273419956608
	2273419956608 [label=AccumulateGrad]
	2273419958816 -> 2273419958528
	2273387644112 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2273387644112 -> 2273419958816
	2273419958816 [label=AccumulateGrad]
	2273419959536 -> 2273419958528
	2273387644208 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2273387644208 -> 2273419959536
	2273419959536 [label=AccumulateGrad]
	2273419953488 -> 2273419958096
	2273419957664 -> 2273419958000
	2273387644592 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2273387644592 -> 2273419957664
	2273419957664 [label=AccumulateGrad]
	2273419953776 -> 2273419959200
	2273387644688 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2273387644688 -> 2273419953776
	2273419953776 [label=AccumulateGrad]
	2273419958864 -> 2273419959200
	2273387644784 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2273387644784 -> 2273419958864
	2273419958864 [label=AccumulateGrad]
	2273419953104 -> 2273419953920
	2273387645168 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2273387645168 -> 2273419953104
	2273419953104 [label=AccumulateGrad]
	2273419955840 -> 2273419958960
	2273387645264 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2273387645264 -> 2273419955840
	2273419955840 [label=AccumulateGrad]
	2273419955936 -> 2273419958960
	2273387645360 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2273387645360 -> 2273419955936
	2273419955936 [label=AccumulateGrad]
	2273419957568 -> 2273419955264
	2273387645744 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2273387645744 -> 2273419957568
	2273419957568 [label=AccumulateGrad]
	2273419956800 -> 2273419954640
	2273387645840 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2273387645840 -> 2273419956800
	2273419956800 [label=AccumulateGrad]
	2273419955456 -> 2273419954640
	2273387645936 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2273387645936 -> 2273419955456
	2273419955456 [label=AccumulateGrad]
	2273419958912 -> 2273419956032
	2273419954496 -> 2273419953584
	2273387646320 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2273387646320 -> 2273419954496
	2273419954496 [label=AccumulateGrad]
	2273419953200 -> 2273419959392
	2273387646416 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2273387646416 -> 2273419953200
	2273419953200 [label=AccumulateGrad]
	2273419959248 -> 2273419959392
	2273387646512 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2273387646512 -> 2273419959248
	2273419959248 [label=AccumulateGrad]
	2273419960064 -> 2273419960016
	2273387646896 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2273387646896 -> 2273419960064
	2273419960064 [label=AccumulateGrad]
	2273419958480 -> 2273419960112
	2273387646992 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2273387646992 -> 2273419958480
	2273419958480 [label=AccumulateGrad]
	2273419960304 -> 2273419960112
	2273387647088 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2273387647088 -> 2273419960304
	2273419960304 [label=AccumulateGrad]
	2273419960448 -> 2273419960400
	2273387647472 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2273387647472 -> 2273419960448
	2273419960448 [label=AccumulateGrad]
	2273419960592 -> 2273419960784
	2273387647568 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2273387647568 -> 2273419960592
	2273419960592 [label=AccumulateGrad]
	2273419960736 -> 2273419960784
	2273387647664 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2273387647664 -> 2273419960736
	2273419960736 [label=AccumulateGrad]
	2273419961024 -> 2273419960928
	2273419960976 -> 2273419961264
	2273387648048 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2273387648048 -> 2273419960976
	2273419960976 [label=AccumulateGrad]
	2273419961504 -> 2273419961408
	2273387648144 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2273387648144 -> 2273419961504
	2273419961504 [label=AccumulateGrad]
	2273419961456 -> 2273419961408
	2273387648240 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2273387648240 -> 2273419961456
	2273419961456 [label=AccumulateGrad]
	2273419961552 -> 2273419961984
	2273387648624 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2273387648624 -> 2273419961552
	2273419961552 [label=AccumulateGrad]
	2273419961888 -> 2273419962080
	2273387648720 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2273387648720 -> 2273419961888
	2273419961888 [label=AccumulateGrad]
	2273419961840 -> 2273419962080
	2273387648816 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2273387648816 -> 2273419961840
	2273419961840 [label=AccumulateGrad]
	2273419962176 -> 2273419962368
	2273387649200 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2273387649200 -> 2273419962176
	2273419962176 [label=AccumulateGrad]
	2273419962560 -> 2273419962320
	2273387649296 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2273387649296 -> 2273419962560
	2273419962560 [label=AccumulateGrad]
	2273419962416 -> 2273419962320
	2273387649392 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2273387649392 -> 2273419962416
	2273419962416 [label=AccumulateGrad]
	2273419962512 -> 2273419962656
	2273419962944 -> 2273419962800
	2273387649776 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2273387649776 -> 2273419962944
	2273419962944 [label=AccumulateGrad]
	2273419962992 -> 2273419963136
	2273387649872 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2273387649872 -> 2273419962992
	2273419962992 [label=AccumulateGrad]
	2273419963424 -> 2273419963136
	2273387649968 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2273387649968 -> 2273419963424
	2273419963424 [label=AccumulateGrad]
	2273419963520 -> 2273419963472
	2273387650352 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2273387650352 -> 2273419963520
	2273419963520 [label=AccumulateGrad]
	2273419963616 -> 2273419963664
	2273387650448 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2273387650448 -> 2273419963616
	2273419963616 [label=AccumulateGrad]
	2273419963808 -> 2273419963664
	2273387650544 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2273387650544 -> 2273419963808
	2273419963808 [label=AccumulateGrad]
	2273419963856 -> 2273419964096
	2273387650928 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2273387650928 -> 2273419963856
	2273419963856 [label=AccumulateGrad]
	2273419964144 -> 2273419964288
	2273387651024 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2273387651024 -> 2273419964144
	2273419964144 [label=AccumulateGrad]
	2273419964384 -> 2273419964288
	2273387651120 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2273387651120 -> 2273419964384
	2273419964384 [label=AccumulateGrad]
	2273419964480 -> 2273419964336
	2273419964624 -> 2273419964960
	2273387652080 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2273387652080 -> 2273419964624
	2273419964624 [label=AccumulateGrad]
	2273419964816 -> 2273419964720
	2273387652176 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2273387652176 -> 2273419964816
	2273419964816 [label=AccumulateGrad]
	2273419965056 -> 2273419964720
	2273387652272 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2273387652272 -> 2273419965056
	2273419965056 [label=AccumulateGrad]
	2273419965344 -> 2273419965296
	2273387652656 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2273387652656 -> 2273419965344
	2273419965344 [label=AccumulateGrad]
	2273419965200 -> 2273419965392
	2273387652752 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2273387652752 -> 2273419965200
	2273419965200 [label=AccumulateGrad]
	2273419965584 -> 2273419965392
	2273387652848 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2273387652848 -> 2273419965584
	2273419965584 [label=AccumulateGrad]
	2273419965728 -> 2273419965680
	2273387653232 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2273387653232 -> 2273419965728
	2273419965728 [label=AccumulateGrad]
	2273419965872 -> 2273419966064
	2273387653328 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2273387653328 -> 2273419965872
	2273419965872 [label=AccumulateGrad]
	2273419966016 -> 2273419966064
	2273387653424 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2273387653424 -> 2273419966016
	2273419966016 [label=AccumulateGrad]
	2273419966304 -> 2273419966208
	2273419966304 [label=CudnnBatchNormBackward0]
	2273419965248 -> 2273419966304
	2273419965248 [label=ConvolutionBackward0]
	2273419964576 -> 2273419965248
	2273419964768 -> 2273419965248
	2273387651504 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2273387651504 -> 2273419964768
	2273419964768 [label=AccumulateGrad]
	2273419965920 -> 2273419966304
	2273387651600 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2273387651600 -> 2273419965920
	2273419965920 [label=AccumulateGrad]
	2273419965776 -> 2273419966304
	2273387651696 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2273387651696 -> 2273419965776
	2273419965776 [label=AccumulateGrad]
	2273419966256 -> 2273419966544
	2273387653808 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2273387653808 -> 2273419966256
	2273419966256 [label=AccumulateGrad]
	2273419966784 -> 2273419966688
	2273387653904 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2273387653904 -> 2273419966784
	2273419966784 [label=AccumulateGrad]
	2273419966736 -> 2273419966688
	2273387654000 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2273387654000 -> 2273419966736
	2273419966736 [label=AccumulateGrad]
	2273419966832 -> 2273419967264
	2273387654384 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2273387654384 -> 2273419966832
	2273419966832 [label=AccumulateGrad]
	2273419967168 -> 2273419967360
	2273387654480 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2273387654480 -> 2273419967168
	2273419967168 [label=AccumulateGrad]
	2273419967120 -> 2273419967360
	2273387654576 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2273387654576 -> 2273419967120
	2273419967120 [label=AccumulateGrad]
	2273419967456 -> 2273419967648
	2273387654960 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2273387654960 -> 2273419967456
	2273419967456 [label=AccumulateGrad]
	2273419967840 -> 2273419967600
	2273387655056 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2273387655056 -> 2273419967840
	2273419967840 [label=AccumulateGrad]
	2273419967696 -> 2273419967600
	2273387655152 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2273387655152 -> 2273419967696
	2273419967696 [label=AccumulateGrad]
	2273419967792 -> 2273419967936
	2273419968224 -> 2273419968080
	2273387655536 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2273387655536 -> 2273419968224
	2273419968224 [label=AccumulateGrad]
	2273419968272 -> 2273419968416
	2273387655632 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2273387655632 -> 2273419968272
	2273419968272 [label=AccumulateGrad]
	2273419959344 -> 2273419968416
	2273387655728 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2273387655728 -> 2273419959344
	2273419959344 [label=AccumulateGrad]
	2273419957472 -> 2273419957424
	2273387656112 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2273387656112 -> 2273419957472
	2273419957472 [label=AccumulateGrad]
	2273419960352 -> 2273419960208
	2273387656208 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2273387656208 -> 2273419960352
	2273419960352 [label=AccumulateGrad]
	2273419960688 -> 2273419960208
	2273387656304 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2273387656304 -> 2273419960688
	2273419960688 [label=AccumulateGrad]
	2273419961168 -> 2273419962272
	2273387656688 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2273387656688 -> 2273419961168
	2273419961168 [label=AccumulateGrad]
	2273419962128 -> 2273419962608
	2273387656784 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2273387656784 -> 2273419962128
	2273419962128 [label=AccumulateGrad]
	2273419962752 -> 2273419962608
	2273387656880 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2273387656880 -> 2273419962752
	2273419962752 [label=AccumulateGrad]
	2273419963232 -> 2273419963088
	2273419964528 -> 2273419966112
	2273419964528 [label=TBackward0]
	2273419963712 -> 2273419964528
	2273387657552 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2273387657552 -> 2273419963712
	2273419963712 [label=AccumulateGrad]
	2273419966112 -> 2273394915088
}
