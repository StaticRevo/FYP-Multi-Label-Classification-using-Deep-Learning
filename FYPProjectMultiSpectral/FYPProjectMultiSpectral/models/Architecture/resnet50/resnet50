digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2492580894512 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2492612854352 [label=AddmmBackward0]
	2492612869520 -> 2492612854352
	2492581386896 [label="fc.bias
 (19)" fillcolor=lightblue]
	2492581386896 -> 2492612869520
	2492612869520 [label=AccumulateGrad]
	2492612867744 -> 2492612854352
	2492612867744 [label=ViewBackward0]
	2492612860448 -> 2492612867744
	2492612860448 [label=MeanBackward1]
	2492612868032 -> 2492612860448
	2492612868032 [label=ReluBackward0]
	2492612865584 -> 2492612868032
	2492612865584 [label=AddBackward0]
	2492612868320 -> 2492612865584
	2492612868320 [label=NativeBatchNormBackward0]
	2492612853824 -> 2492612868320
	2492612853824 [label=ConvolutionBackward0]
	2492612862464 -> 2492612853824
	2492612862464 [label=ReluBackward0]
	2492612865776 -> 2492612862464
	2492612865776 [label=NativeBatchNormBackward0]
	2492612869472 -> 2492612865776
	2492612869472 [label=ConvolutionBackward0]
	2492612866688 -> 2492612869472
	2492612866688 [label=ReluBackward0]
	2492612855888 -> 2492612866688
	2492612855888 [label=NativeBatchNormBackward0]
	2492612860736 -> 2492612855888
	2492612860736 [label=ConvolutionBackward0]
	2492612868080 -> 2492612860736
	2492612868080 [label=ReluBackward0]
	2492612862560 -> 2492612868080
	2492612862560 [label=AddBackward0]
	2492612862176 -> 2492612862560
	2492612862176 [label=NativeBatchNormBackward0]
	2492612854880 -> 2492612862176
	2492612854880 [label=ConvolutionBackward0]
	2492612868176 -> 2492612854880
	2492612868176 [label=ReluBackward0]
	2492612854928 -> 2492612868176
	2492612854928 [label=NativeBatchNormBackward0]
	2492612855936 -> 2492612854928
	2492612855936 [label=ConvolutionBackward0]
	2492612856080 -> 2492612855936
	2492612856080 [label=ReluBackward0]
	2492612866832 -> 2492612856080
	2492612866832 [label=NativeBatchNormBackward0]
	2492612865920 -> 2492612866832
	2492612865920 [label=ConvolutionBackward0]
	2492612859632 -> 2492612865920
	2492612859632 [label=ReluBackward0]
	2492612869376 -> 2492612859632
	2492612869376 [label=AddBackward0]
	2492612862128 -> 2492612869376
	2492612862128 [label=NativeBatchNormBackward0]
	2492612865392 -> 2492612862128
	2492612865392 [label=ConvolutionBackward0]
	2492612862848 -> 2492612865392
	2492612862848 [label=ReluBackward0]
	2492612869616 -> 2492612862848
	2492612869616 [label=NativeBatchNormBackward0]
	2492612868128 -> 2492612869616
	2492612868128 [label=ConvolutionBackward0]
	2492612855600 -> 2492612868128
	2492612855600 [label=ReluBackward0]
	2492612857664 -> 2492612855600
	2492612857664 [label=NativeBatchNormBackward0]
	2492612864912 -> 2492612857664
	2492612864912 [label=ConvolutionBackward0]
	2492612861696 -> 2492612864912
	2492612861696 [label=ReluBackward0]
	2492612866448 -> 2492612861696
	2492612866448 [label=AddBackward0]
	2492612855840 -> 2492612866448
	2492612855840 [label=NativeBatchNormBackward0]
	2492612865056 -> 2492612855840
	2492612865056 [label=ConvolutionBackward0]
	2492612859296 -> 2492612865056
	2492612859296 [label=ReluBackward0]
	2492612867360 -> 2492612859296
	2492612867360 [label=NativeBatchNormBackward0]
	2492572178832 -> 2492612867360
	2492572178832 [label=ConvolutionBackward0]
	2492578644704 -> 2492572178832
	2492578644704 [label=ReluBackward0]
	2492578644848 -> 2492578644704
	2492578644848 [label=NativeBatchNormBackward0]
	2492578644944 -> 2492578644848
	2492578644944 [label=ConvolutionBackward0]
	2492612858336 -> 2492578644944
	2492612858336 [label=ReluBackward0]
	2492578645232 -> 2492612858336
	2492578645232 [label=AddBackward0]
	2492578645328 -> 2492578645232
	2492578645328 [label=NativeBatchNormBackward0]
	2492578645472 -> 2492578645328
	2492578645472 [label=ConvolutionBackward0]
	2492578645664 -> 2492578645472
	2492578645664 [label=ReluBackward0]
	2492578645808 -> 2492578645664
	2492578645808 [label=NativeBatchNormBackward0]
	2492578645904 -> 2492578645808
	2492578645904 [label=ConvolutionBackward0]
	2492578646096 -> 2492578645904
	2492578646096 [label=ReluBackward0]
	2492578646240 -> 2492578646096
	2492578646240 [label=NativeBatchNormBackward0]
	2492578646336 -> 2492578646240
	2492578646336 [label=ConvolutionBackward0]
	2492578645280 -> 2492578646336
	2492578645280 [label=ReluBackward0]
	2492578646624 -> 2492578645280
	2492578646624 [label=AddBackward0]
	2492578646720 -> 2492578646624
	2492578646720 [label=NativeBatchNormBackward0]
	2492578646864 -> 2492578646720
	2492578646864 [label=ConvolutionBackward0]
	2492578647056 -> 2492578646864
	2492578647056 [label=ReluBackward0]
	2492578647200 -> 2492578647056
	2492578647200 [label=NativeBatchNormBackward0]
	2492578647296 -> 2492578647200
	2492578647296 [label=ConvolutionBackward0]
	2492578647488 -> 2492578647296
	2492578647488 [label=ReluBackward0]
	2492578647632 -> 2492578647488
	2492578647632 [label=NativeBatchNormBackward0]
	2492578647728 -> 2492578647632
	2492578647728 [label=ConvolutionBackward0]
	2492578646672 -> 2492578647728
	2492578646672 [label=ReluBackward0]
	2492578648016 -> 2492578646672
	2492578648016 [label=AddBackward0]
	2492578648112 -> 2492578648016
	2492578648112 [label=NativeBatchNormBackward0]
	2492578648256 -> 2492578648112
	2492578648256 [label=ConvolutionBackward0]
	2492578648448 -> 2492578648256
	2492578648448 [label=ReluBackward0]
	2492578648592 -> 2492578648448
	2492578648592 [label=NativeBatchNormBackward0]
	2492578648688 -> 2492578648592
	2492578648688 [label=ConvolutionBackward0]
	2492578648880 -> 2492578648688
	2492578648880 [label=ReluBackward0]
	2492578649024 -> 2492578648880
	2492578649024 [label=NativeBatchNormBackward0]
	2492578649120 -> 2492578649024
	2492578649120 [label=ConvolutionBackward0]
	2492578648064 -> 2492578649120
	2492578648064 [label=ReluBackward0]
	2492578649408 -> 2492578648064
	2492578649408 [label=AddBackward0]
	2492578649504 -> 2492578649408
	2492578649504 [label=NativeBatchNormBackward0]
	2492578649648 -> 2492578649504
	2492578649648 [label=ConvolutionBackward0]
	2492578649840 -> 2492578649648
	2492578649840 [label=ReluBackward0]
	2492578649984 -> 2492578649840
	2492578649984 [label=NativeBatchNormBackward0]
	2492578650080 -> 2492578649984
	2492578650080 [label=ConvolutionBackward0]
	2492578650272 -> 2492578650080
	2492578650272 [label=ReluBackward0]
	2492578650416 -> 2492578650272
	2492578650416 [label=NativeBatchNormBackward0]
	2492578650512 -> 2492578650416
	2492578650512 [label=ConvolutionBackward0]
	2492578649456 -> 2492578650512
	2492578649456 [label=ReluBackward0]
	2492578650800 -> 2492578649456
	2492578650800 [label=AddBackward0]
	2492578650896 -> 2492578650800
	2492578650896 [label=NativeBatchNormBackward0]
	2492578651040 -> 2492578650896
	2492578651040 [label=ConvolutionBackward0]
	2492578651232 -> 2492578651040
	2492578651232 [label=ReluBackward0]
	2492578651376 -> 2492578651232
	2492578651376 [label=NativeBatchNormBackward0]
	2492578651472 -> 2492578651376
	2492578651472 [label=ConvolutionBackward0]
	2492578651664 -> 2492578651472
	2492578651664 [label=ReluBackward0]
	2492578651808 -> 2492578651664
	2492578651808 [label=NativeBatchNormBackward0]
	2492578651904 -> 2492578651808
	2492578651904 [label=ConvolutionBackward0]
	2492578652096 -> 2492578651904
	2492578652096 [label=ReluBackward0]
	2492578652240 -> 2492578652096
	2492578652240 [label=AddBackward0]
	2492578652336 -> 2492578652240
	2492578652336 [label=NativeBatchNormBackward0]
	2492578652480 -> 2492578652336
	2492578652480 [label=ConvolutionBackward0]
	2492578652672 -> 2492578652480
	2492578652672 [label=ReluBackward0]
	2492578652816 -> 2492578652672
	2492578652816 [label=NativeBatchNormBackward0]
	2492578652912 -> 2492578652816
	2492578652912 [label=ConvolutionBackward0]
	2492578653104 -> 2492578652912
	2492578653104 [label=ReluBackward0]
	2492578653248 -> 2492578653104
	2492578653248 [label=NativeBatchNormBackward0]
	2492578653344 -> 2492578653248
	2492578653344 [label=ConvolutionBackward0]
	2492578652288 -> 2492578653344
	2492578652288 [label=ReluBackward0]
	2492578653632 -> 2492578652288
	2492578653632 [label=AddBackward0]
	2492578653728 -> 2492578653632
	2492578653728 [label=NativeBatchNormBackward0]
	2492578653872 -> 2492578653728
	2492578653872 [label=ConvolutionBackward0]
	2492578654064 -> 2492578653872
	2492578654064 [label=ReluBackward0]
	2492578654208 -> 2492578654064
	2492578654208 [label=NativeBatchNormBackward0]
	2492578654304 -> 2492578654208
	2492578654304 [label=ConvolutionBackward0]
	2492578654496 -> 2492578654304
	2492578654496 [label=ReluBackward0]
	2492578654640 -> 2492578654496
	2492578654640 [label=NativeBatchNormBackward0]
	2492578654736 -> 2492578654640
	2492578654736 [label=ConvolutionBackward0]
	2492578653680 -> 2492578654736
	2492578653680 [label=ReluBackward0]
	2492578655024 -> 2492578653680
	2492578655024 [label=AddBackward0]
	2492578655120 -> 2492578655024
	2492578655120 [label=NativeBatchNormBackward0]
	2492578655264 -> 2492578655120
	2492578655264 [label=ConvolutionBackward0]
	2492578655456 -> 2492578655264
	2492578655456 [label=ReluBackward0]
	2492578655600 -> 2492578655456
	2492578655600 [label=NativeBatchNormBackward0]
	2492578655696 -> 2492578655600
	2492578655696 [label=ConvolutionBackward0]
	2492578655888 -> 2492578655696
	2492578655888 [label=ReluBackward0]
	2492578656032 -> 2492578655888
	2492578656032 [label=NativeBatchNormBackward0]
	2492578656128 -> 2492578656032
	2492578656128 [label=ConvolutionBackward0]
	2492578655072 -> 2492578656128
	2492578655072 [label=ReluBackward0]
	2492578656416 -> 2492578655072
	2492578656416 [label=AddBackward0]
	2492578656512 -> 2492578656416
	2492578656512 [label=NativeBatchNormBackward0]
	2492578656656 -> 2492578656512
	2492578656656 [label=ConvolutionBackward0]
	2492578656848 -> 2492578656656
	2492578656848 [label=ReluBackward0]
	2492578656992 -> 2492578656848
	2492578656992 [label=NativeBatchNormBackward0]
	2492578657088 -> 2492578656992
	2492578657088 [label=ConvolutionBackward0]
	2492578657280 -> 2492578657088
	2492578657280 [label=ReluBackward0]
	2492578657424 -> 2492578657280
	2492578657424 [label=NativeBatchNormBackward0]
	2492578657520 -> 2492578657424
	2492578657520 [label=ConvolutionBackward0]
	2492578657712 -> 2492578657520
	2492578657712 [label=ReluBackward0]
	2492578657856 -> 2492578657712
	2492578657856 [label=AddBackward0]
	2492578657952 -> 2492578657856
	2492578657952 [label=NativeBatchNormBackward0]
	2492578658096 -> 2492578657952
	2492578658096 [label=ConvolutionBackward0]
	2492578658288 -> 2492578658096
	2492578658288 [label=ReluBackward0]
	2492578658432 -> 2492578658288
	2492578658432 [label=NativeBatchNormBackward0]
	2492578658528 -> 2492578658432
	2492578658528 [label=ConvolutionBackward0]
	2492578658720 -> 2492578658528
	2492578658720 [label=ReluBackward0]
	2492578658864 -> 2492578658720
	2492578658864 [label=NativeBatchNormBackward0]
	2492578658960 -> 2492578658864
	2492578658960 [label=ConvolutionBackward0]
	2492578657904 -> 2492578658960
	2492578657904 [label=ReluBackward0]
	2492578659248 -> 2492578657904
	2492578659248 [label=AddBackward0]
	2492578659344 -> 2492578659248
	2492578659344 [label=NativeBatchNormBackward0]
	2492578659488 -> 2492578659344
	2492578659488 [label=ConvolutionBackward0]
	2492578659680 -> 2492578659488
	2492578659680 [label=ReluBackward0]
	2492578659824 -> 2492578659680
	2492578659824 [label=NativeBatchNormBackward0]
	2492578659920 -> 2492578659824
	2492578659920 [label=ConvolutionBackward0]
	2492578660112 -> 2492578659920
	2492578660112 [label=ReluBackward0]
	2492578660256 -> 2492578660112
	2492578660256 [label=NativeBatchNormBackward0]
	2492578660304 -> 2492578660256
	2492578660304 [label=ConvolutionBackward0]
	2492578659296 -> 2492578660304
	2492578659296 [label=ReluBackward0]
	2492580987232 -> 2492578659296
	2492580987232 [label=AddBackward0]
	2492580987328 -> 2492580987232
	2492580987328 [label=NativeBatchNormBackward0]
	2492580987472 -> 2492580987328
	2492580987472 [label=ConvolutionBackward0]
	2492580987664 -> 2492580987472
	2492580987664 [label=ReluBackward0]
	2492580987808 -> 2492580987664
	2492580987808 [label=NativeBatchNormBackward0]
	2492580987904 -> 2492580987808
	2492580987904 [label=ConvolutionBackward0]
	2492580988096 -> 2492580987904
	2492580988096 [label=ReluBackward0]
	2492580988240 -> 2492580988096
	2492580988240 [label=NativeBatchNormBackward0]
	2492580988336 -> 2492580988240
	2492580988336 [label=ConvolutionBackward0]
	2492580988528 -> 2492580988336
	2492580988528 [label=MaxPool2DWithIndicesBackward0]
	2492580988672 -> 2492580988528
	2492580988672 [label=ReluBackward0]
	2492580988768 -> 2492580988672
	2492580988768 [label=NativeBatchNormBackward0]
	2492580988864 -> 2492580988768
	2492580988864 [label=ConvolutionBackward0]
	2492580989056 -> 2492580988864
	2492581386704 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2492581386704 -> 2492580989056
	2492580989056 [label=AccumulateGrad]
	2492580988816 -> 2492580988768
	2492612586640 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2492612586640 -> 2492580988816
	2492580988816 [label=AccumulateGrad]
	2492580988576 -> 2492580988768
	2492612586352 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2492612586352 -> 2492580988576
	2492580988576 [label=AccumulateGrad]
	2492580988480 -> 2492580988336
	2492612586256 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2492612586256 -> 2492580988480
	2492580988480 [label=AccumulateGrad]
	2492580988288 -> 2492580988240
	2492612586160 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2492612586160 -> 2492580988288
	2492580988288 [label=AccumulateGrad]
	2492580988144 -> 2492580988240
	2492612585392 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2492612585392 -> 2492580988144
	2492580988144 [label=AccumulateGrad]
	2492580988048 -> 2492580987904
	2492612585488 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2492612585488 -> 2492580988048
	2492580988048 [label=AccumulateGrad]
	2492580987856 -> 2492580987808
	2492612585584 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2492612585584 -> 2492580987856
	2492580987856 [label=AccumulateGrad]
	2492580987712 -> 2492580987808
	2492612585680 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2492612585680 -> 2492580987712
	2492580987712 [label=AccumulateGrad]
	2492580987616 -> 2492580987472
	2492612585968 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2492612585968 -> 2492580987616
	2492580987616 [label=AccumulateGrad]
	2492580987424 -> 2492580987328
	2492612585008 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2492612585008 -> 2492580987424
	2492580987424 [label=AccumulateGrad]
	2492580987376 -> 2492580987328
	2492612584912 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2492612584912 -> 2492580987376
	2492580987376 [label=AccumulateGrad]
	2492580987280 -> 2492580987232
	2492580987280 [label=NativeBatchNormBackward0]
	2492580988000 -> 2492580987280
	2492580988000 [label=ConvolutionBackward0]
	2492580988528 -> 2492580988000
	2492580988384 -> 2492580988000
	2492612586832 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2492612586832 -> 2492580988384
	2492580988384 [label=AccumulateGrad]
	2492580987568 -> 2492580987280
	2492612586928 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2492612586928 -> 2492580987568
	2492580987568 [label=AccumulateGrad]
	2492580987520 -> 2492580987280
	2492612587024 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2492612587024 -> 2492580987520
	2492580987520 [label=AccumulateGrad]
	2492580987136 -> 2492578660304
	2492612584240 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2492612584240 -> 2492580987136
	2492580987136 [label=AccumulateGrad]
	2492578660160 -> 2492578660256
	2492612583664 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2492612583664 -> 2492578660160
	2492578660160 [label=AccumulateGrad]
	2492580986944 -> 2492578660256
	2492612583952 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2492612583952 -> 2492580986944
	2492580986944 [label=AccumulateGrad]
	2492578660064 -> 2492578659920
	2492612584816 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2492612584816 -> 2492578660064
	2492578660064 [label=AccumulateGrad]
	2492578659872 -> 2492578659824
	2492612584720 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2492612584720 -> 2492578659872
	2492578659872 [label=AccumulateGrad]
	2492578659728 -> 2492578659824
	2492581851024 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2492581851024 -> 2492578659728
	2492578659728 [label=AccumulateGrad]
	2492578659632 -> 2492578659488
	2492581851408 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2492581851408 -> 2492578659632
	2492578659632 [label=AccumulateGrad]
	2492578659440 -> 2492578659344
	2492581851504 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2492581851504 -> 2492578659440
	2492578659440 [label=AccumulateGrad]
	2492578659392 -> 2492578659344
	2492581851600 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2492581851600 -> 2492578659392
	2492578659392 [label=AccumulateGrad]
	2492578659296 -> 2492578659248
	2492578659152 -> 2492578658960
	2492581851984 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2492581851984 -> 2492578659152
	2492578659152 [label=AccumulateGrad]
	2492578658912 -> 2492578658864
	2492581852080 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2492581852080 -> 2492578658912
	2492578658912 [label=AccumulateGrad]
	2492578658768 -> 2492578658864
	2492581852176 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2492581852176 -> 2492578658768
	2492578658768 [label=AccumulateGrad]
	2492578658672 -> 2492578658528
	2492581852560 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2492581852560 -> 2492578658672
	2492578658672 [label=AccumulateGrad]
	2492578658480 -> 2492578658432
	2492581852656 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2492581852656 -> 2492578658480
	2492578658480 [label=AccumulateGrad]
	2492578658336 -> 2492578658432
	2492581852752 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2492581852752 -> 2492578658336
	2492578658336 [label=AccumulateGrad]
	2492578658240 -> 2492578658096
	2492581853136 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2492581853136 -> 2492578658240
	2492578658240 [label=AccumulateGrad]
	2492578658048 -> 2492578657952
	2492581853232 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2492581853232 -> 2492578658048
	2492578658048 [label=AccumulateGrad]
	2492578658000 -> 2492578657952
	2492581853328 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2492581853328 -> 2492578658000
	2492578658000 [label=AccumulateGrad]
	2492578657904 -> 2492578657856
	2492578657664 -> 2492578657520
	2492581854288 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2492581854288 -> 2492578657664
	2492578657664 [label=AccumulateGrad]
	2492578657472 -> 2492578657424
	2492581854384 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2492581854384 -> 2492578657472
	2492578657472 [label=AccumulateGrad]
	2492578657328 -> 2492578657424
	2492581854480 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2492581854480 -> 2492578657328
	2492578657328 [label=AccumulateGrad]
	2492578657232 -> 2492578657088
	2492581854864 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2492581854864 -> 2492578657232
	2492578657232 [label=AccumulateGrad]
	2492578657040 -> 2492578656992
	2492581854960 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2492581854960 -> 2492578657040
	2492578657040 [label=AccumulateGrad]
	2492578656896 -> 2492578656992
	2492581855056 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2492581855056 -> 2492578656896
	2492578656896 [label=AccumulateGrad]
	2492578656800 -> 2492578656656
	2492581396752 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2492581396752 -> 2492578656800
	2492578656800 [label=AccumulateGrad]
	2492578656608 -> 2492578656512
	2492581396848 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2492581396848 -> 2492578656608
	2492578656608 [label=AccumulateGrad]
	2492578656560 -> 2492578656512
	2492581396944 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2492581396944 -> 2492578656560
	2492578656560 [label=AccumulateGrad]
	2492578656464 -> 2492578656416
	2492578656464 [label=NativeBatchNormBackward0]
	2492578657184 -> 2492578656464
	2492578657184 [label=ConvolutionBackward0]
	2492578657712 -> 2492578657184
	2492578657568 -> 2492578657184
	2492581853712 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2492581853712 -> 2492578657568
	2492578657568 [label=AccumulateGrad]
	2492578656752 -> 2492578656464
	2492581853808 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2492581853808 -> 2492578656752
	2492578656752 [label=AccumulateGrad]
	2492578656704 -> 2492578656464
	2492581853904 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2492581853904 -> 2492578656704
	2492578656704 [label=AccumulateGrad]
	2492578656320 -> 2492578656128
	2492581397328 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2492581397328 -> 2492578656320
	2492578656320 [label=AccumulateGrad]
	2492578656080 -> 2492578656032
	2492581397424 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2492581397424 -> 2492578656080
	2492578656080 [label=AccumulateGrad]
	2492578655936 -> 2492578656032
	2492581397520 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2492581397520 -> 2492578655936
	2492578655936 [label=AccumulateGrad]
	2492578655840 -> 2492578655696
	2492581397904 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2492581397904 -> 2492578655840
	2492578655840 [label=AccumulateGrad]
	2492578655648 -> 2492578655600
	2492581398000 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2492581398000 -> 2492578655648
	2492578655648 [label=AccumulateGrad]
	2492578655504 -> 2492578655600
	2492581398096 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2492581398096 -> 2492578655504
	2492578655504 [label=AccumulateGrad]
	2492578655408 -> 2492578655264
	2492581398480 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2492581398480 -> 2492578655408
	2492578655408 [label=AccumulateGrad]
	2492578655216 -> 2492578655120
	2492581398576 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2492581398576 -> 2492578655216
	2492578655216 [label=AccumulateGrad]
	2492578655168 -> 2492578655120
	2492581398672 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2492581398672 -> 2492578655168
	2492578655168 [label=AccumulateGrad]
	2492578655072 -> 2492578655024
	2492578654928 -> 2492578654736
	2492581399056 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2492581399056 -> 2492578654928
	2492578654928 [label=AccumulateGrad]
	2492578654688 -> 2492578654640
	2492581399152 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2492581399152 -> 2492578654688
	2492578654688 [label=AccumulateGrad]
	2492578654544 -> 2492578654640
	2492581399248 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2492581399248 -> 2492578654544
	2492578654544 [label=AccumulateGrad]
	2492578654448 -> 2492578654304
	2492581399632 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2492581399632 -> 2492578654448
	2492578654448 [label=AccumulateGrad]
	2492578654256 -> 2492578654208
	2492581399728 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2492581399728 -> 2492578654256
	2492578654256 [label=AccumulateGrad]
	2492578654112 -> 2492578654208
	2492581399824 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2492581399824 -> 2492578654112
	2492578654112 [label=AccumulateGrad]
	2492578654016 -> 2492578653872
	2492581400208 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2492581400208 -> 2492578654016
	2492578654016 [label=AccumulateGrad]
	2492578653824 -> 2492578653728
	2492581400304 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2492581400304 -> 2492578653824
	2492578653824 [label=AccumulateGrad]
	2492578653776 -> 2492578653728
	2492581400400 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2492581400400 -> 2492578653776
	2492578653776 [label=AccumulateGrad]
	2492578653680 -> 2492578653632
	2492578653536 -> 2492578653344
	2492581400784 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2492581400784 -> 2492578653536
	2492578653536 [label=AccumulateGrad]
	2492578653296 -> 2492578653248
	2492581400880 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2492581400880 -> 2492578653296
	2492578653296 [label=AccumulateGrad]
	2492578653152 -> 2492578653248
	2492581400976 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2492581400976 -> 2492578653152
	2492578653152 [label=AccumulateGrad]
	2492578653056 -> 2492578652912
	2492581401360 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2492581401360 -> 2492578653056
	2492578653056 [label=AccumulateGrad]
	2492578652864 -> 2492578652816
	2492581401456 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2492581401456 -> 2492578652864
	2492578652864 [label=AccumulateGrad]
	2492578652720 -> 2492578652816
	2492581401552 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2492581401552 -> 2492578652720
	2492578652720 [label=AccumulateGrad]
	2492578652624 -> 2492578652480
	2492581401936 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2492581401936 -> 2492578652624
	2492578652624 [label=AccumulateGrad]
	2492578652432 -> 2492578652336
	2492581402032 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2492581402032 -> 2492578652432
	2492578652432 [label=AccumulateGrad]
	2492578652384 -> 2492578652336
	2492581402128 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2492581402128 -> 2492578652384
	2492578652384 [label=AccumulateGrad]
	2492578652288 -> 2492578652240
	2492578652048 -> 2492578651904
	2492581403088 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2492581403088 -> 2492578652048
	2492578652048 [label=AccumulateGrad]
	2492578651856 -> 2492578651808
	2492581403184 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2492581403184 -> 2492578651856
	2492578651856 [label=AccumulateGrad]
	2492578651712 -> 2492578651808
	2492581403280 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2492581403280 -> 2492578651712
	2492578651712 [label=AccumulateGrad]
	2492578651616 -> 2492578651472
	2492581403664 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2492581403664 -> 2492578651616
	2492578651616 [label=AccumulateGrad]
	2492578651424 -> 2492578651376
	2492581403760 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2492581403760 -> 2492578651424
	2492578651424 [label=AccumulateGrad]
	2492578651280 -> 2492578651376
	2492581403856 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2492581403856 -> 2492578651280
	2492578651280 [label=AccumulateGrad]
	2492578651184 -> 2492578651040
	2492581404240 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2492581404240 -> 2492578651184
	2492578651184 [label=AccumulateGrad]
	2492578650992 -> 2492578650896
	2492581404336 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2492581404336 -> 2492578650992
	2492578650992 [label=AccumulateGrad]
	2492578650944 -> 2492578650896
	2492581404432 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2492581404432 -> 2492578650944
	2492578650944 [label=AccumulateGrad]
	2492578650848 -> 2492578650800
	2492578650848 [label=NativeBatchNormBackward0]
	2492578651568 -> 2492578650848
	2492578651568 [label=ConvolutionBackward0]
	2492578652096 -> 2492578651568
	2492578651952 -> 2492578651568
	2492581402512 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2492581402512 -> 2492578651952
	2492578651952 [label=AccumulateGrad]
	2492578651136 -> 2492578650848
	2492581402608 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2492581402608 -> 2492578651136
	2492578651136 [label=AccumulateGrad]
	2492578651088 -> 2492578650848
	2492581402704 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2492581402704 -> 2492578651088
	2492578651088 [label=AccumulateGrad]
	2492578650704 -> 2492578650512
	2492581404816 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2492581404816 -> 2492578650704
	2492578650704 [label=AccumulateGrad]
	2492578650464 -> 2492578650416
	2492581404912 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2492581404912 -> 2492578650464
	2492578650464 [label=AccumulateGrad]
	2492578650320 -> 2492578650416
	2492581405008 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2492581405008 -> 2492578650320
	2492578650320 [label=AccumulateGrad]
	2492578650224 -> 2492578650080
	2492581405392 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2492581405392 -> 2492578650224
	2492578650224 [label=AccumulateGrad]
	2492578650032 -> 2492578649984
	2492581405488 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2492581405488 -> 2492578650032
	2492578650032 [label=AccumulateGrad]
	2492578649888 -> 2492578649984
	2492581405584 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2492581405584 -> 2492578649888
	2492578649888 [label=AccumulateGrad]
	2492578649792 -> 2492578649648
	2492581405968 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2492581405968 -> 2492578649792
	2492578649792 [label=AccumulateGrad]
	2492578649600 -> 2492578649504
	2492581406064 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2492581406064 -> 2492578649600
	2492578649600 [label=AccumulateGrad]
	2492578649552 -> 2492578649504
	2492581406160 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2492581406160 -> 2492578649552
	2492578649552 [label=AccumulateGrad]
	2492578649456 -> 2492578649408
	2492578649312 -> 2492578649120
	2492581406544 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2492581406544 -> 2492578649312
	2492578649312 [label=AccumulateGrad]
	2492578649072 -> 2492578649024
	2492581406640 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2492581406640 -> 2492578649072
	2492578649072 [label=AccumulateGrad]
	2492578648928 -> 2492578649024
	2492581406736 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2492581406736 -> 2492578648928
	2492578648928 [label=AccumulateGrad]
	2492578648832 -> 2492578648688
	2492581407120 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2492581407120 -> 2492578648832
	2492578648832 [label=AccumulateGrad]
	2492578648640 -> 2492578648592
	2492581407216 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2492581407216 -> 2492578648640
	2492578648640 [label=AccumulateGrad]
	2492578648496 -> 2492578648592
	2492581407312 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2492581407312 -> 2492578648496
	2492578648496 [label=AccumulateGrad]
	2492578648400 -> 2492578648256
	2492581407696 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2492581407696 -> 2492578648400
	2492578648400 [label=AccumulateGrad]
	2492578648208 -> 2492578648112
	2492581407792 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2492581407792 -> 2492578648208
	2492578648208 [label=AccumulateGrad]
	2492578648160 -> 2492578648112
	2492581407888 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2492581407888 -> 2492578648160
	2492578648160 [label=AccumulateGrad]
	2492578648064 -> 2492578648016
	2492578647920 -> 2492578647728
	2492581408272 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2492581408272 -> 2492578647920
	2492578647920 [label=AccumulateGrad]
	2492578647680 -> 2492578647632
	2492581408368 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2492581408368 -> 2492578647680
	2492578647680 [label=AccumulateGrad]
	2492578647536 -> 2492578647632
	2492581408464 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2492581408464 -> 2492578647536
	2492578647536 [label=AccumulateGrad]
	2492578647440 -> 2492578647296
	2492581408848 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2492581408848 -> 2492578647440
	2492578647440 [label=AccumulateGrad]
	2492578647248 -> 2492578647200
	2492581408944 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2492581408944 -> 2492578647248
	2492578647248 [label=AccumulateGrad]
	2492578647104 -> 2492578647200
	2492581409040 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2492581409040 -> 2492578647104
	2492578647104 [label=AccumulateGrad]
	2492578647008 -> 2492578646864
	2492581409424 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2492581409424 -> 2492578647008
	2492578647008 [label=AccumulateGrad]
	2492578646816 -> 2492578646720
	2492581409520 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2492581409520 -> 2492578646816
	2492578646816 [label=AccumulateGrad]
	2492578646768 -> 2492578646720
	2492581409616 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2492581409616 -> 2492578646768
	2492578646768 [label=AccumulateGrad]
	2492578646672 -> 2492578646624
	2492578646528 -> 2492578646336
	2492581410000 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2492581410000 -> 2492578646528
	2492578646528 [label=AccumulateGrad]
	2492578646288 -> 2492578646240
	2492581410096 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2492581410096 -> 2492578646288
	2492578646288 [label=AccumulateGrad]
	2492578646144 -> 2492578646240
	2492581410192 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2492581410192 -> 2492578646144
	2492578646144 [label=AccumulateGrad]
	2492578646048 -> 2492578645904
	2492581410576 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2492581410576 -> 2492578646048
	2492578646048 [label=AccumulateGrad]
	2492578645856 -> 2492578645808
	2492581410672 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2492581410672 -> 2492578645856
	2492578645856 [label=AccumulateGrad]
	2492578645712 -> 2492578645808
	2492581410768 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2492581410768 -> 2492578645712
	2492578645712 [label=AccumulateGrad]
	2492578645616 -> 2492578645472
	2492581411152 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2492581411152 -> 2492578645616
	2492578645616 [label=AccumulateGrad]
	2492578645424 -> 2492578645328
	2492581411248 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2492581411248 -> 2492578645424
	2492578645424 [label=AccumulateGrad]
	2492578645376 -> 2492578645328
	2492581411344 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2492581411344 -> 2492578645376
	2492578645376 [label=AccumulateGrad]
	2492578645280 -> 2492578645232
	2492578645136 -> 2492578644944
	2492581411728 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2492581411728 -> 2492578645136
	2492578645136 [label=AccumulateGrad]
	2492578644896 -> 2492578644848
	2492581411824 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2492581411824 -> 2492578644896
	2492578644896 [label=AccumulateGrad]
	2492578644560 -> 2492578644848
	2492581411920 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2492581411920 -> 2492578644560
	2492578644560 [label=AccumulateGrad]
	2492578644080 -> 2492572178832
	2492581412304 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2492581412304 -> 2492578644080
	2492578644080 [label=AccumulateGrad]
	2492612869904 -> 2492612867360
	2492581412400 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2492581412400 -> 2492612869904
	2492612869904 [label=AccumulateGrad]
	2492578644176 -> 2492612867360
	2492581412496 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2492581412496 -> 2492578644176
	2492578644176 [label=AccumulateGrad]
	2492612858720 -> 2492612865056
	2492581380176 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2492581380176 -> 2492612858720
	2492612858720 [label=AccumulateGrad]
	2492612861840 -> 2492612855840
	2492581380272 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2492581380272 -> 2492612861840
	2492612861840 [label=AccumulateGrad]
	2492612858912 -> 2492612855840
	2492581380368 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2492581380368 -> 2492612858912
	2492612858912 [label=AccumulateGrad]
	2492612858336 -> 2492612866448
	2492612866976 -> 2492612864912
	2492581381328 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2492581381328 -> 2492612866976
	2492612866976 [label=AccumulateGrad]
	2492612856176 -> 2492612857664
	2492581381424 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2492581381424 -> 2492612856176
	2492612856176 [label=AccumulateGrad]
	2492612864288 -> 2492612857664
	2492581381520 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2492581381520 -> 2492612864288
	2492612864288 [label=AccumulateGrad]
	2492612863136 -> 2492612868128
	2492581381904 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2492581381904 -> 2492612863136
	2492612863136 [label=AccumulateGrad]
	2492612859584 -> 2492612869616
	2492581382000 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2492581382000 -> 2492612859584
	2492612859584 [label=AccumulateGrad]
	2492612863760 -> 2492612869616
	2492581382096 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2492581382096 -> 2492612863760
	2492612863760 [label=AccumulateGrad]
	2492612864816 -> 2492612865392
	2492581382480 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2492581382480 -> 2492612864816
	2492612864816 [label=AccumulateGrad]
	2492612856896 -> 2492612862128
	2492581382576 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2492581382576 -> 2492612856896
	2492612856896 [label=AccumulateGrad]
	2492612860544 -> 2492612862128
	2492581382672 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2492581382672 -> 2492612860544
	2492612860544 [label=AccumulateGrad]
	2492612865536 -> 2492612869376
	2492612865536 [label=NativeBatchNormBackward0]
	2492612857136 -> 2492612865536
	2492612857136 [label=ConvolutionBackward0]
	2492612861696 -> 2492612857136
	2492612857808 -> 2492612857136
	2492581380752 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2492581380752 -> 2492612857808
	2492612857808 [label=AccumulateGrad]
	2492612854496 -> 2492612865536
	2492581380848 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2492581380848 -> 2492612854496
	2492612854496 [label=AccumulateGrad]
	2492612858576 -> 2492612865536
	2492581380944 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2492581380944 -> 2492612858576
	2492612858576 [label=AccumulateGrad]
	2492612861120 -> 2492612865920
	2492581383056 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2492581383056 -> 2492612861120
	2492612861120 [label=AccumulateGrad]
	2492612869424 -> 2492612866832
	2492581383152 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2492581383152 -> 2492612869424
	2492612869424 [label=AccumulateGrad]
	2492612868560 -> 2492612866832
	2492581383248 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2492581383248 -> 2492612868560
	2492612868560 [label=AccumulateGrad]
	2492612868944 -> 2492612855936
	2492581383632 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2492581383632 -> 2492612868944
	2492612868944 [label=AccumulateGrad]
	2492612855552 -> 2492612854928
	2492581383728 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2492581383728 -> 2492612855552
	2492612855552 [label=AccumulateGrad]
	2492612862944 -> 2492612854928
	2492581383824 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2492581383824 -> 2492612862944
	2492612862944 [label=AccumulateGrad]
	2492612861360 -> 2492612854880
	2492581384208 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2492581384208 -> 2492612861360
	2492612861360 [label=AccumulateGrad]
	2492612858480 -> 2492612862176
	2492581384304 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2492581384304 -> 2492612858480
	2492612858480 [label=AccumulateGrad]
	2492612863664 -> 2492612862176
	2492581384400 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2492581384400 -> 2492612863664
	2492612863664 [label=AccumulateGrad]
	2492612859632 -> 2492612862560
	2492612857616 -> 2492612860736
	2492581384784 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2492581384784 -> 2492612857616
	2492612857616 [label=AccumulateGrad]
	2492612863712 -> 2492612855888
	2492581384880 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2492581384880 -> 2492612863712
	2492612863712 [label=AccumulateGrad]
	2492612858000 -> 2492612855888
	2492581384976 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2492581384976 -> 2492612858000
	2492612858000 [label=AccumulateGrad]
	2492612869328 -> 2492612869472
	2492581385360 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2492581385360 -> 2492612869328
	2492612869328 [label=AccumulateGrad]
	2492612860400 -> 2492612865776
	2492581385456 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2492581385456 -> 2492612860400
	2492612860400 [label=AccumulateGrad]
	2492612859872 -> 2492612865776
	2492581385552 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2492581385552 -> 2492612859872
	2492612859872 [label=AccumulateGrad]
	2492612863952 -> 2492612853824
	2492581385936 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2492581385936 -> 2492612863952
	2492612863952 [label=AccumulateGrad]
	2492612867120 -> 2492612868320
	2492581386032 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2492581386032 -> 2492612867120
	2492612867120 [label=AccumulateGrad]
	2492612865344 -> 2492612868320
	2492581386128 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2492581386128 -> 2492612865344
	2492612865344 [label=AccumulateGrad]
	2492612868080 -> 2492612865584
	2492612869280 -> 2492612854352
	2492612869280 [label=TBackward0]
	2492612865296 -> 2492612869280
	2492581386800 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2492581386800 -> 2492612865296
	2492612865296 [label=AccumulateGrad]
	2492612854352 -> 2492580894512
}
