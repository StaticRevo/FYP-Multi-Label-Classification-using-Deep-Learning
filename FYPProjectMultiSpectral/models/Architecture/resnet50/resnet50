digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2310960228272 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2310917712592 [label=AddmmBackward0]
	2310917715472 -> 2310917712592
	2310957612976 [label="fc.bias
 (19)" fillcolor=lightblue]
	2310957612976 -> 2310917715472
	2310917715472 [label=AccumulateGrad]
	2310917717536 -> 2310917712592
	2310917717536 [label=ViewBackward0]
	2310917710864 -> 2310917717536
	2310917710864 [label=MeanBackward1]
	2310917716432 -> 2310917710864
	2310917716432 [label=ReluBackward0]
	2310917705056 -> 2310917716432
	2310917705056 [label=AddBackward0]
	2310917713168 -> 2310917705056
	2310917713168 [label=CudnnBatchNormBackward0]
	2310917703664 -> 2310917713168
	2310917703664 [label=ConvolutionBackward0]
	2310917709712 -> 2310917703664
	2310917709712 [label=ReluBackward0]
	2310917703040 -> 2310917709712
	2310917703040 [label=CudnnBatchNormBackward0]
	2310917714560 -> 2310917703040
	2310917714560 [label=ConvolutionBackward0]
	2310917717920 -> 2310917714560
	2310917717920 [label=ReluBackward0]
	2310917717728 -> 2310917717920
	2310917717728 [label=CudnnBatchNormBackward0]
	2310917713840 -> 2310917717728
	2310917713840 [label=ConvolutionBackward0]
	2310917704624 -> 2310917713840
	2310917704624 [label=ReluBackward0]
	2310917710816 -> 2310917704624
	2310917710816 [label=AddBackward0]
	2310917707792 -> 2310917710816
	2310917707792 [label=CudnnBatchNormBackward0]
	2310917708896 -> 2310917707792
	2310917708896 [label=ConvolutionBackward0]
	2310917711440 -> 2310917708896
	2310917711440 [label=ReluBackward0]
	2310917706208 -> 2310917711440
	2310917706208 [label=CudnnBatchNormBackward0]
	2310917702368 -> 2310917706208
	2310917702368 [label=ConvolutionBackward0]
	2310917712544 -> 2310917702368
	2310917712544 [label=ReluBackward0]
	2310917703424 -> 2310917712544
	2310917703424 [label=CudnnBatchNormBackward0]
	2310917709904 -> 2310917703424
	2310917709904 [label=ConvolutionBackward0]
	2310917704288 -> 2310917709904
	2310917704288 [label=ReluBackward0]
	2310917711488 -> 2310917704288
	2310917711488 [label=AddBackward0]
	2310917711008 -> 2310917711488
	2310917711008 [label=CudnnBatchNormBackward0]
	2310917704864 -> 2310917711008
	2310917704864 [label=ConvolutionBackward0]
	2310917714512 -> 2310917704864
	2310917714512 [label=ReluBackward0]
	2310917715616 -> 2310917714512
	2310917715616 [label=CudnnBatchNormBackward0]
	2310917702752 -> 2310917715616
	2310917702752 [label=ConvolutionBackward0]
	2310917708944 -> 2310917702752
	2310917708944 [label=ReluBackward0]
	2310917702032 -> 2310917708944
	2310917702032 [label=CudnnBatchNormBackward0]
	2310917716192 -> 2310917702032
	2310917716192 [label=ConvolutionBackward0]
	2310917717344 -> 2310917716192
	2310917717344 [label=ReluBackward0]
	2310917707744 -> 2310917717344
	2310917707744 [label=AddBackward0]
	2310917712160 -> 2310917707744
	2310917712160 [label=CudnnBatchNormBackward0]
	2310917704096 -> 2310917712160
	2310917704096 [label=ConvolutionBackward0]
	2310917710048 -> 2310917704096
	2310917710048 [label=ReluBackward0]
	2310917707120 -> 2310917710048
	2310917707120 [label=CudnnBatchNormBackward0]
	2310917717584 -> 2310917707120
	2310917717584 [label=ConvolutionBackward0]
	2310917710576 -> 2310917717584
	2310917710576 [label=ReluBackward0]
	2310917713024 -> 2310917710576
	2310917713024 [label=CudnnBatchNormBackward0]
	2310917717632 -> 2310917713024
	2310917717632 [label=ConvolutionBackward0]
	2310917702464 -> 2310917717632
	2310917702464 [label=ReluBackward0]
	2310917712016 -> 2310917702464
	2310917712016 [label=AddBackward0]
	2310917717104 -> 2310917712016
	2310917717104 [label=CudnnBatchNormBackward0]
	2310917716336 -> 2310917717104
	2310917716336 [label=ConvolutionBackward0]
	2310917717056 -> 2310917716336
	2310917717056 [label=ReluBackward0]
	2310917715184 -> 2310917717056
	2310917715184 [label=CudnnBatchNormBackward0]
	2310917706112 -> 2310917715184
	2310917706112 [label=ConvolutionBackward0]
	2310917702320 -> 2310917706112
	2310917702320 [label=ReluBackward0]
	2310917713792 -> 2310917702320
	2310917713792 [label=CudnnBatchNormBackward0]
	2310917708368 -> 2310917713792
	2310917708368 [label=ConvolutionBackward0]
	2310917713648 -> 2310917708368
	2310917713648 [label=ReluBackward0]
	2310917703568 -> 2310917713648
	2310917703568 [label=AddBackward0]
	2310917706592 -> 2310917703568
	2310917706592 [label=CudnnBatchNormBackward0]
	2310917705248 -> 2310917706592
	2310917705248 [label=ConvolutionBackward0]
	2310917706688 -> 2310917705248
	2310917706688 [label=ReluBackward0]
	2310917702560 -> 2310917706688
	2310917702560 [label=CudnnBatchNormBackward0]
	2310917713216 -> 2310917702560
	2310917713216 [label=ConvolutionBackward0]
	2310960057792 -> 2310917713216
	2310960057792 [label=ReluBackward0]
	2310960057936 -> 2310960057792
	2310960057936 [label=CudnnBatchNormBackward0]
	2310960058032 -> 2310960057936
	2310960058032 [label=ConvolutionBackward0]
	2310917704768 -> 2310960058032
	2310917704768 [label=ReluBackward0]
	2310960058320 -> 2310917704768
	2310960058320 [label=AddBackward0]
	2310960058416 -> 2310960058320
	2310960058416 [label=CudnnBatchNormBackward0]
	2310960058560 -> 2310960058416
	2310960058560 [label=ConvolutionBackward0]
	2310960058752 -> 2310960058560
	2310960058752 [label=ReluBackward0]
	2310960058896 -> 2310960058752
	2310960058896 [label=CudnnBatchNormBackward0]
	2310960058992 -> 2310960058896
	2310960058992 [label=ConvolutionBackward0]
	2310960059184 -> 2310960058992
	2310960059184 [label=ReluBackward0]
	2310960059328 -> 2310960059184
	2310960059328 [label=CudnnBatchNormBackward0]
	2310960059424 -> 2310960059328
	2310960059424 [label=ConvolutionBackward0]
	2310960058368 -> 2310960059424
	2310960058368 [label=ReluBackward0]
	2310960059712 -> 2310960058368
	2310960059712 [label=AddBackward0]
	2310960059808 -> 2310960059712
	2310960059808 [label=CudnnBatchNormBackward0]
	2310960059952 -> 2310960059808
	2310960059952 [label=ConvolutionBackward0]
	2310960060144 -> 2310960059952
	2310960060144 [label=ReluBackward0]
	2310960060288 -> 2310960060144
	2310960060288 [label=CudnnBatchNormBackward0]
	2310960060384 -> 2310960060288
	2310960060384 [label=ConvolutionBackward0]
	2310960060576 -> 2310960060384
	2310960060576 [label=ReluBackward0]
	2310960060720 -> 2310960060576
	2310960060720 [label=CudnnBatchNormBackward0]
	2310960060816 -> 2310960060720
	2310960060816 [label=ConvolutionBackward0]
	2310960059760 -> 2310960060816
	2310960059760 [label=ReluBackward0]
	2310960061104 -> 2310960059760
	2310960061104 [label=AddBackward0]
	2310960061200 -> 2310960061104
	2310960061200 [label=CudnnBatchNormBackward0]
	2310960061344 -> 2310960061200
	2310960061344 [label=ConvolutionBackward0]
	2310960061536 -> 2310960061344
	2310960061536 [label=ReluBackward0]
	2310960061680 -> 2310960061536
	2310960061680 [label=CudnnBatchNormBackward0]
	2310960061776 -> 2310960061680
	2310960061776 [label=ConvolutionBackward0]
	2310960061968 -> 2310960061776
	2310960061968 [label=ReluBackward0]
	2310960062112 -> 2310960061968
	2310960062112 [label=CudnnBatchNormBackward0]
	2310960062208 -> 2310960062112
	2310960062208 [label=ConvolutionBackward0]
	2310960062400 -> 2310960062208
	2310960062400 [label=ReluBackward0]
	2310960062544 -> 2310960062400
	2310960062544 [label=AddBackward0]
	2310960062640 -> 2310960062544
	2310960062640 [label=CudnnBatchNormBackward0]
	2310960062784 -> 2310960062640
	2310960062784 [label=ConvolutionBackward0]
	2310960062976 -> 2310960062784
	2310960062976 [label=ReluBackward0]
	2310960063120 -> 2310960062976
	2310960063120 [label=CudnnBatchNormBackward0]
	2310960063216 -> 2310960063120
	2310960063216 [label=ConvolutionBackward0]
	2310960063408 -> 2310960063216
	2310960063408 [label=ReluBackward0]
	2310960063552 -> 2310960063408
	2310960063552 [label=CudnnBatchNormBackward0]
	2310960063648 -> 2310960063552
	2310960063648 [label=ConvolutionBackward0]
	2310960062592 -> 2310960063648
	2310960062592 [label=ReluBackward0]
	2310960063936 -> 2310960062592
	2310960063936 [label=AddBackward0]
	2310960064032 -> 2310960063936
	2310960064032 [label=CudnnBatchNormBackward0]
	2310960064176 -> 2310960064032
	2310960064176 [label=ConvolutionBackward0]
	2310960064368 -> 2310960064176
	2310960064368 [label=ReluBackward0]
	2310960064512 -> 2310960064368
	2310960064512 [label=CudnnBatchNormBackward0]
	2310960064608 -> 2310960064512
	2310960064608 [label=ConvolutionBackward0]
	2310960064800 -> 2310960064608
	2310960064800 [label=ReluBackward0]
	2310960064944 -> 2310960064800
	2310960064944 [label=CudnnBatchNormBackward0]
	2310960065040 -> 2310960064944
	2310960065040 [label=ConvolutionBackward0]
	2310960063984 -> 2310960065040
	2310960063984 [label=ReluBackward0]
	2310960065328 -> 2310960063984
	2310960065328 [label=AddBackward0]
	2310960065424 -> 2310960065328
	2310960065424 [label=CudnnBatchNormBackward0]
	2310960065568 -> 2310960065424
	2310960065568 [label=ConvolutionBackward0]
	2310960065760 -> 2310960065568
	2310960065760 [label=ReluBackward0]
	2310960065904 -> 2310960065760
	2310960065904 [label=CudnnBatchNormBackward0]
	2310960066000 -> 2310960065904
	2310960066000 [label=ConvolutionBackward0]
	2310960066192 -> 2310960066000
	2310960066192 [label=ReluBackward0]
	2310960066336 -> 2310960066192
	2310960066336 [label=CudnnBatchNormBackward0]
	2310960066432 -> 2310960066336
	2310960066432 [label=ConvolutionBackward0]
	2310960065376 -> 2310960066432
	2310960065376 [label=ReluBackward0]
	2310960066720 -> 2310960065376
	2310960066720 [label=AddBackward0]
	2310960066816 -> 2310960066720
	2310960066816 [label=CudnnBatchNormBackward0]
	2310960066960 -> 2310960066816
	2310960066960 [label=ConvolutionBackward0]
	2310960067152 -> 2310960066960
	2310960067152 [label=ReluBackward0]
	2310960067296 -> 2310960067152
	2310960067296 [label=CudnnBatchNormBackward0]
	2310960067392 -> 2310960067296
	2310960067392 [label=ConvolutionBackward0]
	2310960067584 -> 2310960067392
	2310960067584 [label=ReluBackward0]
	2310960067728 -> 2310960067584
	2310960067728 [label=CudnnBatchNormBackward0]
	2310960067824 -> 2310960067728
	2310960067824 [label=ConvolutionBackward0]
	2310960068016 -> 2310960067824
	2310960068016 [label=ReluBackward0]
	2310960068160 -> 2310960068016
	2310960068160 [label=AddBackward0]
	2310960068256 -> 2310960068160
	2310960068256 [label=CudnnBatchNormBackward0]
	2310960068400 -> 2310960068256
	2310960068400 [label=ConvolutionBackward0]
	2310960068592 -> 2310960068400
	2310960068592 [label=ReluBackward0]
	2310960068736 -> 2310960068592
	2310960068736 [label=CudnnBatchNormBackward0]
	2310960068832 -> 2310960068736
	2310960068832 [label=ConvolutionBackward0]
	2310960069024 -> 2310960068832
	2310960069024 [label=ReluBackward0]
	2310960069168 -> 2310960069024
	2310960069168 [label=CudnnBatchNormBackward0]
	2310960069264 -> 2310960069168
	2310960069264 [label=ConvolutionBackward0]
	2310960068208 -> 2310960069264
	2310960068208 [label=ReluBackward0]
	2310960069552 -> 2310960068208
	2310960069552 [label=AddBackward0]
	2310960069648 -> 2310960069552
	2310960069648 [label=CudnnBatchNormBackward0]
	2310960069792 -> 2310960069648
	2310960069792 [label=ConvolutionBackward0]
	2310960069984 -> 2310960069792
	2310960069984 [label=ReluBackward0]
	2310960070128 -> 2310960069984
	2310960070128 [label=CudnnBatchNormBackward0]
	2310960070224 -> 2310960070128
	2310960070224 [label=ConvolutionBackward0]
	2310960070416 -> 2310960070224
	2310960070416 [label=ReluBackward0]
	2310960070560 -> 2310960070416
	2310960070560 [label=CudnnBatchNormBackward0]
	2310960070608 -> 2310960070560
	2310960070608 [label=ConvolutionBackward0]
	2310960069600 -> 2310960070608
	2310960069600 [label=ReluBackward0]
	2310925975904 -> 2310960069600
	2310925975904 [label=AddBackward0]
	2310925976000 -> 2310925975904
	2310925976000 [label=CudnnBatchNormBackward0]
	2310925976144 -> 2310925976000
	2310925976144 [label=ConvolutionBackward0]
	2310925976336 -> 2310925976144
	2310925976336 [label=ReluBackward0]
	2310925976480 -> 2310925976336
	2310925976480 [label=CudnnBatchNormBackward0]
	2310925976576 -> 2310925976480
	2310925976576 [label=ConvolutionBackward0]
	2310925976768 -> 2310925976576
	2310925976768 [label=ReluBackward0]
	2310925976912 -> 2310925976768
	2310925976912 [label=CudnnBatchNormBackward0]
	2310925977008 -> 2310925976912
	2310925977008 [label=ConvolutionBackward0]
	2310925977200 -> 2310925977008
	2310925977200 [label=MaxPool2DWithIndicesBackward0]
	2310925977344 -> 2310925977200
	2310925977344 [label=ReluBackward0]
	2310925977440 -> 2310925977344
	2310925977440 [label=CudnnBatchNormBackward0]
	2310925977536 -> 2310925977440
	2310925977536 [label=ConvolutionBackward0]
	2310925977728 -> 2310925977536
	2310925895536 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2310925895536 -> 2310925977728
	2310925977728 [label=AccumulateGrad]
	2310925977488 -> 2310925977440
	2310957580944 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2310957580944 -> 2310925977488
	2310925977488 [label=AccumulateGrad]
	2310925977248 -> 2310925977440
	2310957581040 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2310957581040 -> 2310925977248
	2310925977248 [label=AccumulateGrad]
	2310925977152 -> 2310925977008
	2310957582000 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2310957582000 -> 2310925977152
	2310925977152 [label=AccumulateGrad]
	2310925976960 -> 2310925976912
	2310957582096 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2310957582096 -> 2310925976960
	2310925976960 [label=AccumulateGrad]
	2310925976816 -> 2310925976912
	2310957582192 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2310957582192 -> 2310925976816
	2310925976816 [label=AccumulateGrad]
	2310925976720 -> 2310925976576
	2310957582672 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2310957582672 -> 2310925976720
	2310925976720 [label=AccumulateGrad]
	2310925976528 -> 2310925976480
	2310957582768 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2310957582768 -> 2310925976528
	2310925976528 [label=AccumulateGrad]
	2310925976384 -> 2310925976480
	2310957582864 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2310957582864 -> 2310925976384
	2310925976384 [label=AccumulateGrad]
	2310925976288 -> 2310925976144
	2310957583248 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2310957583248 -> 2310925976288
	2310925976288 [label=AccumulateGrad]
	2310925976096 -> 2310925976000
	2310957583344 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2310957583344 -> 2310925976096
	2310925976096 [label=AccumulateGrad]
	2310925976048 -> 2310925976000
	2310957583440 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2310957583440 -> 2310925976048
	2310925976048 [label=AccumulateGrad]
	2310925975952 -> 2310925975904
	2310925975952 [label=CudnnBatchNormBackward0]
	2310925976672 -> 2310925975952
	2310925976672 [label=ConvolutionBackward0]
	2310925977200 -> 2310925976672
	2310925977056 -> 2310925976672
	2310957581424 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2310957581424 -> 2310925977056
	2310925977056 [label=AccumulateGrad]
	2310925976240 -> 2310925975952
	2310957581520 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2310957581520 -> 2310925976240
	2310925976240 [label=AccumulateGrad]
	2310925976192 -> 2310925975952
	2310957581616 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2310957581616 -> 2310925976192
	2310925976192 [label=AccumulateGrad]
	2310925975808 -> 2310960070608
	2310957583824 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2310957583824 -> 2310925975808
	2310925975808 [label=AccumulateGrad]
	2310960070464 -> 2310960070560
	2310957583920 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2310957583920 -> 2310960070464
	2310960070464 [label=AccumulateGrad]
	2310925975616 -> 2310960070560
	2310957584016 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2310957584016 -> 2310925975616
	2310925975616 [label=AccumulateGrad]
	2310960070368 -> 2310960070224
	2310957584400 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2310957584400 -> 2310960070368
	2310960070368 [label=AccumulateGrad]
	2310960070176 -> 2310960070128
	2310957584496 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2310957584496 -> 2310960070176
	2310960070176 [label=AccumulateGrad]
	2310960070032 -> 2310960070128
	2310957584592 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2310957584592 -> 2310960070032
	2310960070032 [label=AccumulateGrad]
	2310960069936 -> 2310960069792
	2310957584976 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2310957584976 -> 2310960069936
	2310960069936 [label=AccumulateGrad]
	2310960069744 -> 2310960069648
	2310957585072 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2310957585072 -> 2310960069744
	2310960069744 [label=AccumulateGrad]
	2310960069696 -> 2310960069648
	2310957585168 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2310957585168 -> 2310960069696
	2310960069696 [label=AccumulateGrad]
	2310960069600 -> 2310960069552
	2310960069456 -> 2310960069264
	2310957585552 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2310957585552 -> 2310960069456
	2310960069456 [label=AccumulateGrad]
	2310960069216 -> 2310960069168
	2310957585648 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2310957585648 -> 2310960069216
	2310960069216 [label=AccumulateGrad]
	2310960069072 -> 2310960069168
	2310957585744 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2310957585744 -> 2310960069072
	2310960069072 [label=AccumulateGrad]
	2310960068976 -> 2310960068832
	2310957586128 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2310957586128 -> 2310960068976
	2310960068976 [label=AccumulateGrad]
	2310960068784 -> 2310960068736
	2310957586224 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2310957586224 -> 2310960068784
	2310960068784 [label=AccumulateGrad]
	2310960068640 -> 2310960068736
	2310957586320 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2310957586320 -> 2310960068640
	2310960068640 [label=AccumulateGrad]
	2310960068544 -> 2310960068400
	2310957586704 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2310957586704 -> 2310960068544
	2310960068544 [label=AccumulateGrad]
	2310960068352 -> 2310960068256
	2310957586800 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2310957586800 -> 2310960068352
	2310960068352 [label=AccumulateGrad]
	2310960068304 -> 2310960068256
	2310957586896 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2310957586896 -> 2310960068304
	2310960068304 [label=AccumulateGrad]
	2310960068208 -> 2310960068160
	2310960067968 -> 2310960067824
	2310957587856 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2310957587856 -> 2310960067968
	2310960067968 [label=AccumulateGrad]
	2310960067776 -> 2310960067728
	2310957587952 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2310957587952 -> 2310960067776
	2310960067776 [label=AccumulateGrad]
	2310960067632 -> 2310960067728
	2310957588048 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2310957588048 -> 2310960067632
	2310960067632 [label=AccumulateGrad]
	2310960067536 -> 2310960067392
	2310957588432 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2310957588432 -> 2310960067536
	2310960067536 [label=AccumulateGrad]
	2310960067344 -> 2310960067296
	2310957588528 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2310957588528 -> 2310960067344
	2310960067344 [label=AccumulateGrad]
	2310960067200 -> 2310960067296
	2310957588624 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2310957588624 -> 2310960067200
	2310960067200 [label=AccumulateGrad]
	2310960067104 -> 2310960066960
	2310957589008 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2310957589008 -> 2310960067104
	2310960067104 [label=AccumulateGrad]
	2310960066912 -> 2310960066816
	2310957589104 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2310957589104 -> 2310960066912
	2310960066912 [label=AccumulateGrad]
	2310960066864 -> 2310960066816
	2310957589200 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2310957589200 -> 2310960066864
	2310960066864 [label=AccumulateGrad]
	2310960066768 -> 2310960066720
	2310960066768 [label=CudnnBatchNormBackward0]
	2310960067488 -> 2310960066768
	2310960067488 [label=ConvolutionBackward0]
	2310960068016 -> 2310960067488
	2310960067872 -> 2310960067488
	2310957587280 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2310957587280 -> 2310960067872
	2310960067872 [label=AccumulateGrad]
	2310960067056 -> 2310960066768
	2310957587376 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2310957587376 -> 2310960067056
	2310960067056 [label=AccumulateGrad]
	2310960067008 -> 2310960066768
	2310957587472 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2310957587472 -> 2310960067008
	2310960067008 [label=AccumulateGrad]
	2310960066624 -> 2310960066432
	2310957589584 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2310957589584 -> 2310960066624
	2310960066624 [label=AccumulateGrad]
	2310960066384 -> 2310960066336
	2310957589680 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2310957589680 -> 2310960066384
	2310960066384 [label=AccumulateGrad]
	2310960066240 -> 2310960066336
	2310957589776 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2310957589776 -> 2310960066240
	2310960066240 [label=AccumulateGrad]
	2310960066144 -> 2310960066000
	2310957590160 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2310957590160 -> 2310960066144
	2310960066144 [label=AccumulateGrad]
	2310960065952 -> 2310960065904
	2310957590256 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2310957590256 -> 2310960065952
	2310960065952 [label=AccumulateGrad]
	2310960065808 -> 2310960065904
	2310957590352 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2310957590352 -> 2310960065808
	2310960065808 [label=AccumulateGrad]
	2310960065712 -> 2310960065568
	2310957590736 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2310957590736 -> 2310960065712
	2310960065712 [label=AccumulateGrad]
	2310960065520 -> 2310960065424
	2310957590832 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2310957590832 -> 2310960065520
	2310960065520 [label=AccumulateGrad]
	2310960065472 -> 2310960065424
	2310957590928 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2310957590928 -> 2310960065472
	2310960065472 [label=AccumulateGrad]
	2310960065376 -> 2310960065328
	2310960065232 -> 2310960065040
	2310957591312 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2310957591312 -> 2310960065232
	2310960065232 [label=AccumulateGrad]
	2310960064992 -> 2310960064944
	2310957591408 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2310957591408 -> 2310960064992
	2310960064992 [label=AccumulateGrad]
	2310960064848 -> 2310960064944
	2310957591504 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2310957591504 -> 2310960064848
	2310960064848 [label=AccumulateGrad]
	2310960064752 -> 2310960064608
	2310957591888 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2310957591888 -> 2310960064752
	2310960064752 [label=AccumulateGrad]
	2310960064560 -> 2310960064512
	2310957591984 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2310957591984 -> 2310960064560
	2310960064560 [label=AccumulateGrad]
	2310960064416 -> 2310960064512
	2310957592080 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2310957592080 -> 2310960064416
	2310960064416 [label=AccumulateGrad]
	2310960064320 -> 2310960064176
	2310957592464 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2310957592464 -> 2310960064320
	2310960064320 [label=AccumulateGrad]
	2310960064128 -> 2310960064032
	2310957592560 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2310957592560 -> 2310960064128
	2310960064128 [label=AccumulateGrad]
	2310960064080 -> 2310960064032
	2310957592656 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2310957592656 -> 2310960064080
	2310960064080 [label=AccumulateGrad]
	2310960063984 -> 2310960063936
	2310960063840 -> 2310960063648
	2310957593040 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2310957593040 -> 2310960063840
	2310960063840 [label=AccumulateGrad]
	2310960063600 -> 2310960063552
	2310957593136 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2310957593136 -> 2310960063600
	2310960063600 [label=AccumulateGrad]
	2310960063456 -> 2310960063552
	2310957593232 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2310957593232 -> 2310960063456
	2310960063456 [label=AccumulateGrad]
	2310960063360 -> 2310960063216
	2310957593616 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2310957593616 -> 2310960063360
	2310960063360 [label=AccumulateGrad]
	2310960063168 -> 2310960063120
	2310957593712 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2310957593712 -> 2310960063168
	2310960063168 [label=AccumulateGrad]
	2310960063024 -> 2310960063120
	2310957593808 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2310957593808 -> 2310960063024
	2310960063024 [label=AccumulateGrad]
	2310960062928 -> 2310960062784
	2310957594192 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2310957594192 -> 2310960062928
	2310960062928 [label=AccumulateGrad]
	2310960062736 -> 2310960062640
	2310957594288 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2310957594288 -> 2310960062736
	2310960062736 [label=AccumulateGrad]
	2310960062688 -> 2310960062640
	2310957594384 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2310957594384 -> 2310960062688
	2310960062688 [label=AccumulateGrad]
	2310960062592 -> 2310960062544
	2310960062352 -> 2310960062208
	2310957595344 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2310957595344 -> 2310960062352
	2310960062352 [label=AccumulateGrad]
	2310960062160 -> 2310960062112
	2310957595440 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2310957595440 -> 2310960062160
	2310960062160 [label=AccumulateGrad]
	2310960062016 -> 2310960062112
	2310957595536 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2310957595536 -> 2310960062016
	2310960062016 [label=AccumulateGrad]
	2310960061920 -> 2310960061776
	2310957595920 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2310957595920 -> 2310960061920
	2310960061920 [label=AccumulateGrad]
	2310960061728 -> 2310960061680
	2310957596016 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2310957596016 -> 2310960061728
	2310960061728 [label=AccumulateGrad]
	2310960061584 -> 2310960061680
	2310957596112 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2310957596112 -> 2310960061584
	2310960061584 [label=AccumulateGrad]
	2310960061488 -> 2310960061344
	2310957596496 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2310957596496 -> 2310960061488
	2310960061488 [label=AccumulateGrad]
	2310960061296 -> 2310960061200
	2310957596592 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2310957596592 -> 2310960061296
	2310960061296 [label=AccumulateGrad]
	2310960061248 -> 2310960061200
	2310952648784 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2310952648784 -> 2310960061248
	2310960061248 [label=AccumulateGrad]
	2310960061152 -> 2310960061104
	2310960061152 [label=CudnnBatchNormBackward0]
	2310960061872 -> 2310960061152
	2310960061872 [label=ConvolutionBackward0]
	2310960062400 -> 2310960061872
	2310960062256 -> 2310960061872
	2310957594768 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2310957594768 -> 2310960062256
	2310960062256 [label=AccumulateGrad]
	2310960061440 -> 2310960061152
	2310957594864 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2310957594864 -> 2310960061440
	2310960061440 [label=AccumulateGrad]
	2310960061392 -> 2310960061152
	2310957594960 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2310957594960 -> 2310960061392
	2310960061392 [label=AccumulateGrad]
	2310960061008 -> 2310960060816
	2310918472528 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2310918472528 -> 2310960061008
	2310960061008 [label=AccumulateGrad]
	2310960060768 -> 2310960060720
	2310918473680 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2310918473680 -> 2310960060768
	2310960060768 [label=AccumulateGrad]
	2310960060624 -> 2310960060720
	2310918472240 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2310918472240 -> 2310960060624
	2310960060624 [label=AccumulateGrad]
	2310960060528 -> 2310960060384
	2310918477904 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2310918477904 -> 2310960060528
	2310960060528 [label=AccumulateGrad]
	2310960060336 -> 2310960060288
	2310918478000 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2310918478000 -> 2310960060336
	2310960060336 [label=AccumulateGrad]
	2310960060192 -> 2310960060288
	2310918478096 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2310918478096 -> 2310960060192
	2310960060192 [label=AccumulateGrad]
	2310960060096 -> 2310960059952
	2310918478480 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2310918478480 -> 2310960060096
	2310960060096 [label=AccumulateGrad]
	2310960059904 -> 2310960059808
	2310918478576 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2310918478576 -> 2310960059904
	2310960059904 [label=AccumulateGrad]
	2310960059856 -> 2310960059808
	2310918478672 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2310918478672 -> 2310960059856
	2310960059856 [label=AccumulateGrad]
	2310960059760 -> 2310960059712
	2310960059616 -> 2310960059424
	2310918479056 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2310918479056 -> 2310960059616
	2310960059616 [label=AccumulateGrad]
	2310960059376 -> 2310960059328
	2310918479152 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2310918479152 -> 2310960059376
	2310960059376 [label=AccumulateGrad]
	2310960059232 -> 2310960059328
	2310918479248 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2310918479248 -> 2310960059232
	2310960059232 [label=AccumulateGrad]
	2310960059136 -> 2310960058992
	2310918479632 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2310918479632 -> 2310960059136
	2310960059136 [label=AccumulateGrad]
	2310960058944 -> 2310960058896
	2310918479728 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2310918479728 -> 2310960058944
	2310960058944 [label=AccumulateGrad]
	2310960058800 -> 2310960058896
	2310918479824 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2310918479824 -> 2310960058800
	2310960058800 [label=AccumulateGrad]
	2310960058704 -> 2310960058560
	2310918480208 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2310918480208 -> 2310960058704
	2310960058704 [label=AccumulateGrad]
	2310960058512 -> 2310960058416
	2310918480304 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2310918480304 -> 2310960058512
	2310960058512 [label=AccumulateGrad]
	2310960058464 -> 2310960058416
	2310918480400 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2310918480400 -> 2310960058464
	2310960058464 [label=AccumulateGrad]
	2310960058368 -> 2310960058320
	2310960058224 -> 2310960058032
	2310918480784 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2310918480784 -> 2310960058224
	2310960058224 [label=AccumulateGrad]
	2310960057984 -> 2310960057936
	2310918480880 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2310918480880 -> 2310960057984
	2310960057984 [label=AccumulateGrad]
	2310960057840 -> 2310960057936
	2310918480976 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2310918480976 -> 2310960057840
	2310960057840 [label=AccumulateGrad]
	2310960057744 -> 2310917713216
	2310918481360 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2310918481360 -> 2310960057744
	2310960057744 [label=AccumulateGrad]
	2310917711200 -> 2310917702560
	2310918481456 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2310918481456 -> 2310917711200
	2310917711200 [label=AccumulateGrad]
	2310960057600 -> 2310917702560
	2310918481552 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2310918481552 -> 2310960057600
	2310960057600 [label=AccumulateGrad]
	2310917707168 -> 2310917705248
	2310918481936 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2310918481936 -> 2310917707168
	2310917707168 [label=AccumulateGrad]
	2310917715280 -> 2310917706592
	2310918482032 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2310918482032 -> 2310917715280
	2310917715280 [label=AccumulateGrad]
	2310917713504 -> 2310917706592
	2310918482128 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2310918482128 -> 2310917713504
	2310917713504 [label=AccumulateGrad]
	2310917704768 -> 2310917703568
	2310917704576 -> 2310917708368
	2310918482512 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2310918482512 -> 2310917704576
	2310917704576 [label=AccumulateGrad]
	2310917707456 -> 2310917713792
	2310918482608 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2310918482608 -> 2310917707456
	2310917707456 [label=AccumulateGrad]
	2310917708416 -> 2310917713792
	2310918482704 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2310918482704 -> 2310917708416
	2310917708416 [label=AccumulateGrad]
	2310917712928 -> 2310917706112
	2310918483088 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2310918483088 -> 2310917712928
	2310917712928 [label=AccumulateGrad]
	2310917716960 -> 2310917715184
	2310918483184 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2310918483184 -> 2310917716960
	2310917716960 [label=AccumulateGrad]
	2310917716720 -> 2310917715184
	2310918483280 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2310918483280 -> 2310917716720
	2310917716720 [label=AccumulateGrad]
	2310917714032 -> 2310917716336
	2310918483664 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2310918483664 -> 2310917714032
	2310917714032 [label=AccumulateGrad]
	2310917708512 -> 2310917717104
	2310918483760 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2310918483760 -> 2310917708512
	2310917708512 [label=AccumulateGrad]
	2310917702416 -> 2310917717104
	2310918483856 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2310918483856 -> 2310917702416
	2310917702416 [label=AccumulateGrad]
	2310917713648 -> 2310917712016
	2310917715328 -> 2310917717632
	2310918484240 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2310918484240 -> 2310917715328
	2310917715328 [label=AccumulateGrad]
	2310917702176 -> 2310917713024
	2310918484336 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2310918484336 -> 2310917702176
	2310917702176 [label=AccumulateGrad]
	2310917712736 -> 2310917713024
	2310918484432 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2310918484432 -> 2310917712736
	2310917712736 [label=AccumulateGrad]
	2310917714752 -> 2310917717584
	2310918484816 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2310918484816 -> 2310917714752
	2310917714752 [label=AccumulateGrad]
	2310917717296 -> 2310917707120
	2310918484912 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2310918484912 -> 2310917717296
	2310917717296 [label=AccumulateGrad]
	2310917713552 -> 2310917707120
	2310918485008 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2310918485008 -> 2310917713552
	2310917713552 [label=AccumulateGrad]
	2310917709808 -> 2310917704096
	2310918485392 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2310918485392 -> 2310917709808
	2310917709808 [label=AccumulateGrad]
	2310917704528 -> 2310917712160
	2310918485488 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2310918485488 -> 2310917704528
	2310917704528 [label=AccumulateGrad]
	2310917715856 -> 2310917712160
	2310918485584 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2310918485584 -> 2310917715856
	2310917715856 [label=AccumulateGrad]
	2310917702464 -> 2310917707744
	2310917711920 -> 2310917716192
	2310918486640 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2310918486640 -> 2310917711920
	2310917711920 [label=AccumulateGrad]
	2310917714080 -> 2310917702032
	2310918486736 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2310918486736 -> 2310917714080
	2310917714080 [label=AccumulateGrad]
	2310917709760 -> 2310917702032
	2310918486832 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2310918486832 -> 2310917709760
	2310917709760 [label=AccumulateGrad]
	2310917714224 -> 2310917702752
	2310918487216 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2310918487216 -> 2310917714224
	2310917714224 [label=AccumulateGrad]
	2310917710096 -> 2310917715616
	2310918487312 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2310918487312 -> 2310917710096
	2310917710096 [label=AccumulateGrad]
	2310917715808 -> 2310917715616
	2310918487408 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2310918487408 -> 2310917715808
	2310917715808 [label=AccumulateGrad]
	2310917709568 -> 2310917704864
	2310918487792 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2310918487792 -> 2310917709568
	2310917709568 [label=AccumulateGrad]
	2310917702800 -> 2310917711008
	2310918487888 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2310918487888 -> 2310917702800
	2310917702800 [label=AccumulateGrad]
	2310917703136 -> 2310917711008
	2310918487984 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2310918487984 -> 2310917703136
	2310917703136 [label=AccumulateGrad]
	2310917704816 -> 2310917711488
	2310917704816 [label=CudnnBatchNormBackward0]
	2310917717200 -> 2310917704816
	2310917717200 [label=ConvolutionBackward0]
	2310917717344 -> 2310917717200
	2310917707696 -> 2310917717200
	2310918486064 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2310918486064 -> 2310917707696
	2310917707696 [label=AccumulateGrad]
	2310917709472 -> 2310917704816
	2310918486160 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2310918486160 -> 2310917709472
	2310917709472 [label=AccumulateGrad]
	2310917715952 -> 2310917704816
	2310918486256 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2310918486256 -> 2310917715952
	2310917715952 [label=AccumulateGrad]
	2310917715568 -> 2310917709904
	2310959986640 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2310959986640 -> 2310917715568
	2310917715568 [label=AccumulateGrad]
	2310917708032 -> 2310917703424
	2310959986736 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2310959986736 -> 2310917708032
	2310917708032 [label=AccumulateGrad]
	2310917708272 -> 2310917703424
	2310959986832 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2310959986832 -> 2310917708272
	2310917708272 [label=AccumulateGrad]
	2310917711536 -> 2310917702368
	2310959987216 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2310959987216 -> 2310917711536
	2310917711536 [label=AccumulateGrad]
	2310917709136 -> 2310917706208
	2310959987312 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2310959987312 -> 2310917709136
	2310917709136 [label=AccumulateGrad]
	2310917708656 -> 2310917706208
	2310959987408 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2310959987408 -> 2310917708656
	2310917708656 [label=AccumulateGrad]
	2310917711776 -> 2310917708896
	2310959987792 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2310959987792 -> 2310917711776
	2310917711776 [label=AccumulateGrad]
	2310917717248 -> 2310917707792
	2310959987888 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2310959987888 -> 2310917717248
	2310917717248 [label=AccumulateGrad]
	2310917712256 -> 2310917707792
	2310959987984 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2310959987984 -> 2310917712256
	2310917712256 [label=AccumulateGrad]
	2310917704288 -> 2310917710816
	2310917706064 -> 2310917713840
	2310959988368 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2310959988368 -> 2310917706064
	2310917706064 [label=AccumulateGrad]
	2310917710624 -> 2310917717728
	2310959988464 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2310959988464 -> 2310917710624
	2310917710624 [label=AccumulateGrad]
	2310917717776 -> 2310917717728
	2310959988560 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2310959988560 -> 2310917717776
	2310917717776 [label=AccumulateGrad]
	2310917702656 -> 2310917714560
	2310925893904 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2310925893904 -> 2310917702656
	2310917702656 [label=AccumulateGrad]
	2310917711872 -> 2310917703040
	2310925894000 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2310925894000 -> 2310917711872
	2310917711872 [label=AccumulateGrad]
	2310917703712 -> 2310917703040
	2310925894096 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2310925894096 -> 2310917703712
	2310917703712 [label=AccumulateGrad]
	2310917712640 -> 2310917703664
	2310925894480 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2310925894480 -> 2310917712640
	2310917712640 [label=AccumulateGrad]
	2310917709232 -> 2310917713168
	2310925894576 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2310925894576 -> 2310917709232
	2310917709232 [label=AccumulateGrad]
	2310917711392 -> 2310917713168
	2310925894672 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2310925894672 -> 2310917711392
	2310917711392 [label=AccumulateGrad]
	2310917704624 -> 2310917705056
	2310917707936 -> 2310917712592
	2310917707936 [label=TBackward0]
	2310917707408 -> 2310917707936
	2310925895344 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2310925895344 -> 2310917707408
	2310917707408 [label=AccumulateGrad]
	2310917712592 -> 2310960228272
}
