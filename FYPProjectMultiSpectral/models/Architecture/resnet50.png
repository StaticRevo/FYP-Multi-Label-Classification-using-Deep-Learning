digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2203364058096 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2204347462464 [label=AddmmBackward0]
	2204347453872 -> 2204347462464
	2203364016048 [label="fc.bias
 (19)" fillcolor=lightblue]
	2203364016048 -> 2204347453872
	2204347453872 [label=AccumulateGrad]
	2204347451616 -> 2204347462464
	2204347451616 [label=ViewBackward0]
	2204347463184 -> 2204347451616
	2204347463184 [label=MeanBackward1]
	2204347464096 -> 2204347463184
	2204347464096 [label=ReluBackward0]
	2204347454160 -> 2204347464096
	2204347454160 [label=AddBackward0]
	2204347450224 -> 2204347454160
	2204347450224 [label=CudnnBatchNormBackward0]
	2204347452528 -> 2204347450224
	2204347452528 [label=ConvolutionBackward0]
	2204347449696 -> 2204347452528
	2204347449696 [label=ReluBackward0]
	2204347455120 -> 2204347449696
	2204347455120 [label=CudnnBatchNormBackward0]
	2204347465632 -> 2204347455120
	2204347465632 [label=ConvolutionBackward0]
	2204347465536 -> 2204347465632
	2204347465536 [label=ReluBackward0]
	2204347464864 -> 2204347465536
	2204347464864 [label=CudnnBatchNormBackward0]
	2204347460688 -> 2204347464864
	2204347460688 [label=ConvolutionBackward0]
	2204347456608 -> 2204347460688
	2204347456608 [label=ReluBackward0]
	2204347464624 -> 2204347456608
	2204347464624 [label=AddBackward0]
	2204347449456 -> 2204347464624
	2204347449456 [label=CudnnBatchNormBackward0]
	2204347449888 -> 2204347449456
	2204347449888 [label=ConvolutionBackward0]
	2204347463088 -> 2204347449888
	2204347463088 [label=ReluBackward0]
	2204347462560 -> 2204347463088
	2204347462560 [label=CudnnBatchNormBackward0]
	2204347454016 -> 2204347462560
	2204347454016 [label=ConvolutionBackward0]
	2204347456704 -> 2204347454016
	2204347456704 [label=ReluBackward0]
	2204347459968 -> 2204347456704
	2204347459968 [label=CudnnBatchNormBackward0]
	2204347452336 -> 2204347459968
	2204347452336 [label=ConvolutionBackward0]
	2204347461648 -> 2204347452336
	2204347461648 [label=ReluBackward0]
	2204347459920 -> 2204347461648
	2204347459920 [label=AddBackward0]
	2204347454496 -> 2204347459920
	2204347454496 [label=CudnnBatchNormBackward0]
	2204347455936 -> 2204347454496
	2204347455936 [label=ConvolutionBackward0]
	2204347465344 -> 2204347455936
	2204347465344 [label=ReluBackward0]
	2204347457904 -> 2204347465344
	2204347457904 [label=CudnnBatchNormBackward0]
	2204347454880 -> 2204347457904
	2204347454880 [label=ConvolutionBackward0]
	2204347464576 -> 2204347454880
	2204347464576 [label=ReluBackward0]
	2204347453296 -> 2204347464576
	2204347453296 [label=CudnnBatchNormBackward0]
	2204347452048 -> 2204347453296
	2204347452048 [label=ConvolutionBackward0]
	2204963602640 -> 2204347452048
	2204963602640 [label=ReluBackward0]
	2204963602784 -> 2204963602640
	2204963602784 [label=AddBackward0]
	2204963602880 -> 2204963602784
	2204963602880 [label=CudnnBatchNormBackward0]
	2204963603024 -> 2204963602880
	2204963603024 [label=ConvolutionBackward0]
	2204963603216 -> 2204963603024
	2204963603216 [label=ReluBackward0]
	2204963603360 -> 2204963603216
	2204963603360 [label=CudnnBatchNormBackward0]
	2204963603456 -> 2204963603360
	2204963603456 [label=ConvolutionBackward0]
	2204963603648 -> 2204963603456
	2204963603648 [label=ReluBackward0]
	2204963603792 -> 2204963603648
	2204963603792 [label=CudnnBatchNormBackward0]
	2204963603888 -> 2204963603792
	2204963603888 [label=ConvolutionBackward0]
	2204963602832 -> 2204963603888
	2204963602832 [label=ReluBackward0]
	2204963604176 -> 2204963602832
	2204963604176 [label=AddBackward0]
	2204963604272 -> 2204963604176
	2204963604272 [label=CudnnBatchNormBackward0]
	2204963604416 -> 2204963604272
	2204963604416 [label=ConvolutionBackward0]
	2204963604608 -> 2204963604416
	2204963604608 [label=ReluBackward0]
	2204963604752 -> 2204963604608
	2204963604752 [label=CudnnBatchNormBackward0]
	2204963604848 -> 2204963604752
	2204963604848 [label=ConvolutionBackward0]
	2204963605040 -> 2204963604848
	2204963605040 [label=ReluBackward0]
	2204963605184 -> 2204963605040
	2204963605184 [label=CudnnBatchNormBackward0]
	2204963605280 -> 2204963605184
	2204963605280 [label=ConvolutionBackward0]
	2204963604224 -> 2204963605280
	2204963604224 [label=ReluBackward0]
	2204963605568 -> 2204963604224
	2204963605568 [label=AddBackward0]
	2204963605664 -> 2204963605568
	2204963605664 [label=CudnnBatchNormBackward0]
	2204963605808 -> 2204963605664
	2204963605808 [label=ConvolutionBackward0]
	2204963606000 -> 2204963605808
	2204963606000 [label=ReluBackward0]
	2204963606144 -> 2204963606000
	2204963606144 [label=CudnnBatchNormBackward0]
	2204963606240 -> 2204963606144
	2204963606240 [label=ConvolutionBackward0]
	2204963606432 -> 2204963606240
	2204963606432 [label=ReluBackward0]
	2204963606576 -> 2204963606432
	2204963606576 [label=CudnnBatchNormBackward0]
	2204963606672 -> 2204963606576
	2204963606672 [label=ConvolutionBackward0]
	2204963605616 -> 2204963606672
	2204963605616 [label=ReluBackward0]
	2204963606960 -> 2204963605616
	2204963606960 [label=AddBackward0]
	2204963607056 -> 2204963606960
	2204963607056 [label=CudnnBatchNormBackward0]
	2204963607200 -> 2204963607056
	2204963607200 [label=ConvolutionBackward0]
	2204963607392 -> 2204963607200
	2204963607392 [label=ReluBackward0]
	2204963607536 -> 2204963607392
	2204963607536 [label=CudnnBatchNormBackward0]
	2204963607632 -> 2204963607536
	2204963607632 [label=ConvolutionBackward0]
	2204963607824 -> 2204963607632
	2204963607824 [label=ReluBackward0]
	2204963607968 -> 2204963607824
	2204963607968 [label=CudnnBatchNormBackward0]
	2204963608064 -> 2204963607968
	2204963608064 [label=ConvolutionBackward0]
	2204963607008 -> 2204963608064
	2204963607008 [label=ReluBackward0]
	2204963608352 -> 2204963607008
	2204963608352 [label=AddBackward0]
	2204963608448 -> 2204963608352
	2204963608448 [label=CudnnBatchNormBackward0]
	2204963608592 -> 2204963608448
	2204963608592 [label=ConvolutionBackward0]
	2204963608784 -> 2204963608592
	2204963608784 [label=ReluBackward0]
	2204963608928 -> 2204963608784
	2204963608928 [label=CudnnBatchNormBackward0]
	2204963609024 -> 2204963608928
	2204963609024 [label=ConvolutionBackward0]
	2204963609216 -> 2204963609024
	2204963609216 [label=ReluBackward0]
	2204963609360 -> 2204963609216
	2204963609360 [label=CudnnBatchNormBackward0]
	2204963609456 -> 2204963609360
	2204963609456 [label=ConvolutionBackward0]
	2204963608400 -> 2204963609456
	2204963608400 [label=ReluBackward0]
	2204963609744 -> 2204963608400
	2204963609744 [label=AddBackward0]
	2204963609840 -> 2204963609744
	2204963609840 [label=CudnnBatchNormBackward0]
	2204963609984 -> 2204963609840
	2204963609984 [label=ConvolutionBackward0]
	2204963610176 -> 2204963609984
	2204963610176 [label=ReluBackward0]
	2204963610320 -> 2204963610176
	2204963610320 [label=CudnnBatchNormBackward0]
	2204963610416 -> 2204963610320
	2204963610416 [label=ConvolutionBackward0]
	2204963610608 -> 2204963610416
	2204963610608 [label=ReluBackward0]
	2204963610752 -> 2204963610608
	2204963610752 [label=CudnnBatchNormBackward0]
	2204963610848 -> 2204963610752
	2204963610848 [label=ConvolutionBackward0]
	2204963611040 -> 2204963610848
	2204963611040 [label=ReluBackward0]
	2204963611184 -> 2204963611040
	2204963611184 [label=AddBackward0]
	2204963611280 -> 2204963611184
	2204963611280 [label=CudnnBatchNormBackward0]
	2204963611424 -> 2204963611280
	2204963611424 [label=ConvolutionBackward0]
	2204963611616 -> 2204963611424
	2204963611616 [label=ReluBackward0]
	2204963611760 -> 2204963611616
	2204963611760 [label=CudnnBatchNormBackward0]
	2204963611856 -> 2204963611760
	2204963611856 [label=ConvolutionBackward0]
	2204963612048 -> 2204963611856
	2204963612048 [label=ReluBackward0]
	2204963612192 -> 2204963612048
	2204963612192 [label=CudnnBatchNormBackward0]
	2204963612288 -> 2204963612192
	2204963612288 [label=ConvolutionBackward0]
	2204963611232 -> 2204963612288
	2204963611232 [label=ReluBackward0]
	2204963612576 -> 2204963611232
	2204963612576 [label=AddBackward0]
	2204963612672 -> 2204963612576
	2204963612672 [label=CudnnBatchNormBackward0]
	2204963612816 -> 2204963612672
	2204963612816 [label=ConvolutionBackward0]
	2204963613008 -> 2204963612816
	2204963613008 [label=ReluBackward0]
	2204963613152 -> 2204963613008
	2204963613152 [label=CudnnBatchNormBackward0]
	2204963613248 -> 2204963613152
	2204963613248 [label=ConvolutionBackward0]
	2204963613440 -> 2204963613248
	2204963613440 [label=ReluBackward0]
	2204963613584 -> 2204963613440
	2204963613584 [label=CudnnBatchNormBackward0]
	2204963613680 -> 2204963613584
	2204963613680 [label=ConvolutionBackward0]
	2204963612624 -> 2204963613680
	2204963612624 [label=ReluBackward0]
	2204963613968 -> 2204963612624
	2204963613968 [label=AddBackward0]
	2204963614064 -> 2204963613968
	2204963614064 [label=CudnnBatchNormBackward0]
	2204963614208 -> 2204963614064
	2204963614208 [label=ConvolutionBackward0]
	2204963614400 -> 2204963614208
	2204963614400 [label=ReluBackward0]
	2204963614544 -> 2204963614400
	2204963614544 [label=CudnnBatchNormBackward0]
	2204963614640 -> 2204963614544
	2204963614640 [label=ConvolutionBackward0]
	2204963614832 -> 2204963614640
	2204963614832 [label=ReluBackward0]
	2204963614976 -> 2204963614832
	2204963614976 [label=CudnnBatchNormBackward0]
	2204963615072 -> 2204963614976
	2204963615072 [label=ConvolutionBackward0]
	2204963614016 -> 2204963615072
	2204963614016 [label=ReluBackward0]
	2204963615360 -> 2204963614016
	2204963615360 [label=AddBackward0]
	2204963615456 -> 2204963615360
	2204963615456 [label=CudnnBatchNormBackward0]
	2204963615600 -> 2204963615456
	2204963615600 [label=ConvolutionBackward0]
	2204963615792 -> 2204963615600
	2204963615792 [label=ReluBackward0]
	2204963615936 -> 2204963615792
	2204963615936 [label=CudnnBatchNormBackward0]
	2204963616032 -> 2204963615936
	2204963616032 [label=ConvolutionBackward0]
	2204963616224 -> 2204963616032
	2204963616224 [label=ReluBackward0]
	2204963616368 -> 2204963616224
	2204963616368 [label=CudnnBatchNormBackward0]
	2204963616464 -> 2204963616368
	2204963616464 [label=ConvolutionBackward0]
	2204963616656 -> 2204963616464
	2204963616656 [label=ReluBackward0]
	2204963617040 -> 2204963616656
	2204963617040 [label=AddBackward0]
	2204963616704 -> 2204963617040
	2204963616704 [label=CudnnBatchNormBackward0]
	2205236520144 -> 2204963616704
	2205236520144 [label=ConvolutionBackward0]
	2205236520336 -> 2205236520144
	2205236520336 [label=ReluBackward0]
	2205236520480 -> 2205236520336
	2205236520480 [label=CudnnBatchNormBackward0]
	2205236520576 -> 2205236520480
	2205236520576 [label=ConvolutionBackward0]
	2205236520768 -> 2205236520576
	2205236520768 [label=ReluBackward0]
	2205236520912 -> 2205236520768
	2205236520912 [label=CudnnBatchNormBackward0]
	2205236521008 -> 2205236520912
	2205236521008 [label=ConvolutionBackward0]
	2205236520000 -> 2205236521008
	2205236520000 [label=ReluBackward0]
	2205236521296 -> 2205236520000
	2205236521296 [label=AddBackward0]
	2205236521392 -> 2205236521296
	2205236521392 [label=CudnnBatchNormBackward0]
	2205236521536 -> 2205236521392
	2205236521536 [label=ConvolutionBackward0]
	2205236521728 -> 2205236521536
	2205236521728 [label=ReluBackward0]
	2205236521872 -> 2205236521728
	2205236521872 [label=CudnnBatchNormBackward0]
	2205236521968 -> 2205236521872
	2205236521968 [label=ConvolutionBackward0]
	2205236522160 -> 2205236521968
	2205236522160 [label=ReluBackward0]
	2205236522304 -> 2205236522160
	2205236522304 [label=CudnnBatchNormBackward0]
	2205236522400 -> 2205236522304
	2205236522400 [label=ConvolutionBackward0]
	2205236521344 -> 2205236522400
	2205236521344 [label=ReluBackward0]
	2205236522688 -> 2205236521344
	2205236522688 [label=AddBackward0]
	2205236522784 -> 2205236522688
	2205236522784 [label=CudnnBatchNormBackward0]
	2205236522928 -> 2205236522784
	2205236522928 [label=ConvolutionBackward0]
	2205236523120 -> 2205236522928
	2205236523120 [label=ReluBackward0]
	2205236523264 -> 2205236523120
	2205236523264 [label=CudnnBatchNormBackward0]
	2205236523360 -> 2205236523264
	2205236523360 [label=ConvolutionBackward0]
	2205236523552 -> 2205236523360
	2205236523552 [label=ReluBackward0]
	2205236523696 -> 2205236523552
	2205236523696 [label=CudnnBatchNormBackward0]
	2205236523792 -> 2205236523696
	2205236523792 [label=ConvolutionBackward0]
	2205236523984 -> 2205236523792
	2205236523984 [label=MaxPool2DWithIndicesBackward0]
	2205236524128 -> 2205236523984
	2205236524128 [label=ReluBackward0]
	2205236524224 -> 2205236524128
	2205236524224 [label=CudnnBatchNormBackward0]
	2205236524320 -> 2205236524224
	2205236524320 [label=ConvolutionBackward0]
	2205236524512 -> 2205236524320
	2204875702832 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2204875702832 -> 2205236524512
	2205236524512 [label=AccumulateGrad]
	2205236524272 -> 2205236524224
	2204963832080 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2204963832080 -> 2205236524272
	2205236524272 [label=AccumulateGrad]
	2205236524032 -> 2205236524224
	2204963832176 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2204963832176 -> 2205236524032
	2205236524032 [label=AccumulateGrad]
	2205236523936 -> 2205236523792
	2204963833136 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2204963833136 -> 2205236523936
	2205236523936 [label=AccumulateGrad]
	2205236523744 -> 2205236523696
	2204963833232 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2204963833232 -> 2205236523744
	2205236523744 [label=AccumulateGrad]
	2205236523600 -> 2205236523696
	2204963833328 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2204963833328 -> 2205236523600
	2205236523600 [label=AccumulateGrad]
	2205236523504 -> 2205236523360
	2204963833808 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2204963833808 -> 2205236523504
	2205236523504 [label=AccumulateGrad]
	2205236523312 -> 2205236523264
	2204963833904 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2204963833904 -> 2205236523312
	2205236523312 [label=AccumulateGrad]
	2205236523168 -> 2205236523264
	2204963834000 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2204963834000 -> 2205236523168
	2205236523168 [label=AccumulateGrad]
	2205236523072 -> 2205236522928
	2204963834384 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2204963834384 -> 2205236523072
	2205236523072 [label=AccumulateGrad]
	2205236522880 -> 2205236522784
	2204963834480 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2204963834480 -> 2205236522880
	2205236522880 [label=AccumulateGrad]
	2205236522832 -> 2205236522784
	2204963834576 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2204963834576 -> 2205236522832
	2205236522832 [label=AccumulateGrad]
	2205236522736 -> 2205236522688
	2205236522736 [label=CudnnBatchNormBackward0]
	2205236523456 -> 2205236522736
	2205236523456 [label=ConvolutionBackward0]
	2205236523984 -> 2205236523456
	2205236523840 -> 2205236523456
	2204963832560 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2204963832560 -> 2205236523840
	2205236523840 [label=AccumulateGrad]
	2205236523024 -> 2205236522736
	2204963832656 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2204963832656 -> 2205236523024
	2205236523024 [label=AccumulateGrad]
	2205236522976 -> 2205236522736
	2204963832752 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2204963832752 -> 2205236522976
	2205236522976 [label=AccumulateGrad]
	2205236522592 -> 2205236522400
	2204963834960 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2204963834960 -> 2205236522592
	2205236522592 [label=AccumulateGrad]
	2205236522352 -> 2205236522304
	2204963835056 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2204963835056 -> 2205236522352
	2205236522352 [label=AccumulateGrad]
	2205236522208 -> 2205236522304
	2204963835152 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2204963835152 -> 2205236522208
	2205236522208 [label=AccumulateGrad]
	2205236522112 -> 2205236521968
	2204963835536 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2204963835536 -> 2205236522112
	2205236522112 [label=AccumulateGrad]
	2205236521920 -> 2205236521872
	2204963835632 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2204963835632 -> 2205236521920
	2205236521920 [label=AccumulateGrad]
	2205236521776 -> 2205236521872
	2204963835728 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2204963835728 -> 2205236521776
	2205236521776 [label=AccumulateGrad]
	2205236521680 -> 2205236521536
	2204963836112 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2204963836112 -> 2205236521680
	2205236521680 [label=AccumulateGrad]
	2205236521488 -> 2205236521392
	2204963836208 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2204963836208 -> 2205236521488
	2205236521488 [label=AccumulateGrad]
	2205236521440 -> 2205236521392
	2204963836304 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2204963836304 -> 2205236521440
	2205236521440 [label=AccumulateGrad]
	2205236521344 -> 2205236521296
	2205236521200 -> 2205236521008
	2204963836688 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2204963836688 -> 2205236521200
	2205236521200 [label=AccumulateGrad]
	2205236520960 -> 2205236520912
	2204963836784 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2204963836784 -> 2205236520960
	2205236520960 [label=AccumulateGrad]
	2205236520816 -> 2205236520912
	2204963836880 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2204963836880 -> 2205236520816
	2205236520816 [label=AccumulateGrad]
	2205236520720 -> 2205236520576
	2204963837264 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2204963837264 -> 2205236520720
	2205236520720 [label=AccumulateGrad]
	2205236520528 -> 2205236520480
	2204963837360 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2204963837360 -> 2205236520528
	2205236520528 [label=AccumulateGrad]
	2205236520384 -> 2205236520480
	2204963837456 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2204963837456 -> 2205236520384
	2205236520384 [label=AccumulateGrad]
	2205236520288 -> 2205236520144
	2204963837840 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2204963837840 -> 2205236520288
	2205236520288 [label=AccumulateGrad]
	2205236520096 -> 2204963616704
	2204963837936 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2204963837936 -> 2205236520096
	2205236520096 [label=AccumulateGrad]
	2205236520048 -> 2204963616704
	2204963838032 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2204963838032 -> 2205236520048
	2205236520048 [label=AccumulateGrad]
	2205236520000 -> 2204963617040
	2204963616608 -> 2204963616464
	2205236693328 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2205236693328 -> 2204963616608
	2204963616608 [label=AccumulateGrad]
	2204963616416 -> 2204963616368
	2205236693424 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2205236693424 -> 2204963616416
	2204963616416 [label=AccumulateGrad]
	2204963616272 -> 2204963616368
	2205236693520 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2205236693520 -> 2204963616272
	2204963616272 [label=AccumulateGrad]
	2204963616176 -> 2204963616032
	2205236693904 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205236693904 -> 2204963616176
	2204963616176 [label=AccumulateGrad]
	2204963615984 -> 2204963615936
	2205236694000 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2205236694000 -> 2204963615984
	2204963615984 [label=AccumulateGrad]
	2204963615840 -> 2204963615936
	2205236694096 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2205236694096 -> 2204963615840
	2204963615840 [label=AccumulateGrad]
	2204963615744 -> 2204963615600
	2205236694480 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205236694480 -> 2204963615744
	2204963615744 [label=AccumulateGrad]
	2204963615552 -> 2204963615456
	2205236694576 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2205236694576 -> 2204963615552
	2204963615552 [label=AccumulateGrad]
	2204963615504 -> 2204963615456
	2205236694672 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2205236694672 -> 2204963615504
	2204963615504 [label=AccumulateGrad]
	2204963615408 -> 2204963615360
	2204963615408 [label=CudnnBatchNormBackward0]
	2204963616128 -> 2204963615408
	2204963616128 [label=ConvolutionBackward0]
	2204963616656 -> 2204963616128
	2204963616512 -> 2204963616128
	2204963838608 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2204963838608 -> 2204963616512
	2204963616512 [label=AccumulateGrad]
	2204963615696 -> 2204963615408
	2205236692848 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2205236692848 -> 2204963615696
	2204963615696 [label=AccumulateGrad]
	2204963615648 -> 2204963615408
	2205236692944 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2205236692944 -> 2204963615648
	2204963615648 [label=AccumulateGrad]
	2204963615264 -> 2204963615072
	2205236695056 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2205236695056 -> 2204963615264
	2204963615264 [label=AccumulateGrad]
	2204963615024 -> 2204963614976
	2205236695152 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2205236695152 -> 2204963615024
	2204963615024 [label=AccumulateGrad]
	2204963614880 -> 2204963614976
	2205236695248 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2205236695248 -> 2204963614880
	2204963614880 [label=AccumulateGrad]
	2204963614784 -> 2204963614640
	2205236695632 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205236695632 -> 2204963614784
	2204963614784 [label=AccumulateGrad]
	2204963614592 -> 2204963614544
	2205236695728 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2205236695728 -> 2204963614592
	2204963614592 [label=AccumulateGrad]
	2204963614448 -> 2204963614544
	2205236695824 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2205236695824 -> 2204963614448
	2204963614448 [label=AccumulateGrad]
	2204963614352 -> 2204963614208
	2205236696208 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205236696208 -> 2204963614352
	2204963614352 [label=AccumulateGrad]
	2204963614160 -> 2204963614064
	2205236696304 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2205236696304 -> 2204963614160
	2204963614160 [label=AccumulateGrad]
	2204963614112 -> 2204963614064
	2205236696400 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2205236696400 -> 2204963614112
	2204963614112 [label=AccumulateGrad]
	2204963614016 -> 2204963613968
	2204963613872 -> 2204963613680
	2205236696784 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2205236696784 -> 2204963613872
	2204963613872 [label=AccumulateGrad]
	2204963613632 -> 2204963613584
	2205236696880 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2205236696880 -> 2204963613632
	2204963613632 [label=AccumulateGrad]
	2204963613488 -> 2204963613584
	2205236696976 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2205236696976 -> 2204963613488
	2204963613488 [label=AccumulateGrad]
	2204963613392 -> 2204963613248
	2205236697360 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205236697360 -> 2204963613392
	2204963613392 [label=AccumulateGrad]
	2204963613200 -> 2204963613152
	2205236697456 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2205236697456 -> 2204963613200
	2204963613200 [label=AccumulateGrad]
	2204963613056 -> 2204963613152
	2205236697552 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2205236697552 -> 2204963613056
	2204963613056 [label=AccumulateGrad]
	2204963612960 -> 2204963612816
	2205236697936 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205236697936 -> 2204963612960
	2204963612960 [label=AccumulateGrad]
	2204963612768 -> 2204963612672
	2205236698032 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2205236698032 -> 2204963612768
	2204963612768 [label=AccumulateGrad]
	2204963612720 -> 2204963612672
	2205236698128 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2205236698128 -> 2204963612720
	2204963612720 [label=AccumulateGrad]
	2204963612624 -> 2204963612576
	2204963612480 -> 2204963612288
	2205236698512 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2205236698512 -> 2204963612480
	2204963612480 [label=AccumulateGrad]
	2204963612240 -> 2204963612192
	2205236698608 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2205236698608 -> 2204963612240
	2204963612240 [label=AccumulateGrad]
	2204963612096 -> 2204963612192
	2205236698704 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2205236698704 -> 2204963612096
	2204963612096 [label=AccumulateGrad]
	2204963612000 -> 2204963611856
	2205236699088 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205236699088 -> 2204963612000
	2204963612000 [label=AccumulateGrad]
	2204963611808 -> 2204963611760
	2205236699184 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2205236699184 -> 2204963611808
	2204963611808 [label=AccumulateGrad]
	2204963611664 -> 2204963611760
	2205236699280 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2205236699280 -> 2204963611664
	2204963611664 [label=AccumulateGrad]
	2204963611568 -> 2204963611424
	2205236699664 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205236699664 -> 2204963611568
	2204963611568 [label=AccumulateGrad]
	2204963611376 -> 2204963611280
	2205236699760 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2205236699760 -> 2204963611376
	2204963611376 [label=AccumulateGrad]
	2204963611328 -> 2204963611280
	2205236699856 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2205236699856 -> 2204963611328
	2204963611328 [label=AccumulateGrad]
	2204963611232 -> 2204963611184
	2204963610992 -> 2204963610848
	2205236700816 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2205236700816 -> 2204963610992
	2204963610992 [label=AccumulateGrad]
	2204963610800 -> 2204963610752
	2205236700912 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2205236700912 -> 2204963610800
	2204963610800 [label=AccumulateGrad]
	2204963610656 -> 2204963610752
	2205236701008 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2205236701008 -> 2204963610656
	2204963610656 [label=AccumulateGrad]
	2204963610560 -> 2204963610416
	2205236701392 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205236701392 -> 2204963610560
	2204963610560 [label=AccumulateGrad]
	2204963610368 -> 2204963610320
	2205236701488 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2205236701488 -> 2204963610368
	2204963610368 [label=AccumulateGrad]
	2204963610224 -> 2204963610320
	2205236701584 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2205236701584 -> 2204963610224
	2204963610224 [label=AccumulateGrad]
	2204963610128 -> 2204963609984
	2205236701968 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205236701968 -> 2204963610128
	2204963610128 [label=AccumulateGrad]
	2204963609936 -> 2204963609840
	2205236702064 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2205236702064 -> 2204963609936
	2204963609936 [label=AccumulateGrad]
	2204963609888 -> 2204963609840
	2205236702160 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2205236702160 -> 2204963609888
	2204963609888 [label=AccumulateGrad]
	2204963609792 -> 2204963609744
	2204963609792 [label=CudnnBatchNormBackward0]
	2204963610512 -> 2204963609792
	2204963610512 [label=ConvolutionBackward0]
	2204963611040 -> 2204963610512
	2204963610896 -> 2204963610512
	2205236700240 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2205236700240 -> 2204963610896
	2204963610896 [label=AccumulateGrad]
	2204963610080 -> 2204963609792
	2205236700336 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2205236700336 -> 2204963610080
	2204963610080 [label=AccumulateGrad]
	2204963610032 -> 2204963609792
	2205236700432 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2205236700432 -> 2204963610032
	2204963610032 [label=AccumulateGrad]
	2204963609648 -> 2204963609456
	2205236702544 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205236702544 -> 2204963609648
	2204963609648 [label=AccumulateGrad]
	2204963609408 -> 2204963609360
	2205236702640 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2205236702640 -> 2204963609408
	2204963609408 [label=AccumulateGrad]
	2204963609264 -> 2204963609360
	2205236702736 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2205236702736 -> 2204963609264
	2204963609264 [label=AccumulateGrad]
	2204963609168 -> 2204963609024
	2205236703120 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205236703120 -> 2204963609168
	2204963609168 [label=AccumulateGrad]
	2204963608976 -> 2204963608928
	2205236703216 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2205236703216 -> 2204963608976
	2204963608976 [label=AccumulateGrad]
	2204963608832 -> 2204963608928
	2205236703312 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2205236703312 -> 2204963608832
	2204963608832 [label=AccumulateGrad]
	2204963608736 -> 2204963608592
	2205236703696 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205236703696 -> 2204963608736
	2204963608736 [label=AccumulateGrad]
	2204963608544 -> 2204963608448
	2205236703792 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2205236703792 -> 2204963608544
	2204963608544 [label=AccumulateGrad]
	2204963608496 -> 2204963608448
	2205236703888 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2205236703888 -> 2204963608496
	2204963608496 [label=AccumulateGrad]
	2204963608400 -> 2204963608352
	2204963608256 -> 2204963608064
	2205236704272 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205236704272 -> 2204963608256
	2204963608256 [label=AccumulateGrad]
	2204963608016 -> 2204963607968
	2205236704368 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2205236704368 -> 2204963608016
	2204963608016 [label=AccumulateGrad]
	2204963607872 -> 2204963607968
	2205236704464 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2205236704464 -> 2204963607872
	2204963607872 [label=AccumulateGrad]
	2204963607776 -> 2204963607632
	2205236704848 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205236704848 -> 2204963607776
	2204963607776 [label=AccumulateGrad]
	2204963607584 -> 2204963607536
	2205236704944 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2205236704944 -> 2204963607584
	2204963607584 [label=AccumulateGrad]
	2204963607440 -> 2204963607536
	2205236705040 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2205236705040 -> 2204963607440
	2204963607440 [label=AccumulateGrad]
	2204963607344 -> 2204963607200
	2205236705424 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205236705424 -> 2204963607344
	2204963607344 [label=AccumulateGrad]
	2204963607152 -> 2204963607056
	2205236705520 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2205236705520 -> 2204963607152
	2204963607152 [label=AccumulateGrad]
	2204963607104 -> 2204963607056
	2205236705616 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2205236705616 -> 2204963607104
	2204963607104 [label=AccumulateGrad]
	2204963607008 -> 2204963606960
	2204963606864 -> 2204963606672
	2205236706000 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205236706000 -> 2204963606864
	2204963606864 [label=AccumulateGrad]
	2204963606624 -> 2204963606576
	2205236706096 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2205236706096 -> 2204963606624
	2204963606624 [label=AccumulateGrad]
	2204963606480 -> 2204963606576
	2205236706192 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2205236706192 -> 2204963606480
	2204963606480 [label=AccumulateGrad]
	2204963606384 -> 2204963606240
	2205236706576 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205236706576 -> 2204963606384
	2204963606384 [label=AccumulateGrad]
	2204963606192 -> 2204963606144
	2205236706672 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2205236706672 -> 2204963606192
	2204963606192 [label=AccumulateGrad]
	2204963606048 -> 2204963606144
	2205236706768 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2205236706768 -> 2204963606048
	2204963606048 [label=AccumulateGrad]
	2204963605952 -> 2204963605808
	2205236707152 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205236707152 -> 2204963605952
	2204963605952 [label=AccumulateGrad]
	2204963605760 -> 2204963605664
	2205236707248 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2205236707248 -> 2204963605760
	2204963605760 [label=AccumulateGrad]
	2204963605712 -> 2204963605664
	2205158047824 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2205158047824 -> 2204963605712
	2204963605712 [label=AccumulateGrad]
	2204963605616 -> 2204963605568
	2204963605472 -> 2204963605280
	2205158048208 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205158048208 -> 2204963605472
	2204963605472 [label=AccumulateGrad]
	2204963605232 -> 2204963605184
	2205158048304 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2205158048304 -> 2204963605232
	2204963605232 [label=AccumulateGrad]
	2204963605088 -> 2204963605184
	2205158048400 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2205158048400 -> 2204963605088
	2204963605088 [label=AccumulateGrad]
	2204963604992 -> 2204963604848
	2205158048784 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205158048784 -> 2204963604992
	2204963604992 [label=AccumulateGrad]
	2204963604800 -> 2204963604752
	2205158048880 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2205158048880 -> 2204963604800
	2204963604800 [label=AccumulateGrad]
	2204963604656 -> 2204963604752
	2205158048976 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2205158048976 -> 2204963604656
	2204963604656 [label=AccumulateGrad]
	2204963604560 -> 2204963604416
	2205158049360 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205158049360 -> 2204963604560
	2204963604560 [label=AccumulateGrad]
	2204963604368 -> 2204963604272
	2205158049456 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2205158049456 -> 2204963604368
	2204963604368 [label=AccumulateGrad]
	2204963604320 -> 2204963604272
	2205158049552 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2205158049552 -> 2204963604320
	2204963604320 [label=AccumulateGrad]
	2204963604224 -> 2204963604176
	2204963604080 -> 2204963603888
	2204584781168 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2204584781168 -> 2204963604080
	2204963604080 [label=AccumulateGrad]
	2204963603840 -> 2204963603792
	2204584783760 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2204584783760 -> 2204963603840
	2204963603840 [label=AccumulateGrad]
	2204963603696 -> 2204963603792
	2204584783856 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2204584783856 -> 2204963603696
	2204963603696 [label=AccumulateGrad]
	2204963603600 -> 2204963603456
	2204584784240 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2204584784240 -> 2204963603600
	2204963603600 [label=AccumulateGrad]
	2204963603408 -> 2204963603360
	2204584784336 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2204584784336 -> 2204963603408
	2204963603408 [label=AccumulateGrad]
	2204963603264 -> 2204963603360
	2204584784432 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2204584784432 -> 2204963603264
	2204963603264 [label=AccumulateGrad]
	2204963603168 -> 2204963603024
	2204584784816 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2204584784816 -> 2204963603168
	2204963603168 [label=AccumulateGrad]
	2204963602976 -> 2204963602880
	2204584784912 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2204584784912 -> 2204963602976
	2204963602976 [label=AccumulateGrad]
	2204963602928 -> 2204963602880
	2204584785008 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2204584785008 -> 2204963602928
	2204963602928 [label=AccumulateGrad]
	2204963602832 -> 2204963602784
	2204963602592 -> 2204347452048
	2204584786064 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2204584786064 -> 2204963602592
	2204963602592 [label=AccumulateGrad]
	2204347450512 -> 2204347453296
	2204584786160 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2204584786160 -> 2204347450512
	2204347450512 [label=AccumulateGrad]
	2204347450320 -> 2204347453296
	2204584786256 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2204584786256 -> 2204347450320
	2204347450320 [label=AccumulateGrad]
	2204347462512 -> 2204347454880
	2204584786640 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2204584786640 -> 2204347462512
	2204347462512 [label=AccumulateGrad]
	2204347457184 -> 2204347457904
	2204584786736 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2204584786736 -> 2204347457184
	2204347457184 [label=AccumulateGrad]
	2204347450272 -> 2204347457904
	2204584786832 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2204584786832 -> 2204347450272
	2204347450272 [label=AccumulateGrad]
	2204347462704 -> 2204347455936
	2204584787216 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2204584787216 -> 2204347462704
	2204347462704 [label=AccumulateGrad]
	2204347453584 -> 2204347454496
	2204584787312 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2204584787312 -> 2204347453584
	2204347453584 [label=AccumulateGrad]
	2204347463808 -> 2204347454496
	2204584787408 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2204584787408 -> 2204347463808
	2204347463808 [label=AccumulateGrad]
	2204347452576 -> 2204347459920
	2204347452576 [label=CudnnBatchNormBackward0]
	2204347457520 -> 2204347452576
	2204347457520 [label=ConvolutionBackward0]
	2204963602640 -> 2204347457520
	2204347459536 -> 2204347457520
	2204584785488 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2204584785488 -> 2204347459536
	2204347459536 [label=AccumulateGrad]
	2204347458720 -> 2204347452576
	2204584785584 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2204584785584 -> 2204347458720
	2204347458720 [label=AccumulateGrad]
	2204347456560 -> 2204347452576
	2204584785680 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2204584785680 -> 2204347456560
	2204347456560 [label=AccumulateGrad]
	2204347459200 -> 2204347452336
	2204584787792 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2204584787792 -> 2204347459200
	2204347459200 [label=AccumulateGrad]
	2204347452000 -> 2204347459968
	2204584787888 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2204584787888 -> 2204347452000
	2204347452000 [label=AccumulateGrad]
	2204347462176 -> 2204347459968
	2204584778480 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2204584778480 -> 2204347462176
	2204347462176 [label=AccumulateGrad]
	2204347461888 -> 2204347454016
	2203363800272 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2203363800272 -> 2204347461888
	2204347461888 [label=AccumulateGrad]
	2204347459392 -> 2204347462560
	2203363800368 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2203363800368 -> 2204347459392
	2204347459392 [label=AccumulateGrad]
	2204347455360 -> 2204347462560
	2203363800464 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2203363800464 -> 2204347455360
	2204347455360 [label=AccumulateGrad]
	2204347449552 -> 2204347449888
	2203363800848 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2203363800848 -> 2204347449552
	2204347449552 [label=AccumulateGrad]
	2204347456368 -> 2204347449456
	2203363800944 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2203363800944 -> 2204347456368
	2204347456368 [label=AccumulateGrad]
	2204347455024 -> 2204347449456
	2203363801040 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2203363801040 -> 2204347455024
	2204347455024 [label=AccumulateGrad]
	2204347461648 -> 2204347464624
	2204347461792 -> 2204347460688
	2203363801424 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2203363801424 -> 2204347461792
	2204347461792 [label=AccumulateGrad]
	2204347458912 -> 2204347464864
	2203363801520 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2203363801520 -> 2204347458912
	2204347458912 [label=AccumulateGrad]
	2204347452960 -> 2204347464864
	2203363801616 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2203363801616 -> 2204347452960
	2204347452960 [label=AccumulateGrad]
	2204347450464 -> 2204347465632
	2203363802000 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2203363802000 -> 2204347450464
	2204347450464 [label=AccumulateGrad]
	2204347456464 -> 2204347455120
	2203363802096 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2203363802096 -> 2204347456464
	2204347456464 [label=AccumulateGrad]
	2204347450032 -> 2204347455120
	2203363802192 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2203363802192 -> 2204347450032
	2204347450032 [label=AccumulateGrad]
	2204347457088 -> 2204347452528
	2203363802576 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2203363802576 -> 2204347457088
	2204347457088 [label=AccumulateGrad]
	2204347450992 -> 2204347450224
	2203363802672 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2203363802672 -> 2204347450992
	2204347450992 [label=AccumulateGrad]
	2204347451472 -> 2204347450224
	2203363802768 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2203363802768 -> 2204347451472
	2204347451472 [label=AccumulateGrad]
	2204347456608 -> 2204347454160
	2204347451088 -> 2204347462464
	2204347451088 [label=TBackward0]
	2204347462656 -> 2204347451088
	2203363803056 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2203363803056 -> 2204347462656
	2204347462656 [label=AccumulateGrad]
	2204347462464 -> 2203364058096
}
