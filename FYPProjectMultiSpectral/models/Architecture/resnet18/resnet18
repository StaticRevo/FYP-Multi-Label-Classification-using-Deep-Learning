digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1908216310640 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1908245112640 [label=AddmmBackward0]
	1908245112784 -> 1908245112640
	1908207813840 [label="fc.bias
 (19)" fillcolor=lightblue]
	1908207813840 -> 1908245112784
	1908245112784 [label=AccumulateGrad]
	1908245112832 -> 1908245112640
	1908245112832 [label=ViewBackward0]
	1908245112928 -> 1908245112832
	1908245112928 [label=MeanBackward1]
	1908245113072 -> 1908245112928
	1908245113072 [label=ReluBackward0]
	1908245113168 -> 1908245113072
	1908245113168 [label=AddBackward0]
	1908245113264 -> 1908245113168
	1908245113264 [label=CudnnBatchNormBackward0]
	1908245113408 -> 1908245113264
	1908245113408 [label=ConvolutionBackward0]
	1908245113600 -> 1908245113408
	1908245113600 [label=ReluBackward0]
	1908245113744 -> 1908245113600
	1908245113744 [label=CudnnBatchNormBackward0]
	1908245113840 -> 1908245113744
	1908245113840 [label=ConvolutionBackward0]
	1908245113216 -> 1908245113840
	1908245113216 [label=ReluBackward0]
	1908245114128 -> 1908245113216
	1908245114128 [label=AddBackward0]
	1908245114224 -> 1908245114128
	1908245114224 [label=CudnnBatchNormBackward0]
	1908245114368 -> 1908245114224
	1908245114368 [label=ConvolutionBackward0]
	1908245114560 -> 1908245114368
	1908245114560 [label=ReluBackward0]
	1908245114704 -> 1908245114560
	1908245114704 [label=CudnnBatchNormBackward0]
	1908245114800 -> 1908245114704
	1908245114800 [label=ConvolutionBackward0]
	1908245114992 -> 1908245114800
	1908245114992 [label=ReluBackward0]
	1908245115136 -> 1908245114992
	1908245115136 [label=AddBackward0]
	1908245115232 -> 1908245115136
	1908245115232 [label=CudnnBatchNormBackward0]
	1908245115376 -> 1908245115232
	1908245115376 [label=ConvolutionBackward0]
	1908245115568 -> 1908245115376
	1908245115568 [label=ReluBackward0]
	1908245115712 -> 1908245115568
	1908245115712 [label=CudnnBatchNormBackward0]
	1908245115808 -> 1908245115712
	1908245115808 [label=ConvolutionBackward0]
	1908245115184 -> 1908245115808
	1908245115184 [label=ReluBackward0]
	1908245110192 -> 1908245115184
	1908245110192 [label=AddBackward0]
	1908245110528 -> 1908245110192
	1908245110528 [label=CudnnBatchNormBackward0]
	1908245110624 -> 1908245110528
	1908245110624 [label=ConvolutionBackward0]
	1908245110816 -> 1908245110624
	1908245110816 [label=ReluBackward0]
	1908245110960 -> 1908245110816
	1908245110960 [label=CudnnBatchNormBackward0]
	1908245111056 -> 1908245110960
	1908245111056 [label=ConvolutionBackward0]
	1908245111248 -> 1908245111056
	1908245111248 [label=ReluBackward0]
	1908245111392 -> 1908245111248
	1908245111392 [label=AddBackward0]
	1908245111488 -> 1908245111392
	1908245111488 [label=CudnnBatchNormBackward0]
	1908245111632 -> 1908245111488
	1908245111632 [label=ConvolutionBackward0]
	1908245111824 -> 1908245111632
	1908245111824 [label=ReluBackward0]
	1908245111968 -> 1908245111824
	1908245111968 [label=CudnnBatchNormBackward0]
	1908245112064 -> 1908245111968
	1908245112064 [label=ConvolutionBackward0]
	1908245111440 -> 1908245112064
	1908245111440 [label=ReluBackward0]
	1908245116096 -> 1908245111440
	1908245116096 [label=AddBackward0]
	1908245116192 -> 1908245116096
	1908245116192 [label=CudnnBatchNormBackward0]
	1908245116336 -> 1908245116192
	1908245116336 [label=ConvolutionBackward0]
	1908245116528 -> 1908245116336
	1908245116528 [label=ReluBackward0]
	1908245116672 -> 1908245116528
	1908245116672 [label=CudnnBatchNormBackward0]
	1908245116768 -> 1908245116672
	1908245116768 [label=ConvolutionBackward0]
	1908245116960 -> 1908245116768
	1908245116960 [label=ReluBackward0]
	1908245117104 -> 1908245116960
	1908245117104 [label=AddBackward0]
	1908245117200 -> 1908245117104
	1908245117200 [label=CudnnBatchNormBackward0]
	1908245117344 -> 1908245117200
	1908245117344 [label=ConvolutionBackward0]
	1908245117536 -> 1908245117344
	1908245117536 [label=ReluBackward0]
	1908245117680 -> 1908245117536
	1908245117680 [label=CudnnBatchNormBackward0]
	1908245117776 -> 1908245117680
	1908245117776 [label=ConvolutionBackward0]
	1908245117152 -> 1908245117776
	1908245117152 [label=ReluBackward0]
	1908245118064 -> 1908245117152
	1908245118064 [label=AddBackward0]
	1908245118160 -> 1908245118064
	1908245118160 [label=CudnnBatchNormBackward0]
	1908245118304 -> 1908245118160
	1908245118304 [label=ConvolutionBackward0]
	1908245118496 -> 1908245118304
	1908245118496 [label=ReluBackward0]
	1908245118640 -> 1908245118496
	1908245118640 [label=CudnnBatchNormBackward0]
	1908245118736 -> 1908245118640
	1908245118736 [label=ConvolutionBackward0]
	1908245118112 -> 1908245118736
	1908245118112 [label=MaxPool2DWithIndicesBackward0]
	1908245118832 -> 1908245118112
	1908245118832 [label=ReluBackward0]
	1908212793552 -> 1908245118832
	1908212793552 [label=CudnnBatchNormBackward0]
	1908212793648 -> 1908212793552
	1908212793648 [label=ConvolutionBackward0]
	1908212793840 -> 1908212793648
	1908245181360 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1908245181360 -> 1908212793840
	1908212793840 [label=AccumulateGrad]
	1908212793600 -> 1908212793552
	1908245169264 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1908245169264 -> 1908212793600
	1908212793600 [label=AccumulateGrad]
	1908212793408 -> 1908212793552
	1908245169360 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1908245169360 -> 1908212793408
	1908212793408 [label=AccumulateGrad]
	1908245118928 -> 1908245118736
	1908245169840 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1908245169840 -> 1908245118928
	1908245118928 [label=AccumulateGrad]
	1908245118688 -> 1908245118640
	1908245169936 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1908245169936 -> 1908245118688
	1908245118688 [label=AccumulateGrad]
	1908245118544 -> 1908245118640
	1908245170032 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1908245170032 -> 1908245118544
	1908245118544 [label=AccumulateGrad]
	1908245118448 -> 1908245118304
	1908245170416 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1908245170416 -> 1908245118448
	1908245118448 [label=AccumulateGrad]
	1908245118256 -> 1908245118160
	1908245170512 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1908245170512 -> 1908245118256
	1908245118256 [label=AccumulateGrad]
	1908245118208 -> 1908245118160
	1908245170608 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1908245170608 -> 1908245118208
	1908245118208 [label=AccumulateGrad]
	1908245118112 -> 1908245118064
	1908245117968 -> 1908245117776
	1908245170992 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1908245170992 -> 1908245117968
	1908245117968 [label=AccumulateGrad]
	1908245117728 -> 1908245117680
	1908245171088 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1908245171088 -> 1908245117728
	1908245117728 [label=AccumulateGrad]
	1908245117584 -> 1908245117680
	1908245171184 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1908245171184 -> 1908245117584
	1908245117584 [label=AccumulateGrad]
	1908245117488 -> 1908245117344
	1908245171568 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1908245171568 -> 1908245117488
	1908245117488 [label=AccumulateGrad]
	1908245117296 -> 1908245117200
	1908245171664 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1908245171664 -> 1908245117296
	1908245117296 [label=AccumulateGrad]
	1908245117248 -> 1908245117200
	1908245171760 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1908245171760 -> 1908245117248
	1908245117248 [label=AccumulateGrad]
	1908245117152 -> 1908245117104
	1908245116912 -> 1908245116768
	1908245172720 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1908245172720 -> 1908245116912
	1908245116912 [label=AccumulateGrad]
	1908245116720 -> 1908245116672
	1908245172816 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1908245172816 -> 1908245116720
	1908245116720 [label=AccumulateGrad]
	1908245116576 -> 1908245116672
	1908245172912 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1908245172912 -> 1908245116576
	1908245116576 [label=AccumulateGrad]
	1908245116480 -> 1908245116336
	1908245173296 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1908245173296 -> 1908245116480
	1908245116480 [label=AccumulateGrad]
	1908245116288 -> 1908245116192
	1908245173392 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1908245173392 -> 1908245116288
	1908245116288 [label=AccumulateGrad]
	1908245116240 -> 1908245116192
	1908245173488 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1908245173488 -> 1908245116240
	1908245116240 [label=AccumulateGrad]
	1908245116144 -> 1908245116096
	1908245116144 [label=CudnnBatchNormBackward0]
	1908245116864 -> 1908245116144
	1908245116864 [label=ConvolutionBackward0]
	1908245116960 -> 1908245116864
	1908245117008 -> 1908245116864
	1908245172144 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1908245172144 -> 1908245117008
	1908245117008 [label=AccumulateGrad]
	1908245116432 -> 1908245116144
	1908245172240 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1908245172240 -> 1908245116432
	1908245116432 [label=AccumulateGrad]
	1908245116384 -> 1908245116144
	1908245172336 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1908245172336 -> 1908245116384
	1908245116384 [label=AccumulateGrad]
	1908245110288 -> 1908245112064
	1908245173872 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1908245173872 -> 1908245110288
	1908245110288 [label=AccumulateGrad]
	1908245112016 -> 1908245111968
	1908245173968 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1908245173968 -> 1908245112016
	1908245112016 [label=AccumulateGrad]
	1908245111872 -> 1908245111968
	1908245174064 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1908245174064 -> 1908245111872
	1908245111872 [label=AccumulateGrad]
	1908245111776 -> 1908245111632
	1908245174448 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1908245174448 -> 1908245111776
	1908245111776 [label=AccumulateGrad]
	1908245111584 -> 1908245111488
	1908245174544 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1908245174544 -> 1908245111584
	1908245111584 [label=AccumulateGrad]
	1908245111536 -> 1908245111488
	1908245174640 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1908245174640 -> 1908245111536
	1908245111536 [label=AccumulateGrad]
	1908245111440 -> 1908245111392
	1908245111200 -> 1908245111056
	1908245175600 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1908245175600 -> 1908245111200
	1908245111200 [label=AccumulateGrad]
	1908245111008 -> 1908245110960
	1908245175696 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1908245175696 -> 1908245111008
	1908245111008 [label=AccumulateGrad]
	1908245110864 -> 1908245110960
	1908245175792 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1908245175792 -> 1908245110864
	1908245110864 [label=AccumulateGrad]
	1908245110768 -> 1908245110624
	1908245176176 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1908245176176 -> 1908245110768
	1908245110768 [label=AccumulateGrad]
	1908245110576 -> 1908245110528
	1908245176272 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1908245176272 -> 1908245110576
	1908245110576 [label=AccumulateGrad]
	1908245110672 -> 1908245110528
	1908245176368 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1908245176368 -> 1908245110672
	1908245110672 [label=AccumulateGrad]
	1908245110432 -> 1908245110192
	1908245110432 [label=CudnnBatchNormBackward0]
	1908245111152 -> 1908245110432
	1908245111152 [label=ConvolutionBackward0]
	1908245111248 -> 1908245111152
	1908245111296 -> 1908245111152
	1908245175024 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1908245175024 -> 1908245111296
	1908245111296 [label=AccumulateGrad]
	1908245110720 -> 1908245110432
	1908245175120 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1908245175120 -> 1908245110720
	1908245110720 [label=AccumulateGrad]
	1908245110480 -> 1908245110432
	1908245175216 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1908245175216 -> 1908245110480
	1908245110480 [label=AccumulateGrad]
	1908245116000 -> 1908245115808
	1908245176752 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1908245176752 -> 1908245116000
	1908245116000 [label=AccumulateGrad]
	1908245115760 -> 1908245115712
	1908245176848 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1908245176848 -> 1908245115760
	1908245115760 [label=AccumulateGrad]
	1908245115616 -> 1908245115712
	1908245176944 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1908245176944 -> 1908245115616
	1908245115616 [label=AccumulateGrad]
	1908245115520 -> 1908245115376
	1908245177328 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1908245177328 -> 1908245115520
	1908245115520 [label=AccumulateGrad]
	1908245115328 -> 1908245115232
	1908245177424 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1908245177424 -> 1908245115328
	1908245115328 [label=AccumulateGrad]
	1908245115280 -> 1908245115232
	1908245177520 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1908245177520 -> 1908245115280
	1908245115280 [label=AccumulateGrad]
	1908245115184 -> 1908245115136
	1908245114944 -> 1908245114800
	1908245178480 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1908245178480 -> 1908245114944
	1908245114944 [label=AccumulateGrad]
	1908245114752 -> 1908245114704
	1908245178576 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1908245178576 -> 1908245114752
	1908245114752 [label=AccumulateGrad]
	1908245114608 -> 1908245114704
	1908245178672 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1908245178672 -> 1908245114608
	1908245114608 [label=AccumulateGrad]
	1908245114512 -> 1908245114368
	1908245179056 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1908245179056 -> 1908245114512
	1908245114512 [label=AccumulateGrad]
	1908245114320 -> 1908245114224
	1908245179152 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1908245179152 -> 1908245114320
	1908245114320 [label=AccumulateGrad]
	1908245114272 -> 1908245114224
	1908245179248 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1908245179248 -> 1908245114272
	1908245114272 [label=AccumulateGrad]
	1908245114176 -> 1908245114128
	1908245114176 [label=CudnnBatchNormBackward0]
	1908245114896 -> 1908245114176
	1908245114896 [label=ConvolutionBackward0]
	1908245114992 -> 1908245114896
	1908245115040 -> 1908245114896
	1908245177904 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1908245177904 -> 1908245115040
	1908245115040 [label=AccumulateGrad]
	1908245114464 -> 1908245114176
	1908245178000 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1908245178000 -> 1908245114464
	1908245114464 [label=AccumulateGrad]
	1908245114416 -> 1908245114176
	1908245178096 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1908245178096 -> 1908245114416
	1908245114416 [label=AccumulateGrad]
	1908245114032 -> 1908245113840
	1908245179632 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1908245179632 -> 1908245114032
	1908245114032 [label=AccumulateGrad]
	1908245113792 -> 1908245113744
	1908245179728 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1908245179728 -> 1908245113792
	1908245113792 [label=AccumulateGrad]
	1908245113648 -> 1908245113744
	1908245179824 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1908245179824 -> 1908245113648
	1908245113648 [label=AccumulateGrad]
	1908245113552 -> 1908245113408
	1908245180208 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1908245180208 -> 1908245113552
	1908245113552 [label=AccumulateGrad]
	1908245113360 -> 1908245113264
	1908245180304 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1908245180304 -> 1908245113360
	1908245113360 [label=AccumulateGrad]
	1908245113312 -> 1908245113264
	1908245180400 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1908245180400 -> 1908245113312
	1908245113312 [label=AccumulateGrad]
	1908245113216 -> 1908245113168
	1908245112880 -> 1908245112640
	1908245112880 [label=TBackward0]
	1908245113120 -> 1908245112880
	1908245181072 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1908245181072 -> 1908245113120
	1908245113120 [label=AccumulateGrad]
	1908245112640 -> 1908216310640
}
