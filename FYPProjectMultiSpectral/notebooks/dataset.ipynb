{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Library modules\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platformâ€™s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass, field  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch and Deep Learning Libaries\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "from torch.nn import functional as F  # Functional module for defining functions and loss functions.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Data handling and batching\n",
    "import torchvision  # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Image datasets and transformations.\n",
    "import torchvision.datasets as datasets  # Specific datasets for vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "from torchvision.datasets import MNIST, EuroSAT  # Standard datasets.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations.\n",
    "from torchvision.models import ResNet18_Weights  # ResNet-18 model with pretrained weights.\n",
    "from torchsummary import summary  # Model summary.\n",
    "import torchmetrics  # Model evaluation metrics.\n",
    "from torchmetrics import MeanMetric, Accuracy  # Accuracy metrics.\n",
    "from torchmetrics.classification import (\n",
    "    MultilabelF1Score, MultilabelRecall, MultilabelPrecision, MultilabelAccuracy\n",
    ")  # Classification metrics.\n",
    "from torchviz import make_dot  # Model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for class imbalance.\n",
    "from torchcam.methods import GradCAM  # Grad-CAM for model interpretability.\n",
    "from torchcam.utils import overlay_mask  # Overlay mask for visualizations.\n",
    "import pytorch_lightning as pl  # Training management.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, Callback  # Callbacks.\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Logger for TensorBoard.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation.\n",
    "from rasterio.enums import Resampling  # Resampling for raster resizing.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation, Analysis and Visualization Libraries\n",
    "import pandas as pd  # Data analysis and manipulation.\n",
    "import numpy as np  # Array operations and computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics.\n",
    "import matplotlib.pyplot as plt  # Static and interactive plotting.\n",
    "import seaborn as sns  # High-level interface for statistical graphics.\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm  # Progress bar for loops.\n",
    "from PIL import Image  # Image handling and manipulation.\n",
    "import ast  # Parsing Python code.\n",
    "import requests  # HTTP requests.\n",
    "import zstandard as zstd  # Compression and decompression.\n",
    "from collections import Counter  # Counting hashable objects.\n",
    "import certifi  # Certificates for HTTPS.\n",
    "import ssl  # Secure connections.\n",
    "import urllib.request  # URL handling.\n",
    "import kaggle  # Kaggle API for datasets.\n",
    "from IPython.display import Image  # Display images in notebooks.\n",
    "from pathlib import Path # File system path handling.\n",
    "from typing import Dict, List, Tuple  # Type hints.\n",
    "import sys  # System-specific parameters and functions.\n",
    "import time # Time access and conversions.\n",
    "import logging # Logging facility for Python.\n",
    "import json # JSON encoder and decoder.\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from contextlib import redirect_stdout\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "# Custom Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda (GPU: NVIDIA GeForce RTX 3050)\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42  \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"upb\"\n",
    "\n",
    "\n",
    "# Render plots\n",
    "%matplotlib inline\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device} {'(GPU: ' + torch.cuda.get_device_name(0) + ')' if device.type == 'cuda' else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clean_and_parse_labels(label_string):\n",
    "    cleaned_labels = label_string.replace(\" '\", \", '\").replace(\"[\", \"[\").replace(\"]\", \"]\")\n",
    "    return ast.literal_eval(cleaned_labels)\n",
    "\n",
    "# Function to normalize class weights\n",
    "def normalize_class_weights(class_weights):\n",
    "    total_weight = sum(class_weights)\n",
    "    return [weight / total_weight for weight in class_weights]\n",
    "\n",
    "# Function to calculate class weights based on the label counts of each category\n",
    "def calculate_class_weights(metadata_csv):\n",
    "    metadata_csv['labels'] = metadata_csv['labels'].apply(clean_and_parse_labels)\n",
    "\n",
    "    class_labels = set()\n",
    "    for labels in metadata_csv['labels']:\n",
    "        class_labels.update(labels)\n",
    "\n",
    "    label_counts = metadata_csv['labels'].explode().value_counts()\n",
    "    total_counts = label_counts.sum()\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = {label: total_counts / count for label, count in label_counts.items()}\n",
    "    class_weights_array = np.array([class_weights[label] for label in class_labels])\n",
    "    \n",
    "    # Normalize class weights\n",
    "    normalized_class_weights = normalize_class_weights(class_weights_array)\n",
    "    \n",
    "    return class_weights, normalized_class_weights\n",
    "\n",
    "# Function used to calculate the class labels within the metadata\n",
    "def calculate_class_labels(metadata_csv):\n",
    "    metadata_csv['labels'] = metadata_csv['labels'].apply(clean_and_parse_labels)\n",
    "\n",
    "    class_labels = set()\n",
    "    for labels in metadata_csv['labels']:\n",
    "        class_labels.update(labels)\n",
    "\n",
    "    return class_labels\n",
    "\n",
    "def decode_target(\n",
    "    target: list,\n",
    "    text_labels: bool = False,\n",
    "    threshold: float = 0.4,\n",
    "    cls_labels: dict = None,\n",
    "):\n",
    "    result = []\n",
    "    for i, x in enumerate(target):\n",
    "        if x >= threshold:\n",
    "            if text_labels:\n",
    "                result.append(cls_labels[i] + \"(\" + str(i) + \")\")\n",
    "            else:\n",
    "                result.append(str(i))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def get_band_indices(band_names, all_band_names):\n",
    "    return [all_band_names.index(band) for band in band_names]\n",
    "\n",
    "\n",
    "def get_labels_for_image(image_path, model, transform, patch_to_labels):\n",
    "    # Load and preprocess the image\n",
    "    with rasterio.open(image_path) as src:\n",
    "        bands = [2, 3, 4]  # Bands to combine for display\n",
    "        image = np.stack([src.read(band) for band in bands], axis=-1)\n",
    "        image = transform(image).unsqueeze(0).to(model.device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Get the predicted labels\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(image).sigmoid() > 0.5  # Apply sigmoid and threshold at 0.5\n",
    "        preds = preds.cpu().numpy().astype(int).flatten()\n",
    "\n",
    "    # Get the true labels\n",
    "    patch_id = os.path.basename(image_path).split('.')[0]\n",
    "    true_labels = patch_to_labels[patch_id]\n",
    "\n",
    "    return preds, true_labels, image\n",
    "\n",
    "def display_image(image_path):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        bands = [2, 3, 4]  # Bands to combine for display\n",
    "        image = np.stack([src.read(band) for band in bands], axis=-1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Image with Bands 2, 3, and 4\")\n",
    "        plt.show()\n",
    "\n",
    "def display_image_and_labels(image_path, model, transform, patch_to_labels):\n",
    "    # Display the image\n",
    "    display_image(image_path)\n",
    "\n",
    "    # Get predicted and true labels\n",
    "    preds, true_labels, _ = get_labels_for_image(image_path, model, transform, patch_to_labels)\n",
    "    print(f\"Predicted Labels: {preds}\")\n",
    "    print(f\"True Labels: {true_labels}\")\n",
    "\n",
    "def extract_number(string):\n",
    "    number_str = string.split('%')[0]\n",
    "    try:\n",
    "        number = float(number_str)\n",
    "        if number.is_integer():\n",
    "            return int(number)\n",
    "        return number\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Cannot extract a number from the string: {string}\")\n",
    "    \n",
    "\n",
    "# Define the hook function\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandNormalisation:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i in range(image.shape[0]):\n",
    "            image[i] = (image[i] - self.mean[i]) / self.std[i]\n",
    "        return image\n",
    "    \n",
    "class BandUnnormalisation:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i in range(image.shape[0]):\n",
    "            image[i] = (image[i] * self.std[i]) + self.mean[i]\n",
    "        return image\n",
    "    \n",
    "class ToTensor:\n",
    "    def __call__(self, image):\n",
    "        return torch.tensor(image, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: Configuration file for the project\n",
    "# Helper functions\n",
    "def denormalize(tensors, *, mean, std):\n",
    "    for c in range(12):\n",
    "        tensors[:, c, :, :].mul_(std[c]).add_(mean[c])\n",
    "\n",
    "    return torch.clamp(tensors, min=0.0, max=1.0)\n",
    "\n",
    "def encode_label(label: list, num_classes=19):\n",
    "    target = torch.zeros(num_classes)\n",
    "    for l in label:\n",
    "        if l in DatasetConfig.class_labels_dict:\n",
    "            target[DatasetConfig.class_labels_dict[l]] = 1.0\n",
    "    return target\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    metadata_path = r\"C:\\\\Users\\\\isaac\\\\Desktop\\BigEarthTests\\\\50%_BigEarthNet\\\\metadata_50_percent.csv\"\n",
    "    dataset_paths = {\n",
    "        \"0.5\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\0.5%_BigEarthNet\\CombinedImages\",\n",
    "        \"1\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\1%_BigEarthNet\\CombinedImages\",\n",
    "        \"5\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\5%_BigEarthNet\\CombinedImages\",\n",
    "        \"10\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\10%_BigEarthNet\\CombinedImages\",\n",
    "        \"50\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\50%_BigEarthNet\\CombinedImages\",\n",
    "        \"100\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\100%_BigEarthNet\\CombinedImages\"\n",
    "    }\n",
    "    metadata_paths = {\n",
    "        \"0.5\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\0.5%_BigEarthNet\\metadata_0.5_percent.csv\",\n",
    "        \"1\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\1%_BigEarthNet\\metadata_1_percent.csv\",\n",
    "        \"5\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\5%_BigEarthNet\\metadata_5_percent.csv\",\n",
    "        \"10\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\10%_BigEarthNet\\metadata_10_percent.csv\",\n",
    "        \"50\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\50%_BigEarthNet\\metadata_50_percent.csv\",\n",
    "        \"100\": r\"C:\\Users\\isaac\\Desktop\\BigEarthTests\\100%_BigEarthNet\\metadata_100_percent.csv\"\n",
    "    }\n",
    "    unwanted_metadata_file: str = r'C:\\Users\\isaac\\Downloads\\metadata_for_patches_with_snow_cloud_or_shadow.parquet'\n",
    "    unwanted_metadata_csv = pd.read_parquet(unwanted_metadata_file)\n",
    "\n",
    "    class_labels = calculate_class_labels(pd.read_csv(metadata_path))\n",
    "    class_labels = class_labels\n",
    "    class_labels_dict = {label: idx for idx, label in enumerate(class_labels)}\n",
    "    reversed_class_labels_dict = {idx: label for label, idx in class_labels_dict.items()}\n",
    "\n",
    "    num_classes: int = 19\n",
    "    band_channels: int = 12\n",
    "    valid_pct: float = 0.1\n",
    "    img_size: int = 120\n",
    "\n",
    "    rgb_bands = [\"B04\", \"B03\", \"B02\"]\n",
    "    rgb_nir_bands = [\"B04\", \"B03\", \"B02\", \"B08\"]\n",
    "    rgb_swir_bands = [\"B04\", \"B03\", \"B02\", \"B11\", \"B12\"]\n",
    "    rgb_nir_swir_bands = [\"B04\", \"B03\", \"B02\", \"B08\", \"B11\", \"B12\"]\n",
    "    all_imp_bands = [ \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B11\", \"B12\"]\n",
    "    all_bands = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]\n",
    "    \n",
    "    band_stats = {\n",
    "        \"mean\": {\n",
    "            \"B01\": 359.93681858037576,\n",
    "            \"B02\": 437.7795146920668,\n",
    "            \"B03\": 626.9061237185847,\n",
    "            \"B04\": 605.0589129818594,\n",
    "            \"B05\": 971.6512098450492,\n",
    "            \"B06\": 1821.9817358749056,\n",
    "            \"B07\": 2108.096240315571,\n",
    "            \"B08\": 2256.3215618504346,\n",
    "            \"B8A\": 2310.6351913265307,\n",
    "            \"B09\": 2311.6085833217353,\n",
    "            \"B11\": 1608.6865167942176,\n",
    "            \"B12\": 1017.1259618291762\n",
    "        },\n",
    "        \"std\": {\n",
    "            \"B01\": 583.5085769396974,\n",
    "            \"B02\": 648.4384481402268,\n",
    "            \"B03\": 639.2669907766995,\n",
    "            \"B04\": 717.5748664544205,\n",
    "            \"B05\": 761.8971822905785,\n",
    "            \"B06\": 1090.758232889144,\n",
    "            \"B07\": 1256.5524552734478,\n",
    "            \"B08\": 1349.2050493390414,\n",
    "            \"B8A\": 1287.1124261320342,\n",
    "            \"B09\": 1297.654379610044,\n",
    "            \"B11\": 1057.3350765979644,\n",
    "            \"B12\": 802.1790763840752\n",
    "        }\n",
    "    }\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    num_epochs: int = 10\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = os.cpu_count() // 2\n",
    "    learning_rate: float = 0.0001\n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 1e-4\n",
    "    lr_step_size: int = 7\n",
    "    lr_factor: float = 0.1\n",
    "    patience: int = 5\n",
    "    lr_patience: int = 5\n",
    "    dropout: float = 0.5\n",
    "\n",
    "    model_names: list = field(default_factory=lambda: [\n",
    "        'resnet18', \n",
    "        'resnet34', \n",
    "        'resnet50', \n",
    "        'resnet101', \n",
    "        'resnet152', \n",
    "        'densenet121', \n",
    "        'densenet169', \n",
    "        'densenet201', \n",
    "        'densenet161',\n",
    "        'efficientnet-b0',\n",
    "        'vgg16',\n",
    "        'vgg19'\n",
    "    ])\n",
    "\n",
    "@dataclass\n",
    "class ModuleConfig:\n",
    "    reduction: int = 16\n",
    "    ratio: int = 8\n",
    "    kernel_size: int = 3\n",
    "    dropout_rt: float = 0.1\n",
    "    activation: type = nn.ReLU\n",
    "\n",
    "@dataclass\n",
    "class TransformsConfig:\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=(120, 120), scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.33))\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((120, 120))\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((120, 120))\n",
    "    ])\n",
    "\n",
    "    normalisations = transforms.Compose([\n",
    "        BandNormalisation(\n",
    "            mean=[DatasetConfig.band_stats[\"mean\"][band] for band in DatasetConfig.all_bands],\n",
    "            std=[DatasetConfig.band_stats[\"std\"][band] for band in DatasetConfig.all_bands]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UnormalisationConfig:\n",
    "    unormalization = BandUnnormalisation(\n",
    "        mean=[DatasetConfig.band_stats[\"mean\"][band] for band in DatasetConfig.all_bands],\n",
    "        std=[DatasetConfig.band_stats[\"std\"][band] for band in DatasetConfig.all_bands]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetDatasetTIF(Dataset):\n",
    "    def __init__(self, *, df, root_dir, transforms=None, normalisation=None, is_test=False, selected_bands=None, metadata_csv=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.normalisation = normalisation\n",
    "        self.is_test = is_test\n",
    "        self.selected_bands = selected_bands if selected_bands is not None else DatasetConfig.rgb_bands\n",
    "        self.metadata = metadata_csv\n",
    "\n",
    "        self.image_paths = list(Path(root_dir).rglob(\"*.tif\"))\n",
    "        self.patch_to_labels = dict(zip(self.metadata['patch_id'], self.metadata['labels']))\n",
    "        self.image_paths = list(Path(root_dir).rglob(\"*.tif\"))\n",
    "\n",
    "        self.selected_band_indices = get_band_indices(self.selected_bands, DatasetConfig.all_bands)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = src.read()  \n",
    "            image = image[self.selected_band_indices, :, :]\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        if self.normalisation:\n",
    "            image = self.normalisation(image)\n",
    "\n",
    "        label = self.get_label(image_path)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_label(self, img_path):\n",
    "        img_path = Path(img_path) \n",
    "        patch_id = img_path.stem\n",
    "        labels = self.patch_to_labels.get(patch_id, None)\n",
    "\n",
    "        if labels is None:\n",
    "            return torch.zeros(DatasetConfig.num_classes)  \n",
    "    \n",
    "        if isinstance(labels, str):\n",
    "            cleaned_labels = labels.replace(\" '\", \", '\").replace(\"[\", \"[\").replace(\"]\", \"]\")\n",
    "            labels =  ast.literal_eval(cleaned_labels)\n",
    "        \n",
    "        encoded = encode_label(labels)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_csv = pd.read_csv(DatasetConfig.metadata_paths['0.5'])\n",
    "full_df = metadata_csv\n",
    "dataset_dir = DatasetConfig.dataset_paths['0.5']\n",
    "bands = DatasetConfig.all_bands\n",
    "dataset = BigEarthNetDatasetTIF(df=full_df, root_dir=dataset_dir, transforms=None, normalisation=TransformsConfig.normalisations, selected_bands=bands, metadata_csv=metadata_csv)\n",
    "dataset2 = BigEarthNetDatasetTIF(df=full_df, root_dir=dataset_dir, transforms=TransformsConfig.train_transforms, normalisation=TransformsConfig.normalisations, selected_bands=bands, metadata_csv=metadata_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 2352\n",
      "Number of classes: 19\n",
      "Image shape: torch.Size([12, 120, 120])\n",
      "Label shape: torch.Size([19])\n",
      "tensor([[[-0.2467, -0.2467, -0.2467,  ..., -0.2364, -0.2364, -0.2364],\n",
      "         [-0.2467, -0.2467, -0.2467,  ..., -0.2364, -0.2364, -0.2364],\n",
      "         [-0.2467, -0.2467, -0.2467,  ..., -0.2364, -0.2364, -0.2364],\n",
      "         ...,\n",
      "         [-0.2587, -0.2587, -0.2587,  ..., -0.2038, -0.2038, -0.2038],\n",
      "         [-0.2587, -0.2587, -0.2587,  ..., -0.2038, -0.2038, -0.2038],\n",
      "         [-0.2587, -0.2587, -0.2587,  ..., -0.2038, -0.2038, -0.2038]],\n",
      "\n",
      "        [[-0.3991, -0.3837, -0.4099,  ..., -0.4238, -0.4083, -0.3991],\n",
      "         [-0.3898, -0.4315, -0.4700,  ..., -0.4114, -0.4022, -0.4022],\n",
      "         [-0.3975, -0.4376, -0.4361,  ..., -0.4068, -0.3991, -0.4068],\n",
      "         ...,\n",
      "         [-0.4160, -0.3852, -0.4407,  ..., -0.3945, -0.3713, -0.3713],\n",
      "         [-0.4268, -0.4114, -0.4238,  ..., -0.3898, -0.3929, -0.3775],\n",
      "         [-0.3960, -0.3929, -0.4238,  ..., -0.3682, -0.3729, -0.3621]],\n",
      "\n",
      "        [[-0.4285, -0.4472, -0.3737,  ..., -0.4598, -0.4332, -0.4113],\n",
      "         [-0.3831, -0.4425, -0.3768,  ..., -0.4034, -0.4003, -0.4113],\n",
      "         [-0.4300, -0.4598, -0.4379,  ..., -0.4066, -0.4332, -0.4300],\n",
      "         ...,\n",
      "         [-0.4926, -0.4410, -0.4191,  ..., -0.4113, -0.3956, -0.3581],\n",
      "         [-0.4504, -0.4817, -0.4363,  ..., -0.3675, -0.3221, -0.2689],\n",
      "         [-0.4504, -0.5348, -0.4441,  ..., -0.3690, -0.2908, -0.2580]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.6407,  0.6407,  0.6407,  ...,  0.7255,  0.7255,  0.7255],\n",
      "         [ 0.6407,  0.6407,  0.6407,  ...,  0.7255,  0.7255,  0.7255],\n",
      "         [ 0.6407,  0.6407,  0.6407,  ...,  0.7255,  0.7255,  0.7255],\n",
      "         ...,\n",
      "         [ 0.1768,  0.1768,  0.1768,  ...,  0.9351,  0.9351,  0.9351],\n",
      "         [ 0.1768,  0.1768,  0.1768,  ...,  0.9351,  0.9351,  0.9351],\n",
      "         [ 0.1768,  0.1768,  0.1768,  ...,  0.9351,  0.9351,  0.9351]],\n",
      "\n",
      "        [[-0.5936, -0.5936, -0.3165,  ..., -0.7838, -0.5577, -0.5577],\n",
      "         [-0.5936, -0.5936, -0.3165,  ..., -0.7838, -0.5577, -0.5577],\n",
      "         [-0.5369, -0.5369, -0.4102,  ..., -0.7053, -0.6750, -0.6750],\n",
      "         ...,\n",
      "         [-0.5076, -0.5076, -0.6684,  ..., -0.3421, -0.2125, -0.2125],\n",
      "         [-0.7261, -0.7261, -0.7374,  ..., -0.3355, -0.2106, -0.2106],\n",
      "         [-0.7261, -0.7261, -0.7374,  ..., -0.3355, -0.2106, -0.2106]],\n",
      "\n",
      "        [[-0.7581, -0.7581, -0.5474,  ..., -0.8478, -0.7344, -0.7344],\n",
      "         [-0.7581, -0.7581, -0.5474,  ..., -0.8478, -0.7344, -0.7344],\n",
      "         [-0.7307, -0.7307, -0.5711,  ..., -0.8042, -0.7930, -0.7930],\n",
      "         ...,\n",
      "         [-0.6359, -0.6359, -0.7544,  ..., -0.5761, -0.4801, -0.4801],\n",
      "         [-0.7855, -0.7855, -0.8005,  ..., -0.6272, -0.5212, -0.5212],\n",
      "         [-0.7855, -0.7855, -0.8005,  ..., -0.6272, -0.5212, -0.5212]]])\n",
      "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.])\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset: \" + str(len(dataset)))\n",
    "print(\"Number of classes: \" + str(DatasetConfig.num_classes))\n",
    "image, label = dataset[0]\n",
    "print(\"Image shape: \" + str(image.shape))\n",
    "print(\"Label shape: \" + str(label.shape))\n",
    "\n",
    "print(image)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([12, 120, 120])\n",
      "Label shape: torch.Size([19])\n",
      "tensor([[[-0.6168, -0.6168, -0.6168,  ..., -0.6168, -0.6168, -0.6168],\n",
      "         [-0.6168, -0.6168, -0.6168,  ..., -0.6168, -0.6168, -0.6168],\n",
      "         [-0.6168, -0.6168, -0.6168,  ..., -0.6168, -0.6168, -0.6168],\n",
      "         ...,\n",
      "         [-0.6168, -0.6168, -0.6168,  ..., -0.6168, -0.6168, -0.6168],\n",
      "         [-0.6168, -0.6168, -0.6168,  ..., -0.6168, -0.6168, -0.6168],\n",
      "         [-0.6168, -0.6168, -0.6168,  ..., -0.6168, -0.6168, -0.6168]],\n",
      "\n",
      "        [[-0.6751, -0.6751, -0.6751,  ..., -0.6751, -0.6751, -0.6751],\n",
      "         [-0.6751, -0.6751, -0.6751,  ..., -0.6751, -0.6751, -0.6751],\n",
      "         [-0.6751, -0.6751, -0.6751,  ..., -0.6751, -0.6751, -0.6751],\n",
      "         ...,\n",
      "         [-0.6751, -0.6751, -0.6751,  ..., -0.6751, -0.6751, -0.6751],\n",
      "         [-0.6751, -0.6751, -0.6751,  ..., -0.6751, -0.6751, -0.6751],\n",
      "         [-0.6751, -0.6751, -0.6751,  ..., -0.6751, -0.6751, -0.6751]],\n",
      "\n",
      "        [[-0.9807, -0.9807, -0.9807,  ..., -0.9807, -0.9807, -0.9807],\n",
      "         [-0.9807, -0.9807, -0.9807,  ..., -0.9807, -0.9807, -0.9807],\n",
      "         [-0.9807, -0.9807, -0.9807,  ..., -0.9807, -0.9807, -0.9807],\n",
      "         ...,\n",
      "         [-0.9807, -0.9807, -0.9807,  ..., -0.9807, -0.9807, -0.9807],\n",
      "         [-0.9807, -0.9807, -0.9807,  ..., -0.9807, -0.9807, -0.9807],\n",
      "         [-0.9807, -0.9807, -0.9807,  ..., -0.9807, -0.9807, -0.9807]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7814, -1.7814, -1.7814,  ..., -1.7814, -1.7814, -1.7814],\n",
      "         [-1.7814, -1.7814, -1.7814,  ..., -1.7814, -1.7814, -1.7814],\n",
      "         [-1.7814, -1.7814, -1.7814,  ..., -1.7814, -1.7814, -1.7814],\n",
      "         ...,\n",
      "         [-1.7814, -1.7814, -1.7814,  ..., -1.7814, -1.7814, -1.7814],\n",
      "         [-1.7814, -1.7814, -1.7814,  ..., -1.7814, -1.7814, -1.7814],\n",
      "         [-1.7814, -1.7814, -1.7814,  ..., -1.7814, -1.7814, -1.7814]],\n",
      "\n",
      "        [[-1.5215, -1.5215, -1.5215,  ..., -1.5215, -1.5215, -1.5215],\n",
      "         [-1.5215, -1.5215, -1.5215,  ..., -1.5215, -1.5215, -1.5215],\n",
      "         [-1.5215, -1.5215, -1.5215,  ..., -1.5215, -1.5215, -1.5215],\n",
      "         ...,\n",
      "         [-1.5215, -1.5215, -1.5215,  ..., -1.5215, -1.5215, -1.5215],\n",
      "         [-1.5215, -1.5215, -1.5215,  ..., -1.5215, -1.5215, -1.5215],\n",
      "         [-1.5215, -1.5215, -1.5215,  ..., -1.5215, -1.5215, -1.5215]],\n",
      "\n",
      "        [[-1.2680, -1.2680, -1.2680,  ..., -1.2680, -1.2680, -1.2680],\n",
      "         [-1.2680, -1.2680, -1.2680,  ..., -1.2680, -1.2680, -1.2680],\n",
      "         [-1.2680, -1.2680, -1.2680,  ..., -1.2680, -1.2680, -1.2680],\n",
      "         ...,\n",
      "         [-1.2680, -1.2680, -1.2680,  ..., -1.2680, -1.2680, -1.2680],\n",
      "         [-1.2680, -1.2680, -1.2680,  ..., -1.2680, -1.2680, -1.2680],\n",
      "         [-1.2680, -1.2680, -1.2680,  ..., -1.2680, -1.2680, -1.2680]]])\n",
      "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.])\n"
     ]
    }
   ],
   "source": [
    "image2, label2 = dataset2[0]\n",
    "print(\"Image shape: \" + str(image2.shape))\n",
    "print(\"Label shape: \" + str(label2.shape))\n",
    "\n",
    "print(image2)\n",
    "print(label2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
