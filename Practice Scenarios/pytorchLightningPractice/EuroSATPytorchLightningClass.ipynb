{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python Libraries\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platformâ€™s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch-related Libraries (Deep Learning)\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Dataset and DataLoader for managing and batching data.\n",
    "import torchvision # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Datasets and transformations for image processing.\n",
    "import torchvision.datasets as datasets  # Datasets for computer vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Make grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations for image preprocessing.\n",
    "from torchsummary import summary # PyTorch model summary for Keras-like model summary.\n",
    "from torchviz import make_dot  # PyTorch model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for handling class imbalance in object detection.\n",
    "from torchmetrics import MeanMetric  # Intersection over Union (IoU) metric for object detection.\n",
    "from torchmetrics.classification import MultilabelF1Score, MultilabelRecall, MultilabelPrecision, MultilabelAccuracy  # Multilabel classification metrics.\n",
    "\n",
    "import pytorch_lightning as pl  # PyTorch Lightning for high-level training loops.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor # Callbacks for model checkpointing and learning rate monitoring.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Library for reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation functions.\n",
    "from rasterio.enums import Resampling  # Resampling methods used for resizing raster data.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation and Analysis Libraries\n",
    "import pandas as pd  # Data analysis and manipulation library for DataFrames and CSVs.\n",
    "import numpy as np  # Numpy for array operations and numerical computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics for classification models.\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating static and interactive visualizations.\n",
    "import seaborn as sns  # High-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm  # Progress bar for loops and processes.\n",
    "from PIL import Image  # Image handling, opening, manipulating, and saving.\n",
    "import ast  # Abstract Syntax Trees for parsing Python code.\n",
    "import requests  # HTTP library for sending requests.\n",
    "import zstandard as zstd  # Zstandard compression for fast compression and decompression.\n",
    "from collections import Counter # Counter for counting hashable objects.\n",
    "import certifi  # Certificates for verifying HTTPS requests.\n",
    "import ssl  # Secure Sockets Layer for secure connections.\n",
    "import urllib.request  # URL handling for requests.\n",
    "import kaggle # Kaggle API for downloading datasets.\n",
    "import zipfile # Work with ZIP archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)  # Set seed for reproducibility.\n",
    "\n",
    "dataset_path = r'D:\\Datasets\\eurosat\\2750' # Path to the EuroSAT dataset.\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')  # Set the default FP32 matmul precision to 'medium'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    img_size: int = 64  \n",
    "    input_size: int = 224\n",
    "    img_mean = [0.485, 0.456, 0.406]\n",
    "    img_std = [0.229, 0.224, 0.225]\n",
    "    num_classes: int = 10\n",
    "    num_epochs: int = 10\n",
    "    num_workers: int = 1\n",
    "    \n",
    "    batch_size: int = 64\n",
    "    train_size: float = 0.8\n",
    "    learning_rate: float = 0.001\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataset(Dataset):\n",
    "    # Initialize the EuroSAT dataset class\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset  # Store the dataset\n",
    "        self.transform = transform  # Store the optional transformation function\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]  # Get the image and label from the dataset at the specified index\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transformation if provided\n",
    "\n",
    "        return image, label  # Return the image and label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)  # Return the length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:  27000\n",
      "Class names: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'] and number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Create an ImageFolder dataset instance for the specified directory\n",
    "dataset = datasets.ImageFolder(dataset_path)\n",
    "print('Dataset length: ', len(dataset))\n",
    "\n",
    "class_names = dataset.classes  # Get the class names from the dataset\n",
    "num_classes = len(class_names)  # Get the number of classes\n",
    "\n",
    "print(f'Class names: {class_names} and number of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(Config.input_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(Config.img_mean, Config.img_std)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(Config.img_size),\n",
    "    transforms.CenterCrop(Config.img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(Config.img_mean, Config.img_std)\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset_path, train_size=0.8, batch_size=16, num_workers=2):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Define transformations\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not os.path.exists(self.dataset_path):\n",
    "            kaggle.api.dataset_download_files('apollo2506/eurosat-dataset', path=self.dataset_path, unzip=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Initialize the dataset without transformations\n",
    "        self.dataset = datasets.ImageFolder(self.dataset_path)\n",
    "\n",
    "        # Generate indices\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        split = int(np.floor(self.train_size * len(self.dataset)))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "        # Create subsets and apply transformations\n",
    "        self.train_dataset = EuroSATDataset(Subset(self.dataset, train_indices), transform=self.train_transform)\n",
    "        self.test_dataset = EuroSATDataset(Subset(self.dataset, test_indices), transform=self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, len(class_names))\n",
    "        self.model = self.model.to(Config.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=Config.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return TF.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a batch of data from the validation DataLoader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading batches:   0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "data_module = EuroSATDataModule()\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "valid_loader = data_module.val_dataloader()\n",
    "\n",
    "# Fetch a batch of data from the validation DataLoader with a progress bar\n",
    "print(\"Fetching a batch of data from the validation DataLoader...\")\n",
    "for images, labels in tqdm(valid_loader, desc=\"Loading batches\"):\n",
    "    print(f'Batch shape: {images.shape}, Labels shape: {labels.shape}')\n",
    "    break  # Just to test one batch\n",
    "print(\"Batch fetched successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params | Mode \n",
      "-----------------------------------------\n",
      "0 | model | ResNet | 11.2 M | train\n",
      "-----------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "68        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a25c4c0317845688928ed4d6962751b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "model = EuroSATModel()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=Config.num_epochs)  # Use gpus=1 if you have a GPU\n",
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
