{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70661e5-8ba8-4acc-9af5-0374651ce0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File handling and manipulation\n",
    "import os  # Provides functions for interacting with the operating system, such as reading and writing files and directories.\n",
    "import shutil  # Offers high-level file operations like copying and moving files.\n",
    "from PIL import Image  # Python Imaging Library (PIL) for opening, manipulating, and saving image files.\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd  # Data analysis and manipulation library providing data structures like DataFrames.\n",
    "import numpy as np  # Provides support for large arrays and matrices, along with mathematical functions to operate on these arrays.\n",
    "\n",
    "# Geospatial data processing\n",
    "import rasterio  # Library for reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject # Functions for transforming and resizing raster data.\n",
    "from rasterio.enums import Resampling # Functions for transforming and resizing raster data.\n",
    "from rasterio.plot import show  # Function for visualizing raster data.\n",
    "\n",
    "# Visualization and plotting\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating static, animated, and interactive visualizations in Python.\n",
    "import seaborn as sns  # Data visualization library based on matplotlib, providing a high-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "# Utilities\n",
    "import random  # Provides functions for generating random numbers and performing random operations.\n",
    "from tqdm import tqdm  # Library for displaying progress bars in loops or processes.\n",
    "import textwrap  # Utilities for wrapping and formatting text to fit a specific width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704ad7ff-9e60-4d1b-aadc-7e3105057d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata.parquet')\n",
    "# Load the metadata for the snow/cloud/shadow data\n",
    "snow_cloud_shadow_metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata_for_patches_with_snow_cloud_or_shadow.parquet')\n",
    "\n",
    "# Base directories\n",
    "source_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\BigEarthSubset1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99317162-bfa5-4fe6-b515-6cb2c21c758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame contains 480038 rows.\n",
      "\n",
      "Column Names:\n",
      "Index(['patch_id', 'labels', 'split', 'country', 's1_name', 's2v1_name',\n",
      "       'contains_seasonal_snow', 'contains_cloud_or_shadow'],\n",
      "      dtype='object') \n",
      "\n",
      "First few rows of the DataFrame:\n",
      "                                            patch_id  \\\n",
      "0  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "1  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "2  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "3  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "4  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "\n",
      "                                              labels split  country  \\\n",
      "0  [Arable land, Broad-leaved forest, Mixed fores...  test  Austria   \n",
      "1  [Arable land, Broad-leaved forest, Inland wate...  test  Austria   \n",
      "2  [Arable land, Broad-leaved forest, Coniferous ...  test  Austria   \n",
      "3  [Broad-leaved forest, Complex cultivation patt...  test  Austria   \n",
      "4  [Broad-leaved forest, Complex cultivation patt...  test  Austria   \n",
      "\n",
      "                                        s1_name  \\\n",
      "0  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_26_57   \n",
      "1  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_55   \n",
      "2  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_56   \n",
      "3  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_57   \n",
      "4  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_58   \n",
      "\n",
      "                          s2v1_name  contains_seasonal_snow  \\\n",
      "0  S2A_MSIL2A_20170613T101031_26_57                   False   \n",
      "1  S2A_MSIL2A_20170613T101031_27_55                   False   \n",
      "2  S2A_MSIL2A_20170613T101031_27_56                   False   \n",
      "3  S2A_MSIL2A_20170613T101031_27_57                   False   \n",
      "4  S2A_MSIL2A_20170613T101031_27_58                   False   \n",
      "\n",
      "   contains_cloud_or_shadow  \n",
      "0                     False  \n",
      "1                     False  \n",
      "2                     False  \n",
      "3                     False  \n",
      "4                     False  \n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the DataFrame\n",
    "num_rows = metadata_df.shape[0]\n",
    "\n",
    "# Print the number of rows in the DataFrame, followed by a blank line\n",
    "print(f\"The DataFrame contains {num_rows} rows.\\n\")\n",
    "\n",
    "# Display the column names in the DataFrame, followed by a blank line\n",
    "print(\"Column Names:\")\n",
    "print(metadata_df.columns, \"\\n\")\n",
    "\n",
    "# Display the first few rows of the DataFrame with a preceding message\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a94c1d0-a005-48f7-97df-474a4f6ba635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels:\n",
      "1. Arable land\n",
      "2. Broad-leaved forest\n",
      "3. Mixed forest\n",
      "4. Pastures\n",
      "5. Inland waters\n",
      "6. Coniferous forest\n",
      "7. Complex cultivation patterns\n",
      "8. Land principally occupied by agriculture, with significant areas of natural vegetation\n",
      "9. Urban fabric\n",
      "10. Industrial or commercial units\n",
      "11. Inland wetlands\n",
      "12. Transitional woodland, shrub\n",
      "13. Natural grassland and sparsely vegetated areas\n",
      "14. Moors, heathland and sclerophyllous vegetation\n",
      "15. Marine waters\n",
      "16. Coastal wetlands\n",
      "17. Permanent crops\n",
      "18. Beaches, dunes, sands\n",
      "19. Agro-forestry areas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique labels from the metadata dataframe\n",
    "unique_labels = metadata_df['labels'].explode().unique()\n",
    "\n",
    "# Print the unique labels in a numbered table format\n",
    "print(\"Unique Labels:\")\n",
    "for i, label in enumerate(unique_labels, start=1):\n",
    "    print(f\"{i}. {label}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d9067c-cc08-4051-907c-4449e4c6bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_vector(labels, categories):\n",
    "    vector = [1 if category in labels else 0 for category in categories]\n",
    "    return vector\n",
    "\n",
    "# Step 3: Apply the function to each row in the DataFrame\n",
    "metadata_df['binary_vector'] = metadata_df['labels'].apply(lambda labels: create_binary_vector(labels, unique_labels))\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "#print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae140aa4-495e-4a68-8170-34428a34dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame contains 69450 rows.\n",
      "\n",
      "Column Names:\n",
      "Index(['patch_id', 'labels', 'split', 'country', 's1_name', 's2v1_name',\n",
      "       'contains_seasonal_snow', 'contains_cloud_or_shadow'],\n",
      "      dtype='object') \n",
      "\n",
      "First few rows of the DataFrame:\n",
      "                                            patch_id  \\\n",
      "0  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "1  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "2  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "3  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "4  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_4...   \n",
      "\n",
      "                                              labels  split  country  \\\n",
      "0                   [Arable land, Coniferous forest]   test  Austria   \n",
      "1  [Arable land, Coniferous forest, Land principa...   test  Austria   \n",
      "2  [Broad-leaved forest, Coniferous forest, Indus...  train  Austria   \n",
      "3  [Arable land, Broad-leaved forest, Land princi...   test  Austria   \n",
      "4  [Arable land, Broad-leaved forest, Land princi...   test  Austria   \n",
      "\n",
      "                                        s1_name  \\\n",
      "0  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_38_58   \n",
      "1  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_38_59   \n",
      "2  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_38_87   \n",
      "3  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_39_59   \n",
      "4  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_40_59   \n",
      "\n",
      "                          s2v1_name  contains_seasonal_snow  \\\n",
      "0  S2A_MSIL2A_20170613T101031_38_58                   False   \n",
      "1  S2A_MSIL2A_20170613T101031_38_59                   False   \n",
      "2  S2A_MSIL2A_20170613T101031_38_87                   False   \n",
      "3  S2A_MSIL2A_20170613T101031_39_59                   False   \n",
      "4  S2A_MSIL2A_20170613T101031_40_59                   False   \n",
      "\n",
      "   contains_cloud_or_shadow  \n",
      "0                      True  \n",
      "1                      True  \n",
      "2                      True  \n",
      "3                      True  \n",
      "4                      True  \n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the DataFrame\n",
    "num_rows = snow_cloud_shadow_metadata_df.shape[0]\n",
    "\n",
    "# Print the number of rows in the DataFrame, followed by a blank line\n",
    "print(f\"The DataFrame contains {num_rows} rows.\\n\")\n",
    "\n",
    "# Display the column names in the DataFrame, followed by a blank line\n",
    "print(\"Column Names:\")\n",
    "print(snow_cloud_shadow_metadata_df.columns, \"\\n\")\n",
    "\n",
    "# Display the first few rows of the DataFrame with a preceding message\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(snow_cloud_shadow_metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04fbb98-cc39-476f-9bd6-7ccef240fbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 3/3 [05:34<00:00, 111.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total True (contains_cloud_or_shadow): 0\n",
      "Total False (contains_cloud_or_shadow): 14987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the metadata DataFrame\n",
    "snow_cloud_shadow_metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata_for_patches_with_snow_cloud_or_shadow.parquet')\n",
    "\n",
    "# Initialize counters\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "# Base directory path\n",
    "base_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\BigEarthSubset1'\n",
    "\n",
    "# Iterate through all folders in the base directory\n",
    "for folder_name in tqdm(os.listdir(base_dir), desc=\"Processing folders\"):\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    for patch_name in os.listdir(folder_path):\n",
    "        patch_path = os.path.join(folder_path, patch_name)\n",
    "\n",
    "        # Filter the DataFrame to get the row with the matching patch_id\n",
    "        matching_row = snow_cloud_shadow_metadata_df.loc[metadata_df['patch_id'] == patch_name]\n",
    "\n",
    "         # If a match is found, extract the labels\n",
    "        if not matching_row.empty:\n",
    "            contains_cloud_or_shadow = matching_row['contains_cloud_or_shadow'].values[0]\n",
    "            if contains_cloud_or_shadow:\n",
    "                true_count += 1\n",
    "                shutil.rmtree(patch_path)\n",
    "            else:\n",
    "                false_count += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total True (contains_cloud_or_shadow): {true_count}\")\n",
    "print(f\"Total False (contains_cloud_or_shadow): {false_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bad63e38-42bd-4a9c-a8ae-2bac29b4a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 69450/69450 [03:37<00:00, 320.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total deleted folders: 69450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Base directories\n",
    "source_dir = r'D:\\Datasets\\BigEarthNet-S2\\100%BigEarthNet'\n",
    "\n",
    "# Initialize a counter for deleted folders\n",
    "deleted_folders_count = 0\n",
    "\n",
    "# Loop through each row in the DataFrame with a progress bar\n",
    "for index, row in tqdm(snow_cloud_shadow_metadata_df.iterrows(), total=len(snow_cloud_shadow_metadata_df), desc=\"Processing files\"):\n",
    "    patch_id = row['patch_id']  # Adjust if the column name differs\n",
    "    # Extract the folder name (all but the last part of the patch_id)\n",
    "    folder_name = '_'.join(patch_id.split('_')[:-2])\n",
    "    dest_dir = os.path.join(source_dir, folder_name, patch_id)  # Use os.path.join for better path handling\n",
    "    \n",
    "    # If dest_dir exists, delete it\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)  # Remove the directory and its contents\n",
    "        deleted_folders_count += 1  # Increment the counter\n",
    "\n",
    "# Print the total number of deleted folders\n",
    "print(f\"Total deleted folders: {deleted_folders_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a619b40-3cd2-47af-88a5-3d2b5dd8847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 115/115 [5:06:14<00:00, 159.78s/it] \n"
     ]
    }
   ],
   "source": [
    "# Base directory containing all data\n",
    "base_dir = r'D:\\Datasets\\BigEarthNet-S2\\100%BigEarthNet'\n",
    "# Directories for subsets\n",
    "subsets = {\n",
    "    '50%': r'D:\\Datasets\\BigEarthNet-S2\\50%BigEarthNet',\n",
    "    '10%': r'D:\\Datasets\\BigEarthNet-S2\\10%BigEarthNet',\n",
    "    '1%': r'D:\\Datasets\\BigEarthNet-S2\\1%BigEarthNet'\n",
    "}\n",
    "\n",
    "# Create subset directories if they don't exist\n",
    "for subset_dir in subsets.values():\n",
    "    os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through all folders in the base directory\n",
    "for folder in tqdm(os.listdir(base_dir), desc=\"Processing folders\"):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # Check if the current path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # List all subfolders\n",
    "        subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "        \n",
    "        # Calculate number of subfolders for each subset\n",
    "        num_subfolders = len(subfolders)\n",
    "        num_50_percent = max(1, num_subfolders // 2)  # Ensure at least one folder is copied\n",
    "        num_10_percent = max(1, num_subfolders // 10)\n",
    "        num_1_percent = max(1, num_subfolders // 100)\n",
    "\n",
    "        # Randomly select subfolders for each subset\n",
    "        selected_50 = random.sample(subfolders, num_50_percent)\n",
    "        selected_10 = random.sample(subfolders, num_10_percent)\n",
    "        selected_1 = random.sample(subfolders, num_1_percent)\n",
    "\n",
    "        # Copy selected subfolders to the respective subset directories\n",
    "        for selected in selected_50:\n",
    "            shutil.copytree(os.path.join(folder_path, selected), os.path.join(subsets['50%'], folder, selected))\n",
    "\n",
    "        for selected in selected_10:\n",
    "            shutil.copytree(os.path.join(folder_path, selected), os.path.join(subsets['10%'], folder, selected))\n",
    "\n",
    "        for selected in selected_1:\n",
    "            shutil.copytree(os.path.join(folder_path, selected), os.path.join(subsets['1%'], folder, selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db687ba8-644a-4c45-a706-361872f549f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_subfolders(base_dir, folder):\n",
    "    # Dictionary to hold folder counts\n",
    "    folder_counts = {}\n",
    "    total_subfolders = 0  # Initialize total subfolder counter\n",
    "    \n",
    "    # Iterate through all folders in the base directory\n",
    "    for folder in tqdm(os.listdir(base_dir), desc=\"Processing folders\"):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        \n",
    "        # Check if the current path is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Count subdirectories within this folder\n",
    "            subfolder_count = sum(os.path.isdir(os.path.join(folder_path, subfolder)) for subfolder in os.listdir(folder_path))\n",
    "            folder_counts[folder] = subfolder_count\n",
    "        \n",
    "            # Update total subfolder count\n",
    "            total_subfolders += subfolder_count\n",
    "\n",
    "    # Print total subfolders\n",
    "    return total_subfolders, folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca28a9fc-4c57-47ed-adea-740e5fc35cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 115/115 [02:46<00:00,  1.44s/it]\n",
      "Processing folders: 100%|██████████| 115/115 [01:08<00:00,  1.69it/s]\n",
      "Processing folders: 100%|██████████| 115/115 [00:11<00:00,  9.91it/s]\n",
      "Processing folders: 100%|██████████| 115/115 [00:00<00:00, 518.06it/s]\n"
     ]
    }
   ],
   "source": [
    "full_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\100%BigEarthNet', '100%BigEarthNet')\n",
    "half_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\50%BigEarthNet', '50%BigEarthNet' )\n",
    "tenth_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\10%BigEarthNet', '10%BigEarthNet' )\n",
    "hundredth_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\1%BigEarthNet', '1%BigEarthNet' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "381ba19d-0d14-4074-9d45-a9d7c8b20c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subfolder count in full dataset: 480038\n",
      "\n",
      "Folder: 50%BigEarthNet | Subfolder Count: 239988 | Percentage: 49.99%\n",
      "Folder: 10%BigEarthNet | Subfolder Count: 47948 | Percentage: 9.99%\n",
      "Folder: 1%BigEarthNet | Subfolder Count: 4750 | Percentage: 0.99%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate and display subfolder count and percentage\n",
    "def display_percentage(partial_count, full_count, folder_name):\n",
    "    percentage = (partial_count / full_count) * 100\n",
    "    print(f\"Folder: {folder_name} | Subfolder Count: {partial_count} | Percentage: {percentage:.2f}%\")\n",
    "\n",
    "# Display the counts and percentages for each folder\n",
    "print(f\"Total subfolder count in full dataset: {full_subfolder_count}\\n\")\n",
    "display_percentage(half_subfolder_count, full_subfolder_count, '50%BigEarthNet')\n",
    "display_percentage(tenth_subfolder_count, full_subfolder_count, '10%BigEarthNet')\n",
    "display_percentage(hundredth_subfolder_count, full_subfolder_count, '1%BigEarthNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d7b54-a315-44ea-87be-eafbe846569e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
