digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2680217800144 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2680272984944 [label=SigmoidBackward0]
	2680272984416 -> 2680272984944
	2680272984416 [label=AddmmBackward0]
	2680272992720 -> 2680272984416
	2680216482928 [label="model.fc.bias
 (19)" fillcolor=lightblue]
	2680216482928 -> 2680272992720
	2680272992720 [label=AccumulateGrad]
	2680272993584 -> 2680272984416
	2680272993584 [label=ViewBackward0]
	2680272984320 -> 2680272993584
	2680272984320 [label=MeanBackward1]
	2680272984704 -> 2680272984320
	2680272984704 [label=ReluBackward0]
	2680272980816 -> 2680272984704
	2680272980816 [label=AddBackward0]
	2680217468688 -> 2680272980816
	2680217468688 [label=CudnnBatchNormBackward0]
	2680217468832 -> 2680217468688
	2680217468832 [label=ConvolutionBackward0]
	2680217467392 -> 2680217468832
	2680217467392 [label=ReluBackward0]
	2680217466768 -> 2680217467392
	2680217466768 [label=CudnnBatchNormBackward0]
	2680217459616 -> 2680217466768
	2680217459616 [label=ConvolutionBackward0]
	2680217462496 -> 2680217459616
	2680217462496 [label=ReluBackward0]
	2680217468160 -> 2680217462496
	2680217468160 [label=AddBackward0]
	2680217768368 -> 2680217468160
	2680217768368 [label=CudnnBatchNormBackward0]
	2680217768224 -> 2680217768368
	2680217768224 [label=ConvolutionBackward0]
	2680217768128 -> 2680217768224
	2680217768128 [label=ReluBackward0]
	2680217768032 -> 2680217768128
	2680217768032 [label=CudnnBatchNormBackward0]
	2680217768848 -> 2680217768032
	2680217768848 [label=ConvolutionBackward0]
	2680217767648 -> 2680217768848
	2680217767648 [label=ReluBackward0]
	2680217767552 -> 2680217767648
	2680217767552 [label=AddBackward0]
	2680217767456 -> 2680217767552
	2680217767456 [label=CudnnBatchNormBackward0]
	2680217767312 -> 2680217767456
	2680217767312 [label=ConvolutionBackward0]
	2680217767216 -> 2680217767312
	2680217767216 [label=ReluBackward0]
	2680217767120 -> 2680217767216
	2680217767120 [label=CudnnBatchNormBackward0]
	2680217767024 -> 2680217767120
	2680217767024 [label=ConvolutionBackward0]
	2680217767504 -> 2680217767024
	2680217767504 [label=ReluBackward0]
	2680217766880 -> 2680217767504
	2680217766880 [label=AddBackward0]
	2680217766784 -> 2680217766880
	2680217766784 [label=CudnnBatchNormBackward0]
	2680217766640 -> 2680217766784
	2680217766640 [label=ConvolutionBackward0]
	2680217766544 -> 2680217766640
	2680217766544 [label=ReluBackward0]
	2680217766448 -> 2680217766544
	2680217766448 [label=CudnnBatchNormBackward0]
	2680217766352 -> 2680217766448
	2680217766352 [label=ConvolutionBackward0]
	2680217766256 -> 2680217766352
	2680217766256 [label=ReluBackward0]
	2680217766160 -> 2680217766256
	2680217766160 [label=AddBackward0]
	2680217766064 -> 2680217766160
	2680217766064 [label=CudnnBatchNormBackward0]
	2680217765920 -> 2680217766064
	2680217765920 [label=ConvolutionBackward0]
	2680217765824 -> 2680217765920
	2680217765824 [label=ReluBackward0]
	2680217765536 -> 2680217765824
	2680217765536 [label=CudnnBatchNormBackward0]
	2680217765728 -> 2680217765536
	2680217765728 [label=ConvolutionBackward0]
	2680217766112 -> 2680217765728
	2680217766112 [label=ReluBackward0]
	2680217765296 -> 2680217766112
	2680217765296 [label=AddBackward0]
	2680217768752 -> 2680217765296
	2680217768752 [label=CudnnBatchNormBackward0]
	2680217768656 -> 2680217768752
	2680217768656 [label=ConvolutionBackward0]
	2680217764048 -> 2680217768656
	2680217764048 [label=ReluBackward0]
	2680217764096 -> 2680217764048
	2680217764096 [label=CudnnBatchNormBackward0]
	2680217764624 -> 2680217764096
	2680217764624 [label=ConvolutionBackward0]
	2680217765008 -> 2680217764624
	2680217765008 [label=ReluBackward0]
	2680217768704 -> 2680217765008
	2680217768704 [label=AddBackward0]
	2680217767888 -> 2680217768704
	2680217767888 [label=CudnnBatchNormBackward0]
	2680217768896 -> 2680217767888
	2680217768896 [label=ConvolutionBackward0]
	2680217768992 -> 2680217768896
	2680217768992 [label=ReluBackward0]
	2680217769088 -> 2680217768992
	2680217769088 [label=CudnnBatchNormBackward0]
	2680217769184 -> 2680217769088
	2680217769184 [label=ConvolutionBackward0]
	2680217768800 -> 2680217769184
	2680217768800 [label=ReluBackward0]
	2680217769328 -> 2680217768800
	2680217769328 [label=AddBackward0]
	2680217769424 -> 2680217769328
	2680217769424 [label=CudnnBatchNormBackward0]
	2680217769568 -> 2680217769424
	2680217769568 [label=ConvolutionBackward0]
	2680217769664 -> 2680217769568
	2680217769664 [label=ReluBackward0]
	2680217769760 -> 2680217769664
	2680217769760 [label=CudnnBatchNormBackward0]
	2680217769856 -> 2680217769760
	2680217769856 [label=ConvolutionBackward0]
	2680217769376 -> 2680217769856
	2680217769376 [label=MaxPool2DWithIndicesBackward0]
	2680217770000 -> 2680217769376
	2680217770000 [label=ReluBackward0]
	2680217770096 -> 2680217770000
	2680217770096 [label=CudnnBatchNormBackward0]
	2680217770192 -> 2680217770096
	2680217770192 [label=ConvolutionBackward0]
	2680217765632 -> 2680217770192
	2680214043152 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2680214043152 -> 2680217765632
	2680217765632 [label=AccumulateGrad]
	2680217765392 -> 2680217770096
	2680216436272 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	2680216436272 -> 2680217765392
	2680217765392 [label=AccumulateGrad]
	2680217765056 -> 2680217770096
	2680216434256 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	2680216434256 -> 2680217765056
	2680217765056 [label=AccumulateGrad]
	2680217765152 -> 2680217769856
	2680216470064 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2680216470064 -> 2680217765152
	2680217765152 [label=AccumulateGrad]
	2680217764912 -> 2680217769760
	2680216470160 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2680216470160 -> 2680217764912
	2680217764912 [label=AccumulateGrad]
	2680217764768 -> 2680217769760
	2680216470256 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2680216470256 -> 2680217764768
	2680217764768 [label=AccumulateGrad]
	2680217764672 -> 2680217769568
	2680216470640 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2680216470640 -> 2680217764672
	2680217764672 [label=AccumulateGrad]
	2680217764480 -> 2680217769424
	2680216470736 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2680216470736 -> 2680217764480
	2680217764480 [label=AccumulateGrad]
	2680217764432 -> 2680217769424
	2680216470832 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2680216470832 -> 2680217764432
	2680217764432 [label=AccumulateGrad]
	2680217769376 -> 2680217769328
	2680217764192 -> 2680217769184
	2680216471216 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2680216471216 -> 2680217764192
	2680217764192 [label=AccumulateGrad]
	2680217763952 -> 2680217769088
	2680216471312 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2680216471312 -> 2680217763952
	2680217763952 [label=AccumulateGrad]
	2680217763904 -> 2680217769088
	2680216471408 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2680216471408 -> 2680217763904
	2680217763904 [label=AccumulateGrad]
	2680217468736 -> 2680217768896
	2680216471696 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2680216471696 -> 2680217468736
	2680217468736 [label=AccumulateGrad]
	2680217468544 -> 2680217767888
	2680216471792 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2680216471792 -> 2680217468544
	2680217468544 [label=AccumulateGrad]
	2680217468496 -> 2680217767888
	2680216471888 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2680216471888 -> 2680217468496
	2680217468496 [label=AccumulateGrad]
	2680217768800 -> 2680217768704
	2680217467968 -> 2680217764624
	2680216474576 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2680216474576 -> 2680217467968
	2680217467968 [label=AccumulateGrad]
	2680217468112 -> 2680217764096
	2680216474672 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2680216474672 -> 2680217468112
	2680217468112 [label=AccumulateGrad]
	2680217467824 -> 2680217764096
	2680216474768 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2680216474768 -> 2680217467824
	2680217467824 [label=AccumulateGrad]
	2680217467920 -> 2680217768656
	2680216475152 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2680216475152 -> 2680217467920
	2680217467920 [label=AccumulateGrad]
	2680217467536 -> 2680217768752
	2680216475248 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2680216475248 -> 2680217467536
	2680217467536 [label=AccumulateGrad]
	2680217452608 -> 2680217768752
	2680216475344 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2680216475344 -> 2680217452608
	2680217452608 [label=AccumulateGrad]
	2680217765488 -> 2680217765296
	2680217765488 [label=CudnnBatchNormBackward0]
	2680272981344 -> 2680217765488
	2680272981344 [label=ConvolutionBackward0]
	2680217765008 -> 2680272981344
	2680272981968 -> 2680272981344
	2680216472272 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2680216472272 -> 2680272981968
	2680272981968 [label=AccumulateGrad]
	2680272981056 -> 2680217765488
	2680216474096 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2680216474096 -> 2680272981056
	2680272981056 [label=AccumulateGrad]
	2680272982112 -> 2680217765488
	2680216474192 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2680216474192 -> 2680272982112
	2680272982112 [label=AccumulateGrad]
	2680217454576 -> 2680217765728
	2680216475728 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2680216475728 -> 2680217454576
	2680217454576 [label=AccumulateGrad]
	2680217467248 -> 2680217765536
	2680216475824 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2680216475824 -> 2680217467248
	2680217467248 [label=AccumulateGrad]
	2680217466816 -> 2680217765536
	2680216475920 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2680216475920 -> 2680217466816
	2680217466816 [label=AccumulateGrad]
	2680217466960 -> 2680217765920
	2680216476304 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2680216476304 -> 2680217466960
	2680217466960 [label=AccumulateGrad]
	2680217466624 -> 2680217766064
	2680216476400 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2680216476400 -> 2680217466624
	2680217466624 [label=AccumulateGrad]
	2680217465520 -> 2680217766064
	2680216476496 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2680216476496 -> 2680217465520
	2680217465520 [label=AccumulateGrad]
	2680217766112 -> 2680217766160
	2680217466048 -> 2680217766352
	2680216477456 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2680216477456 -> 2680217466048
	2680217466048 [label=AccumulateGrad]
	2680217458944 -> 2680217766448
	2680216477552 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2680216477552 -> 2680217458944
	2680217458944 [label=AccumulateGrad]
	2680217466720 -> 2680217766448
	2680216477648 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2680216477648 -> 2680217466720
	2680217466720 [label=AccumulateGrad]
	2680217462688 -> 2680217766640
	2680216478032 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2680216478032 -> 2680217462688
	2680217462688 [label=AccumulateGrad]
	2680217461488 -> 2680217766784
	2680216478128 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2680216478128 -> 2680217461488
	2680217461488 [label=AccumulateGrad]
	2680217461872 -> 2680217766784
	2680216478224 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2680216478224 -> 2680217461872
	2680217461872 [label=AccumulateGrad]
	2680217766832 -> 2680217766880
	2680217766832 [label=CudnnBatchNormBackward0]
	2680272989888 -> 2680217766832
	2680272989888 [label=ConvolutionBackward0]
	2680217766256 -> 2680272989888
	2680272988304 -> 2680272989888
	2680216476880 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2680216476880 -> 2680272988304
	2680272988304 [label=AccumulateGrad]
	2680272990608 -> 2680217766832
	2680216476976 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2680216476976 -> 2680272990608
	2680272990608 [label=AccumulateGrad]
	2680272984752 -> 2680217766832
	2680216477072 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2680216477072 -> 2680272984752
	2680272984752 [label=AccumulateGrad]
	2680217462112 -> 2680217767024
	2680216478608 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2680216478608 -> 2680217462112
	2680217462112 [label=AccumulateGrad]
	2680217462880 -> 2680217767120
	2680216478704 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2680216478704 -> 2680217462880
	2680217462880 [label=AccumulateGrad]
	2680217462256 -> 2680217767120
	2680216478800 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2680216478800 -> 2680217462256
	2680217462256 [label=AccumulateGrad]
	2680217463408 -> 2680217767312
	2680216479184 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2680216479184 -> 2680217463408
	2680217463408 [label=AccumulateGrad]
	2680217462640 -> 2680217767456
	2680216479280 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2680216479280 -> 2680217462640
	2680217462640 [label=AccumulateGrad]
	2680217464368 -> 2680217767456
	2680216479376 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2680216479376 -> 2680217464368
	2680217464368 [label=AccumulateGrad]
	2680217767504 -> 2680217767552
	2680217464464 -> 2680217768848
	2680216480336 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2680216480336 -> 2680217464464
	2680217464464 [label=AccumulateGrad]
	2680217464128 -> 2680217768032
	2680216480432 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2680216480432 -> 2680217464128
	2680217464128 [label=AccumulateGrad]
	2680217464560 -> 2680217768032
	2680216480528 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2680216480528 -> 2680217464560
	2680217464560 [label=AccumulateGrad]
	2680217463840 -> 2680217768224
	2680216480912 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2680216480912 -> 2680217463840
	2680217463840 [label=AccumulateGrad]
	2680217464752 -> 2680217768368
	2680216481008 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2680216481008 -> 2680217464752
	2680217464752 [label=AccumulateGrad]
	2680217466288 -> 2680217768368
	2680216481104 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2680216481104 -> 2680217466288
	2680217466288 [label=AccumulateGrad]
	2680217768416 -> 2680217468160
	2680217768416 [label=CudnnBatchNormBackward0]
	2680272978032 -> 2680217768416
	2680272978032 [label=ConvolutionBackward0]
	2680217767648 -> 2680272978032
	2680272990944 -> 2680272978032
	2680216479760 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2680216479760 -> 2680272990944
	2680272990944 [label=AccumulateGrad]
	2680272980768 -> 2680217768416
	2680216479856 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2680216479856 -> 2680272980768
	2680272980768 [label=AccumulateGrad]
	2680272990032 -> 2680217768416
	2680216479952 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2680216479952 -> 2680272990032
	2680272990032 [label=AccumulateGrad]
	2680217459760 -> 2680217459616
	2680216481488 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2680216481488 -> 2680217459760
	2680217459760 [label=AccumulateGrad]
	2680217462592 -> 2680217466768
	2680216481584 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2680216481584 -> 2680217462592
	2680217462592 [label=AccumulateGrad]
	2680217461248 -> 2680217466768
	2680216481680 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2680216481680 -> 2680217461248
	2680217461248 [label=AccumulateGrad]
	2680217458032 -> 2680217468832
	2680216482064 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2680216482064 -> 2680217458032
	2680217458032 [label=AccumulateGrad]
	2680217466864 -> 2680217468688
	2680216482160 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2680216482160 -> 2680217466864
	2680217466864 [label=AccumulateGrad]
	2680217466000 -> 2680217468688
	2680216482256 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2680216482256 -> 2680217466000
	2680217466000 [label=AccumulateGrad]
	2680217462496 -> 2680272980816
	2680272979280 -> 2680272984416
	2680272979280 [label=TBackward0]
	2680272978320 -> 2680272979280
	2680216482832 [label="model.fc.weight
 (19, 512)" fillcolor=lightblue]
	2680216482832 -> 2680272978320
	2680272978320 [label=AccumulateGrad]
	2680272984944 -> 2680217800144
}
