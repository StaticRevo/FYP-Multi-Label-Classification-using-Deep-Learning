digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2865851861712 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2865845207872 [label=AddmmBackward0]
	2865845208016 -> 2865845207872
	2865983957488 [label="fc.bias
 (19)" fillcolor=lightblue]
	2865983957488 -> 2865845208016
	2865845208016 [label=AccumulateGrad]
	2865845208064 -> 2865845207872
	2865845208064 [label=ViewBackward0]
	2865845208160 -> 2865845208064
	2865845208160 [label=MeanBackward1]
	2865845208304 -> 2865845208160
	2865845208304 [label=ReluBackward0]
	2865845208400 -> 2865845208304
	2865845208400 [label=AddBackward0]
	2865845208496 -> 2865845208400
	2865845208496 [label=CudnnBatchNormBackward0]
	2865845208640 -> 2865845208496
	2865845208640 [label=ConvolutionBackward0]
	2865845208832 -> 2865845208640
	2865845208832 [label=ReluBackward0]
	2865845208976 -> 2865845208832
	2865845208976 [label=CudnnBatchNormBackward0]
	2865845209072 -> 2865845208976
	2865845209072 [label=ConvolutionBackward0]
	2865845208448 -> 2865845209072
	2865845208448 [label=ReluBackward0]
	2865845209216 -> 2865845208448
	2865845209216 [label=AddBackward0]
	2865845209312 -> 2865845209216
	2865845209312 [label=CudnnBatchNormBackward0]
	2865845209456 -> 2865845209312
	2865845209456 [label=ConvolutionBackward0]
	2865845209648 -> 2865845209456
	2865845209648 [label=ReluBackward0]
	2865845209792 -> 2865845209648
	2865845209792 [label=CudnnBatchNormBackward0]
	2865845209888 -> 2865845209792
	2865845209888 [label=ConvolutionBackward0]
	2865845210080 -> 2865845209888
	2865845210080 [label=ReluBackward0]
	2865845210224 -> 2865845210080
	2865845210224 [label=AddBackward0]
	2865845210320 -> 2865845210224
	2865845210320 [label=CudnnBatchNormBackward0]
	2865845210464 -> 2865845210320
	2865845210464 [label=ConvolutionBackward0]
	2865845210656 -> 2865845210464
	2865845210656 [label=ReluBackward0]
	2865845210800 -> 2865845210656
	2865845210800 [label=CudnnBatchNormBackward0]
	2865845210896 -> 2865845210800
	2865845210896 [label=ConvolutionBackward0]
	2865845210272 -> 2865845210896
	2865845210272 [label=ReluBackward0]
	2865845211184 -> 2865845210272
	2865845211184 [label=AddBackward0]
	2865845211280 -> 2865845211184
	2865845211280 [label=CudnnBatchNormBackward0]
	2865845211424 -> 2865845211280
	2865845211424 [label=ConvolutionBackward0]
	2865845211616 -> 2865845211424
	2865845211616 [label=ReluBackward0]
	2865845211760 -> 2865845211616
	2865845211760 [label=CudnnBatchNormBackward0]
	2865845211856 -> 2865845211760
	2865845211856 [label=ConvolutionBackward0]
	2865845212048 -> 2865845211856
	2865845212048 [label=ReluBackward0]
	2865845212192 -> 2865845212048
	2865845212192 [label=AddBackward0]
	2865845212288 -> 2865845212192
	2865845212288 [label=CudnnBatchNormBackward0]
	2865845212432 -> 2865845212288
	2865845212432 [label=ConvolutionBackward0]
	2865845212624 -> 2865845212432
	2865845212624 [label=ReluBackward0]
	2865845212768 -> 2865845212624
	2865845212768 [label=CudnnBatchNormBackward0]
	2865845212864 -> 2865845212768
	2865845212864 [label=ConvolutionBackward0]
	2865845212240 -> 2865845212864
	2865845212240 [label=ReluBackward0]
	2865845213152 -> 2865845212240
	2865845213152 [label=AddBackward0]
	2865845213248 -> 2865845213152
	2865845213248 [label=CudnnBatchNormBackward0]
	2865845213392 -> 2865845213248
	2865845213392 [label=ConvolutionBackward0]
	2865845213584 -> 2865845213392
	2865845213584 [label=ReluBackward0]
	2865845213728 -> 2865845213584
	2865845213728 [label=CudnnBatchNormBackward0]
	2865845213824 -> 2865845213728
	2865845213824 [label=ConvolutionBackward0]
	2865845214016 -> 2865845213824
	2865845214016 [label=ReluBackward0]
	2865845214160 -> 2865845214016
	2865845214160 [label=AddBackward0]
	2865845214256 -> 2865845214160
	2865845214256 [label=CudnnBatchNormBackward0]
	2865848594352 -> 2865845214256
	2865848594352 [label=ConvolutionBackward0]
	2865848594544 -> 2865848594352
	2865848594544 [label=ReluBackward0]
	2865848594688 -> 2865848594544
	2865848594688 [label=CudnnBatchNormBackward0]
	2865848594784 -> 2865848594688
	2865848594784 [label=ConvolutionBackward0]
	2865845214208 -> 2865848594784
	2865845214208 [label=ReluBackward0]
	2865848595072 -> 2865845214208
	2865848595072 [label=AddBackward0]
	2865848595168 -> 2865848595072
	2865848595168 [label=CudnnBatchNormBackward0]
	2865848595312 -> 2865848595168
	2865848595312 [label=ConvolutionBackward0]
	2865848595504 -> 2865848595312
	2865848595504 [label=ReluBackward0]
	2865848595648 -> 2865848595504
	2865848595648 [label=CudnnBatchNormBackward0]
	2865848595744 -> 2865848595648
	2865848595744 [label=ConvolutionBackward0]
	2865848595120 -> 2865848595744
	2865848595120 [label=MaxPool2DWithIndicesBackward0]
	2865848596032 -> 2865848595120
	2865848596032 [label=ReluBackward0]
	2865848596128 -> 2865848596032
	2865848596128 [label=CudnnBatchNormBackward0]
	2865848596224 -> 2865848596128
	2865848596224 [label=ConvolutionBackward0]
	2865848596416 -> 2865848596224
	2865845235024 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2865845235024 -> 2865848596416
	2865848596416 [label=AccumulateGrad]
	2865848596176 -> 2865848596128
	2865882103248 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2865882103248 -> 2865848596176
	2865848596176 [label=AccumulateGrad]
	2865848595840 -> 2865848596128
	2865882103344 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2865882103344 -> 2865848595840
	2865848595840 [label=AccumulateGrad]
	2865848595936 -> 2865848595744
	2865845223504 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2865845223504 -> 2865848595936
	2865848595936 [label=AccumulateGrad]
	2865848595696 -> 2865848595648
	2865845223600 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2865845223600 -> 2865848595696
	2865848595696 [label=AccumulateGrad]
	2865848595552 -> 2865848595648
	2865845223696 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2865845223696 -> 2865848595552
	2865848595552 [label=AccumulateGrad]
	2865848595456 -> 2865848595312
	2865845224080 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2865845224080 -> 2865848595456
	2865848595456 [label=AccumulateGrad]
	2865848595264 -> 2865848595168
	2865845224176 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2865845224176 -> 2865848595264
	2865848595264 [label=AccumulateGrad]
	2865848595216 -> 2865848595168
	2865845224272 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2865845224272 -> 2865848595216
	2865848595216 [label=AccumulateGrad]
	2865848595120 -> 2865848595072
	2865848594976 -> 2865848594784
	2865845224656 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2865845224656 -> 2865848594976
	2865848594976 [label=AccumulateGrad]
	2865848594736 -> 2865848594688
	2865845224752 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2865845224752 -> 2865848594736
	2865848594736 [label=AccumulateGrad]
	2865848594592 -> 2865848594688
	2865845224848 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2865845224848 -> 2865848594592
	2865848594592 [label=AccumulateGrad]
	2865848594496 -> 2865848594352
	2865845225232 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2865845225232 -> 2865848594496
	2865848594496 [label=AccumulateGrad]
	2865848594304 -> 2865845214256
	2865845225328 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2865845225328 -> 2865848594304
	2865848594304 [label=AccumulateGrad]
	2865848594256 -> 2865845214256
	2865845225424 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2865845225424 -> 2865848594256
	2865848594256 [label=AccumulateGrad]
	2865845214208 -> 2865845214160
	2865845213968 -> 2865845213824
	2865845226384 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2865845226384 -> 2865845213968
	2865845213968 [label=AccumulateGrad]
	2865845213776 -> 2865845213728
	2865845226480 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2865845226480 -> 2865845213776
	2865845213776 [label=AccumulateGrad]
	2865845213632 -> 2865845213728
	2865845226576 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2865845226576 -> 2865845213632
	2865845213632 [label=AccumulateGrad]
	2865845213536 -> 2865845213392
	2865845226960 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2865845226960 -> 2865845213536
	2865845213536 [label=AccumulateGrad]
	2865845213344 -> 2865845213248
	2865845227056 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2865845227056 -> 2865845213344
	2865845213344 [label=AccumulateGrad]
	2865845213296 -> 2865845213248
	2865845227152 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2865845227152 -> 2865845213296
	2865845213296 [label=AccumulateGrad]
	2865845213200 -> 2865845213152
	2865845213200 [label=CudnnBatchNormBackward0]
	2865845213920 -> 2865845213200
	2865845213920 [label=ConvolutionBackward0]
	2865845214016 -> 2865845213920
	2865845214112 -> 2865845213920
	2865845225808 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2865845225808 -> 2865845214112
	2865845214112 [label=AccumulateGrad]
	2865845213488 -> 2865845213200
	2865845225904 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2865845225904 -> 2865845213488
	2865845213488 [label=AccumulateGrad]
	2865845213440 -> 2865845213200
	2865845226000 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2865845226000 -> 2865845213440
	2865845213440 [label=AccumulateGrad]
	2865845213056 -> 2865845212864
	2865845227536 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2865845227536 -> 2865845213056
	2865845213056 [label=AccumulateGrad]
	2865845212816 -> 2865845212768
	2865845227632 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2865845227632 -> 2865845212816
	2865845212816 [label=AccumulateGrad]
	2865845212672 -> 2865845212768
	2865845227728 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2865845227728 -> 2865845212672
	2865845212672 [label=AccumulateGrad]
	2865845212576 -> 2865845212432
	2865845228112 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2865845228112 -> 2865845212576
	2865845212576 [label=AccumulateGrad]
	2865845212384 -> 2865845212288
	2865845228208 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2865845228208 -> 2865845212384
	2865845212384 [label=AccumulateGrad]
	2865845212336 -> 2865845212288
	2865845228304 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2865845228304 -> 2865845212336
	2865845212336 [label=AccumulateGrad]
	2865845212240 -> 2865845212192
	2865845212000 -> 2865845211856
	2865845229264 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2865845229264 -> 2865845212000
	2865845212000 [label=AccumulateGrad]
	2865845211808 -> 2865845211760
	2865845229360 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2865845229360 -> 2865845211808
	2865845211808 [label=AccumulateGrad]
	2865845211664 -> 2865845211760
	2865845229456 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2865845229456 -> 2865845211664
	2865845211664 [label=AccumulateGrad]
	2865845211568 -> 2865845211424
	2865845229840 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2865845229840 -> 2865845211568
	2865845211568 [label=AccumulateGrad]
	2865845211376 -> 2865845211280
	2865845229936 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2865845229936 -> 2865845211376
	2865845211376 [label=AccumulateGrad]
	2865845211328 -> 2865845211280
	2865845230032 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2865845230032 -> 2865845211328
	2865845211328 [label=AccumulateGrad]
	2865845211232 -> 2865845211184
	2865845211232 [label=CudnnBatchNormBackward0]
	2865845211952 -> 2865845211232
	2865845211952 [label=ConvolutionBackward0]
	2865845212048 -> 2865845211952
	2865845212096 -> 2865845211952
	2865845228688 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2865845228688 -> 2865845212096
	2865845212096 [label=AccumulateGrad]
	2865845211520 -> 2865845211232
	2865845228784 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2865845228784 -> 2865845211520
	2865845211520 [label=AccumulateGrad]
	2865845211472 -> 2865845211232
	2865845228880 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2865845228880 -> 2865845211472
	2865845211472 [label=AccumulateGrad]
	2865845211088 -> 2865845210896
	2865845230416 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2865845230416 -> 2865845211088
	2865845211088 [label=AccumulateGrad]
	2865845210848 -> 2865845210800
	2865845230512 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2865845230512 -> 2865845210848
	2865845210848 [label=AccumulateGrad]
	2865845210704 -> 2865845210800
	2865845230608 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2865845230608 -> 2865845210704
	2865845210704 [label=AccumulateGrad]
	2865845210608 -> 2865845210464
	2865845230992 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2865845230992 -> 2865845210608
	2865845210608 [label=AccumulateGrad]
	2865845210416 -> 2865845210320
	2865845231088 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2865845231088 -> 2865845210416
	2865845210416 [label=AccumulateGrad]
	2865845210368 -> 2865845210320
	2865845231184 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2865845231184 -> 2865845210368
	2865845210368 [label=AccumulateGrad]
	2865845210272 -> 2865845210224
	2865845210032 -> 2865845209888
	2865845232144 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2865845232144 -> 2865845210032
	2865845210032 [label=AccumulateGrad]
	2865845209840 -> 2865845209792
	2865845232240 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2865845232240 -> 2865845209840
	2865845209840 [label=AccumulateGrad]
	2865845209696 -> 2865845209792
	2865845232336 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2865845232336 -> 2865845209696
	2865845209696 [label=AccumulateGrad]
	2865845209600 -> 2865845209456
	2865845232720 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2865845232720 -> 2865845209600
	2865845209600 [label=AccumulateGrad]
	2865845209408 -> 2865845209312
	2865845232816 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2865845232816 -> 2865845209408
	2865845209408 [label=AccumulateGrad]
	2865845209360 -> 2865845209312
	2865845232912 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2865845232912 -> 2865845209360
	2865845209360 [label=AccumulateGrad]
	2865845209264 -> 2865845209216
	2865845209264 [label=CudnnBatchNormBackward0]
	2865845209984 -> 2865845209264
	2865845209984 [label=ConvolutionBackward0]
	2865845210080 -> 2865845209984
	2865845210128 -> 2865845209984
	2865845231568 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2865845231568 -> 2865845210128
	2865845210128 [label=AccumulateGrad]
	2865845209552 -> 2865845209264
	2865845231664 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2865845231664 -> 2865845209552
	2865845209552 [label=AccumulateGrad]
	2865845209504 -> 2865845209264
	2865845231760 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2865845231760 -> 2865845209504
	2865845209504 [label=AccumulateGrad]
	2865845207296 -> 2865845209072
	2865845233296 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2865845233296 -> 2865845207296
	2865845207296 [label=AccumulateGrad]
	2865845209024 -> 2865845208976
	2865845233392 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2865845233392 -> 2865845209024
	2865845209024 [label=AccumulateGrad]
	2865845208880 -> 2865845208976
	2865845233488 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2865845233488 -> 2865845208880
	2865845208880 [label=AccumulateGrad]
	2865845208784 -> 2865845208640
	2865845233872 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2865845233872 -> 2865845208784
	2865845208784 [label=AccumulateGrad]
	2865845208592 -> 2865845208496
	2865845233968 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2865845233968 -> 2865845208592
	2865845208592 [label=AccumulateGrad]
	2865845208544 -> 2865845208496
	2865845234064 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2865845234064 -> 2865845208544
	2865845208544 [label=AccumulateGrad]
	2865845208448 -> 2865845208400
	2865845208112 -> 2865845207872
	2865845208112 [label=TBackward0]
	2865845208352 -> 2865845208112
	2865845234736 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2865845234736 -> 2865845208352
	2865845208352 [label=AccumulateGrad]
	2865845207872 -> 2865851861712
}
