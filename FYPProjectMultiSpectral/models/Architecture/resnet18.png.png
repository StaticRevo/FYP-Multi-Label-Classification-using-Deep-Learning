digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1572649951376 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1572826104736 [label=AddmmBackward0]
	1572826104880 -> 1572826104736
	1572794613488 [label="fc.bias
 (19)" fillcolor=lightblue]
	1572794613488 -> 1572826104880
	1572826104880 [label=AccumulateGrad]
	1572826104928 -> 1572826104736
	1572826104928 [label=ViewBackward0]
	1572826105024 -> 1572826104928
	1572826105024 [label=MeanBackward1]
	1572826105168 -> 1572826105024
	1572826105168 [label=ReluBackward0]
	1572826105264 -> 1572826105168
	1572826105264 [label=AddBackward0]
	1572826105360 -> 1572826105264
	1572826105360 [label=CudnnBatchNormBackward0]
	1572826105504 -> 1572826105360
	1572826105504 [label=ConvolutionBackward0]
	1572826105696 -> 1572826105504
	1572826105696 [label=ReluBackward0]
	1572826105840 -> 1572826105696
	1572826105840 [label=CudnnBatchNormBackward0]
	1572826105936 -> 1572826105840
	1572826105936 [label=ConvolutionBackward0]
	1572826105312 -> 1572826105936
	1572826105312 [label=ReluBackward0]
	1572826106080 -> 1572826105312
	1572826106080 [label=AddBackward0]
	1572826106176 -> 1572826106080
	1572826106176 [label=CudnnBatchNormBackward0]
	1572826106320 -> 1572826106176
	1572826106320 [label=ConvolutionBackward0]
	1572826106512 -> 1572826106320
	1572826106512 [label=ReluBackward0]
	1572826106656 -> 1572826106512
	1572826106656 [label=CudnnBatchNormBackward0]
	1572826106752 -> 1572826106656
	1572826106752 [label=ConvolutionBackward0]
	1572826106944 -> 1572826106752
	1572826106944 [label=ReluBackward0]
	1572826107088 -> 1572826106944
	1572826107088 [label=AddBackward0]
	1572826107184 -> 1572826107088
	1572826107184 [label=CudnnBatchNormBackward0]
	1572826107328 -> 1572826107184
	1572826107328 [label=ConvolutionBackward0]
	1572826107520 -> 1572826107328
	1572826107520 [label=ReluBackward0]
	1572826107664 -> 1572826107520
	1572826107664 [label=CudnnBatchNormBackward0]
	1572826107760 -> 1572826107664
	1572826107760 [label=ConvolutionBackward0]
	1572826107136 -> 1572826107760
	1572826107136 [label=ReluBackward0]
	1572826108048 -> 1572826107136
	1572826108048 [label=AddBackward0]
	1572826108144 -> 1572826108048
	1572826108144 [label=CudnnBatchNormBackward0]
	1572826108288 -> 1572826108144
	1572826108288 [label=ConvolutionBackward0]
	1572826108480 -> 1572826108288
	1572826108480 [label=ReluBackward0]
	1572826108624 -> 1572826108480
	1572826108624 [label=CudnnBatchNormBackward0]
	1572826108720 -> 1572826108624
	1572826108720 [label=ConvolutionBackward0]
	1572826108912 -> 1572826108720
	1572826108912 [label=ReluBackward0]
	1572826109056 -> 1572826108912
	1572826109056 [label=AddBackward0]
	1572826109152 -> 1572826109056
	1572826109152 [label=CudnnBatchNormBackward0]
	1572826109296 -> 1572826109152
	1572826109296 [label=ConvolutionBackward0]
	1572826109488 -> 1572826109296
	1572826109488 [label=ReluBackward0]
	1572826109632 -> 1572826109488
	1572826109632 [label=CudnnBatchNormBackward0]
	1572826109728 -> 1572826109632
	1572826109728 [label=ConvolutionBackward0]
	1572826109104 -> 1572826109728
	1572826109104 [label=ReluBackward0]
	1572826110016 -> 1572826109104
	1572826110016 [label=AddBackward0]
	1572826110112 -> 1572826110016
	1572826110112 [label=CudnnBatchNormBackward0]
	1572826110256 -> 1572826110112
	1572826110256 [label=ConvolutionBackward0]
	1572826110448 -> 1572826110256
	1572826110448 [label=ReluBackward0]
	1572826110592 -> 1572826110448
	1572826110592 [label=CudnnBatchNormBackward0]
	1572826110688 -> 1572826110592
	1572826110688 [label=ConvolutionBackward0]
	1572826110880 -> 1572826110688
	1572826110880 [label=ReluBackward0]
	1572826111024 -> 1572826110880
	1572826111024 [label=AddBackward0]
	1572826111120 -> 1572826111024
	1572826111120 [label=CudnnBatchNormBackward0]
	1572826111264 -> 1572826111120
	1572826111264 [label=ConvolutionBackward0]
	1572826111456 -> 1572826111264
	1572826111456 [label=ReluBackward0]
	1572826111600 -> 1572826111456
	1572826111600 [label=CudnnBatchNormBackward0]
	1572826111696 -> 1572826111600
	1572826111696 [label=ConvolutionBackward0]
	1572826111072 -> 1572826111696
	1572826111072 [label=ReluBackward0]
	1572826111984 -> 1572826111072
	1572826111984 [label=AddBackward0]
	1572826112080 -> 1572826111984
	1572826112080 [label=CudnnBatchNormBackward0]
	1572826112224 -> 1572826112080
	1572826112224 [label=ConvolutionBackward0]
	1572826112416 -> 1572826112224
	1572826112416 [label=ReluBackward0]
	1572826112560 -> 1572826112416
	1572826112560 [label=CudnnBatchNormBackward0]
	1572826112656 -> 1572826112560
	1572826112656 [label=ConvolutionBackward0]
	1572826112032 -> 1572826112656
	1572826112032 [label=MaxPool2DWithIndicesBackward0]
	1572826112944 -> 1572826112032
	1572826112944 [label=ReluBackward0]
	1572826113040 -> 1572826112944
	1572826113040 [label=CudnnBatchNormBackward0]
	1572826113136 -> 1572826113040
	1572826113136 [label=ConvolutionBackward0]
	1572826113328 -> 1572826113136
	1572830689040 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1572830689040 -> 1572826113328
	1572826113328 [label=AccumulateGrad]
	1572826113088 -> 1572826113040
	1572636808720 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1572636808720 -> 1572826113088
	1572826113088 [label=AccumulateGrad]
	1572826112752 -> 1572826113040
	1572636806224 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1572636806224 -> 1572826112752
	1572826112752 [label=AccumulateGrad]
	1572826112848 -> 1572826112656
	1572603612240 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1572603612240 -> 1572826112848
	1572826112848 [label=AccumulateGrad]
	1572826112608 -> 1572826112560
	1572603610992 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1572603610992 -> 1572826112608
	1572826112608 [label=AccumulateGrad]
	1572826112464 -> 1572826112560
	1572830677712 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1572830677712 -> 1572826112464
	1572826112464 [label=AccumulateGrad]
	1572826112368 -> 1572826112224
	1572830678096 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1572830678096 -> 1572826112368
	1572826112368 [label=AccumulateGrad]
	1572826112176 -> 1572826112080
	1572830678192 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1572830678192 -> 1572826112176
	1572826112176 [label=AccumulateGrad]
	1572826112128 -> 1572826112080
	1572830678288 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1572830678288 -> 1572826112128
	1572826112128 [label=AccumulateGrad]
	1572826112032 -> 1572826111984
	1572826111888 -> 1572826111696
	1572830678672 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1572830678672 -> 1572826111888
	1572826111888 [label=AccumulateGrad]
	1572826111648 -> 1572826111600
	1572830678768 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1572830678768 -> 1572826111648
	1572826111648 [label=AccumulateGrad]
	1572826111504 -> 1572826111600
	1572830678864 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1572830678864 -> 1572826111504
	1572826111504 [label=AccumulateGrad]
	1572826111408 -> 1572826111264
	1572830679248 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1572830679248 -> 1572826111408
	1572826111408 [label=AccumulateGrad]
	1572826111216 -> 1572826111120
	1572830679344 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1572830679344 -> 1572826111216
	1572826111216 [label=AccumulateGrad]
	1572826111168 -> 1572826111120
	1572830679440 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1572830679440 -> 1572826111168
	1572826111168 [label=AccumulateGrad]
	1572826111072 -> 1572826111024
	1572826110832 -> 1572826110688
	1572830680400 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1572830680400 -> 1572826110832
	1572826110832 [label=AccumulateGrad]
	1572826110640 -> 1572826110592
	1572830680496 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1572830680496 -> 1572826110640
	1572826110640 [label=AccumulateGrad]
	1572826110496 -> 1572826110592
	1572830680592 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1572830680592 -> 1572826110496
	1572826110496 [label=AccumulateGrad]
	1572826110400 -> 1572826110256
	1572830680976 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1572830680976 -> 1572826110400
	1572826110400 [label=AccumulateGrad]
	1572826110208 -> 1572826110112
	1572830681072 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1572830681072 -> 1572826110208
	1572826110208 [label=AccumulateGrad]
	1572826110160 -> 1572826110112
	1572830681168 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1572830681168 -> 1572826110160
	1572826110160 [label=AccumulateGrad]
	1572826110064 -> 1572826110016
	1572826110064 [label=CudnnBatchNormBackward0]
	1572826110784 -> 1572826110064
	1572826110784 [label=ConvolutionBackward0]
	1572826110880 -> 1572826110784
	1572826110928 -> 1572826110784
	1572830679824 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1572830679824 -> 1572826110928
	1572826110928 [label=AccumulateGrad]
	1572826110352 -> 1572826110064
	1572830679920 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1572830679920 -> 1572826110352
	1572826110352 [label=AccumulateGrad]
	1572826110304 -> 1572826110064
	1572830680016 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1572830680016 -> 1572826110304
	1572826110304 [label=AccumulateGrad]
	1572826109920 -> 1572826109728
	1572830681552 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1572830681552 -> 1572826109920
	1572826109920 [label=AccumulateGrad]
	1572826109680 -> 1572826109632
	1572830681648 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1572830681648 -> 1572826109680
	1572826109680 [label=AccumulateGrad]
	1572826109536 -> 1572826109632
	1572830681744 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1572830681744 -> 1572826109536
	1572826109536 [label=AccumulateGrad]
	1572826109440 -> 1572826109296
	1572830682128 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1572830682128 -> 1572826109440
	1572826109440 [label=AccumulateGrad]
	1572826109248 -> 1572826109152
	1572830682224 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1572830682224 -> 1572826109248
	1572826109248 [label=AccumulateGrad]
	1572826109200 -> 1572826109152
	1572830682320 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1572830682320 -> 1572826109200
	1572826109200 [label=AccumulateGrad]
	1572826109104 -> 1572826109056
	1572826108864 -> 1572826108720
	1572830683280 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1572830683280 -> 1572826108864
	1572826108864 [label=AccumulateGrad]
	1572826108672 -> 1572826108624
	1572830683376 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1572830683376 -> 1572826108672
	1572826108672 [label=AccumulateGrad]
	1572826108528 -> 1572826108624
	1572830683472 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1572830683472 -> 1572826108528
	1572826108528 [label=AccumulateGrad]
	1572826108432 -> 1572826108288
	1572830683856 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1572830683856 -> 1572826108432
	1572826108432 [label=AccumulateGrad]
	1572826108240 -> 1572826108144
	1572830683952 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1572830683952 -> 1572826108240
	1572826108240 [label=AccumulateGrad]
	1572826108192 -> 1572826108144
	1572830684048 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1572830684048 -> 1572826108192
	1572826108192 [label=AccumulateGrad]
	1572826108096 -> 1572826108048
	1572826108096 [label=CudnnBatchNormBackward0]
	1572826108816 -> 1572826108096
	1572826108816 [label=ConvolutionBackward0]
	1572826108912 -> 1572826108816
	1572826108960 -> 1572826108816
	1572830682704 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1572830682704 -> 1572826108960
	1572826108960 [label=AccumulateGrad]
	1572826108384 -> 1572826108096
	1572830682800 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1572830682800 -> 1572826108384
	1572826108384 [label=AccumulateGrad]
	1572826108336 -> 1572826108096
	1572830682896 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1572830682896 -> 1572826108336
	1572826108336 [label=AccumulateGrad]
	1572826107952 -> 1572826107760
	1572830684432 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1572830684432 -> 1572826107952
	1572826107952 [label=AccumulateGrad]
	1572826107712 -> 1572826107664
	1572830684528 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1572830684528 -> 1572826107712
	1572826107712 [label=AccumulateGrad]
	1572826107568 -> 1572826107664
	1572830684624 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1572830684624 -> 1572826107568
	1572826107568 [label=AccumulateGrad]
	1572826107472 -> 1572826107328
	1572830685008 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1572830685008 -> 1572826107472
	1572826107472 [label=AccumulateGrad]
	1572826107280 -> 1572826107184
	1572830685104 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1572830685104 -> 1572826107280
	1572826107280 [label=AccumulateGrad]
	1572826107232 -> 1572826107184
	1572830685200 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1572830685200 -> 1572826107232
	1572826107232 [label=AccumulateGrad]
	1572826107136 -> 1572826107088
	1572826106896 -> 1572826106752
	1572830686160 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1572830686160 -> 1572826106896
	1572826106896 [label=AccumulateGrad]
	1572826106704 -> 1572826106656
	1572830686256 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1572830686256 -> 1572826106704
	1572826106704 [label=AccumulateGrad]
	1572826106560 -> 1572826106656
	1572830686352 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1572830686352 -> 1572826106560
	1572826106560 [label=AccumulateGrad]
	1572826106464 -> 1572826106320
	1572830686736 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1572830686736 -> 1572826106464
	1572826106464 [label=AccumulateGrad]
	1572826106272 -> 1572826106176
	1572830686832 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1572830686832 -> 1572826106272
	1572826106272 [label=AccumulateGrad]
	1572826106224 -> 1572826106176
	1572830686928 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1572830686928 -> 1572826106224
	1572826106224 [label=AccumulateGrad]
	1572826106128 -> 1572826106080
	1572826106128 [label=CudnnBatchNormBackward0]
	1572826106848 -> 1572826106128
	1572826106848 [label=ConvolutionBackward0]
	1572826106944 -> 1572826106848
	1572826106992 -> 1572826106848
	1572830685584 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1572830685584 -> 1572826106992
	1572826106992 [label=AccumulateGrad]
	1572826106416 -> 1572826106128
	1572830685680 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1572830685680 -> 1572826106416
	1572826106416 [label=AccumulateGrad]
	1572826106368 -> 1572826106128
	1572830685776 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1572830685776 -> 1572826106368
	1572826106368 [label=AccumulateGrad]
	1572826104160 -> 1572826105936
	1572830687312 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1572830687312 -> 1572826104160
	1572826104160 [label=AccumulateGrad]
	1572826105888 -> 1572826105840
	1572830687408 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1572830687408 -> 1572826105888
	1572826105888 [label=AccumulateGrad]
	1572826105744 -> 1572826105840
	1572830687504 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1572830687504 -> 1572826105744
	1572826105744 [label=AccumulateGrad]
	1572826105648 -> 1572826105504
	1572830687888 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1572830687888 -> 1572826105648
	1572826105648 [label=AccumulateGrad]
	1572826105456 -> 1572826105360
	1572830687984 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1572830687984 -> 1572826105456
	1572826105456 [label=AccumulateGrad]
	1572826105408 -> 1572826105360
	1572830688080 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1572830688080 -> 1572826105408
	1572826105408 [label=AccumulateGrad]
	1572826105312 -> 1572826105264
	1572826104976 -> 1572826104736
	1572826104976 [label=TBackward0]
	1572826105216 -> 1572826104976
	1572830688752 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1572830688752 -> 1572826105216
	1572826105216 [label=AccumulateGrad]
	1572826104736 -> 1572649951376
}
