digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1877529565360 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1877555254832 [label=AddmmBackward0]
	1877555250416 -> 1877555254832
	1877530111888 [label="fc.bias
 (19)" fillcolor=lightblue]
	1877530111888 -> 1877555250416
	1877555250416 [label=AccumulateGrad]
	1877555252096 -> 1877555254832
	1877555252096 [label=ViewBackward0]
	1877555262128 -> 1877555252096
	1877555262128 [label=MeanBackward1]
	1877555254352 -> 1877555262128
	1877555254352 [label=ReluBackward0]
	1877555264768 -> 1877555254352
	1877555264768 [label=AddBackward0]
	1877555252144 -> 1877555264768
	1877555252144 [label=CudnnBatchNormBackward0]
	1877555256272 -> 1877555252144
	1877555256272 [label=ConvolutionBackward0]
	1877555264288 -> 1877555256272
	1877555264288 [label=ReluBackward0]
	1877555264864 -> 1877555264288
	1877555264864 [label=CudnnBatchNormBackward0]
	1877555259728 -> 1877555264864
	1877555259728 [label=ConvolutionBackward0]
	1877555253920 -> 1877555259728
	1877555253920 [label=ReluBackward0]
	1877555254064 -> 1877555253920
	1877555254064 [label=AddBackward0]
	1877555264960 -> 1877555254064
	1877555264960 [label=CudnnBatchNormBackward0]
	1877555257280 -> 1877555264960
	1877555257280 [label=ConvolutionBackward0]
	1877555258000 -> 1877555257280
	1877555258000 [label=ReluBackward0]
	1877555260064 -> 1877555258000
	1877555260064 [label=CudnnBatchNormBackward0]
	1877554223264 -> 1877555260064
	1877554223264 [label=ConvolutionBackward0]
	1877554223888 -> 1877554223264
	1877554223888 [label=ReluBackward0]
	1877554217216 -> 1877554223888
	1877554217216 [label=AddBackward0]
	1877554225328 -> 1877554217216
	1877554225328 [label=CudnnBatchNormBackward0]
	1877554221440 -> 1877554225328
	1877554221440 [label=ConvolutionBackward0]
	1877554221248 -> 1877554221440
	1877554221248 [label=ReluBackward0]
	1877554218368 -> 1877554221248
	1877554218368 [label=CudnnBatchNormBackward0]
	1877554225760 -> 1877554218368
	1877554225760 [label=ConvolutionBackward0]
	1877554222112 -> 1877554225760
	1877554222112 [label=ReluBackward0]
	1877554223600 -> 1877554222112
	1877554223600 [label=AddBackward0]
	1877554224752 -> 1877554223600
	1877554224752 [label=CudnnBatchNormBackward0]
	1877554224560 -> 1877554224752
	1877554224560 [label=ConvolutionBackward0]
	1877554226384 -> 1877554224560
	1877554226384 [label=ReluBackward0]
	1877554222400 -> 1877554226384
	1877554222400 [label=CudnnBatchNormBackward0]
	1877554224320 -> 1877554222400
	1877554224320 [label=ConvolutionBackward0]
	1877554226288 -> 1877554224320
	1877554226288 [label=ReluBackward0]
	1877554219472 -> 1877554226288
	1877554219472 [label=AddBackward0]
	1877554226576 -> 1877554219472
	1877554226576 [label=CudnnBatchNormBackward0]
	1877554223216 -> 1877554226576
	1877554223216 [label=ConvolutionBackward0]
	1877554222736 -> 1877554223216
	1877554222736 [label=ReluBackward0]
	1877554224032 -> 1877554222736
	1877554224032 [label=CudnnBatchNormBackward0]
	1877554222256 -> 1877554224032
	1877554222256 [label=ConvolutionBackward0]
	1877554224416 -> 1877554222256
	1877554224416 [label=ReluBackward0]
	1877554223840 -> 1877554224416
	1877554223840 [label=AddBackward0]
	1877554223552 -> 1877554223840
	1877554223552 [label=CudnnBatchNormBackward0]
	1877554227248 -> 1877554223552
	1877554227248 [label=ConvolutionBackward0]
	1877554222304 -> 1877554227248
	1877554222304 [label=ReluBackward0]
	1877554224608 -> 1877554222304
	1877554224608 [label=CudnnBatchNormBackward0]
	1877554226864 -> 1877554224608
	1877554226864 [label=ConvolutionBackward0]
	1877554227392 -> 1877554226864
	1877554227392 [label=ReluBackward0]
	1877554221056 -> 1877554227392
	1877554221056 [label=AddBackward0]
	1877554225472 -> 1877554221056
	1877554225472 [label=CudnnBatchNormBackward0]
	1877554227824 -> 1877554225472
	1877554227824 [label=ConvolutionBackward0]
	1877554222352 -> 1877554227824
	1877554222352 [label=ReluBackward0]
	1877554226720 -> 1877554222352
	1877554226720 [label=CudnnBatchNormBackward0]
	1877554218848 -> 1877554226720
	1877554218848 [label=ConvolutionBackward0]
	1877554223360 -> 1877554218848
	1877554223360 [label=ReluBackward0]
	1877554218656 -> 1877554223360
	1877554218656 [label=AddBackward0]
	1877554217696 -> 1877554218656
	1877554217696 [label=CudnnBatchNormBackward0]
	1877554219568 -> 1877554217696
	1877554219568 [label=ConvolutionBackward0]
	1877554222928 -> 1877554219568
	1877554222928 [label=ReluBackward0]
	1877554220288 -> 1877554222928
	1877554220288 [label=CudnnBatchNormBackward0]
	1877554227056 -> 1877554220288
	1877554227056 [label=ConvolutionBackward0]
	1877554227920 -> 1877554227056
	1877554227920 [label=MaxPool2DWithIndicesBackward0]
	1877554221776 -> 1877554227920
	1877554221776 [label=ReluBackward0]
	1877554218224 -> 1877554221776
	1877554218224 [label=CudnnBatchNormBackward0]
	1877554225232 -> 1877554218224
	1877554225232 [label=ConvolutionBackward0]
	1877554221920 -> 1877554225232
	1877530111696 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1877530111696 -> 1877554221920
	1877554221920 [label=AccumulateGrad]
	1877554223024 -> 1877554218224
	1877549158320 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1877549158320 -> 1877554223024
	1877554223024 [label=AccumulateGrad]
	1877554227488 -> 1877554218224
	1877549157552 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1877549157552 -> 1877554227488
	1877554227488 [label=AccumulateGrad]
	1877554227728 -> 1877554227056
	1877549157648 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1877549157648 -> 1877554227728
	1877554227728 [label=AccumulateGrad]
	1877554217024 -> 1877554220288
	1877549157744 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1877549157744 -> 1877554217024
	1877554217024 [label=AccumulateGrad]
	1877554218176 -> 1877554220288
	1877549157840 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1877549157840 -> 1877554218176
	1877554218176 [label=AccumulateGrad]
	1877554220096 -> 1877554219568
	1877549158128 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1877549158128 -> 1877554220096
	1877554220096 [label=AccumulateGrad]
	1877554227152 -> 1877554217696
	1877549157168 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1877549157168 -> 1877554227152
	1877554227152 [label=AccumulateGrad]
	1877554219232 -> 1877554217696
	1877549157072 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1877549157072 -> 1877554219232
	1877554219232 [label=AccumulateGrad]
	1877554227920 -> 1877554218656
	1877554220912 -> 1877554218848
	1877549156208 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1877549156208 -> 1877554220912
	1877554220912 [label=AccumulateGrad]
	1877554224848 -> 1877554226720
	1877549156400 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1877549156400 -> 1877554224848
	1877554224848 [label=AccumulateGrad]
	1877554224464 -> 1877554226720
	1877549156496 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1877549156496 -> 1877554224464
	1877554224464 [label=AccumulateGrad]
	1877554220576 -> 1877554227824
	1877549156976 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1877549156976 -> 1877554220576
	1877554220576 [label=AccumulateGrad]
	1877554218464 -> 1877554225472
	1877549156880 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1877549156880 -> 1877554218464
	1877554218464 [label=AccumulateGrad]
	1877554224704 -> 1877554225472
	1877549155920 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1877549155920 -> 1877554224704
	1877554224704 [label=AccumulateGrad]
	1877554223360 -> 1877554221056
	1877554225520 -> 1877554226864
	1877549155440 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1877549155440 -> 1877554225520
	1877554225520 [label=AccumulateGrad]
	1877554222976 -> 1877554224608
	1877549155728 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1877549155728 -> 1877554222976
	1877554222976 [label=AccumulateGrad]
	1877554226960 -> 1877554224608
	1877549155632 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1877549155632 -> 1877554226960
	1877554226960 [label=AccumulateGrad]
	1877554220816 -> 1877554227248
	1877530104016 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1877530104016 -> 1877554220816
	1877554220816 [label=AccumulateGrad]
	1877554227008 -> 1877554223552
	1877530104112 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1877530104112 -> 1877554227008
	1877554227008 [label=AccumulateGrad]
	1877554218512 -> 1877554223552
	1877530104208 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1877530104208 -> 1877554218512
	1877554218512 [label=AccumulateGrad]
	1877554225280 -> 1877554223840
	1877554225280 [label=CudnnBatchNormBackward0]
	1877554224176 -> 1877554225280
	1877554224176 [label=ConvolutionBackward0]
	1877554227392 -> 1877554224176
	1877554221152 -> 1877554224176
	1877549155536 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1877549155536 -> 1877554221152
	1877554221152 [label=AccumulateGrad]
	1877554227104 -> 1877554225280
	1877549154576 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1877549154576 -> 1877554227104
	1877554227104 [label=AccumulateGrad]
	1877554224656 -> 1877554225280
	1877549154384 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1877549154384 -> 1877554224656
	1877554224656 [label=AccumulateGrad]
	1877554221872 -> 1877554222256
	1877530104592 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1877530104592 -> 1877554221872
	1877554221872 [label=AccumulateGrad]
	1877554217552 -> 1877554224032
	1877530104688 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1877530104688 -> 1877554217552
	1877554217552 [label=AccumulateGrad]
	1877554219904 -> 1877554224032
	1877530104784 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1877530104784 -> 1877554219904
	1877554219904 [label=AccumulateGrad]
	1877554225376 -> 1877554223216
	1877530105168 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1877530105168 -> 1877554225376
	1877554225376 [label=AccumulateGrad]
	1877554224512 -> 1877554226576
	1877530105264 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1877530105264 -> 1877554224512
	1877554224512 [label=AccumulateGrad]
	1877554221536 -> 1877554226576
	1877530105360 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1877530105360 -> 1877554221536
	1877554221536 [label=AccumulateGrad]
	1877554224416 -> 1877554219472
	1877554220384 -> 1877554224320
	1877530106320 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1877530106320 -> 1877554220384
	1877554220384 [label=AccumulateGrad]
	1877554228160 -> 1877554222400
	1877530106416 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1877530106416 -> 1877554228160
	1877554228160 [label=AccumulateGrad]
	1877554222160 -> 1877554222400
	1877530106512 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1877530106512 -> 1877554222160
	1877554222160 [label=AccumulateGrad]
	1877554220528 -> 1877554224560
	1877530106896 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1877530106896 -> 1877554220528
	1877554220528 [label=AccumulateGrad]
	1877554220624 -> 1877554224752
	1877530106992 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1877530106992 -> 1877554220624
	1877554220624 [label=AccumulateGrad]
	1877554225088 -> 1877554224752
	1877530107088 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1877530107088 -> 1877554225088
	1877554225088 [label=AccumulateGrad]
	1877554217504 -> 1877554223600
	1877554217504 [label=CudnnBatchNormBackward0]
	1877554225040 -> 1877554217504
	1877554225040 [label=ConvolutionBackward0]
	1877554226288 -> 1877554225040
	1877554223792 -> 1877554225040
	1877530105744 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1877530105744 -> 1877554223792
	1877554223792 [label=AccumulateGrad]
	1877554226816 -> 1877554217504
	1877530105840 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1877530105840 -> 1877554226816
	1877554226816 [label=AccumulateGrad]
	1877554228112 -> 1877554217504
	1877530105936 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1877530105936 -> 1877554228112
	1877554228112 [label=AccumulateGrad]
	1877554217840 -> 1877554225760
	1877530107472 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1877530107472 -> 1877554217840
	1877554217840 [label=AccumulateGrad]
	1877554221632 -> 1877554218368
	1877530107568 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1877530107568 -> 1877554221632
	1877554221632 [label=AccumulateGrad]
	1877554218800 -> 1877554218368
	1877530107664 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1877530107664 -> 1877554218800
	1877554218800 [label=AccumulateGrad]
	1877554219856 -> 1877554221440
	1877530108048 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1877530108048 -> 1877554219856
	1877554219856 [label=AccumulateGrad]
	1877554222208 -> 1877554225328
	1877530108144 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1877530108144 -> 1877554222208
	1877554222208 [label=AccumulateGrad]
	1877554219184 -> 1877554225328
	1877530108240 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1877530108240 -> 1877554219184
	1877554219184 [label=AccumulateGrad]
	1877554222112 -> 1877554217216
	1877554226240 -> 1877554223264
	1877530109200 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1877530109200 -> 1877554226240
	1877554226240 [label=AccumulateGrad]
	1877554227440 -> 1877555260064
	1877530109296 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1877530109296 -> 1877554227440
	1877554227440 [label=AccumulateGrad]
	1877554218320 -> 1877555260064
	1877530109392 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1877530109392 -> 1877554218320
	1877554218320 [label=AccumulateGrad]
	1877555262560 -> 1877555257280
	1877530109776 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1877530109776 -> 1877555262560
	1877555262560 [label=AccumulateGrad]
	1877555265200 -> 1877555264960
	1877530109872 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1877530109872 -> 1877555265200
	1877555265200 [label=AccumulateGrad]
	1877555250752 -> 1877555264960
	1877530109968 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1877530109968 -> 1877555250752
	1877555250752 [label=AccumulateGrad]
	1877555250896 -> 1877555254064
	1877555250896 [label=CudnnBatchNormBackward0]
	1877555255600 -> 1877555250896
	1877555255600 [label=ConvolutionBackward0]
	1877554223888 -> 1877555255600
	1877554218560 -> 1877555255600
	1877530108624 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1877530108624 -> 1877554218560
	1877554218560 [label=AccumulateGrad]
	1877555256176 -> 1877555250896
	1877530108720 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1877530108720 -> 1877555256176
	1877555256176 [label=AccumulateGrad]
	1877555255216 -> 1877555250896
	1877530108816 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1877530108816 -> 1877555255216
	1877555255216 [label=AccumulateGrad]
	1877555259488 -> 1877555259728
	1877530110352 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1877530110352 -> 1877555259488
	1877555259488 [label=AccumulateGrad]
	1877555258096 -> 1877555264864
	1877530110448 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1877530110448 -> 1877555258096
	1877555258096 [label=AccumulateGrad]
	1877555257568 -> 1877555264864
	1877530110544 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1877530110544 -> 1877555257568
	1877555257568 [label=AccumulateGrad]
	1877555251808 -> 1877555256272
	1877530110928 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1877530110928 -> 1877555251808
	1877555251808 [label=AccumulateGrad]
	1877555263664 -> 1877555252144
	1877530111024 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1877530111024 -> 1877555263664
	1877555263664 [label=AccumulateGrad]
	1877555263280 -> 1877555252144
	1877530111120 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1877530111120 -> 1877555263280
	1877555263280 [label=AccumulateGrad]
	1877555253920 -> 1877555264768
	1877555253776 -> 1877555254832
	1877555253776 [label=TBackward0]
	1877555254544 -> 1877555253776
	1877530111792 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1877530111792 -> 1877555254544
	1877555254544 [label=AccumulateGrad]
	1877555254832 -> 1877529565360
}
