digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2847848802128 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2847837564272 [label=AddmmBackward0]
	2847837557840 -> 2847837564272
	2847846530992 [label="fc.bias
 (19)" fillcolor=lightblue]
	2847846530992 -> 2847837557840
	2847837557840 [label=AccumulateGrad]
	2847837563696 -> 2847837564272
	2847837563696 [label=ViewBackward0]
	2847837556976 -> 2847837563696
	2847837556976 [label=MeanBackward1]
	2847837567440 -> 2847837556976
	2847837567440 [label=ReluBackward0]
	2847837556592 -> 2847837567440
	2847837556592 [label=AddBackward0]
	2847837553664 -> 2847837556592
	2847837553664 [label=CudnnBatchNormBackward0]
	2847837557216 -> 2847837553664
	2847837557216 [label=ConvolutionBackward0]
	2847837561392 -> 2847837557216
	2847837561392 [label=ReluBackward0]
	2847837564608 -> 2847837561392
	2847837564608 [label=CudnnBatchNormBackward0]
	2847837567728 -> 2847837564608
	2847837567728 [label=ConvolutionBackward0]
	2847837567488 -> 2847837567728
	2847837567488 [label=ReluBackward0]
	2847837565088 -> 2847837567488
	2847837565088 [label=CudnnBatchNormBackward0]
	2847837562400 -> 2847837565088
	2847837562400 [label=ConvolutionBackward0]
	2847837554912 -> 2847837562400
	2847837554912 [label=ReluBackward0]
	2847837568352 -> 2847837554912
	2847837568352 [label=AddBackward0]
	2847837568448 -> 2847837568352
	2847837568448 [label=CudnnBatchNormBackward0]
	2847837568592 -> 2847837568448
	2847837568592 [label=ConvolutionBackward0]
	2847837568784 -> 2847837568592
	2847837568784 [label=ReluBackward0]
	2847837568928 -> 2847837568784
	2847837568928 [label=CudnnBatchNormBackward0]
	2847837554096 -> 2847837568928
	2847837554096 [label=ConvolutionBackward0]
	2847837553712 -> 2847837554096
	2847837553712 [label=ReluBackward0]
	2847837563792 -> 2847837553712
	2847837563792 [label=CudnnBatchNormBackward0]
	2847837557696 -> 2847837563792
	2847837557696 [label=ConvolutionBackward0]
	2847837568400 -> 2847837557696
	2847837568400 [label=ReluBackward0]
	2847837566864 -> 2847837568400
	2847837566864 [label=AddBackward0]
	2847837562208 -> 2847837566864
	2847837562208 [label=CudnnBatchNormBackward0]
	2847837554192 -> 2847837562208
	2847837554192 [label=ConvolutionBackward0]
	2847837559232 -> 2847837554192
	2847837559232 [label=ReluBackward0]
	2847837555440 -> 2847837559232
	2847837555440 [label=CudnnBatchNormBackward0]
	2847837562688 -> 2847837555440
	2847837562688 [label=ConvolutionBackward0]
	2847837560672 -> 2847837562688
	2847837560672 [label=ReluBackward0]
	2847837561728 -> 2847837560672
	2847837561728 [label=CudnnBatchNormBackward0]
	2847837565232 -> 2847837561728
	2847837565232 [label=ConvolutionBackward0]
	2847837559184 -> 2847837565232
	2847837559184 [label=ReluBackward0]
	2847837556688 -> 2847837559184
	2847837556688 [label=AddBackward0]
	2847837558512 -> 2847837556688
	2847837558512 [label=CudnnBatchNormBackward0]
	2847837557024 -> 2847837558512
	2847837557024 [label=ConvolutionBackward0]
	2847837562736 -> 2847837557024
	2847837562736 [label=ReluBackward0]
	2847837561056 -> 2847837562736
	2847837561056 [label=CudnnBatchNormBackward0]
	2847837562544 -> 2847837561056
	2847837562544 [label=ConvolutionBackward0]
	2847837561104 -> 2847837562544
	2847837561104 [label=ReluBackward0]
	2847837566192 -> 2847837561104
	2847837566192 [label=CudnnBatchNormBackward0]
	2847837562832 -> 2847837566192
	2847837562832 [label=ConvolutionBackward0]
	2847837561632 -> 2847837562832
	2847837561632 [label=ReluBackward0]
	2847837561680 -> 2847837561632
	2847837561680 [label=AddBackward0]
	2847837556736 -> 2847837561680
	2847837556736 [label=CudnnBatchNormBackward0]
	2847837558416 -> 2847837556736
	2847837558416 [label=ConvolutionBackward0]
	2847837564176 -> 2847837558416
	2847837564176 [label=ReluBackward0]
	2847837553808 -> 2847837564176
	2847837553808 [label=CudnnBatchNormBackward0]
	2847837553040 -> 2847837553808
	2847837553040 [label=ConvolutionBackward0]
	2847837567152 -> 2847837553040
	2847837567152 [label=ReluBackward0]
	2847837566240 -> 2847837567152
	2847837566240 [label=CudnnBatchNormBackward0]
	2847837563168 -> 2847837566240
	2847837563168 [label=ConvolutionBackward0]
	2847837559856 -> 2847837563168
	2847837559856 [label=ReluBackward0]
	2847837556112 -> 2847837559856
	2847837556112 [label=AddBackward0]
	2847837558800 -> 2847837556112
	2847837558800 [label=CudnnBatchNormBackward0]
	2847837564224 -> 2847837558800
	2847837564224 [label=ConvolutionBackward0]
	2847837556208 -> 2847837564224
	2847837556208 [label=ReluBackward0]
	2847837556160 -> 2847837556208
	2847837556160 [label=CudnnBatchNormBackward0]
	2847837553952 -> 2847837556160
	2847837553952 [label=ConvolutionBackward0]
	2847837560960 -> 2847837553952
	2847837560960 [label=ReluBackward0]
	2847837558176 -> 2847837560960
	2847837558176 [label=CudnnBatchNormBackward0]
	2847837556352 -> 2847837558176
	2847837556352 [label=ConvolutionBackward0]
	2847837558848 -> 2847837556352
	2847837558848 [label=ReluBackward0]
	2847848349264 -> 2847837558848
	2847848349264 [label=AddBackward0]
	2847848349360 -> 2847848349264
	2847848349360 [label=CudnnBatchNormBackward0]
	2847848349504 -> 2847848349360
	2847848349504 [label=ConvolutionBackward0]
	2847848349648 -> 2847848349504
	2847848349648 [label=ReluBackward0]
	2847843057872 -> 2847848349648
	2847843057872 [label=CudnnBatchNormBackward0]
	2847843057968 -> 2847843057872
	2847843057968 [label=ConvolutionBackward0]
	2847843058160 -> 2847843057968
	2847843058160 [label=ReluBackward0]
	2847843058304 -> 2847843058160
	2847843058304 [label=CudnnBatchNormBackward0]
	2847843058400 -> 2847843058304
	2847843058400 [label=ConvolutionBackward0]
	2847848349312 -> 2847843058400
	2847848349312 [label=ReluBackward0]
	2847843058688 -> 2847848349312
	2847843058688 [label=AddBackward0]
	2847843058784 -> 2847843058688
	2847843058784 [label=CudnnBatchNormBackward0]
	2847843058928 -> 2847843058784
	2847843058928 [label=ConvolutionBackward0]
	2847843059120 -> 2847843058928
	2847843059120 [label=ReluBackward0]
	2847843059264 -> 2847843059120
	2847843059264 [label=CudnnBatchNormBackward0]
	2847843059360 -> 2847843059264
	2847843059360 [label=ConvolutionBackward0]
	2847843059552 -> 2847843059360
	2847843059552 [label=ReluBackward0]
	2847843059696 -> 2847843059552
	2847843059696 [label=CudnnBatchNormBackward0]
	2847843059792 -> 2847843059696
	2847843059792 [label=ConvolutionBackward0]
	2847843058736 -> 2847843059792
	2847843058736 [label=ReluBackward0]
	2847843060080 -> 2847843058736
	2847843060080 [label=AddBackward0]
	2847843060176 -> 2847843060080
	2847843060176 [label=CudnnBatchNormBackward0]
	2847843060320 -> 2847843060176
	2847843060320 [label=ConvolutionBackward0]
	2847843060512 -> 2847843060320
	2847843060512 [label=ReluBackward0]
	2847843060656 -> 2847843060512
	2847843060656 [label=CudnnBatchNormBackward0]
	2847843060752 -> 2847843060656
	2847843060752 [label=ConvolutionBackward0]
	2847843060944 -> 2847843060752
	2847843060944 [label=ReluBackward0]
	2847843061088 -> 2847843060944
	2847843061088 [label=CudnnBatchNormBackward0]
	2847843061184 -> 2847843061088
	2847843061184 [label=ConvolutionBackward0]
	2847843061376 -> 2847843061184
	2847843061376 [label=ReluBackward0]
	2847843061520 -> 2847843061376
	2847843061520 [label=AddBackward0]
	2847843061616 -> 2847843061520
	2847843061616 [label=CudnnBatchNormBackward0]
	2847843061760 -> 2847843061616
	2847843061760 [label=ConvolutionBackward0]
	2847843061952 -> 2847843061760
	2847843061952 [label=ReluBackward0]
	2847843062096 -> 2847843061952
	2847843062096 [label=CudnnBatchNormBackward0]
	2847843062192 -> 2847843062096
	2847843062192 [label=ConvolutionBackward0]
	2847843062384 -> 2847843062192
	2847843062384 [label=ReluBackward0]
	2847843062528 -> 2847843062384
	2847843062528 [label=CudnnBatchNormBackward0]
	2847843062624 -> 2847843062528
	2847843062624 [label=ConvolutionBackward0]
	2847843061568 -> 2847843062624
	2847843061568 [label=ReluBackward0]
	2847843062912 -> 2847843061568
	2847843062912 [label=AddBackward0]
	2847843063008 -> 2847843062912
	2847843063008 [label=CudnnBatchNormBackward0]
	2847843063152 -> 2847843063008
	2847843063152 [label=ConvolutionBackward0]
	2847843063344 -> 2847843063152
	2847843063344 [label=ReluBackward0]
	2847843063488 -> 2847843063344
	2847843063488 [label=CudnnBatchNormBackward0]
	2847843063584 -> 2847843063488
	2847843063584 [label=ConvolutionBackward0]
	2847843063776 -> 2847843063584
	2847843063776 [label=ReluBackward0]
	2847843063920 -> 2847843063776
	2847843063920 [label=CudnnBatchNormBackward0]
	2847843064016 -> 2847843063920
	2847843064016 [label=ConvolutionBackward0]
	2847843062960 -> 2847843064016
	2847843062960 [label=ReluBackward0]
	2847843064304 -> 2847843062960
	2847843064304 [label=AddBackward0]
	2847843064400 -> 2847843064304
	2847843064400 [label=CudnnBatchNormBackward0]
	2847843064544 -> 2847843064400
	2847843064544 [label=ConvolutionBackward0]
	2847843064736 -> 2847843064544
	2847843064736 [label=ReluBackward0]
	2847843064880 -> 2847843064736
	2847843064880 [label=CudnnBatchNormBackward0]
	2847843064976 -> 2847843064880
	2847843064976 [label=ConvolutionBackward0]
	2847843065168 -> 2847843064976
	2847843065168 [label=ReluBackward0]
	2847843065312 -> 2847843065168
	2847843065312 [label=CudnnBatchNormBackward0]
	2847843065408 -> 2847843065312
	2847843065408 [label=ConvolutionBackward0]
	2847843064352 -> 2847843065408
	2847843064352 [label=ReluBackward0]
	2847843065696 -> 2847843064352
	2847843065696 [label=AddBackward0]
	2847843065792 -> 2847843065696
	2847843065792 [label=CudnnBatchNormBackward0]
	2847843065936 -> 2847843065792
	2847843065936 [label=ConvolutionBackward0]
	2847843066128 -> 2847843065936
	2847843066128 [label=ReluBackward0]
	2847843066272 -> 2847843066128
	2847843066272 [label=CudnnBatchNormBackward0]
	2847843066368 -> 2847843066272
	2847843066368 [label=ConvolutionBackward0]
	2847843066560 -> 2847843066368
	2847843066560 [label=ReluBackward0]
	2847843066704 -> 2847843066560
	2847843066704 [label=CudnnBatchNormBackward0]
	2847843066800 -> 2847843066704
	2847843066800 [label=ConvolutionBackward0]
	2847843066992 -> 2847843066800
	2847843066992 [label=ReluBackward0]
	2847843067136 -> 2847843066992
	2847843067136 [label=AddBackward0]
	2847843067232 -> 2847843067136
	2847843067232 [label=CudnnBatchNormBackward0]
	2847843067376 -> 2847843067232
	2847843067376 [label=ConvolutionBackward0]
	2847843067568 -> 2847843067376
	2847843067568 [label=ReluBackward0]
	2847843067712 -> 2847843067568
	2847843067712 [label=CudnnBatchNormBackward0]
	2847843067808 -> 2847843067712
	2847843067808 [label=ConvolutionBackward0]
	2847843068000 -> 2847843067808
	2847843068000 [label=ReluBackward0]
	2847843068144 -> 2847843068000
	2847843068144 [label=CudnnBatchNormBackward0]
	2847843068240 -> 2847843068144
	2847843068240 [label=ConvolutionBackward0]
	2847843067184 -> 2847843068240
	2847843067184 [label=ReluBackward0]
	2847843068528 -> 2847843067184
	2847843068528 [label=AddBackward0]
	2847843068624 -> 2847843068528
	2847843068624 [label=CudnnBatchNormBackward0]
	2847843068768 -> 2847843068624
	2847843068768 [label=ConvolutionBackward0]
	2847843068960 -> 2847843068768
	2847843068960 [label=ReluBackward0]
	2847843069104 -> 2847843068960
	2847843069104 [label=CudnnBatchNormBackward0]
	2847843069200 -> 2847843069104
	2847843069200 [label=ConvolutionBackward0]
	2847843069392 -> 2847843069200
	2847843069392 [label=ReluBackward0]
	2847843069536 -> 2847843069392
	2847843069536 [label=CudnnBatchNormBackward0]
	2847843069632 -> 2847843069536
	2847843069632 [label=ConvolutionBackward0]
	2847843068576 -> 2847843069632
	2847843068576 [label=ReluBackward0]
	2847843069920 -> 2847843068576
	2847843069920 [label=AddBackward0]
	2847843070016 -> 2847843069920
	2847843070016 [label=CudnnBatchNormBackward0]
	2847843070160 -> 2847843070016
	2847843070160 [label=ConvolutionBackward0]
	2847843070352 -> 2847843070160
	2847843070352 [label=ReluBackward0]
	2847843070496 -> 2847843070352
	2847843070496 [label=CudnnBatchNormBackward0]
	2847843070592 -> 2847843070496
	2847843070592 [label=ConvolutionBackward0]
	2847843070784 -> 2847843070592
	2847843070784 [label=ReluBackward0]
	2847843070928 -> 2847843070784
	2847843070928 [label=CudnnBatchNormBackward0]
	2847843071024 -> 2847843070928
	2847843071024 [label=ConvolutionBackward0]
	2847843071216 -> 2847843071024
	2847843071216 [label=MaxPool2DWithIndicesBackward0]
	2847843071360 -> 2847843071216
	2847843071360 [label=ReluBackward0]
	2847843071456 -> 2847843071360
	2847843071456 [label=CudnnBatchNormBackward0]
	2847843071552 -> 2847843071456
	2847843071552 [label=ConvolutionBackward0]
	2847843071744 -> 2847843071552
	2847851146480 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2847851146480 -> 2847843071744
	2847843071744 [label=AccumulateGrad]
	2847843071504 -> 2847843071456
	2847837683888 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2847837683888 -> 2847843071504
	2847843071504 [label=AccumulateGrad]
	2847843071264 -> 2847843071456
	2847837686768 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2847837686768 -> 2847843071264
	2847843071264 [label=AccumulateGrad]
	2847843071168 -> 2847843071024
	2847837690224 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2847837690224 -> 2847843071168
	2847843071168 [label=AccumulateGrad]
	2847843070976 -> 2847843070928
	2847837690320 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2847837690320 -> 2847843070976
	2847843070976 [label=AccumulateGrad]
	2847843070832 -> 2847843070928
	2847837690416 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2847837690416 -> 2847843070832
	2847843070832 [label=AccumulateGrad]
	2847843070736 -> 2847843070592
	2847837690896 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2847837690896 -> 2847843070736
	2847843070736 [label=AccumulateGrad]
	2847843070544 -> 2847843070496
	2847837690992 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2847837690992 -> 2847843070544
	2847843070544 [label=AccumulateGrad]
	2847843070400 -> 2847843070496
	2847837691088 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2847837691088 -> 2847843070400
	2847843070400 [label=AccumulateGrad]
	2847843070304 -> 2847843070160
	2847837691472 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2847837691472 -> 2847843070304
	2847843070304 [label=AccumulateGrad]
	2847843070112 -> 2847843070016
	2847837691568 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2847837691568 -> 2847843070112
	2847843070112 [label=AccumulateGrad]
	2847843070064 -> 2847843070016
	2847837691664 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2847837691664 -> 2847843070064
	2847843070064 [label=AccumulateGrad]
	2847843069968 -> 2847843069920
	2847843069968 [label=CudnnBatchNormBackward0]
	2847843070688 -> 2847843069968
	2847843070688 [label=ConvolutionBackward0]
	2847843071216 -> 2847843070688
	2847843071072 -> 2847843070688
	2847837689648 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2847837689648 -> 2847843071072
	2847843071072 [label=AccumulateGrad]
	2847843070256 -> 2847843069968
	2847837689744 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2847837689744 -> 2847843070256
	2847843070256 [label=AccumulateGrad]
	2847843070208 -> 2847843069968
	2847837689840 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2847837689840 -> 2847843070208
	2847843070208 [label=AccumulateGrad]
	2847843069824 -> 2847843069632
	2847837692048 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2847837692048 -> 2847843069824
	2847843069824 [label=AccumulateGrad]
	2847843069584 -> 2847843069536
	2847837692144 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2847837692144 -> 2847843069584
	2847843069584 [label=AccumulateGrad]
	2847843069440 -> 2847843069536
	2847837692240 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2847837692240 -> 2847843069440
	2847843069440 [label=AccumulateGrad]
	2847843069344 -> 2847843069200
	2847837692624 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2847837692624 -> 2847843069344
	2847843069344 [label=AccumulateGrad]
	2847843069152 -> 2847843069104
	2847837692720 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2847837692720 -> 2847843069152
	2847843069152 [label=AccumulateGrad]
	2847843069008 -> 2847843069104
	2847837692816 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2847837692816 -> 2847843069008
	2847843069008 [label=AccumulateGrad]
	2847843068912 -> 2847843068768
	2847837693200 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2847837693200 -> 2847843068912
	2847843068912 [label=AccumulateGrad]
	2847843068720 -> 2847843068624
	2847837693296 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2847837693296 -> 2847843068720
	2847843068720 [label=AccumulateGrad]
	2847843068672 -> 2847843068624
	2847837693392 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2847837693392 -> 2847843068672
	2847843068672 [label=AccumulateGrad]
	2847843068576 -> 2847843068528
	2847843068432 -> 2847843068240
	2847837693776 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2847837693776 -> 2847843068432
	2847843068432 [label=AccumulateGrad]
	2847843068192 -> 2847843068144
	2847837693872 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2847837693872 -> 2847843068192
	2847843068192 [label=AccumulateGrad]
	2847843068048 -> 2847843068144
	2847837693968 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2847837693968 -> 2847843068048
	2847843068048 [label=AccumulateGrad]
	2847843067952 -> 2847843067808
	2847837694352 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2847837694352 -> 2847843067952
	2847843067952 [label=AccumulateGrad]
	2847843067760 -> 2847843067712
	2847837694448 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2847837694448 -> 2847843067760
	2847843067760 [label=AccumulateGrad]
	2847843067616 -> 2847843067712
	2847837694544 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2847837694544 -> 2847843067616
	2847843067616 [label=AccumulateGrad]
	2847843067520 -> 2847843067376
	2847837694928 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2847837694928 -> 2847843067520
	2847843067520 [label=AccumulateGrad]
	2847843067328 -> 2847843067232
	2847837695024 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2847837695024 -> 2847843067328
	2847843067328 [label=AccumulateGrad]
	2847843067280 -> 2847843067232
	2847837695120 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2847837695120 -> 2847843067280
	2847843067280 [label=AccumulateGrad]
	2847843067184 -> 2847843067136
	2847843066944 -> 2847843066800
	2847837696080 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2847837696080 -> 2847843066944
	2847843066944 [label=AccumulateGrad]
	2847843066752 -> 2847843066704
	2847837696176 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2847837696176 -> 2847843066752
	2847843066752 [label=AccumulateGrad]
	2847843066608 -> 2847843066704
	2847837696272 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2847837696272 -> 2847843066608
	2847843066608 [label=AccumulateGrad]
	2847843066512 -> 2847843066368
	2847837696656 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2847837696656 -> 2847843066512
	2847843066512 [label=AccumulateGrad]
	2847843066320 -> 2847843066272
	2847837696752 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2847837696752 -> 2847843066320
	2847843066320 [label=AccumulateGrad]
	2847843066176 -> 2847843066272
	2847837696848 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2847837696848 -> 2847843066176
	2847843066176 [label=AccumulateGrad]
	2847843066080 -> 2847843065936
	2847837697232 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2847837697232 -> 2847843066080
	2847843066080 [label=AccumulateGrad]
	2847843065888 -> 2847843065792
	2847837697328 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2847837697328 -> 2847843065888
	2847843065888 [label=AccumulateGrad]
	2847843065840 -> 2847843065792
	2847837697424 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2847837697424 -> 2847843065840
	2847843065840 [label=AccumulateGrad]
	2847843065744 -> 2847843065696
	2847843065744 [label=CudnnBatchNormBackward0]
	2847843066464 -> 2847843065744
	2847843066464 [label=ConvolutionBackward0]
	2847843066992 -> 2847843066464
	2847843066848 -> 2847843066464
	2847837695504 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2847837695504 -> 2847843066848
	2847843066848 [label=AccumulateGrad]
	2847843066032 -> 2847843065744
	2847837695600 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2847837695600 -> 2847843066032
	2847843066032 [label=AccumulateGrad]
	2847843065984 -> 2847843065744
	2847837695696 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2847837695696 -> 2847843065984
	2847843065984 [label=AccumulateGrad]
	2847843065600 -> 2847843065408
	2847837697808 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2847837697808 -> 2847843065600
	2847843065600 [label=AccumulateGrad]
	2847843065360 -> 2847843065312
	2847837697904 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2847837697904 -> 2847843065360
	2847843065360 [label=AccumulateGrad]
	2847843065216 -> 2847843065312
	2847837698000 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2847837698000 -> 2847843065216
	2847843065216 [label=AccumulateGrad]
	2847843065120 -> 2847843064976
	2847837698384 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2847837698384 -> 2847843065120
	2847843065120 [label=AccumulateGrad]
	2847843064928 -> 2847843064880
	2847837698480 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2847837698480 -> 2847843064928
	2847843064928 [label=AccumulateGrad]
	2847843064784 -> 2847843064880
	2847837698576 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2847837698576 -> 2847843064784
	2847843064784 [label=AccumulateGrad]
	2847843064688 -> 2847843064544
	2847837698960 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2847837698960 -> 2847843064688
	2847843064688 [label=AccumulateGrad]
	2847843064496 -> 2847843064400
	2847837699056 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2847837699056 -> 2847843064496
	2847843064496 [label=AccumulateGrad]
	2847843064448 -> 2847843064400
	2847837699152 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2847837699152 -> 2847843064448
	2847843064448 [label=AccumulateGrad]
	2847843064352 -> 2847843064304
	2847843064208 -> 2847843064016
	2847837699536 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2847837699536 -> 2847843064208
	2847843064208 [label=AccumulateGrad]
	2847843063968 -> 2847843063920
	2847837699632 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2847837699632 -> 2847843063968
	2847843063968 [label=AccumulateGrad]
	2847843063824 -> 2847843063920
	2847837699728 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2847837699728 -> 2847843063824
	2847843063824 [label=AccumulateGrad]
	2847843063728 -> 2847843063584
	2847837684080 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2847837684080 -> 2847843063728
	2847843063728 [label=AccumulateGrad]
	2847843063536 -> 2847843063488
	2847848592784 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2847848592784 -> 2847843063536
	2847843063536 [label=AccumulateGrad]
	2847843063392 -> 2847843063488
	2847848592880 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2847848592880 -> 2847843063392
	2847843063392 [label=AccumulateGrad]
	2847843063296 -> 2847843063152
	2847848593264 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2847848593264 -> 2847843063296
	2847843063296 [label=AccumulateGrad]
	2847843063104 -> 2847843063008
	2847848593360 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2847848593360 -> 2847843063104
	2847843063104 [label=AccumulateGrad]
	2847843063056 -> 2847843063008
	2847848593456 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2847848593456 -> 2847843063056
	2847843063056 [label=AccumulateGrad]
	2847843062960 -> 2847843062912
	2847843062816 -> 2847843062624
	2847848593840 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2847848593840 -> 2847843062816
	2847843062816 [label=AccumulateGrad]
	2847843062576 -> 2847843062528
	2847848593936 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2847848593936 -> 2847843062576
	2847843062576 [label=AccumulateGrad]
	2847843062432 -> 2847843062528
	2847848594032 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2847848594032 -> 2847843062432
	2847843062432 [label=AccumulateGrad]
	2847843062336 -> 2847843062192
	2847848594416 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2847848594416 -> 2847843062336
	2847843062336 [label=AccumulateGrad]
	2847843062144 -> 2847843062096
	2847848594512 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2847848594512 -> 2847843062144
	2847843062144 [label=AccumulateGrad]
	2847843062000 -> 2847843062096
	2847848594608 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2847848594608 -> 2847843062000
	2847843062000 [label=AccumulateGrad]
	2847843061904 -> 2847843061760
	2847848594992 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2847848594992 -> 2847843061904
	2847843061904 [label=AccumulateGrad]
	2847843061712 -> 2847843061616
	2847848595088 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2847848595088 -> 2847843061712
	2847843061712 [label=AccumulateGrad]
	2847843061664 -> 2847843061616
	2847848595184 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2847848595184 -> 2847843061664
	2847843061664 [label=AccumulateGrad]
	2847843061568 -> 2847843061520
	2847843061328 -> 2847843061184
	2847843320560 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2847843320560 -> 2847843061328
	2847843061328 [label=AccumulateGrad]
	2847843061136 -> 2847843061088
	2847843320656 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2847843320656 -> 2847843061136
	2847843061136 [label=AccumulateGrad]
	2847843060992 -> 2847843061088
	2847843320752 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2847843320752 -> 2847843060992
	2847843060992 [label=AccumulateGrad]
	2847843060896 -> 2847843060752
	2847843321136 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2847843321136 -> 2847843060896
	2847843060896 [label=AccumulateGrad]
	2847843060704 -> 2847843060656
	2847843321232 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2847843321232 -> 2847843060704
	2847843060704 [label=AccumulateGrad]
	2847843060560 -> 2847843060656
	2847843321328 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2847843321328 -> 2847843060560
	2847843060560 [label=AccumulateGrad]
	2847843060464 -> 2847843060320
	2847843321712 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2847843321712 -> 2847843060464
	2847843060464 [label=AccumulateGrad]
	2847843060272 -> 2847843060176
	2847843321808 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2847843321808 -> 2847843060272
	2847843060272 [label=AccumulateGrad]
	2847843060224 -> 2847843060176
	2847843321904 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2847843321904 -> 2847843060224
	2847843060224 [label=AccumulateGrad]
	2847843060128 -> 2847843060080
	2847843060128 [label=CudnnBatchNormBackward0]
	2847843060848 -> 2847843060128
	2847843060848 [label=ConvolutionBackward0]
	2847843061376 -> 2847843060848
	2847843061232 -> 2847843060848
	2847843319984 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2847843319984 -> 2847843061232
	2847843061232 [label=AccumulateGrad]
	2847843060416 -> 2847843060128
	2847843320080 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2847843320080 -> 2847843060416
	2847843060416 [label=AccumulateGrad]
	2847843060368 -> 2847843060128
	2847843320176 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2847843320176 -> 2847843060368
	2847843060368 [label=AccumulateGrad]
	2847843059984 -> 2847843059792
	2847843322288 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2847843322288 -> 2847843059984
	2847843059984 [label=AccumulateGrad]
	2847843059744 -> 2847843059696
	2847843322384 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2847843322384 -> 2847843059744
	2847843059744 [label=AccumulateGrad]
	2847843059600 -> 2847843059696
	2847843322480 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2847843322480 -> 2847843059600
	2847843059600 [label=AccumulateGrad]
	2847843059504 -> 2847843059360
	2847843322864 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2847843322864 -> 2847843059504
	2847843059504 [label=AccumulateGrad]
	2847843059312 -> 2847843059264
	2847843322960 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2847843322960 -> 2847843059312
	2847843059312 [label=AccumulateGrad]
	2847843059168 -> 2847843059264
	2847843323056 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2847843323056 -> 2847843059168
	2847843059168 [label=AccumulateGrad]
	2847843059072 -> 2847843058928
	2847843323440 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2847843323440 -> 2847843059072
	2847843059072 [label=AccumulateGrad]
	2847843058880 -> 2847843058784
	2847843323536 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2847843323536 -> 2847843058880
	2847843058880 [label=AccumulateGrad]
	2847843058832 -> 2847843058784
	2847843323632 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2847843323632 -> 2847843058832
	2847843058832 [label=AccumulateGrad]
	2847843058736 -> 2847843058688
	2847843058592 -> 2847843058400
	2847843324016 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2847843324016 -> 2847843058592
	2847843058592 [label=AccumulateGrad]
	2847843058352 -> 2847843058304
	2847843324112 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2847843324112 -> 2847843058352
	2847843058352 [label=AccumulateGrad]
	2847843058208 -> 2847843058304
	2847843324208 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2847843324208 -> 2847843058208
	2847843058208 [label=AccumulateGrad]
	2847843058112 -> 2847843057968
	2847843324592 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2847843324592 -> 2847843058112
	2847843058112 [label=AccumulateGrad]
	2847843057920 -> 2847843057872
	2847843324688 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2847843324688 -> 2847843057920
	2847843057920 [label=AccumulateGrad]
	2847843057776 -> 2847843057872
	2847843324784 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2847843324784 -> 2847843057776
	2847843057776 [label=AccumulateGrad]
	2847848349600 -> 2847848349504
	2847843325168 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2847843325168 -> 2847848349600
	2847848349600 [label=AccumulateGrad]
	2847848349456 -> 2847848349360
	2847843325264 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2847843325264 -> 2847848349456
	2847848349456 [label=AccumulateGrad]
	2847848349408 -> 2847848349360
	2847843325360 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2847843325360 -> 2847848349408
	2847848349408 [label=AccumulateGrad]
	2847848349312 -> 2847848349264
	2847837559664 -> 2847837556352
	2847843325744 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2847843325744 -> 2847837559664
	2847837559664 [label=AccumulateGrad]
	2847837564896 -> 2847837558176
	2847843325840 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2847843325840 -> 2847837564896
	2847837564896 [label=AccumulateGrad]
	2847837558080 -> 2847837558176
	2847843325936 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2847843325936 -> 2847837558080
	2847837558080 [label=AccumulateGrad]
	2847837568976 -> 2847837553952
	2847843326320 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2847843326320 -> 2847837568976
	2847837568976 [label=AccumulateGrad]
	2847837567824 -> 2847837556160
	2847843326416 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2847843326416 -> 2847837567824
	2847837567824 [label=AccumulateGrad]
	2847837555008 -> 2847837556160
	2847843326512 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2847843326512 -> 2847837555008
	2847837555008 [label=AccumulateGrad]
	2847837554672 -> 2847837564224
	2847843326896 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2847843326896 -> 2847837554672
	2847837554672 [label=AccumulateGrad]
	2847837559376 -> 2847837558800
	2847843326992 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2847843326992 -> 2847837559376
	2847837559376 [label=AccumulateGrad]
	2847837559904 -> 2847837558800
	2847843327088 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2847843327088 -> 2847837559904
	2847837559904 [label=AccumulateGrad]
	2847837558848 -> 2847837556112
	2847837566288 -> 2847837563168
	2847843327472 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2847843327472 -> 2847837566288
	2847837566288 [label=AccumulateGrad]
	2847837559520 -> 2847837566240
	2847843327568 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2847843327568 -> 2847837559520
	2847837559520 [label=AccumulateGrad]
	2847837554288 -> 2847837566240
	2847843327664 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2847843327664 -> 2847837554288
	2847837554288 [label=AccumulateGrad]
	2847837557312 -> 2847837553040
	2847843328048 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2847843328048 -> 2847837557312
	2847837557312 [label=AccumulateGrad]
	2847837560144 -> 2847837553808
	2847843328144 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2847843328144 -> 2847837560144
	2847837560144 [label=AccumulateGrad]
	2847837562592 -> 2847837553808
	2847843328240 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2847843328240 -> 2847837562592
	2847837562592 [label=AccumulateGrad]
	2847837559088 -> 2847837558416
	2847843328624 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2847843328624 -> 2847837559088
	2847837559088 [label=AccumulateGrad]
	2847837563264 -> 2847837556736
	2847843328720 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2847843328720 -> 2847837563264
	2847837563264 [label=AccumulateGrad]
	2847837557264 -> 2847837556736
	2847843328816 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2847843328816 -> 2847837557264
	2847837557264 [label=AccumulateGrad]
	2847837559856 -> 2847837561680
	2847837566000 -> 2847837562832
	2847843329200 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2847843329200 -> 2847837566000
	2847837566000 [label=AccumulateGrad]
	2847837562112 -> 2847837566192
	2847843329296 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2847843329296 -> 2847837562112
	2847837562112 [label=AccumulateGrad]
	2847837561488 -> 2847837566192
	2847843329392 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2847843329392 -> 2847837561488
	2847837561488 [label=AccumulateGrad]
	2847837554336 -> 2847837562544
	2847843329776 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2847843329776 -> 2847837554336
	2847837554336 [label=AccumulateGrad]
	2847837558320 -> 2847837561056
	2847843329872 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2847843329872 -> 2847837558320
	2847837558320 [label=AccumulateGrad]
	2847837565904 -> 2847837561056
	2847843329968 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2847843329968 -> 2847837565904
	2847837565904 [label=AccumulateGrad]
	2847837561584 -> 2847837557024
	2847843330352 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2847843330352 -> 2847837561584
	2847837561584 [label=AccumulateGrad]
	2847837560528 -> 2847837558512
	2847843330448 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2847843330448 -> 2847837560528
	2847837560528 [label=AccumulateGrad]
	2847837564320 -> 2847837558512
	2847843330544 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2847843330544 -> 2847837564320
	2847837564320 [label=AccumulateGrad]
	2847837561632 -> 2847837556688
	2847837563072 -> 2847837565232
	2847843331600 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2847843331600 -> 2847837563072
	2847837563072 [label=AccumulateGrad]
	2847837566576 -> 2847837561728
	2847843331696 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2847843331696 -> 2847837566576
	2847837566576 [label=AccumulateGrad]
	2847837553904 -> 2847837561728
	2847843331792 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2847843331792 -> 2847837553904
	2847837553904 [label=AccumulateGrad]
	2847837558128 -> 2847837562688
	2847843332176 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2847843332176 -> 2847837558128
	2847837558128 [label=AccumulateGrad]
	2847837555824 -> 2847837555440
	2847843332272 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2847843332272 -> 2847837555824
	2847837555824 [label=AccumulateGrad]
	2847837561008 -> 2847837555440
	2847843332368 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2847843332368 -> 2847837561008
	2847837561008 [label=AccumulateGrad]
	2847837555632 -> 2847837554192
	2847843332752 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2847843332752 -> 2847837555632
	2847837555632 [label=AccumulateGrad]
	2847837567008 -> 2847837562208
	2847843332848 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2847843332848 -> 2847837567008
	2847837567008 [label=AccumulateGrad]
	2847837564560 -> 2847837562208
	2847843332944 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2847843332944 -> 2847837564560
	2847837564560 [label=AccumulateGrad]
	2847837563024 -> 2847837566864
	2847837563024 [label=CudnnBatchNormBackward0]
	2847837564656 -> 2847837563024
	2847837564656 [label=ConvolutionBackward0]
	2847837559184 -> 2847837564656
	2847837567392 -> 2847837564656
	2847843331024 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2847843331024 -> 2847837567392
	2847837567392 [label=AccumulateGrad]
	2847837567200 -> 2847837563024
	2847843331120 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2847843331120 -> 2847837567200
	2847837567200 [label=AccumulateGrad]
	2847837568304 -> 2847837563024
	2847843331216 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2847843331216 -> 2847837568304
	2847837568304 [label=AccumulateGrad]
	2847837553520 -> 2847837557696
	2847843333328 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2847843333328 -> 2847837553520
	2847837553520 [label=AccumulateGrad]
	2847837566912 -> 2847837563792
	2847843333424 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2847843333424 -> 2847837566912
	2847837566912 [label=AccumulateGrad]
	2847837566528 -> 2847837563792
	2847843549360 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2847843549360 -> 2847837566528
	2847837566528 [label=AccumulateGrad]
	2847837559568 -> 2847837554096
	2847843549744 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2847843549744 -> 2847837559568
	2847837559568 [label=AccumulateGrad]
	2847837556496 -> 2847837568928
	2847843549840 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2847843549840 -> 2847837556496
	2847837556496 [label=AccumulateGrad]
	2847837568832 -> 2847837568928
	2847843549936 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2847843549936 -> 2847837568832
	2847837568832 [label=AccumulateGrad]
	2847837568736 -> 2847837568592
	2847843550320 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2847843550320 -> 2847837568736
	2847837568736 [label=AccumulateGrad]
	2847837568544 -> 2847837568448
	2847843550416 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2847843550416 -> 2847837568544
	2847837568544 [label=AccumulateGrad]
	2847837568496 -> 2847837568448
	2847843550512 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2847843550512 -> 2847837568496
	2847837568496 [label=AccumulateGrad]
	2847837568400 -> 2847837568352
	2847837556256 -> 2847837562400
	2847843550896 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2847843550896 -> 2847837556256
	2847837556256 [label=AccumulateGrad]
	2847837558992 -> 2847837565088
	2847843550992 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2847843550992 -> 2847837558992
	2847837558992 [label=AccumulateGrad]
	2847837555344 -> 2847837565088
	2847843551088 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2847843551088 -> 2847837555344
	2847837555344 [label=AccumulateGrad]
	2847837567248 -> 2847837567728
	2847843551472 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2847843551472 -> 2847837567248
	2847837567248 [label=AccumulateGrad]
	2847837560288 -> 2847837564608
	2847843551568 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2847843551568 -> 2847837560288
	2847837560288 [label=AccumulateGrad]
	2847837566096 -> 2847837564608
	2847843551856 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2847843551856 -> 2847837566096
	2847837566096 [label=AccumulateGrad]
	2847837556448 -> 2847837557216
	2847851145424 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2847851145424 -> 2847837556448
	2847837556448 [label=AccumulateGrad]
	2847837565856 -> 2847837553664
	2847851145520 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2847851145520 -> 2847837565856
	2847837565856 [label=AccumulateGrad]
	2847837563552 -> 2847837553664
	2847851145616 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2847851145616 -> 2847837563552
	2847837563552 [label=AccumulateGrad]
	2847837554912 -> 2847837556592
	2847837559328 -> 2847837564272
	2847837559328 [label=TBackward0]
	2847837561872 -> 2847837559328
	2847851146288 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2847851146288 -> 2847837561872
	2847837561872 [label=AccumulateGrad]
	2847837564272 -> 2847848802128
}
