digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1685438024592 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1685439247792 [label=AddmmBackward0]
	1685439247456 -> 1685439247792
	1685440755664 [label="fc.bias
 (19)" fillcolor=lightblue]
	1685440755664 -> 1685439247456
	1685439247456 [label=AccumulateGrad]
	1685439246832 -> 1685439247792
	1685439246832 [label=ViewBackward0]
	1685439246352 -> 1685439246832
	1685439246352 [label=MeanBackward1]
	1685439246016 -> 1685439246352
	1685439246016 [label=ReluBackward0]
	1685439245536 -> 1685439246016
	1685439245536 [label=AddBackward0]
	1685439245056 -> 1685439245536
	1685439245056 [label=CudnnBatchNormBackward0]
	1685439243952 -> 1685439245056
	1685439243952 [label=ConvolutionBackward0]
	1685439242992 -> 1685439243952
	1685439242992 [label=ReluBackward0]
	1685439242656 -> 1685439242992
	1685439242656 [label=CudnnBatchNormBackward0]
	1685439242176 -> 1685439242656
	1685439242176 [label=ConvolutionBackward0]
	1685439244912 -> 1685439242176
	1685439244912 [label=ReluBackward0]
	1685439240736 -> 1685439244912
	1685439240736 [label=AddBackward0]
	1685439240256 -> 1685439240736
	1685439240256 [label=CudnnBatchNormBackward0]
	1685439239152 -> 1685439240256
	1685439239152 [label=ConvolutionBackward0]
	1685439238192 -> 1685439239152
	1685439238192 [label=ReluBackward0]
	1685439237856 -> 1685439238192
	1685439237856 [label=CudnnBatchNormBackward0]
	1685439236512 -> 1685439237856
	1685439236512 [label=ConvolutionBackward0]
	1685439235792 -> 1685439236512
	1685439235792 [label=ReluBackward0]
	1685439249952 -> 1685439235792
	1685439249952 [label=AddBackward0]
	1685439249808 -> 1685439249952
	1685439249808 [label=CudnnBatchNormBackward0]
	1685439249424 -> 1685439249808
	1685439249424 [label=ConvolutionBackward0]
	1685439249568 -> 1685439249424
	1685439249568 [label=ReluBackward0]
	1685439249136 -> 1685439249568
	1685439249136 [label=CudnnBatchNormBackward0]
	1685439249040 -> 1685439249136
	1685439249040 [label=ConvolutionBackward0]
	1685439250048 -> 1685439249040
	1685439250048 [label=ReluBackward0]
	1685439248656 -> 1685439250048
	1685439248656 [label=AddBackward0]
	1685439248560 -> 1685439248656
	1685439248560 [label=CudnnBatchNormBackward0]
	1685439248608 -> 1685439248560
	1685439248608 [label=ConvolutionBackward0]
	1685439247984 -> 1685439248608
	1685439247984 [label=ReluBackward0]
	1685439248032 -> 1685439247984
	1685439248032 [label=CudnnBatchNormBackward0]
	1685439247888 -> 1685439248032
	1685439247888 [label=ConvolutionBackward0]
	1685439247600 -> 1685439247888
	1685439247600 [label=ReluBackward0]
	1685439247648 -> 1685439247600
	1685439247648 [label=AddBackward0]
	1685439247360 -> 1685439247648
	1685439247360 [label=CudnnBatchNormBackward0]
	1685439247120 -> 1685439247360
	1685439247120 [label=ConvolutionBackward0]
	1685439246928 -> 1685439247120
	1685439246928 [label=ReluBackward0]
	1685439246544 -> 1685439246928
	1685439246544 [label=CudnnBatchNormBackward0]
	1685439246784 -> 1685439246544
	1685439246784 [label=ConvolutionBackward0]
	1685439247408 -> 1685439246784
	1685439247408 [label=ReluBackward0]
	1685439246064 -> 1685439247408
	1685439246064 [label=AddBackward0]
	1685439246304 -> 1685439246064
	1685439246304 [label=CudnnBatchNormBackward0]
	1685439245968 -> 1685439246304
	1685439245968 [label=ConvolutionBackward0]
	1685439245680 -> 1685439245968
	1685439245680 [label=ReluBackward0]
	1685439245728 -> 1685439245680
	1685439245728 [label=CudnnBatchNormBackward0]
	1685439245440 -> 1685439245728
	1685439245440 [label=ConvolutionBackward0]
	1685439245344 -> 1685439245440
	1685439245344 [label=ReluBackward0]
	1685439245008 -> 1685439245344
	1685439245008 [label=AddBackward0]
	1685439244816 -> 1685439245008
	1685439244816 [label=CudnnBatchNormBackward0]
	1685439244864 -> 1685439244816
	1685439244864 [label=ConvolutionBackward0]
	1685439244480 -> 1685439244864
	1685439244480 [label=ReluBackward0]
	1685439244240 -> 1685439244480
	1685439244240 [label=CudnnBatchNormBackward0]
	1685439244192 -> 1685439244240
	1685439244192 [label=ConvolutionBackward0]
	1685439244960 -> 1685439244192
	1685439244960 [label=ReluBackward0]
	1685439243760 -> 1685439244960
	1685439243760 [label=AddBackward0]
	1685439243712 -> 1685439243760
	1685439243712 [label=CudnnBatchNormBackward0]
	1685439243520 -> 1685439243712
	1685439243520 [label=ConvolutionBackward0]
	1685439243424 -> 1685439243520
	1685439243424 [label=ReluBackward0]
	1685439243088 -> 1685439243424
	1685439243088 [label=CudnnBatchNormBackward0]
	1685439242896 -> 1685439243088
	1685439242896 [label=ConvolutionBackward0]
	1685439243904 -> 1685439242896
	1685439243904 [label=MaxPool2DWithIndicesBackward0]
	1685439242608 -> 1685439243904
	1685439242608 [label=ReluBackward0]
	1685439242416 -> 1685439242608
	1685439242416 [label=CudnnBatchNormBackward0]
	1685439242320 -> 1685439242416
	1685439242320 [label=ConvolutionBackward0]
	1685439242128 -> 1685439242320
	1685440763888 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1685440763888 -> 1685439242128
	1685439242128 [label=AccumulateGrad]
	1685439242224 -> 1685439242416
	1685440763120 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1685440763120 -> 1685439242224
	1685439242224 [label=AccumulateGrad]
	1685439242800 -> 1685439242416
	1685440762832 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1685440762832 -> 1685439242800
	1685439242800 [label=AccumulateGrad]
	1685439242752 -> 1685439242896
	1685440763312 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1685440763312 -> 1685439242752
	1685439242752 [label=AccumulateGrad]
	1685439243040 -> 1685439243088
	1685440763408 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1685440763408 -> 1685439243040
	1685439243040 [label=AccumulateGrad]
	1685439243232 -> 1685439243088
	1685440763504 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1685440763504 -> 1685439243232
	1685439243232 [label=AccumulateGrad]
	1685439243280 -> 1685439243520
	1685440762736 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1685440762736 -> 1685439243280
	1685439243280 [label=AccumulateGrad]
	1685439243568 -> 1685439243712
	1685440762640 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1685440762640 -> 1685439243568
	1685439243568 [label=AccumulateGrad]
	1685439243808 -> 1685439243712
	1685440761872 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1685440761872 -> 1685439243808
	1685439243808 [label=AccumulateGrad]
	1685439243904 -> 1685439243760
	1685439243856 -> 1685439244192
	1685440761968 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1685440761968 -> 1685439243856
	1685439243856 [label=AccumulateGrad]
	1685439244384 -> 1685439244240
	1685440762064 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1685440762064 -> 1685439244384
	1685439244384 [label=AccumulateGrad]
	1685439244336 -> 1685439244240
	1685440762160 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1685440762160 -> 1685439244336
	1685439244336 [label=AccumulateGrad]
	1685439244528 -> 1685439244864
	1685440762448 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1685440762448 -> 1685439244528
	1685439244528 [label=AccumulateGrad]
	1685439244720 -> 1685439244816
	1685440761488 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1685440761488 -> 1685439244720
	1685439244720 [label=AccumulateGrad]
	1685439244624 -> 1685439244816
	1685440761392 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1685440761392 -> 1685439244624
	1685439244624 [label=AccumulateGrad]
	1685439244960 -> 1685439245008
	1685439245200 -> 1685439245440
	1685440761296 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1685440761296 -> 1685439245200
	1685439245200 [label=AccumulateGrad]
	1685439245488 -> 1685439245728
	1685440761200 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1685440761200 -> 1685439245488
	1685439245488 [label=AccumulateGrad]
	1685439245824 -> 1685439245728
	1685440747504 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1685440747504 -> 1685439245824
	1685439245824 [label=AccumulateGrad]
	1685439245584 -> 1685439245968
	1685440747888 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1685440747888 -> 1685439245584
	1685439245584 [label=AccumulateGrad]
	1685439246208 -> 1685439246304
	1685440747984 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1685440747984 -> 1685439246208
	1685439246208 [label=AccumulateGrad]
	1685439246112 -> 1685439246304
	1685440748080 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1685440748080 -> 1685439246112
	1685439246112 [label=AccumulateGrad]
	1685439246160 -> 1685439246064
	1685439246160 [label=CudnnBatchNormBackward0]
	1685439245104 -> 1685439246160
	1685439245104 [label=ConvolutionBackward0]
	1685439245344 -> 1685439245104
	1685439245152 -> 1685439245104
	1685440758704 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1685440758704 -> 1685439245152
	1685439245152 [label=AccumulateGrad]
	1685439245776 -> 1685439246160
	1685440758896 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1685440758896 -> 1685439245776
	1685439245776 [label=AccumulateGrad]
	1685439245920 -> 1685439246160
	1685440760528 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1685440760528 -> 1685439245920
	1685439245920 [label=AccumulateGrad]
	1685439246400 -> 1685439246784
	1685440748464 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1685440748464 -> 1685439246400
	1685439246400 [label=AccumulateGrad]
	1685439246640 -> 1685439246544
	1685440748560 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1685440748560 -> 1685439246640
	1685439246640 [label=AccumulateGrad]
	1685439246880 -> 1685439246544
	1685440748656 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1685440748656 -> 1685439246880
	1685439246880 [label=AccumulateGrad]
	1685439247168 -> 1685439247120
	1685440749040 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1685440749040 -> 1685439247168
	1685439247168 [label=AccumulateGrad]
	1685439247024 -> 1685439247360
	1685440749136 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1685440749136 -> 1685439247024
	1685439247024 [label=AccumulateGrad]
	1685439247216 -> 1685439247360
	1685440749232 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1685440749232 -> 1685439247216
	1685439247216 [label=AccumulateGrad]
	1685439247408 -> 1685439247648
	1685439247504 -> 1685439247888
	1685440750192 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1685440750192 -> 1685439247504
	1685439247504 [label=AccumulateGrad]
	1685439248128 -> 1685439248032
	1685440750288 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1685440750288 -> 1685439248128
	1685439248128 [label=AccumulateGrad]
	1685439248080 -> 1685439248032
	1685440750384 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1685440750384 -> 1685439248080
	1685439248080 [label=AccumulateGrad]
	1685439248176 -> 1685439248608
	1685440750768 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1685440750768 -> 1685439248176
	1685439248176 [label=AccumulateGrad]
	1685439248512 -> 1685439248560
	1685440750864 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1685440750864 -> 1685439248512
	1685439248512 [label=AccumulateGrad]
	1685439248704 -> 1685439248560
	1685440750960 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1685440750960 -> 1685439248704
	1685439248704 [label=AccumulateGrad]
	1685439248464 -> 1685439248656
	1685439248464 [label=CudnnBatchNormBackward0]
	1685439247696 -> 1685439248464
	1685439247696 [label=ConvolutionBackward0]
	1685439247600 -> 1685439247696
	1685439247744 -> 1685439247696
	1685440749616 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1685440749616 -> 1685439247744
	1685439247744 [label=AccumulateGrad]
	1685439248320 -> 1685439248464
	1685440749712 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1685440749712 -> 1685439248320
	1685439248320 [label=AccumulateGrad]
	1685439248368 -> 1685439248464
	1685440749808 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1685440749808 -> 1685439248368
	1685439248368 [label=AccumulateGrad]
	1685439248848 -> 1685439249040
	1685440751344 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1685440751344 -> 1685439248848
	1685439248848 [label=AccumulateGrad]
	1685439248944 -> 1685439249136
	1685440751440 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1685440751440 -> 1685439248944
	1685439248944 [label=AccumulateGrad]
	1685439249328 -> 1685439249136
	1685440751536 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1685440751536 -> 1685439249328
	1685439249328 [label=AccumulateGrad]
	1685439249472 -> 1685439249424
	1685440751920 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1685440751920 -> 1685439249472
	1685439249472 [label=AccumulateGrad]
	1685439249616 -> 1685439249808
	1685440752016 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1685440752016 -> 1685439249616
	1685439249616 [label=AccumulateGrad]
	1685439249760 -> 1685439249808
	1685440752112 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1685440752112 -> 1685439249760
	1685439249760 [label=AccumulateGrad]
	1685439250048 -> 1685439249952
	1685439236608 -> 1685439236512
	1685440753072 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1685440753072 -> 1685439236608
	1685439236608 [label=AccumulateGrad]
	1685439235552 -> 1685439237856
	1685440753168 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1685440753168 -> 1685439235552
	1685439235552 [label=AccumulateGrad]
	1685439238336 -> 1685439237856
	1685440753264 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1685440753264 -> 1685439238336
	1685439238336 [label=AccumulateGrad]
	1685439238816 -> 1685439239152
	1685440753648 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1685440753648 -> 1685439238816
	1685439238816 [label=AccumulateGrad]
	1685439239776 -> 1685439240256
	1685440753744 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1685440753744 -> 1685439239776
	1685439239776 [label=AccumulateGrad]
	1685439239632 -> 1685439240256
	1685440753840 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1685440753840 -> 1685439239632
	1685439239632 [label=AccumulateGrad]
	1685439240112 -> 1685439240736
	1685439240112 [label=CudnnBatchNormBackward0]
	1685439236800 -> 1685439240112
	1685439236800 [label=ConvolutionBackward0]
	1685439235792 -> 1685439236800
	1685439250384 -> 1685439236800
	1685440752496 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1685440752496 -> 1685439250384
	1685439250384 [label=AccumulateGrad]
	1685439238672 -> 1685439240112
	1685440752592 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1685440752592 -> 1685439238672
	1685439238672 [label=AccumulateGrad]
	1685439239296 -> 1685439240112
	1685440752688 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1685440752688 -> 1685439239296
	1685439239296 [label=AccumulateGrad]
	1685439241216 -> 1685439242176
	1685440754224 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1685440754224 -> 1685439241216
	1685439241216 [label=AccumulateGrad]
	1685439242032 -> 1685439242656
	1685440754320 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1685440754320 -> 1685439242032
	1685439242032 [label=AccumulateGrad]
	1685439243136 -> 1685439242656
	1685440754416 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1685440754416 -> 1685439243136
	1685439243136 [label=AccumulateGrad]
	1685439243616 -> 1685439243952
	1685440754800 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1685440754800 -> 1685439243616
	1685439243616 [label=AccumulateGrad]
	1685439244576 -> 1685439245056
	1685440754896 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1685440754896 -> 1685439244576
	1685439244576 [label=AccumulateGrad]
	1685439244432 -> 1685439245056
	1685440754992 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1685440754992 -> 1685439244432
	1685439244432 [label=AccumulateGrad]
	1685439244912 -> 1685439245536
	1685439246976 -> 1685439247792
	1685439246976 [label=TBackward0]
	1685439245392 -> 1685439246976
	1685440755568 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1685440755568 -> 1685439245392
	1685439245392 [label=AccumulateGrad]
	1685439247792 -> 1685438024592
}
