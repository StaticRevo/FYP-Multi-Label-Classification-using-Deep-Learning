digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2108626465808 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2108713985184 [label=AddmmBackward0]
	2108713985328 -> 2108713985184
	2108709838704 [label="fc.bias
 (19)" fillcolor=lightblue]
	2108709838704 -> 2108713985328
	2108713985328 [label=AccumulateGrad]
	2108713985376 -> 2108713985184
	2108713985376 [label=ViewBackward0]
	2108713985472 -> 2108713985376
	2108713985472 [label=MeanBackward1]
	2108713985616 -> 2108713985472
	2108713985616 [label=ReluBackward0]
	2108713985712 -> 2108713985616
	2108713985712 [label=AddBackward0]
	2108713985808 -> 2108713985712
	2108713985808 [label=CudnnBatchNormBackward0]
	2108713985952 -> 2108713985808
	2108713985952 [label=ConvolutionBackward0]
	2108713986144 -> 2108713985952
	2108713986144 [label=ReluBackward0]
	2108713986288 -> 2108713986144
	2108713986288 [label=CudnnBatchNormBackward0]
	2108713986384 -> 2108713986288
	2108713986384 [label=ConvolutionBackward0]
	2108713985760 -> 2108713986384
	2108713985760 [label=ReluBackward0]
	2108713986672 -> 2108713985760
	2108713986672 [label=AddBackward0]
	2108713986768 -> 2108713986672
	2108713986768 [label=CudnnBatchNormBackward0]
	2108713986912 -> 2108713986768
	2108713986912 [label=ConvolutionBackward0]
	2108713987104 -> 2108713986912
	2108713987104 [label=ReluBackward0]
	2108713987248 -> 2108713987104
	2108713987248 [label=CudnnBatchNormBackward0]
	2108713987344 -> 2108713987248
	2108713987344 [label=ConvolutionBackward0]
	2108713987536 -> 2108713987344
	2108713987536 [label=ReluBackward0]
	2108713987680 -> 2108713987536
	2108713987680 [label=AddBackward0]
	2108713987776 -> 2108713987680
	2108713987776 [label=CudnnBatchNormBackward0]
	2108713987920 -> 2108713987776
	2108713987920 [label=ConvolutionBackward0]
	2108713988112 -> 2108713987920
	2108713988112 [label=ReluBackward0]
	2108713988256 -> 2108713988112
	2108713988256 [label=CudnnBatchNormBackward0]
	2108713988352 -> 2108713988256
	2108713988352 [label=ConvolutionBackward0]
	2108713987728 -> 2108713988352
	2108713987728 [label=ReluBackward0]
	2108713982976 -> 2108713987728
	2108713982976 [label=AddBackward0]
	2108713983072 -> 2108713982976
	2108713983072 [label=CudnnBatchNormBackward0]
	2108713983168 -> 2108713983072
	2108713983168 [label=ConvolutionBackward0]
	2108713983360 -> 2108713983168
	2108713983360 [label=ReluBackward0]
	2108713983504 -> 2108713983360
	2108713983504 [label=CudnnBatchNormBackward0]
	2108713983600 -> 2108713983504
	2108713983600 [label=ConvolutionBackward0]
	2108713983792 -> 2108713983600
	2108713983792 [label=ReluBackward0]
	2108713983936 -> 2108713983792
	2108713983936 [label=AddBackward0]
	2108713984032 -> 2108713983936
	2108713984032 [label=CudnnBatchNormBackward0]
	2108713984176 -> 2108713984032
	2108713984176 [label=ConvolutionBackward0]
	2108713984368 -> 2108713984176
	2108713984368 [label=ReluBackward0]
	2108713984512 -> 2108713984368
	2108713984512 [label=CudnnBatchNormBackward0]
	2108713984608 -> 2108713984512
	2108713984608 [label=ConvolutionBackward0]
	2108713983984 -> 2108713984608
	2108713983984 [label=ReluBackward0]
	2108713988688 -> 2108713983984
	2108713988688 [label=AddBackward0]
	2108713988784 -> 2108713988688
	2108713988784 [label=CudnnBatchNormBackward0]
	2108713988928 -> 2108713988784
	2108713988928 [label=ConvolutionBackward0]
	2108713989120 -> 2108713988928
	2108713989120 [label=ReluBackward0]
	2108713989264 -> 2108713989120
	2108713989264 [label=CudnnBatchNormBackward0]
	2108713989360 -> 2108713989264
	2108713989360 [label=ConvolutionBackward0]
	2108713989552 -> 2108713989360
	2108713989552 [label=ReluBackward0]
	2108713989696 -> 2108713989552
	2108713989696 [label=AddBackward0]
	2108713989792 -> 2108713989696
	2108713989792 [label=CudnnBatchNormBackward0]
	2108713989936 -> 2108713989792
	2108713989936 [label=ConvolutionBackward0]
	2108713990128 -> 2108713989936
	2108713990128 [label=ReluBackward0]
	2108713990272 -> 2108713990128
	2108713990272 [label=CudnnBatchNormBackward0]
	2108713990368 -> 2108713990272
	2108713990368 [label=ConvolutionBackward0]
	2108713989744 -> 2108713990368
	2108713989744 [label=ReluBackward0]
	2108713990656 -> 2108713989744
	2108713990656 [label=AddBackward0]
	2108713990752 -> 2108713990656
	2108713990752 [label=CudnnBatchNormBackward0]
	2108713990896 -> 2108713990752
	2108713990896 [label=ConvolutionBackward0]
	2108713991088 -> 2108713990896
	2108713991088 [label=ReluBackward0]
	2108713991232 -> 2108713991088
	2108713991232 [label=CudnnBatchNormBackward0]
	2108713991328 -> 2108713991232
	2108713991328 [label=ConvolutionBackward0]
	2108713990704 -> 2108713991328
	2108713990704 [label=MaxPool2DWithIndicesBackward0]
	2108713991616 -> 2108713990704
	2108713991616 [label=ReluBackward0]
	2108713991712 -> 2108713991616
	2108713991712 [label=CudnnBatchNormBackward0]
	2108713991808 -> 2108713991712
	2108713991808 [label=ConvolutionBackward0]
	2108713992000 -> 2108713991808
	2108709838512 [label="conv1.weight
 (64, 6, 7, 7)" fillcolor=lightblue]
	2108709838512 -> 2108713992000
	2108713992000 [label=AccumulateGrad]
	2108713991760 -> 2108713991712
	2108714217744 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2108714217744 -> 2108713991760
	2108713991760 [label=AccumulateGrad]
	2108713991424 -> 2108713991712
	2108714217840 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2108714217840 -> 2108713991424
	2108713991424 [label=AccumulateGrad]
	2108713991520 -> 2108713991328
	2108714218224 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2108714218224 -> 2108713991520
	2108713991520 [label=AccumulateGrad]
	2108713991280 -> 2108713991232
	2108714218320 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2108714218320 -> 2108713991280
	2108713991280 [label=AccumulateGrad]
	2108713991136 -> 2108713991232
	2108714218416 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2108714218416 -> 2108713991136
	2108713991136 [label=AccumulateGrad]
	2108713991040 -> 2108713990896
	2108714218800 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2108714218800 -> 2108713991040
	2108713991040 [label=AccumulateGrad]
	2108713990848 -> 2108713990752
	2108714218896 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2108714218896 -> 2108713990848
	2108713990848 [label=AccumulateGrad]
	2108713990800 -> 2108713990752
	2108714218992 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2108714218992 -> 2108713990800
	2108713990800 [label=AccumulateGrad]
	2108713990704 -> 2108713990656
	2108713990560 -> 2108713990368
	2108714219376 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2108714219376 -> 2108713990560
	2108713990560 [label=AccumulateGrad]
	2108713990320 -> 2108713990272
	2108714219472 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2108714219472 -> 2108713990320
	2108713990320 [label=AccumulateGrad]
	2108713990176 -> 2108713990272
	2108714219568 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2108714219568 -> 2108713990176
	2108713990176 [label=AccumulateGrad]
	2108713990080 -> 2108713989936
	2108714219952 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2108714219952 -> 2108713990080
	2108713990080 [label=AccumulateGrad]
	2108713989888 -> 2108713989792
	2108714220048 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2108714220048 -> 2108713989888
	2108713989888 [label=AccumulateGrad]
	2108713989840 -> 2108713989792
	2108714220144 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2108714220144 -> 2108713989840
	2108713989840 [label=AccumulateGrad]
	2108713989744 -> 2108713989696
	2108713989504 -> 2108713989360
	2108714221104 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2108714221104 -> 2108713989504
	2108713989504 [label=AccumulateGrad]
	2108713989312 -> 2108713989264
	2108714221200 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2108714221200 -> 2108713989312
	2108713989312 [label=AccumulateGrad]
	2108713989168 -> 2108713989264
	2108714221296 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2108714221296 -> 2108713989168
	2108713989168 [label=AccumulateGrad]
	2108713989072 -> 2108713988928
	2108709830832 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2108709830832 -> 2108713989072
	2108713989072 [label=AccumulateGrad]
	2108713988880 -> 2108713988784
	2108709830928 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2108709830928 -> 2108713988880
	2108713988880 [label=AccumulateGrad]
	2108713988832 -> 2108713988784
	2108709831024 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2108709831024 -> 2108713988832
	2108713988832 [label=AccumulateGrad]
	2108713988736 -> 2108713988688
	2108713988736 [label=CudnnBatchNormBackward0]
	2108713989456 -> 2108713988736
	2108713989456 [label=ConvolutionBackward0]
	2108713989552 -> 2108713989456
	2108713989600 -> 2108713989456
	2108714220528 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2108714220528 -> 2108713989600
	2108713989600 [label=AccumulateGrad]
	2108713989024 -> 2108713988736
	2108714220624 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2108714220624 -> 2108713989024
	2108713989024 [label=AccumulateGrad]
	2108713988976 -> 2108713988736
	2108714220720 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2108714220720 -> 2108713988976
	2108713988976 [label=AccumulateGrad]
	2108713982928 -> 2108713984608
	2108709831408 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2108709831408 -> 2108713982928
	2108713982928 [label=AccumulateGrad]
	2108713984560 -> 2108713984512
	2108709831504 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2108709831504 -> 2108713984560
	2108713984560 [label=AccumulateGrad]
	2108713984416 -> 2108713984512
	2108709831600 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2108709831600 -> 2108713984416
	2108713984416 [label=AccumulateGrad]
	2108713984320 -> 2108713984176
	2108709831984 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2108709831984 -> 2108713984320
	2108713984320 [label=AccumulateGrad]
	2108713984128 -> 2108713984032
	2108709832080 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2108709832080 -> 2108713984128
	2108713984128 [label=AccumulateGrad]
	2108713984080 -> 2108713984032
	2108709832176 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2108709832176 -> 2108713984080
	2108713984080 [label=AccumulateGrad]
	2108713983984 -> 2108713983936
	2108713983744 -> 2108713983600
	2108709833136 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2108709833136 -> 2108713983744
	2108713983744 [label=AccumulateGrad]
	2108713983552 -> 2108713983504
	2108709833232 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2108709833232 -> 2108713983552
	2108713983552 [label=AccumulateGrad]
	2108713983408 -> 2108713983504
	2108709833328 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2108709833328 -> 2108713983408
	2108713983408 [label=AccumulateGrad]
	2108713983312 -> 2108713983168
	2108709833712 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2108709833712 -> 2108713983312
	2108713983312 [label=AccumulateGrad]
	2108713983120 -> 2108713983072
	2108709833808 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2108709833808 -> 2108713983120
	2108713983120 [label=AccumulateGrad]
	2108713983024 -> 2108713983072
	2108709833904 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2108709833904 -> 2108713983024
	2108713983024 [label=AccumulateGrad]
	2108713982832 -> 2108713982976
	2108713982832 [label=CudnnBatchNormBackward0]
	2108713983696 -> 2108713982832
	2108713983696 [label=ConvolutionBackward0]
	2108713983792 -> 2108713983696
	2108713983840 -> 2108713983696
	2108709832560 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2108709832560 -> 2108713983840
	2108713983840 [label=AccumulateGrad]
	2108713983264 -> 2108713982832
	2108709832656 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2108709832656 -> 2108713983264
	2108713983264 [label=AccumulateGrad]
	2108713983216 -> 2108713982832
	2108709832752 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2108709832752 -> 2108713983216
	2108713983216 [label=AccumulateGrad]
	2108713988544 -> 2108713988352
	2108709834288 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2108709834288 -> 2108713988544
	2108713988544 [label=AccumulateGrad]
	2108713988304 -> 2108713988256
	2108709834384 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2108709834384 -> 2108713988304
	2108713988304 [label=AccumulateGrad]
	2108713988160 -> 2108713988256
	2108709834480 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2108709834480 -> 2108713988160
	2108713988160 [label=AccumulateGrad]
	2108713988064 -> 2108713987920
	2108709834864 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2108709834864 -> 2108713988064
	2108713988064 [label=AccumulateGrad]
	2108713987872 -> 2108713987776
	2108709834960 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2108709834960 -> 2108713987872
	2108713987872 [label=AccumulateGrad]
	2108713987824 -> 2108713987776
	2108709835056 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2108709835056 -> 2108713987824
	2108713987824 [label=AccumulateGrad]
	2108713987728 -> 2108713987680
	2108713987488 -> 2108713987344
	2108709836016 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2108709836016 -> 2108713987488
	2108713987488 [label=AccumulateGrad]
	2108713987296 -> 2108713987248
	2108709836112 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2108709836112 -> 2108713987296
	2108713987296 [label=AccumulateGrad]
	2108713987152 -> 2108713987248
	2108709836208 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2108709836208 -> 2108713987152
	2108713987152 [label=AccumulateGrad]
	2108713987056 -> 2108713986912
	2108709836592 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2108709836592 -> 2108713987056
	2108713987056 [label=AccumulateGrad]
	2108713986864 -> 2108713986768
	2108709836688 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2108709836688 -> 2108713986864
	2108713986864 [label=AccumulateGrad]
	2108713986816 -> 2108713986768
	2108709836784 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2108709836784 -> 2108713986816
	2108713986816 [label=AccumulateGrad]
	2108713986720 -> 2108713986672
	2108713986720 [label=CudnnBatchNormBackward0]
	2108713987440 -> 2108713986720
	2108713987440 [label=ConvolutionBackward0]
	2108713987536 -> 2108713987440
	2108713987584 -> 2108713987440
	2108709835440 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2108709835440 -> 2108713987584
	2108713987584 [label=AccumulateGrad]
	2108713987008 -> 2108713986720
	2108709835536 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2108709835536 -> 2108713987008
	2108713987008 [label=AccumulateGrad]
	2108713986960 -> 2108713986720
	2108709835632 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2108709835632 -> 2108713986960
	2108713986960 [label=AccumulateGrad]
	2108713986576 -> 2108713986384
	2108709837168 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2108709837168 -> 2108713986576
	2108713986576 [label=AccumulateGrad]
	2108713986336 -> 2108713986288
	2108709837264 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2108709837264 -> 2108713986336
	2108713986336 [label=AccumulateGrad]
	2108713986192 -> 2108713986288
	2108709837360 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2108709837360 -> 2108713986192
	2108713986192 [label=AccumulateGrad]
	2108713986096 -> 2108713985952
	2108709837744 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2108709837744 -> 2108713986096
	2108713986096 [label=AccumulateGrad]
	2108713985904 -> 2108713985808
	2108709837840 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2108709837840 -> 2108713985904
	2108713985904 [label=AccumulateGrad]
	2108713985856 -> 2108713985808
	2108709837936 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2108709837936 -> 2108713985856
	2108713985856 [label=AccumulateGrad]
	2108713985760 -> 2108713985712
	2108713985424 -> 2108713985184
	2108713985424 [label=TBackward0]
	2108713985664 -> 2108713985424
	2108709838608 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2108709838608 -> 2108713985664
	2108713985664 [label=AccumulateGrad]
	2108713985184 -> 2108626465808
}
