digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2211062401456 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2210906027248 [label=AddmmBackward0]
	2210906026912 -> 2210906027248
	2210906158768 [label="fc.bias
 (19)" fillcolor=lightblue]
	2210906158768 -> 2210906026912
	2210906026912 [label=AccumulateGrad]
	2210906026288 -> 2210906027248
	2210906026288 [label=ViewBackward0]
	2210906025808 -> 2210906026288
	2210906025808 [label=MeanBackward1]
	2210906025472 -> 2210906025808
	2210906025472 [label=ReluBackward0]
	2210906024992 -> 2210906025472
	2210906024992 [label=AddBackward0]
	2210906024512 -> 2210906024992
	2210906024512 [label=CudnnBatchNormBackward0]
	2210906023408 -> 2210906024512
	2210906023408 [label=ConvolutionBackward0]
	2210906022448 -> 2210906023408
	2210906022448 [label=ReluBackward0]
	2210906022112 -> 2210906022448
	2210906022112 [label=CudnnBatchNormBackward0]
	2210906021632 -> 2210906022112
	2210906021632 [label=ConvolutionBackward0]
	2210906024368 -> 2210906021632
	2210906024368 [label=ReluBackward0]
	2210906020192 -> 2210906024368
	2210906020192 [label=AddBackward0]
	2210906019712 -> 2210906020192
	2210906019712 [label=CudnnBatchNormBackward0]
	2210906018608 -> 2210906019712
	2210906018608 [label=ConvolutionBackward0]
	2210906017648 -> 2210906018608
	2210906017648 [label=ReluBackward0]
	2210906017312 -> 2210906017648
	2210906017312 [label=CudnnBatchNormBackward0]
	2210906016832 -> 2210906017312
	2210906016832 [label=ConvolutionBackward0]
	2210906015872 -> 2210906016832
	2210906015872 [label=ReluBackward0]
	2210906014768 -> 2210906015872
	2210906014768 [label=AddBackward0]
	2210906014288 -> 2210906014768
	2210906014288 [label=CudnnBatchNormBackward0]
	2210906013952 -> 2210906014288
	2210906013952 [label=ConvolutionBackward0]
	2210906029360 -> 2210906013952
	2210906029360 [label=ReluBackward0]
	2210906029408 -> 2210906029360
	2210906029408 [label=CudnnBatchNormBackward0]
	2210906029264 -> 2210906029408
	2210906029264 [label=ConvolutionBackward0]
	2210906014912 -> 2210906029264
	2210906014912 [label=ReluBackward0]
	2210906028928 -> 2210906014912
	2210906028928 [label=AddBackward0]
	2210906028784 -> 2210906028928
	2210906028784 [label=CudnnBatchNormBackward0]
	2210906028400 -> 2210906028784
	2210906028400 [label=ConvolutionBackward0]
	2210906028544 -> 2210906028400
	2210906028544 [label=ReluBackward0]
	2210906028112 -> 2210906028544
	2210906028112 [label=CudnnBatchNormBackward0]
	2210906028016 -> 2210906028112
	2210906028016 [label=ConvolutionBackward0]
	2210906027824 -> 2210906028016
	2210906027824 [label=ReluBackward0]
	2210906027440 -> 2210906027824
	2210906027440 [label=AddBackward0]
	2210906027680 -> 2210906027440
	2210906027680 [label=CudnnBatchNormBackward0]
	2210906027344 -> 2210906027680
	2210906027344 [label=ConvolutionBackward0]
	2210906027056 -> 2210906027344
	2210906027056 [label=ReluBackward0]
	2210906027104 -> 2210906027056
	2210906027104 [label=CudnnBatchNormBackward0]
	2210906026816 -> 2210906027104
	2210906026816 [label=ConvolutionBackward0]
	2210906027536 -> 2210906026816
	2210906027536 [label=ReluBackward0]
	2210906026624 -> 2210906027536
	2210906026624 [label=AddBackward0]
	2210906026336 -> 2210906026624
	2210906026336 [label=CudnnBatchNormBackward0]
	2210906026096 -> 2210906026336
	2210906026096 [label=ConvolutionBackward0]
	2210906025904 -> 2210906026096
	2210906025904 [label=ReluBackward0]
	2210906025520 -> 2210906025904
	2210906025520 [label=CudnnBatchNormBackward0]
	2210906025760 -> 2210906025520
	2210906025760 [label=ConvolutionBackward0]
	2210906025376 -> 2210906025760
	2210906025376 [label=ReluBackward0]
	2210906025136 -> 2210906025376
	2210906025136 [label=AddBackward0]
	2210906025088 -> 2210906025136
	2210906025088 [label=CudnnBatchNormBackward0]
	2210906024896 -> 2210906025088
	2210906024896 [label=ConvolutionBackward0]
	2210906024800 -> 2210906024896
	2210906024800 [label=ReluBackward0]
	2210906024464 -> 2210906024800
	2210906024464 [label=CudnnBatchNormBackward0]
	2210906024272 -> 2210906024464
	2210906024272 [label=ConvolutionBackward0]
	2210906025280 -> 2210906024272
	2210906025280 [label=ReluBackward0]
	2210906023984 -> 2210906025280
	2210906023984 [label=AddBackward0]
	2210906023792 -> 2210906023984
	2210906023792 [label=CudnnBatchNormBackward0]
	2210906023840 -> 2210906023792
	2210906023840 [label=ConvolutionBackward0]
	2210906023456 -> 2210906023840
	2210906023456 [label=ReluBackward0]
	2210906023216 -> 2210906023456
	2210906023216 [label=CudnnBatchNormBackward0]
	2210906023168 -> 2210906023216
	2210906023168 [label=ConvolutionBackward0]
	2210906023936 -> 2210906023168
	2210906023936 [label=MaxPool2DWithIndicesBackward0]
	2210906022736 -> 2210906023936
	2210906022736 [label=ReluBackward0]
	2210906022688 -> 2210906022736
	2210906022688 [label=CudnnBatchNormBackward0]
	2210906022544 -> 2210906022688
	2210906022544 [label=ConvolutionBackward0]
	2210906022256 -> 2210906022544
	2210906158576 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2210906158576 -> 2210906022256
	2210906022256 [label=AccumulateGrad]
	2210906022784 -> 2210906022688
	2210909524848 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2210909524848 -> 2210906022784
	2210906022784 [label=AccumulateGrad]
	2210906023024 -> 2210906022688
	2210909524560 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2210909524560 -> 2210906023024
	2210906023024 [label=AccumulateGrad]
	2210906022832 -> 2210906023168
	2210909525040 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2210909525040 -> 2210906022832
	2210906022832 [label=AccumulateGrad]
	2210906023360 -> 2210906023216
	2210909525136 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2210909525136 -> 2210906023360
	2210906023360 [label=AccumulateGrad]
	2210906023312 -> 2210906023216
	2210909525232 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2210909525232 -> 2210906023312
	2210906023312 [label=AccumulateGrad]
	2210906023504 -> 2210906023840
	2210909524464 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2210909524464 -> 2210906023504
	2210906023504 [label=AccumulateGrad]
	2210906023696 -> 2210906023792
	2210909524368 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2210909524368 -> 2210906023696
	2210906023696 [label=AccumulateGrad]
	2210906023600 -> 2210906023792
	2210909523600 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2210909523600 -> 2210906023600
	2210906023600 [label=AccumulateGrad]
	2210906023936 -> 2210906023984
	2210906024128 -> 2210906024272
	2210909523696 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2210909523696 -> 2210906024128
	2210906024128 [label=AccumulateGrad]
	2210906024416 -> 2210906024464
	2210909523792 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2210909523792 -> 2210906024416
	2210906024416 [label=AccumulateGrad]
	2210906024608 -> 2210906024464
	2210909523888 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2210909523888 -> 2210906024608
	2210906024608 [label=AccumulateGrad]
	2210906024656 -> 2210906024896
	2210909524176 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2210909524176 -> 2210906024656
	2210906024656 [label=AccumulateGrad]
	2210906024944 -> 2210906025088
	2210909523216 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2210909523216 -> 2210906024944
	2210906024944 [label=AccumulateGrad]
	2210906025184 -> 2210906025088
	2210909523120 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2210909523120 -> 2210906025184
	2210906025184 [label=AccumulateGrad]
	2210906025280 -> 2210906025136
	2210906025424 -> 2210906025760
	2210909523024 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2210909523024 -> 2210906025424
	2210906025424 [label=AccumulateGrad]
	2210906025616 -> 2210906025520
	2210909522928 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2210909522928 -> 2210906025616
	2210906025616 [label=AccumulateGrad]
	2210906025856 -> 2210906025520
	2210906150512 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2210906150512 -> 2210906025856
	2210906025856 [label=AccumulateGrad]
	2210906026144 -> 2210906026096
	2210906150896 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2210906150896 -> 2210906026144
	2210906026144 [label=AccumulateGrad]
	2210906026000 -> 2210906026336
	2210906150992 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2210906150992 -> 2210906026000
	2210906026000 [label=AccumulateGrad]
	2210906026192 -> 2210906026336
	2210906151088 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2210906151088 -> 2210906026192
	2210906026192 [label=AccumulateGrad]
	2210906026384 -> 2210906026624
	2210906026384 [label=CudnnBatchNormBackward0]
	2210906025664 -> 2210906026384
	2210906025664 [label=ConvolutionBackward0]
	2210906025376 -> 2210906025664
	2210906025232 -> 2210906025664
	2210909520432 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2210909520432 -> 2210906025232
	2210906025232 [label=AccumulateGrad]
	2210906026048 -> 2210906026384
	2210909520624 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2210909520624 -> 2210906026048
	2210906026048 [label=AccumulateGrad]
	2210906026240 -> 2210906026384
	2210909522256 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2210909522256 -> 2210906026240
	2210906026240 [label=AccumulateGrad]
	2210906026720 -> 2210906026816
	2210906151472 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2210906151472 -> 2210906026720
	2210906026720 [label=AccumulateGrad]
	2210906026864 -> 2210906027104
	2210906151568 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2210906151568 -> 2210906026864
	2210906026864 [label=AccumulateGrad]
	2210906027200 -> 2210906027104
	2210906151664 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2210906151664 -> 2210906027200
	2210906027200 [label=AccumulateGrad]
	2210906026960 -> 2210906027344
	2210906152048 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2210906152048 -> 2210906026960
	2210906026960 [label=AccumulateGrad]
	2210906027584 -> 2210906027680
	2210906152144 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2210906152144 -> 2210906027584
	2210906027584 [label=AccumulateGrad]
	2210906027488 -> 2210906027680
	2210906152240 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2210906152240 -> 2210906027488
	2210906027488 [label=AccumulateGrad]
	2210906027536 -> 2210906027440
	2210906028064 -> 2210906028016
	2210906153200 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2210906153200 -> 2210906028064
	2210906028064 [label=AccumulateGrad]
	2210906027920 -> 2210906028112
	2210906153296 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2210906153296 -> 2210906027920
	2210906027920 [label=AccumulateGrad]
	2210906028304 -> 2210906028112
	2210906153392 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2210906153392 -> 2210906028304
	2210906028304 [label=AccumulateGrad]
	2210906028448 -> 2210906028400
	2210906153776 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2210906153776 -> 2210906028448
	2210906028448 [label=AccumulateGrad]
	2210906028592 -> 2210906028784
	2210906153872 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2210906153872 -> 2210906028592
	2210906028592 [label=AccumulateGrad]
	2210906028736 -> 2210906028784
	2210906153968 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2210906153968 -> 2210906028736
	2210906028736 [label=AccumulateGrad]
	2210906029024 -> 2210906028928
	2210906029024 [label=CudnnBatchNormBackward0]
	2210906027968 -> 2210906029024
	2210906027968 [label=ConvolutionBackward0]
	2210906027824 -> 2210906027968
	2210906027776 -> 2210906027968
	2210906152624 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2210906152624 -> 2210906027776
	2210906027776 [label=AccumulateGrad]
	2210906028640 -> 2210906029024
	2210906152720 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2210906152720 -> 2210906028640
	2210906028640 [label=AccumulateGrad]
	2210906028496 -> 2210906029024
	2210906152816 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2210906152816 -> 2210906028496
	2210906028496 [label=AccumulateGrad]
	2210906028976 -> 2210906029264
	2210906154352 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2210906154352 -> 2210906028976
	2210906028976 [label=AccumulateGrad]
	2210906029504 -> 2210906029408
	2210906154448 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2210906154448 -> 2210906029504
	2210906029504 [label=AccumulateGrad]
	2210906029456 -> 2210906029408
	2210906154544 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2210906154544 -> 2210906029456
	2210906029456 [label=AccumulateGrad]
	2210906029552 -> 2210906013952
	2210906154928 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2210906154928 -> 2210906029552
	2210906029552 [label=AccumulateGrad]
	2210906013808 -> 2210906014288
	2210906155024 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2210906155024 -> 2210906013808
	2210906013808 [label=AccumulateGrad]
	2210906014432 -> 2210906014288
	2210906155120 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2210906155120 -> 2210906014432
	2210906014432 [label=AccumulateGrad]
	2210906014912 -> 2210906014768
	2210906015728 -> 2210906016832
	2210906156080 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2210906156080 -> 2210906015728
	2210906015728 [label=AccumulateGrad]
	2210906016688 -> 2210906017312
	2210906156176 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2210906156176 -> 2210906016688
	2210906016688 [label=AccumulateGrad]
	2210906017792 -> 2210906017312
	2210906156272 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2210906156272 -> 2210906017792
	2210906017792 [label=AccumulateGrad]
	2210906018272 -> 2210906018608
	2210906156656 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2210906156656 -> 2210906018272
	2210906018272 [label=AccumulateGrad]
	2210906019232 -> 2210906019712
	2210906156752 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2210906156752 -> 2210906019232
	2210906019232 [label=AccumulateGrad]
	2210906019088 -> 2210906019712
	2210906156848 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2210906156848 -> 2210906019088
	2210906019088 [label=AccumulateGrad]
	2210906019568 -> 2210906020192
	2210906019568 [label=CudnnBatchNormBackward0]
	2210906016352 -> 2210906019568
	2210906016352 [label=ConvolutionBackward0]
	2210906015872 -> 2210906016352
	2210906015248 -> 2210906016352
	2210906155504 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2210906155504 -> 2210906015248
	2210906015248 [label=AccumulateGrad]
	2210906018128 -> 2210906019568
	2210906155600 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2210906155600 -> 2210906018128
	2210906018128 [label=AccumulateGrad]
	2210906018752 -> 2210906019568
	2210906155696 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2210906155696 -> 2210906018752
	2210906018752 [label=AccumulateGrad]
	2210906020672 -> 2210906021632
	2210906157232 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2210906157232 -> 2210906020672
	2210906020672 [label=AccumulateGrad]
	2210906021488 -> 2210906022112
	2210906157328 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2210906157328 -> 2210906021488
	2210906021488 [label=AccumulateGrad]
	2210906022592 -> 2210906022112
	2210906157424 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2210906157424 -> 2210906022592
	2210906022592 [label=AccumulateGrad]
	2210906023072 -> 2210906023408
	2210906157808 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2210906157808 -> 2210906023072
	2210906023072 [label=AccumulateGrad]
	2210906024032 -> 2210906024512
	2210906157904 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2210906157904 -> 2210906024032
	2210906024032 [label=AccumulateGrad]
	2210906023888 -> 2210906024512
	2210906158000 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2210906158000 -> 2210906023888
	2210906023888 [label=AccumulateGrad]
	2210906024368 -> 2210906024992
	2210906026432 -> 2210906027248
	2210906026432 [label=TBackward0]
	2210906024848 -> 2210906026432
	2210906158672 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2210906158672 -> 2210906024848
	2210906024848 [label=AccumulateGrad]
	2210906027248 -> 2211062401456
}
