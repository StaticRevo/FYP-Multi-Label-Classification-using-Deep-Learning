{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Library modules\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platformâ€™s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch and Deep Learning Libaries\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "from torch.nn import functional as F  # Functional module for defining functions and loss functions.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Data handling and batching\n",
    "from torch.utils.tensorboard import SummaryWriter  # TensorBoard for PyTorch.\n",
    "import torchvision  # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Image datasets and transformations.\n",
    "import torchvision.datasets as datasets  # Specific datasets for vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "from torchvision.datasets import MNIST, EuroSAT  # Standard datasets.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations.\n",
    "from torchvision.models import ResNet18_Weights  # ResNet-18 model with pretrained weights.\n",
    "from torchsummary import summary  # Model summary.\n",
    "import torchmetrics  # Model evaluation metrics.\n",
    "from torchmetrics import MeanMetric, Accuracy  # Accuracy metrics.\n",
    "from torchmetrics.classification import (\n",
    "    MultilabelF1Score, MultilabelRecall, MultilabelPrecision, MultilabelAccuracy\n",
    ")  # Classification metrics.\n",
    "from torchviz import make_dot  # Model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for class imbalance.\n",
    "from torchcam.methods import GradCAM  # Grad-CAM for model interpretability.\n",
    "from torchcam.utils import overlay_mask  # Overlay mask for visualizations.\n",
    "import pytorch_lightning as pl  # Training management.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, Callback  # Callbacks.\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Logger for TensorBoard.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation.\n",
    "from rasterio.enums import Resampling  # Resampling for raster resizing.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation, Analysis and Visualization Libraries\n",
    "import pandas as pd  # Data analysis and manipulation.\n",
    "import numpy as np  # Array operations and computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics.\n",
    "import matplotlib.pyplot as plt  # Static and interactive plotting.\n",
    "import seaborn as sns  # High-level interface for statistical graphics.\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm  # Progress bar for loops.\n",
    "from PIL import Image  # Image handling and manipulation.\n",
    "import ast  # Parsing Python code.\n",
    "import requests  # HTTP requests.\n",
    "import zstandard as zstd  # Compression and decompression.\n",
    "from collections import Counter  # Counting hashable objects.\n",
    "import certifi  # Certificates for HTTPS.\n",
    "import ssl  # Secure connections.\n",
    "import urllib.request  # URL handling.\n",
    "import kaggle  # Kaggle API for datasets.\n",
    "from IPython.display import Image  # Display images in notebooks.\n",
    "from pathlib import Path # File system path handling.\n",
    "from typing import Dict, List, Tuple  # Type hints.\n",
    "import sys  # System-specific parameters and functions.\n",
    "import time # Time access and conversions.\n",
    "\n",
    "# Custom Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    dataset_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%'\n",
    "    combined_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%\\CombinedRGBImages'\n",
    "    metadata_path: str =r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\metadata_50_percent.csv'\n",
    "    metadata_csv = pd.read_csv(metadata_path)\n",
    "    img_size: int = 120\n",
    "    img_mean, img_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    num_classes: int = 19\n",
    "    band_channels: int = 3 #13\n",
    "    valid_pct: float = 0.1\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubsetModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BigEarthNetSubsetModel, self).__init__()\n",
    "        # Load the ResNet-18 model\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        # Modify the final layer to output 19 classes\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, DatasetConfig.num_classes)\n",
    "        # Addition of a sigmoid activation function for muylti-label classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Passing the model to the GPU\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Accuracy metrics\n",
    "        self.train_acc = MultilabelAccuracy(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_acc = MultilabelAccuracy(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_acc = MultilabelAccuracy(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "        # Recall metrics\n",
    "        self.train_recall = MultilabelRecall(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_recall = MultilabelRecall(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_recall = MultilabelRecall(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "        # Precision metrics\n",
    "        self.train_precision = MultilabelPrecision(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_precision = MultilabelPrecision(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_precision = MultilabelPrecision(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "        # F1 Score metrics\n",
    "        self.train_f1 = MultilabelF1Score(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_f1 = MultilabelF1Score(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_f1 = MultilabelF1Score(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.train_acc(logits, y)\n",
    "        recall = self.train_recall(logits, y)\n",
    "        f1 = self.train_f1(logits, y)\n",
    "        precision = self.train_precision(logits, y)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_recall', recall, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_f1', f1, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.val_acc(logits, y)\n",
    "        recall = self.val_recall(logits, y)\n",
    "        f1 = self.val_f1(logits, y)\n",
    "        precision = self.val_precision(logits, y)\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_recall', recall, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.test_acc(logits, y)\n",
    "        recall = self.test_recall(logits, y)\n",
    "        f1 = self.test_f1(logits, y)\n",
    "        precision = self.test_precision(logits, y)\n",
    "\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_recall', recall, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_precision', precision, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 60, 60]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 60, 60]             128\n",
      "              ReLU-3           [-1, 64, 60, 60]               0\n",
      "         MaxPool2d-4           [-1, 64, 30, 30]               0\n",
      "            Conv2d-5           [-1, 64, 30, 30]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 30, 30]             128\n",
      "              ReLU-7           [-1, 64, 30, 30]               0\n",
      "            Conv2d-8           [-1, 64, 30, 30]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 30, 30]             128\n",
      "             ReLU-10           [-1, 64, 30, 30]               0\n",
      "       BasicBlock-11           [-1, 64, 30, 30]               0\n",
      "           Conv2d-12           [-1, 64, 30, 30]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 30, 30]             128\n",
      "             ReLU-14           [-1, 64, 30, 30]               0\n",
      "           Conv2d-15           [-1, 64, 30, 30]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 30, 30]             128\n",
      "             ReLU-17           [-1, 64, 30, 30]               0\n",
      "       BasicBlock-18           [-1, 64, 30, 30]               0\n",
      "           Conv2d-19          [-1, 128, 15, 15]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 15, 15]             256\n",
      "             ReLU-21          [-1, 128, 15, 15]               0\n",
      "           Conv2d-22          [-1, 128, 15, 15]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 15, 15]             256\n",
      "           Conv2d-24          [-1, 128, 15, 15]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 15, 15]             256\n",
      "             ReLU-26          [-1, 128, 15, 15]               0\n",
      "       BasicBlock-27          [-1, 128, 15, 15]               0\n",
      "           Conv2d-28          [-1, 128, 15, 15]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 15, 15]             256\n",
      "             ReLU-30          [-1, 128, 15, 15]               0\n",
      "           Conv2d-31          [-1, 128, 15, 15]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 15, 15]             256\n",
      "             ReLU-33          [-1, 128, 15, 15]               0\n",
      "       BasicBlock-34          [-1, 128, 15, 15]               0\n",
      "           Conv2d-35            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "             ReLU-37            [-1, 256, 8, 8]               0\n",
      "           Conv2d-38            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
      "           Conv2d-40            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
      "             ReLU-42            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-43            [-1, 256, 8, 8]               0\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "             ReLU-46            [-1, 256, 8, 8]               0\n",
      "           Conv2d-47            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 8, 8]             512\n",
      "             ReLU-49            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-50            [-1, 256, 8, 8]               0\n",
      "           Conv2d-51            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-53            [-1, 512, 4, 4]               0\n",
      "           Conv2d-54            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-56            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-58            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-59            [-1, 512, 4, 4]               0\n",
      "           Conv2d-60            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-62            [-1, 512, 4, 4]               0\n",
      "           Conv2d-63            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-65            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-66            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 19]           9,747\n",
      "================================================================\n",
      "Total params: 11,186,259\n",
      "Trainable params: 11,186,259\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.16\n",
      "Forward/backward pass size (MB): 18.38\n",
      "Params size (MB): 42.67\n",
      "Estimated Total Size (MB): 61.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BigEarthNetSubsetModel()\n",
    "summary(model.model, input_size=(DatasetConfig.band_channels, DatasetConfig.img_size, DatasetConfig.img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture saved to C:\\Users\\isaac\\OneDrive\\Documents\\GitHub\\Deep-Learning-Based-Land-Use-Classification-Using-Sentinel-2-Imagery\\FYPProject\\models\\resnet18\\model_architecture.png\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input tensor\n",
    "sample_input = torch.randn(1, DatasetConfig.band_channels, DatasetConfig.img_size, DatasetConfig.img_size).to(device)\n",
    "\n",
    "# Pass the sample input through the model\n",
    "output = model(sample_input)\n",
    "\n",
    "# Visualize the computational graph\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# Specify the directory path where you want to save the image\n",
    "output_dir = r'C:\\Users\\isaac\\OneDrive\\Documents\\GitHub\\Deep-Learning-Based-Land-Use-Classification-Using-Sentinel-2-Imagery\\FYPProject\\models\\resnet18'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the visualization as an image file within the specified directory\n",
    "output_path = os.path.join(output_dir, 'model_architecture')\n",
    "dot.format = 'png'\n",
    "dot.render(output_path)\n",
    "\n",
    "print(f\"Model architecture saved to {output_path}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
