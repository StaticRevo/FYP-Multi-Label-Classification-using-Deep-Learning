digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2202833357520 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2202691007680 [label=AddmmBackward0]
	2202691007344 -> 2202691007680
	2202690729840 [label="fc.bias
 (19)" fillcolor=lightblue]
	2202690729840 -> 2202691007344
	2202691007344 [label=AccumulateGrad]
	2202691006720 -> 2202691007680
	2202691006720 [label=ViewBackward0]
	2202691006240 -> 2202691006720
	2202691006240 [label=MeanBackward1]
	2202691005904 -> 2202691006240
	2202691005904 [label=ReluBackward0]
	2202691005424 -> 2202691005904
	2202691005424 [label=AddBackward0]
	2202691004944 -> 2202691005424
	2202691004944 [label=CudnnBatchNormBackward0]
	2202691003840 -> 2202691004944
	2202691003840 [label=ConvolutionBackward0]
	2202691002880 -> 2202691003840
	2202691002880 [label=ReluBackward0]
	2202691002544 -> 2202691002880
	2202691002544 [label=CudnnBatchNormBackward0]
	2202691002064 -> 2202691002544
	2202691002064 [label=ConvolutionBackward0]
	2202691004800 -> 2202691002064
	2202691004800 [label=ReluBackward0]
	2202691000624 -> 2202691004800
	2202691000624 [label=AddBackward0]
	2202691000144 -> 2202691000624
	2202691000144 [label=CudnnBatchNormBackward0]
	2202690999040 -> 2202691000144
	2202690999040 [label=ConvolutionBackward0]
	2202690998080 -> 2202690999040
	2202690998080 [label=ReluBackward0]
	2202690997744 -> 2202690998080
	2202690997744 [label=CudnnBatchNormBackward0]
	2202690997264 -> 2202690997744
	2202690997264 [label=ConvolutionBackward0]
	2202690996016 -> 2202690997264
	2202690996016 [label=ReluBackward0]
	2202690994960 -> 2202690996016
	2202690994960 [label=AddBackward0]
	2202691010464 -> 2202690994960
	2202691010464 [label=CudnnBatchNormBackward0]
	2202691009984 -> 2202691010464
	2202691009984 [label=ConvolutionBackward0]
	2202691009840 -> 2202691009984
	2202691009840 [label=ReluBackward0]
	2202691009648 -> 2202691009840
	2202691009648 [label=CudnnBatchNormBackward0]
	2202691009312 -> 2202691009648
	2202691009312 [label=ConvolutionBackward0]
	2202690996448 -> 2202691009312
	2202690996448 [label=ReluBackward0]
	2202691009168 -> 2202690996448
	2202691009168 [label=AddBackward0]
	2202691008832 -> 2202691009168
	2202691008832 [label=CudnnBatchNormBackward0]
	2202691008880 -> 2202691008832
	2202691008880 [label=ConvolutionBackward0]
	2202691008544 -> 2202691008880
	2202691008544 [label=ReluBackward0]
	2202691008592 -> 2202691008544
	2202691008592 [label=CudnnBatchNormBackward0]
	2202691008496 -> 2202691008592
	2202691008496 [label=ConvolutionBackward0]
	2202691007872 -> 2202691008496
	2202691007872 [label=ReluBackward0]
	2202691007920 -> 2202691007872
	2202691007920 [label=AddBackward0]
	2202691007776 -> 2202691007920
	2202691007776 [label=CudnnBatchNormBackward0]
	2202691007392 -> 2202691007776
	2202691007392 [label=ConvolutionBackward0]
	2202691007536 -> 2202691007392
	2202691007536 [label=ReluBackward0]
	2202691007104 -> 2202691007536
	2202691007104 [label=CudnnBatchNormBackward0]
	2202691007008 -> 2202691007104
	2202691007008 [label=ConvolutionBackward0]
	2202691008016 -> 2202691007008
	2202691008016 [label=ReluBackward0]
	2202691006624 -> 2202691008016
	2202691006624 [label=AddBackward0]
	2202691006528 -> 2202691006624
	2202691006528 [label=CudnnBatchNormBackward0]
	2202691006576 -> 2202691006528
	2202691006576 [label=ConvolutionBackward0]
	2202691005952 -> 2202691006576
	2202691005952 [label=ReluBackward0]
	2202691006000 -> 2202691005952
	2202691006000 [label=CudnnBatchNormBackward0]
	2202691005856 -> 2202691006000
	2202691005856 [label=ConvolutionBackward0]
	2202691005568 -> 2202691005856
	2202691005568 [label=ReluBackward0]
	2202691005616 -> 2202691005568
	2202691005616 [label=AddBackward0]
	2202691005328 -> 2202691005616
	2202691005328 [label=CudnnBatchNormBackward0]
	2202691005088 -> 2202691005328
	2202691005088 [label=ConvolutionBackward0]
	2202691004896 -> 2202691005088
	2202691004896 [label=ReluBackward0]
	2202691004512 -> 2202691004896
	2202691004512 [label=CudnnBatchNormBackward0]
	2202691004752 -> 2202691004512
	2202691004752 [label=ConvolutionBackward0]
	2202691005376 -> 2202691004752
	2202691005376 [label=ReluBackward0]
	2202691004032 -> 2202691005376
	2202691004032 [label=AddBackward0]
	2202691004272 -> 2202691004032
	2202691004272 [label=CudnnBatchNormBackward0]
	2202691003936 -> 2202691004272
	2202691003936 [label=ConvolutionBackward0]
	2202691003648 -> 2202691003936
	2202691003648 [label=ReluBackward0]
	2202691003696 -> 2202691003648
	2202691003696 [label=CudnnBatchNormBackward0]
	2202691003408 -> 2202691003696
	2202691003408 [label=ConvolutionBackward0]
	2202691004128 -> 2202691003408
	2202691004128 [label=MaxPool2DWithIndicesBackward0]
	2202691003216 -> 2202691004128
	2202691003216 [label=ReluBackward0]
	2202691002928 -> 2202691003216
	2202691002928 [label=CudnnBatchNormBackward0]
	2202691002592 -> 2202691002928
	2202691002592 [label=ConvolutionBackward0]
	2202691002736 -> 2202691002592
	2202690729648 [label="conv1.weight
 (64, 5, 7, 7)" fillcolor=lightblue]
	2202690729648 -> 2202691002736
	2202691002736 [label=AccumulateGrad]
	2202691002784 -> 2202691002928
	2202690704528 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2202690704528 -> 2202691002784
	2202691002784 [label=AccumulateGrad]
	2202691003072 -> 2202691002928
	2202690704240 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2202690704240 -> 2202691003072
	2202691003072 [label=AccumulateGrad]
	2202691003312 -> 2202691003408
	2202690704624 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2202690704624 -> 2202691003312
	2202691003312 [label=AccumulateGrad]
	2202691003456 -> 2202691003696
	2202690704720 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2202690704720 -> 2202691003456
	2202691003456 [label=AccumulateGrad]
	2202691003792 -> 2202691003696
	2202690704816 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2202690704816 -> 2202691003792
	2202691003792 [label=AccumulateGrad]
	2202691003552 -> 2202691003936
	2202690705104 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2202690705104 -> 2202691003552
	2202691003552 [label=AccumulateGrad]
	2202691004176 -> 2202691004272
	2202690704144 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2202690704144 -> 2202691004176
	2202691004176 [label=AccumulateGrad]
	2202691004080 -> 2202691004272
	2202690704048 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2202690704048 -> 2202691004080
	2202691004080 [label=AccumulateGrad]
	2202691004128 -> 2202691004032
	2202691004368 -> 2202691004752
	2202690703184 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2202690703184 -> 2202691004368
	2202691004368 [label=AccumulateGrad]
	2202691004608 -> 2202691004512
	2202690703376 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2202690703376 -> 2202691004608
	2202691004608 [label=AccumulateGrad]
	2202691004848 -> 2202691004512
	2202690703472 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2202690703472 -> 2202691004848
	2202691004848 [label=AccumulateGrad]
	2202691005136 -> 2202691005088
	2202690703952 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2202690703952 -> 2202691005136
	2202691005136 [label=AccumulateGrad]
	2202691004992 -> 2202691005328
	2202690703856 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2202690703856 -> 2202691004992
	2202691004992 [label=AccumulateGrad]
	2202691005184 -> 2202691005328
	2202690702896 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2202690702896 -> 2202691005184
	2202691005184 [label=AccumulateGrad]
	2202691005376 -> 2202691005616
	2202691005472 -> 2202691005856
	2202690702416 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2202690702416 -> 2202691005472
	2202691005472 [label=AccumulateGrad]
	2202691006096 -> 2202691006000
	2202690702704 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2202690702704 -> 2202691006096
	2202691006096 [label=AccumulateGrad]
	2202691006048 -> 2202691006000
	2202690702608 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2202690702608 -> 2202691006048
	2202691006048 [label=AccumulateGrad]
	2202691006144 -> 2202691006576
	2202690721968 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2202690721968 -> 2202691006144
	2202691006144 [label=AccumulateGrad]
	2202691006480 -> 2202691006528
	2202690722064 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2202690722064 -> 2202691006480
	2202691006480 [label=AccumulateGrad]
	2202691006672 -> 2202691006528
	2202690722160 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2202690722160 -> 2202691006672
	2202691006672 [label=AccumulateGrad]
	2202691006432 -> 2202691006624
	2202691006432 [label=CudnnBatchNormBackward0]
	2202691005664 -> 2202691006432
	2202691005664 [label=ConvolutionBackward0]
	2202691005568 -> 2202691005664
	2202691005712 -> 2202691005664
	2202690702512 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2202690702512 -> 2202691005712
	2202691005712 [label=AccumulateGrad]
	2202691006288 -> 2202691006432
	2202690700112 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2202690700112 -> 2202691006288
	2202691006288 [label=AccumulateGrad]
	2202691006336 -> 2202691006432
	2202690700304 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2202690700304 -> 2202691006336
	2202691006336 [label=AccumulateGrad]
	2202691006816 -> 2202691007008
	2202690722544 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2202690722544 -> 2202691006816
	2202691006816 [label=AccumulateGrad]
	2202691006912 -> 2202691007104
	2202690722640 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2202690722640 -> 2202691006912
	2202691006912 [label=AccumulateGrad]
	2202691007296 -> 2202691007104
	2202690722736 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2202690722736 -> 2202691007296
	2202691007296 [label=AccumulateGrad]
	2202691007440 -> 2202691007392
	2202690723120 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2202690723120 -> 2202691007440
	2202691007440 [label=AccumulateGrad]
	2202691007584 -> 2202691007776
	2202690723216 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2202690723216 -> 2202691007584
	2202691007584 [label=AccumulateGrad]
	2202691007728 -> 2202691007776
	2202690723312 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2202690723312 -> 2202691007728
	2202691007728 [label=AccumulateGrad]
	2202691008016 -> 2202691007920
	2202691008064 -> 2202691008496
	2202690724272 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2202690724272 -> 2202691008064
	2202691008064 [label=AccumulateGrad]
	2202691008400 -> 2202691008592
	2202690724368 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2202690724368 -> 2202691008400
	2202691008400 [label=AccumulateGrad]
	2202691008352 -> 2202691008592
	2202690724464 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2202690724464 -> 2202691008352
	2202691008352 [label=AccumulateGrad]
	2202691008688 -> 2202691008880
	2202690724848 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2202690724848 -> 2202691008688
	2202691008688 [label=AccumulateGrad]
	2202691009072 -> 2202691008832
	2202690724944 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2202690724944 -> 2202691009072
	2202691009072 [label=AccumulateGrad]
	2202691008928 -> 2202691008832
	2202690725040 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2202690725040 -> 2202691008928
	2202691008928 [label=AccumulateGrad]
	2202691009024 -> 2202691009168
	2202691009024 [label=CudnnBatchNormBackward0]
	2202691008208 -> 2202691009024
	2202691008208 [label=ConvolutionBackward0]
	2202691007872 -> 2202691008208
	2202691007968 -> 2202691008208
	2202690723696 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2202690723696 -> 2202691007968
	2202691007968 [label=AccumulateGrad]
	2202691008736 -> 2202691009024
	2202690723792 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2202690723792 -> 2202691008736
	2202691008736 [label=AccumulateGrad]
	2202691008976 -> 2202691009024
	2202690723888 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2202690723888 -> 2202691008976
	2202691008976 [label=AccumulateGrad]
	2202691009456 -> 2202691009312
	2202690725424 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2202690725424 -> 2202691009456
	2202691009456 [label=AccumulateGrad]
	2202691009504 -> 2202691009648
	2202690725520 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2202690725520 -> 2202691009504
	2202691009504 [label=AccumulateGrad]
	2202691009936 -> 2202691009648
	2202690725616 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2202690725616 -> 2202691009936
	2202691009936 [label=AccumulateGrad]
	2202691010032 -> 2202691009984
	2202690726000 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2202690726000 -> 2202691010032
	2202691010032 [label=AccumulateGrad]
	2202691010128 -> 2202691010464
	2202690726096 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2202690726096 -> 2202691010128
	2202691010128 [label=AccumulateGrad]
	2202691010176 -> 2202691010464
	2202690726192 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2202690726192 -> 2202691010176
	2202691010176 [label=AccumulateGrad]
	2202690996448 -> 2202690994960
	2202690996112 -> 2202690997264
	2202690727152 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2202690727152 -> 2202690996112
	2202690996112 [label=AccumulateGrad]
	2202690997120 -> 2202690997744
	2202690727248 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2202690727248 -> 2202690997120
	2202690997120 [label=AccumulateGrad]
	2202690998224 -> 2202690997744
	2202690727344 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2202690727344 -> 2202690998224
	2202690998224 [label=AccumulateGrad]
	2202690998704 -> 2202690999040
	2202690727728 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2202690727728 -> 2202690998704
	2202690998704 [label=AccumulateGrad]
	2202690999664 -> 2202691000144
	2202690727824 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2202690727824 -> 2202690999664
	2202690999664 [label=AccumulateGrad]
	2202690999520 -> 2202691000144
	2202690727920 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2202690727920 -> 2202690999520
	2202690999520 [label=AccumulateGrad]
	2202691000000 -> 2202691000624
	2202691000000 [label=CudnnBatchNormBackward0]
	2202690996784 -> 2202691000000
	2202690996784 [label=ConvolutionBackward0]
	2202690996016 -> 2202690996784
	2202690995440 -> 2202690996784
	2202690726576 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2202690726576 -> 2202690995440
	2202690995440 [label=AccumulateGrad]
	2202690998560 -> 2202691000000
	2202690726672 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2202690726672 -> 2202690998560
	2202690998560 [label=AccumulateGrad]
	2202690999184 -> 2202691000000
	2202690726768 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2202690726768 -> 2202690999184
	2202690999184 [label=AccumulateGrad]
	2202691001104 -> 2202691002064
	2202690728304 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2202690728304 -> 2202691001104
	2202691001104 [label=AccumulateGrad]
	2202691001920 -> 2202691002544
	2202690728400 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2202690728400 -> 2202691001920
	2202691001920 [label=AccumulateGrad]
	2202691003024 -> 2202691002544
	2202690728496 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2202690728496 -> 2202691003024
	2202691003024 [label=AccumulateGrad]
	2202691003504 -> 2202691003840
	2202690728880 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2202690728880 -> 2202691003504
	2202691003504 [label=AccumulateGrad]
	2202691004464 -> 2202691004944
	2202690728976 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2202690728976 -> 2202691004464
	2202691004464 [label=AccumulateGrad]
	2202691004320 -> 2202691004944
	2202690729072 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2202690729072 -> 2202691004320
	2202691004320 [label=AccumulateGrad]
	2202691004800 -> 2202691005424
	2202691006864 -> 2202691007680
	2202691006864 [label=TBackward0]
	2202691005280 -> 2202691006864
	2202690729744 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2202690729744 -> 2202691005280
	2202691005280 [label=AccumulateGrad]
	2202691007680 -> 2202833357520
}
