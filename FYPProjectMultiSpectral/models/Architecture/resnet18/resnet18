digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1673415279632 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1673420812544 [label=AddmmBackward0]
	1673420807264 -> 1673420812544
	1673436134000 [label="fc.bias
 (19)" fillcolor=lightblue]
	1673436134000 -> 1673420807264
	1673420807264 [label=AccumulateGrad]
	1673420815760 -> 1673420812544
	1673420815760 [label=ViewBackward0]
	1673420807408 -> 1673420815760
	1673420807408 [label=MeanBackward1]
	1673420813216 -> 1673420807408
	1673420813216 [label=ReluBackward0]
	1673420801312 -> 1673420813216
	1673420801312 [label=AddBackward0]
	1673532588576 -> 1673420801312
	1673532588576 [label=CudnnBatchNormBackward0]
	1673532588192 -> 1673532588576
	1673532588192 [label=ConvolutionBackward0]
	1673532588768 -> 1673532588192
	1673532588768 [label=ReluBackward0]
	1673532589392 -> 1673532588768
	1673532589392 [label=CudnnBatchNormBackward0]
	1673532589488 -> 1673532589392
	1673532589488 [label=ConvolutionBackward0]
	1673532588336 -> 1673532589488
	1673532588336 [label=ReluBackward0]
	1673532590352 -> 1673532588336
	1673532590352 [label=AddBackward0]
	1673532590448 -> 1673532590352
	1673532590448 [label=CudnnBatchNormBackward0]
	1673532590592 -> 1673532590448
	1673532590592 [label=ConvolutionBackward0]
	1673532590784 -> 1673532590592
	1673532590784 [label=ReluBackward0]
	1673532590928 -> 1673532590784
	1673532590928 [label=CudnnBatchNormBackward0]
	1673532591024 -> 1673532590928
	1673532591024 [label=ConvolutionBackward0]
	1673532591216 -> 1673532591024
	1673532591216 [label=ReluBackward0]
	1673532591360 -> 1673532591216
	1673532591360 [label=AddBackward0]
	1673532591456 -> 1673532591360
	1673532591456 [label=CudnnBatchNormBackward0]
	1673532591600 -> 1673532591456
	1673532591600 [label=ConvolutionBackward0]
	1673532591792 -> 1673532591600
	1673532591792 [label=ReluBackward0]
	1673532591936 -> 1673532591792
	1673532591936 [label=CudnnBatchNormBackward0]
	1673532592032 -> 1673532591936
	1673532592032 [label=ConvolutionBackward0]
	1673532591408 -> 1673532592032
	1673532591408 [label=ReluBackward0]
	1673532592320 -> 1673532591408
	1673532592320 [label=AddBackward0]
	1673532592416 -> 1673532592320
	1673532592416 [label=CudnnBatchNormBackward0]
	1673532592560 -> 1673532592416
	1673532592560 [label=ConvolutionBackward0]
	1673532592752 -> 1673532592560
	1673532592752 [label=ReluBackward0]
	1673532592896 -> 1673532592752
	1673532592896 [label=CudnnBatchNormBackward0]
	1673532592992 -> 1673532592896
	1673532592992 [label=ConvolutionBackward0]
	1673532593184 -> 1673532592992
	1673532593184 [label=ReluBackward0]
	1673532590208 -> 1673532593184
	1673532590208 [label=AddBackward0]
	1673532590112 -> 1673532590208
	1673532590112 [label=CudnnBatchNormBackward0]
	1673532589296 -> 1673532590112
	1673532589296 [label=ConvolutionBackward0]
	1673532588624 -> 1673532589296
	1673532588624 [label=ReluBackward0]
	1673532590016 -> 1673532588624
	1673532590016 [label=CudnnBatchNormBackward0]
	1673532588720 -> 1673532590016
	1673532588720 [label=ConvolutionBackward0]
	1673532590160 -> 1673532588720
	1673532590160 [label=ReluBackward0]
	1673532593328 -> 1673532590160
	1673532593328 [label=AddBackward0]
	1673532593424 -> 1673532593328
	1673532593424 [label=CudnnBatchNormBackward0]
	1673532593568 -> 1673532593424
	1673532593568 [label=ConvolutionBackward0]
	1673532593760 -> 1673532593568
	1673532593760 [label=ReluBackward0]
	1673532593904 -> 1673532593760
	1673532593904 [label=CudnnBatchNormBackward0]
	1673532594000 -> 1673532593904
	1673532594000 [label=ConvolutionBackward0]
	1673532594192 -> 1673532594000
	1673532594192 [label=ReluBackward0]
	1673532594336 -> 1673532594192
	1673532594336 [label=AddBackward0]
	1673532594432 -> 1673532594336
	1673532594432 [label=CudnnBatchNormBackward0]
	1673532594576 -> 1673532594432
	1673532594576 [label=ConvolutionBackward0]
	1673532594768 -> 1673532594576
	1673532594768 [label=ReluBackward0]
	1673532594912 -> 1673532594768
	1673532594912 [label=CudnnBatchNormBackward0]
	1673532595008 -> 1673532594912
	1673532595008 [label=ConvolutionBackward0]
	1673532594384 -> 1673532595008
	1673532594384 [label=ReluBackward0]
	1673532595296 -> 1673532594384
	1673532595296 [label=AddBackward0]
	1673532595392 -> 1673532595296
	1673532595392 [label=CudnnBatchNormBackward0]
	1673532595536 -> 1673532595392
	1673532595536 [label=ConvolutionBackward0]
	1673532595728 -> 1673532595536
	1673532595728 [label=ReluBackward0]
	1673532595872 -> 1673532595728
	1673532595872 [label=CudnnBatchNormBackward0]
	1673532595968 -> 1673532595872
	1673532595968 [label=ConvolutionBackward0]
	1673532595344 -> 1673532595968
	1673532595344 [label=MaxPool2DWithIndicesBackward0]
	1673532596256 -> 1673532595344
	1673532596256 [label=ReluBackward0]
	1673532596352 -> 1673532596256
	1673532596352 [label=CudnnBatchNormBackward0]
	1673532596448 -> 1673532596352
	1673532596448 [label=ConvolutionBackward0]
	1673532596640 -> 1673532596448
	1673436119504 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1673436119504 -> 1673532596640
	1673532596640 [label=AccumulateGrad]
	1673532596400 -> 1673532596352
	1673436122096 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1673436122096 -> 1673532596400
	1673532596400 [label=AccumulateGrad]
	1673532596064 -> 1673532596352
	1673436122192 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1673436122192 -> 1673532596064
	1673532596064 [label=AccumulateGrad]
	1673532596160 -> 1673532595968
	1673436122576 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673436122576 -> 1673532596160
	1673532596160 [label=AccumulateGrad]
	1673532595920 -> 1673532595872
	1673436122672 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1673436122672 -> 1673532595920
	1673532595920 [label=AccumulateGrad]
	1673532595776 -> 1673532595872
	1673436122768 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1673436122768 -> 1673532595776
	1673532595776 [label=AccumulateGrad]
	1673532595680 -> 1673532595536
	1673436123152 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673436123152 -> 1673532595680
	1673532595680 [label=AccumulateGrad]
	1673532595488 -> 1673532595392
	1673436123248 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1673436123248 -> 1673532595488
	1673532595488 [label=AccumulateGrad]
	1673532595440 -> 1673532595392
	1673436123344 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1673436123344 -> 1673532595440
	1673532595440 [label=AccumulateGrad]
	1673532595344 -> 1673532595296
	1673532595200 -> 1673532595008
	1673436123728 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673436123728 -> 1673532595200
	1673532595200 [label=AccumulateGrad]
	1673532594960 -> 1673532594912
	1673436123824 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1673436123824 -> 1673532594960
	1673532594960 [label=AccumulateGrad]
	1673532594816 -> 1673532594912
	1673436123920 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1673436123920 -> 1673532594816
	1673532594816 [label=AccumulateGrad]
	1673532594720 -> 1673532594576
	1673436124304 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673436124304 -> 1673532594720
	1673532594720 [label=AccumulateGrad]
	1673532594528 -> 1673532594432
	1673436124400 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1673436124400 -> 1673532594528
	1673532594528 [label=AccumulateGrad]
	1673532594480 -> 1673532594432
	1673436124496 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1673436124496 -> 1673532594480
	1673532594480 [label=AccumulateGrad]
	1673532594384 -> 1673532594336
	1673532594144 -> 1673532594000
	1673436125456 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1673436125456 -> 1673532594144
	1673532594144 [label=AccumulateGrad]
	1673532593952 -> 1673532593904
	1673436125552 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1673436125552 -> 1673532593952
	1673532593952 [label=AccumulateGrad]
	1673532593808 -> 1673532593904
	1673436125648 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1673436125648 -> 1673532593808
	1673532593808 [label=AccumulateGrad]
	1673532593712 -> 1673532593568
	1673436126032 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1673436126032 -> 1673532593712
	1673532593712 [label=AccumulateGrad]
	1673532593520 -> 1673532593424
	1673436126128 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1673436126128 -> 1673532593520
	1673532593520 [label=AccumulateGrad]
	1673532593472 -> 1673532593424
	1673436126224 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1673436126224 -> 1673532593472
	1673532593472 [label=AccumulateGrad]
	1673532593376 -> 1673532593328
	1673532593376 [label=CudnnBatchNormBackward0]
	1673532594096 -> 1673532593376
	1673532594096 [label=ConvolutionBackward0]
	1673532594192 -> 1673532594096
	1673532594240 -> 1673532594096
	1673436124880 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1673436124880 -> 1673532594240
	1673532594240 [label=AccumulateGrad]
	1673532593664 -> 1673532593376
	1673436124976 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1673436124976 -> 1673532593664
	1673532593664 [label=AccumulateGrad]
	1673532593616 -> 1673532593376
	1673436125072 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1673436125072 -> 1673532593616
	1673532593616 [label=AccumulateGrad]
	1673532588480 -> 1673532588720
	1673436126608 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1673436126608 -> 1673532588480
	1673532588480 [label=AccumulateGrad]
	1673532588096 -> 1673532590016
	1673436126704 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1673436126704 -> 1673532588096
	1673532588096 [label=AccumulateGrad]
	1673532589248 -> 1673532590016
	1673436126800 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1673436126800 -> 1673532589248
	1673532589248 [label=AccumulateGrad]
	1673532588144 -> 1673532589296
	1673436127184 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1673436127184 -> 1673532588144
	1673532588144 [label=AccumulateGrad]
	1673532588288 -> 1673532590112
	1673436127280 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1673436127280 -> 1673532588288
	1673532588288 [label=AccumulateGrad]
	1673532590064 -> 1673532590112
	1673436127376 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1673436127376 -> 1673532590064
	1673532590064 [label=AccumulateGrad]
	1673532590160 -> 1673532590208
	1673532593136 -> 1673532592992
	1673436128240 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1673436128240 -> 1673532593136
	1673532593136 [label=AccumulateGrad]
	1673532592944 -> 1673532592896
	1673436128336 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1673436128336 -> 1673532592944
	1673532592944 [label=AccumulateGrad]
	1673532592800 -> 1673532592896
	1673436128432 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1673436128432 -> 1673532592800
	1673532592800 [label=AccumulateGrad]
	1673532592704 -> 1673532592560
	1673436128816 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1673436128816 -> 1673532592704
	1673532592704 [label=AccumulateGrad]
	1673532592512 -> 1673532592416
	1673436128912 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1673436128912 -> 1673532592512
	1673532592512 [label=AccumulateGrad]
	1673532592464 -> 1673532592416
	1673436129008 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1673436129008 -> 1673532592464
	1673532592464 [label=AccumulateGrad]
	1673532592368 -> 1673532592320
	1673532592368 [label=CudnnBatchNormBackward0]
	1673532593088 -> 1673532592368
	1673532593088 [label=ConvolutionBackward0]
	1673532593184 -> 1673532593088
	1673532593232 -> 1673532593088
	1673436127760 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1673436127760 -> 1673532593232
	1673532593232 [label=AccumulateGrad]
	1673532592656 -> 1673532592368
	1673436127856 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1673436127856 -> 1673532592656
	1673532592656 [label=AccumulateGrad]
	1673532592608 -> 1673532592368
	1673436127952 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1673436127952 -> 1673532592608
	1673532592608 [label=AccumulateGrad]
	1673532592224 -> 1673532592032
	1673436129392 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1673436129392 -> 1673532592224
	1673532592224 [label=AccumulateGrad]
	1673532591984 -> 1673532591936
	1673436129488 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1673436129488 -> 1673532591984
	1673532591984 [label=AccumulateGrad]
	1673532591840 -> 1673532591936
	1673436129584 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1673436129584 -> 1673532591840
	1673532591840 [label=AccumulateGrad]
	1673532591744 -> 1673532591600
	1673436129968 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1673436129968 -> 1673532591744
	1673532591744 [label=AccumulateGrad]
	1673532591552 -> 1673532591456
	1673436130064 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1673436130064 -> 1673532591552
	1673532591552 [label=AccumulateGrad]
	1673532591504 -> 1673532591456
	1673436130160 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1673436130160 -> 1673532591504
	1673532591504 [label=AccumulateGrad]
	1673532591408 -> 1673532591360
	1673532591168 -> 1673532591024
	1673436131120 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1673436131120 -> 1673532591168
	1673532591168 [label=AccumulateGrad]
	1673532590976 -> 1673532590928
	1673436131216 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1673436131216 -> 1673532590976
	1673532590976 [label=AccumulateGrad]
	1673532590832 -> 1673532590928
	1673436131312 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1673436131312 -> 1673532590832
	1673532590832 [label=AccumulateGrad]
	1673532590736 -> 1673532590592
	1673436131696 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1673436131696 -> 1673532590736
	1673532590736 [label=AccumulateGrad]
	1673532590544 -> 1673532590448
	1673436131792 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1673436131792 -> 1673532590544
	1673532590544 [label=AccumulateGrad]
	1673532590496 -> 1673532590448
	1673436131888 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1673436131888 -> 1673532590496
	1673532590496 [label=AccumulateGrad]
	1673532590400 -> 1673532590352
	1673532590400 [label=CudnnBatchNormBackward0]
	1673532591120 -> 1673532590400
	1673532591120 [label=ConvolutionBackward0]
	1673532591216 -> 1673532591120
	1673532591264 -> 1673532591120
	1673436130544 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1673436130544 -> 1673532591264
	1673532591264 [label=AccumulateGrad]
	1673532590688 -> 1673532590400
	1673436130640 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1673436130640 -> 1673532590688
	1673532590688 [label=AccumulateGrad]
	1673532590640 -> 1673532590400
	1673436130736 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1673436130736 -> 1673532590640
	1673532590640 [label=AccumulateGrad]
	1673532589728 -> 1673532589488
	1673436132272 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1673436132272 -> 1673532589728
	1673532589728 [label=AccumulateGrad]
	1673532589440 -> 1673532589392
	1673436132368 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1673436132368 -> 1673532589440
	1673532589440 [label=AccumulateGrad]
	1673532588816 -> 1673532589392
	1673436132464 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1673436132464 -> 1673532588816
	1673532588816 [label=AccumulateGrad]
	1673532588864 -> 1673532588192
	1673436132848 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1673436132848 -> 1673532588864
	1673532588864 [label=AccumulateGrad]
	1673532589056 -> 1673532588576
	1673436132944 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1673436132944 -> 1673532589056
	1673532589056 [label=AccumulateGrad]
	1673532589104 -> 1673532588576
	1673436133040 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1673436133040 -> 1673532589104
	1673532589104 [label=AccumulateGrad]
	1673532588336 -> 1673420801312
	1673420813984 -> 1673420812544
	1673420813984 [label=TBackward0]
	1673420804096 -> 1673420813984
	1673436133616 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1673436133616 -> 1673420804096
	1673420804096 [label=AccumulateGrad]
	1673420812544 -> 1673415279632
}
