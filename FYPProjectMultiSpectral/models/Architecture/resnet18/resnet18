digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2465752058864 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2465752282304 [label=AddmmBackward0]
	2465752281968 -> 2465752282304
	2465752889584 [label="fc.bias
 (19)" fillcolor=lightblue]
	2465752889584 -> 2465752281968
	2465752281968 [label=AccumulateGrad]
	2465752281344 -> 2465752282304
	2465752281344 [label=ViewBackward0]
	2465752280864 -> 2465752281344
	2465752280864 [label=MeanBackward1]
	2465752280528 -> 2465752280864
	2465752280528 [label=ReluBackward0]
	2465752280048 -> 2465752280528
	2465752280048 [label=AddBackward0]
	2465752279568 -> 2465752280048
	2465752279568 [label=CudnnBatchNormBackward0]
	2465752278464 -> 2465752279568
	2465752278464 [label=ConvolutionBackward0]
	2465752277504 -> 2465752278464
	2465752277504 [label=ReluBackward0]
	2465752277168 -> 2465752277504
	2465752277168 [label=CudnnBatchNormBackward0]
	2465752276688 -> 2465752277168
	2465752276688 [label=ConvolutionBackward0]
	2465752279424 -> 2465752276688
	2465752279424 [label=ReluBackward0]
	2465752275248 -> 2465752279424
	2465752275248 [label=AddBackward0]
	2465752274768 -> 2465752275248
	2465752274768 [label=CudnnBatchNormBackward0]
	2465752273664 -> 2465752274768
	2465752273664 [label=ConvolutionBackward0]
	2465752272704 -> 2465752273664
	2465752272704 [label=ReluBackward0]
	2465752272368 -> 2465752272704
	2465752272368 [label=CudnnBatchNormBackward0]
	2465752271888 -> 2465752272368
	2465752271888 [label=ConvolutionBackward0]
	2465752270928 -> 2465752271888
	2465752270928 [label=ReluBackward0]
	2465752269824 -> 2465752270928
	2465752269824 [label=AddBackward0]
	2465752269344 -> 2465752269824
	2465752269344 [label=CudnnBatchNormBackward0]
	2465752269008 -> 2465752269344
	2465752269008 [label=ConvolutionBackward0]
	2465752284608 -> 2465752269008
	2465752284608 [label=ReluBackward0]
	2465752284656 -> 2465752284608
	2465752284656 [label=CudnnBatchNormBackward0]
	2465752284560 -> 2465752284656
	2465752284560 [label=ConvolutionBackward0]
	2465752269968 -> 2465752284560
	2465752269968 [label=ReluBackward0]
	2465752284176 -> 2465752269968
	2465752284176 [label=AddBackward0]
	2465752284080 -> 2465752284176
	2465752284080 [label=CudnnBatchNormBackward0]
	2465752283648 -> 2465752284080
	2465752283648 [label=ConvolutionBackward0]
	2465752283504 -> 2465752283648
	2465752283504 [label=ReluBackward0]
	2465752283312 -> 2465752283504
	2465752283312 [label=CudnnBatchNormBackward0]
	2465752282976 -> 2465752283312
	2465752282976 [label=ConvolutionBackward0]
	2465752283120 -> 2465752282976
	2465752283120 [label=ReluBackward0]
	2465752282688 -> 2465752283120
	2465752282688 [label=AddBackward0]
	2465752282592 -> 2465752282688
	2465752282592 [label=CudnnBatchNormBackward0]
	2465752282640 -> 2465752282592
	2465752282640 [label=ConvolutionBackward0]
	2465752282016 -> 2465752282640
	2465752282016 [label=ReluBackward0]
	2465752282064 -> 2465752282016
	2465752282064 [label=CudnnBatchNormBackward0]
	2465752281920 -> 2465752282064
	2465752281920 [label=ConvolutionBackward0]
	2465752282496 -> 2465752281920
	2465752282496 [label=ReluBackward0]
	2465752281584 -> 2465752282496
	2465752281584 [label=AddBackward0]
	2465752281440 -> 2465752281584
	2465752281440 [label=CudnnBatchNormBackward0]
	2465752281056 -> 2465752281440
	2465752281056 [label=ConvolutionBackward0]
	2465752281200 -> 2465752281056
	2465752281200 [label=ReluBackward0]
	2465752280768 -> 2465752281200
	2465752280768 [label=CudnnBatchNormBackward0]
	2465752280672 -> 2465752280768
	2465752280672 [label=ConvolutionBackward0]
	2465752280480 -> 2465752280672
	2465752280480 [label=ReluBackward0]
	2465752280096 -> 2465752280480
	2465752280096 [label=AddBackward0]
	2465752280336 -> 2465752280096
	2465752280336 [label=CudnnBatchNormBackward0]
	2465752280000 -> 2465752280336
	2465752280000 [label=ConvolutionBackward0]
	2465752279712 -> 2465752280000
	2465752279712 [label=ReluBackward0]
	2465752279760 -> 2465752279712
	2465752279760 [label=CudnnBatchNormBackward0]
	2465752279472 -> 2465752279760
	2465752279472 [label=ConvolutionBackward0]
	2465752280192 -> 2465752279472
	2465752280192 [label=ReluBackward0]
	2465752279280 -> 2465752280192
	2465752279280 [label=AddBackward0]
	2465752278992 -> 2465752279280
	2465752278992 [label=CudnnBatchNormBackward0]
	2465752278752 -> 2465752278992
	2465752278752 [label=ConvolutionBackward0]
	2465752278560 -> 2465752278752
	2465752278560 [label=ReluBackward0]
	2465752278176 -> 2465752278560
	2465752278176 [label=CudnnBatchNormBackward0]
	2465752278416 -> 2465752278176
	2465752278416 [label=ConvolutionBackward0]
	2465752279040 -> 2465752278416
	2465752279040 [label=MaxPool2DWithIndicesBackward0]
	2465752277696 -> 2465752279040
	2465752277696 [label=ReluBackward0]
	2465752277936 -> 2465752277696
	2465752277936 [label=CudnnBatchNormBackward0]
	2465752277840 -> 2465752277936
	2465752277840 [label=ConvolutionBackward0]
	2465752277216 -> 2465752277840
	2465752889392 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2465752889392 -> 2465752277216
	2465752277216 [label=AccumulateGrad]
	2465752277744 -> 2465752277936
	2465755944368 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2465755944368 -> 2465752277744
	2465752277744 [label=AccumulateGrad]
	2465752278320 -> 2465752277936
	2465755944080 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2465755944080 -> 2465752278320
	2465752278320 [label=AccumulateGrad]
	2465752278032 -> 2465752278416
	2465755944560 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2465755944560 -> 2465752278032
	2465752278032 [label=AccumulateGrad]
	2465752278272 -> 2465752278176
	2465755944656 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2465755944656 -> 2465752278272
	2465752278272 [label=AccumulateGrad]
	2465752278512 -> 2465752278176
	2465755944752 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2465755944752 -> 2465752278512
	2465752278512 [label=AccumulateGrad]
	2465752278800 -> 2465752278752
	2465755943984 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2465755943984 -> 2465752278800
	2465752278800 [label=AccumulateGrad]
	2465752278656 -> 2465752278992
	2465755943888 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2465755943888 -> 2465752278656
	2465752278656 [label=AccumulateGrad]
	2465752278848 -> 2465752278992
	2465755943120 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2465755943120 -> 2465752278848
	2465752278848 [label=AccumulateGrad]
	2465752279040 -> 2465752279280
	2465752279376 -> 2465752279472
	2465755943216 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2465755943216 -> 2465752279376
	2465752279376 [label=AccumulateGrad]
	2465752279520 -> 2465752279760
	2465755943312 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2465755943312 -> 2465752279520
	2465752279520 [label=AccumulateGrad]
	2465752279856 -> 2465752279760
	2465755943408 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2465755943408 -> 2465752279856
	2465752279856 [label=AccumulateGrad]
	2465752279616 -> 2465752280000
	2465755943696 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2465755943696 -> 2465752279616
	2465752279616 [label=AccumulateGrad]
	2465752280240 -> 2465752280336
	2465755942736 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2465755942736 -> 2465752280240
	2465752280240 [label=AccumulateGrad]
	2465752280144 -> 2465752280336
	2465755942640 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2465755942640 -> 2465752280144
	2465752280144 [label=AccumulateGrad]
	2465752280192 -> 2465752280096
	2465752280720 -> 2465752280672
	2465755942544 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2465755942544 -> 2465752280720
	2465752280720 [label=AccumulateGrad]
	2465752280576 -> 2465752280768
	2465755942448 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2465755942448 -> 2465752280576
	2465752280576 [label=AccumulateGrad]
	2465752280960 -> 2465752280768
	2465752881328 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2465752881328 -> 2465752280960
	2465752280960 [label=AccumulateGrad]
	2465752281104 -> 2465752281056
	2465752881712 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2465752881712 -> 2465752281104
	2465752281104 [label=AccumulateGrad]
	2465752281248 -> 2465752281440
	2465752881808 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2465752881808 -> 2465752281248
	2465752281248 [label=AccumulateGrad]
	2465752281392 -> 2465752281440
	2465752881904 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2465752881904 -> 2465752281392
	2465752281392 [label=AccumulateGrad]
	2465752281680 -> 2465752281584
	2465752281680 [label=CudnnBatchNormBackward0]
	2465752280624 -> 2465752281680
	2465752280624 [label=ConvolutionBackward0]
	2465752280480 -> 2465752280624
	2465752280432 -> 2465752280624
	2465755939952 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2465755939952 -> 2465752280432
	2465752280432 [label=AccumulateGrad]
	2465752281296 -> 2465752281680
	2465755940144 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2465755940144 -> 2465752281296
	2465752281296 [label=AccumulateGrad]
	2465752281152 -> 2465752281680
	2465755941776 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2465755941776 -> 2465752281152
	2465752281152 [label=AccumulateGrad]
	2465752281632 -> 2465752281920
	2465752882288 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2465752882288 -> 2465752281632
	2465752281632 [label=AccumulateGrad]
	2465752282160 -> 2465752282064
	2465752882384 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2465752882384 -> 2465752282160
	2465752282160 [label=AccumulateGrad]
	2465752282112 -> 2465752282064
	2465752882480 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2465752882480 -> 2465752282112
	2465752282112 [label=AccumulateGrad]
	2465752282208 -> 2465752282640
	2465752882864 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2465752882864 -> 2465752282208
	2465752282208 [label=AccumulateGrad]
	2465752282544 -> 2465752282592
	2465752882960 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2465752882960 -> 2465752282544
	2465752282544 [label=AccumulateGrad]
	2465752282736 -> 2465752282592
	2465752883056 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2465752883056 -> 2465752282736
	2465752282736 [label=AccumulateGrad]
	2465752282496 -> 2465752282688
	2465752283024 -> 2465752282976
	2465752884016 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2465752884016 -> 2465752283024
	2465752283024 [label=AccumulateGrad]
	2465752283168 -> 2465752283312
	2465752884112 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2465752884112 -> 2465752283168
	2465752283168 [label=AccumulateGrad]
	2465752283600 -> 2465752283312
	2465752884208 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2465752884208 -> 2465752283600
	2465752283600 [label=AccumulateGrad]
	2465752283696 -> 2465752283648
	2465752884592 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2465752884592 -> 2465752283696
	2465752283696 [label=AccumulateGrad]
	2465752283792 -> 2465752284080
	2465752884688 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2465752884688 -> 2465752283792
	2465752283792 [label=AccumulateGrad]
	2465752283840 -> 2465752284080
	2465752884784 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2465752884784 -> 2465752283840
	2465752283840 [label=AccumulateGrad]
	2465752283984 -> 2465752284176
	2465752283984 [label=CudnnBatchNormBackward0]
	2465752283216 -> 2465752283984
	2465752283216 [label=ConvolutionBackward0]
	2465752283120 -> 2465752283216
	2465752282880 -> 2465752283216
	2465752883440 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2465752883440 -> 2465752282880
	2465752282880 [label=AccumulateGrad]
	2465752283552 -> 2465752283984
	2465752883536 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2465752883536 -> 2465752283552
	2465752283552 [label=AccumulateGrad]
	2465752283456 -> 2465752283984
	2465752883632 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2465752883632 -> 2465752283456
	2465752283456 [label=AccumulateGrad]
	2465752283936 -> 2465752284560
	2465752885168 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2465752885168 -> 2465752283936
	2465752283936 [label=AccumulateGrad]
	2465752284464 -> 2465752284656
	2465752885264 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2465752885264 -> 2465752284464
	2465752284464 [label=AccumulateGrad]
	2465752284416 -> 2465752284656
	2465752885360 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2465752885360 -> 2465752284416
	2465752284416 [label=AccumulateGrad]
	2465752284752 -> 2465752269008
	2465752885744 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2465752885744 -> 2465752284752
	2465752284752 [label=AccumulateGrad]
	2465752268864 -> 2465752269344
	2465752885840 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2465752885840 -> 2465752268864
	2465752268864 [label=AccumulateGrad]
	2465752269488 -> 2465752269344
	2465752885936 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2465752885936 -> 2465752269488
	2465752269488 [label=AccumulateGrad]
	2465752269968 -> 2465752269824
	2465752270784 -> 2465752271888
	2465752886896 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2465752886896 -> 2465752270784
	2465752270784 [label=AccumulateGrad]
	2465752271744 -> 2465752272368
	2465752886992 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2465752886992 -> 2465752271744
	2465752271744 [label=AccumulateGrad]
	2465752272848 -> 2465752272368
	2465752887088 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2465752887088 -> 2465752272848
	2465752272848 [label=AccumulateGrad]
	2465752273328 -> 2465752273664
	2465752887472 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2465752887472 -> 2465752273328
	2465752273328 [label=AccumulateGrad]
	2465752274288 -> 2465752274768
	2465752887568 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2465752887568 -> 2465752274288
	2465752274288 [label=AccumulateGrad]
	2465752274144 -> 2465752274768
	2465752887664 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2465752887664 -> 2465752274144
	2465752274144 [label=AccumulateGrad]
	2465752274624 -> 2465752275248
	2465752274624 [label=CudnnBatchNormBackward0]
	2465752271408 -> 2465752274624
	2465752271408 [label=ConvolutionBackward0]
	2465752270928 -> 2465752271408
	2465752270304 -> 2465752271408
	2465752886320 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2465752886320 -> 2465752270304
	2465752270304 [label=AccumulateGrad]
	2465752273184 -> 2465752274624
	2465752886416 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2465752886416 -> 2465752273184
	2465752273184 [label=AccumulateGrad]
	2465752273808 -> 2465752274624
	2465752886512 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2465752886512 -> 2465752273808
	2465752273808 [label=AccumulateGrad]
	2465752275728 -> 2465752276688
	2465752888048 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2465752888048 -> 2465752275728
	2465752275728 [label=AccumulateGrad]
	2465752276544 -> 2465752277168
	2465752888144 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2465752888144 -> 2465752276544
	2465752276544 [label=AccumulateGrad]
	2465752277648 -> 2465752277168
	2465752888240 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2465752888240 -> 2465752277648
	2465752277648 [label=AccumulateGrad]
	2465752278128 -> 2465752278464
	2465752888624 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2465752888624 -> 2465752278128
	2465752278128 [label=AccumulateGrad]
	2465752279088 -> 2465752279568
	2465752888720 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2465752888720 -> 2465752279088
	2465752279088 [label=AccumulateGrad]
	2465752278944 -> 2465752279568
	2465752888816 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2465752888816 -> 2465752278944
	2465752278944 [label=AccumulateGrad]
	2465752279424 -> 2465752280048
	2465752281488 -> 2465752282304
	2465752281488 [label=TBackward0]
	2465752279904 -> 2465752281488
	2465752889488 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2465752889488 -> 2465752279904
	2465752279904 [label=AccumulateGrad]
	2465752282304 -> 2465752058864
}
