digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1964513480432 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1964520503328 [label=AddmmBackward0]
	1964520503472 -> 1964520503328
	1964508619568 [label="fc.bias
 (19)" fillcolor=lightblue]
	1964508619568 -> 1964520503472
	1964520503472 [label=AccumulateGrad]
	1964520503520 -> 1964520503328
	1964520503520 [label=ViewBackward0]
	1964520503616 -> 1964520503520
	1964520503616 [label=MeanBackward1]
	1964520503760 -> 1964520503616
	1964520503760 [label=ReluBackward0]
	1964520503856 -> 1964520503760
	1964520503856 [label=AddBackward0]
	1964520503952 -> 1964520503856
	1964520503952 [label=CudnnBatchNormBackward0]
	1964520504096 -> 1964520503952
	1964520504096 [label=ConvolutionBackward0]
	1964520504288 -> 1964520504096
	1964520504288 [label=ReluBackward0]
	1964520504432 -> 1964520504288
	1964520504432 [label=CudnnBatchNormBackward0]
	1964520504528 -> 1964520504432
	1964520504528 [label=ConvolutionBackward0]
	1964520503904 -> 1964520504528
	1964520503904 [label=ReluBackward0]
	1964520504672 -> 1964520503904
	1964520504672 [label=AddBackward0]
	1964520504768 -> 1964520504672
	1964520504768 [label=CudnnBatchNormBackward0]
	1964520504912 -> 1964520504768
	1964520504912 [label=ConvolutionBackward0]
	1964520505104 -> 1964520504912
	1964520505104 [label=ReluBackward0]
	1964520505248 -> 1964520505104
	1964520505248 [label=CudnnBatchNormBackward0]
	1964520505296 -> 1964520505248
	1964520505296 [label=ConvolutionBackward0]
	1964516278528 -> 1964520505296
	1964516278528 [label=ReluBackward0]
	1964516278672 -> 1964516278528
	1964516278672 [label=AddBackward0]
	1964516278768 -> 1964516278672
	1964516278768 [label=CudnnBatchNormBackward0]
	1964516278912 -> 1964516278768
	1964516278912 [label=ConvolutionBackward0]
	1964516279104 -> 1964516278912
	1964516279104 [label=ReluBackward0]
	1964516279248 -> 1964516279104
	1964516279248 [label=CudnnBatchNormBackward0]
	1964516279344 -> 1964516279248
	1964516279344 [label=ConvolutionBackward0]
	1964516278720 -> 1964516279344
	1964516278720 [label=ReluBackward0]
	1964516279632 -> 1964516278720
	1964516279632 [label=AddBackward0]
	1964516279728 -> 1964516279632
	1964516279728 [label=CudnnBatchNormBackward0]
	1964516279872 -> 1964516279728
	1964516279872 [label=ConvolutionBackward0]
	1964516280064 -> 1964516279872
	1964516280064 [label=ReluBackward0]
	1964516280208 -> 1964516280064
	1964516280208 [label=CudnnBatchNormBackward0]
	1964516280304 -> 1964516280208
	1964516280304 [label=ConvolutionBackward0]
	1964516280496 -> 1964516280304
	1964516280496 [label=ReluBackward0]
	1964516280640 -> 1964516280496
	1964516280640 [label=AddBackward0]
	1964516280736 -> 1964516280640
	1964516280736 [label=CudnnBatchNormBackward0]
	1964516280880 -> 1964516280736
	1964516280880 [label=ConvolutionBackward0]
	1964516280976 -> 1964516280880
	1964516280976 [label=ReluBackward0]
	1964512748128 -> 1964516280976
	1964512748128 [label=CudnnBatchNormBackward0]
	1964512753600 -> 1964512748128
	1964512753600 [label=ConvolutionBackward0]
	1964516280688 -> 1964512753600
	1964516280688 [label=ReluBackward0]
	1964512749568 -> 1964516280688
	1964512749568 [label=AddBackward0]
	1964512752352 -> 1964512749568
	1964512752352 [label=CudnnBatchNormBackward0]
	1964512754464 -> 1964512752352
	1964512754464 [label=ConvolutionBackward0]
	1964512753264 -> 1964512754464
	1964512753264 [label=ReluBackward0]
	1964512753216 -> 1964512753264
	1964512753216 [label=CudnnBatchNormBackward0]
	1964512751344 -> 1964512753216
	1964512751344 [label=ConvolutionBackward0]
	1964512746400 -> 1964512751344
	1964512746400 [label=ReluBackward0]
	1964512745104 -> 1964512746400
	1964512745104 [label=AddBackward0]
	1964512754848 -> 1964512745104
	1964512754848 [label=CudnnBatchNormBackward0]
	1964512744432 -> 1964512754848
	1964512744432 [label=ConvolutionBackward0]
	1964512746496 -> 1964512744432
	1964512746496 [label=ReluBackward0]
	1964512751536 -> 1964512746496
	1964512751536 [label=CudnnBatchNormBackward0]
	1964512750528 -> 1964512751536
	1964512750528 [label=ConvolutionBackward0]
	1964512751968 -> 1964512750528
	1964512751968 [label=ReluBackward0]
	1964512744816 -> 1964512751968
	1964512744816 [label=AddBackward0]
	1964512753312 -> 1964512744816
	1964512753312 [label=CudnnBatchNormBackward0]
	1964512755568 -> 1964512753312
	1964512755568 [label=ConvolutionBackward0]
	1964512746880 -> 1964512755568
	1964512746880 [label=ReluBackward0]
	1964512750816 -> 1964512746880
	1964512750816 [label=CudnnBatchNormBackward0]
	1964512744336 -> 1964512750816
	1964512744336 [label=ConvolutionBackward0]
	1964512753696 -> 1964512744336
	1964512753696 [label=MaxPool2DWithIndicesBackward0]
	1964512751296 -> 1964512753696
	1964512751296 [label=ReluBackward0]
	1964512752544 -> 1964512751296
	1964512752544 [label=CudnnBatchNormBackward0]
	1964512746016 -> 1964512752544
	1964512746016 [label=ConvolutionBackward0]
	1964512754368 -> 1964512746016
	1964516189744 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1964516189744 -> 1964512754368
	1964512754368 [label=AccumulateGrad]
	1964512750288 -> 1964512752544
	1964517998864 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1964517998864 -> 1964512750288
	1964512750288 [label=AccumulateGrad]
	1964512754896 -> 1964512752544
	1964517998960 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1964517998960 -> 1964512754896
	1964512754896 [label=AccumulateGrad]
	1964512754608 -> 1964512744336
	1964520405328 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1964520405328 -> 1964512754608
	1964512754608 [label=AccumulateGrad]
	1964512752400 -> 1964512750816
	1964520405424 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1964520405424 -> 1964512752400
	1964512752400 [label=AccumulateGrad]
	1964512749280 -> 1964512750816
	1964520405520 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1964520405520 -> 1964512749280
	1964512749280 [label=AccumulateGrad]
	1964512750000 -> 1964512755568
	1964520405904 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1964520405904 -> 1964512750000
	1964512750000 [label=AccumulateGrad]
	1964512749184 -> 1964512753312
	1964520406000 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1964520406000 -> 1964512749184
	1964512749184 [label=AccumulateGrad]
	1964512748944 -> 1964512753312
	1964520406096 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1964520406096 -> 1964512748944
	1964512748944 [label=AccumulateGrad]
	1964512753696 -> 1964512744816
	1964512755424 -> 1964512750528
	1964520406480 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1964520406480 -> 1964512755424
	1964512755424 [label=AccumulateGrad]
	1964512752928 -> 1964512751536
	1964520406576 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1964520406576 -> 1964512752928
	1964512752928 [label=AccumulateGrad]
	1964512754080 -> 1964512751536
	1964520406672 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1964520406672 -> 1964512754080
	1964512754080 [label=AccumulateGrad]
	1964512747792 -> 1964512744432
	1964516180048 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1964516180048 -> 1964512747792
	1964512747792 [label=AccumulateGrad]
	1964512755328 -> 1964512754848
	1964516180144 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1964516180144 -> 1964512755328
	1964512755328 [label=AccumulateGrad]
	1964512750144 -> 1964512754848
	1964516180240 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1964516180240 -> 1964512750144
	1964512750144 [label=AccumulateGrad]
	1964512751968 -> 1964512745104
	1964512747072 -> 1964512751344
	1964516181200 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1964516181200 -> 1964512747072
	1964512747072 [label=AccumulateGrad]
	1964512745296 -> 1964512753216
	1964516181296 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1964516181296 -> 1964512745296
	1964512745296 [label=AccumulateGrad]
	1964512750672 -> 1964512753216
	1964516181392 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1964516181392 -> 1964512750672
	1964512750672 [label=AccumulateGrad]
	1964512755040 -> 1964512754464
	1964516181776 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1964516181776 -> 1964512755040
	1964512755040 [label=AccumulateGrad]
	1964512753552 -> 1964512752352
	1964516181872 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1964516181872 -> 1964512753552
	1964512753552 [label=AccumulateGrad]
	1964512754800 -> 1964512752352
	1964516181968 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1964516181968 -> 1964512754800
	1964512754800 [label=AccumulateGrad]
	1964512751680 -> 1964512749568
	1964512751680 [label=CudnnBatchNormBackward0]
	1964512752064 -> 1964512751680
	1964512752064 [label=ConvolutionBackward0]
	1964512746400 -> 1964512752064
	1964512748656 -> 1964512752064
	1964516180624 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1964516180624 -> 1964512748656
	1964512748656 [label=AccumulateGrad]
	1964512748464 -> 1964512751680
	1964516180720 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1964516180720 -> 1964512748464
	1964512748464 [label=AccumulateGrad]
	1964512753024 -> 1964512751680
	1964516180816 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1964516180816 -> 1964512753024
	1964512753024 [label=AccumulateGrad]
	1964512751248 -> 1964512753600
	1964516182352 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1964516182352 -> 1964512751248
	1964512751248 [label=AccumulateGrad]
	1964512745248 -> 1964512748128
	1964516182448 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1964516182448 -> 1964512745248
	1964512745248 [label=AccumulateGrad]
	1964512754656 -> 1964512748128
	1964516182544 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1964516182544 -> 1964512754656
	1964512754656 [label=AccumulateGrad]
	1964512747120 -> 1964516280880
	1964516182928 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1964516182928 -> 1964512747120
	1964512747120 [label=AccumulateGrad]
	1964516280832 -> 1964516280736
	1964516183024 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1964516183024 -> 1964516280832
	1964516280832 [label=AccumulateGrad]
	1964516280784 -> 1964516280736
	1964516183120 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1964516183120 -> 1964516280784
	1964516280784 [label=AccumulateGrad]
	1964516280688 -> 1964516280640
	1964516280448 -> 1964516280304
	1964516184080 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1964516184080 -> 1964516280448
	1964516280448 [label=AccumulateGrad]
	1964516280256 -> 1964516280208
	1964516184176 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1964516184176 -> 1964516280256
	1964516280256 [label=AccumulateGrad]
	1964516280112 -> 1964516280208
	1964516184272 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1964516184272 -> 1964516280112
	1964516280112 [label=AccumulateGrad]
	1964516280016 -> 1964516279872
	1964516184656 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1964516184656 -> 1964516280016
	1964516280016 [label=AccumulateGrad]
	1964516279824 -> 1964516279728
	1964516184752 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1964516184752 -> 1964516279824
	1964516279824 [label=AccumulateGrad]
	1964516279776 -> 1964516279728
	1964516184848 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1964516184848 -> 1964516279776
	1964516279776 [label=AccumulateGrad]
	1964516279680 -> 1964516279632
	1964516279680 [label=CudnnBatchNormBackward0]
	1964516280400 -> 1964516279680
	1964516280400 [label=ConvolutionBackward0]
	1964516280496 -> 1964516280400
	1964516280544 -> 1964516280400
	1964516183504 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1964516183504 -> 1964516280544
	1964516280544 [label=AccumulateGrad]
	1964516279968 -> 1964516279680
	1964516183600 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1964516183600 -> 1964516279968
	1964516279968 [label=AccumulateGrad]
	1964516279920 -> 1964516279680
	1964516183696 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1964516183696 -> 1964516279920
	1964516279920 [label=AccumulateGrad]
	1964516279536 -> 1964516279344
	1964516185232 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1964516185232 -> 1964516279536
	1964516279536 [label=AccumulateGrad]
	1964516279296 -> 1964516279248
	1964516185328 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1964516185328 -> 1964516279296
	1964516279296 [label=AccumulateGrad]
	1964516279152 -> 1964516279248
	1964516185424 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1964516185424 -> 1964516279152
	1964516279152 [label=AccumulateGrad]
	1964516279056 -> 1964516278912
	1964516185808 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1964516185808 -> 1964516279056
	1964516279056 [label=AccumulateGrad]
	1964516278864 -> 1964516278768
	1964516185904 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1964516185904 -> 1964516278864
	1964516278864 [label=AccumulateGrad]
	1964516278816 -> 1964516278768
	1964516186000 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1964516186000 -> 1964516278816
	1964516278816 [label=AccumulateGrad]
	1964516278720 -> 1964516278672
	1964516278480 -> 1964520505296
	1964516186960 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1964516186960 -> 1964516278480
	1964516278480 [label=AccumulateGrad]
	1964520505152 -> 1964520505248
	1964516187056 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1964516187056 -> 1964520505152
	1964520505152 [label=AccumulateGrad]
	1964516278336 -> 1964520505248
	1964516187152 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1964516187152 -> 1964516278336
	1964516278336 [label=AccumulateGrad]
	1964520505056 -> 1964520504912
	1964516187536 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1964516187536 -> 1964520505056
	1964520505056 [label=AccumulateGrad]
	1964520504864 -> 1964520504768
	1964516187632 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1964516187632 -> 1964520504864
	1964520504864 [label=AccumulateGrad]
	1964520504816 -> 1964520504768
	1964516187728 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1964516187728 -> 1964520504816
	1964520504816 [label=AccumulateGrad]
	1964520504720 -> 1964520504672
	1964520504720 [label=CudnnBatchNormBackward0]
	1964520505200 -> 1964520504720
	1964520505200 [label=ConvolutionBackward0]
	1964516278528 -> 1964520505200
	1964516278576 -> 1964520505200
	1964516186384 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1964516186384 -> 1964516278576
	1964516278576 [label=AccumulateGrad]
	1964520505008 -> 1964520504720
	1964516186480 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1964516186480 -> 1964520505008
	1964520505008 [label=AccumulateGrad]
	1964520504960 -> 1964520504720
	1964516186576 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1964516186576 -> 1964520504960
	1964520504960 [label=AccumulateGrad]
	1964520502752 -> 1964520504528
	1964516188112 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1964516188112 -> 1964520502752
	1964520502752 [label=AccumulateGrad]
	1964520504480 -> 1964520504432
	1964516188208 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1964516188208 -> 1964520504480
	1964520504480 [label=AccumulateGrad]
	1964520504336 -> 1964520504432
	1964516188304 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1964516188304 -> 1964520504336
	1964520504336 [label=AccumulateGrad]
	1964520504240 -> 1964520504096
	1964516188688 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1964516188688 -> 1964520504240
	1964520504240 [label=AccumulateGrad]
	1964520504048 -> 1964520503952
	1964516188784 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1964516188784 -> 1964520504048
	1964520504048 [label=AccumulateGrad]
	1964520504000 -> 1964520503952
	1964516188880 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1964516188880 -> 1964520504000
	1964520504000 [label=AccumulateGrad]
	1964520503904 -> 1964520503856
	1964520503568 -> 1964520503328
	1964520503568 [label=TBackward0]
	1964520503808 -> 1964520503568
	1964516189360 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1964516189360 -> 1964520503808
	1964520503808 [label=AccumulateGrad]
	1964520503328 -> 1964513480432
}
