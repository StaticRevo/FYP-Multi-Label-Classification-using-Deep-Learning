digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2185060875248 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2185001989888 [label=AddmmBackward0]
	2185001988784 -> 2185001989888
	2185086090896 [label="fc.bias
 (19)" fillcolor=lightblue]
	2185086090896 -> 2185001988784
	2185001988784 [label=AccumulateGrad]
	2185001988928 -> 2185001989888
	2185001988928 [label=ViewBackward0]
	2185001988448 -> 2185001988928
	2185001988448 [label=MeanBackward1]
	2185001987344 -> 2185001988448
	2185001987344 [label=ReluBackward0]
	2185001986864 -> 2185001987344
	2185001986864 [label=AddBackward0]
	2185001986384 -> 2185001986864
	2185001986384 [label=CudnnBatchNormBackward0]
	2185001986048 -> 2185001986384
	2185001986048 [label=ConvolutionBackward0]
	2185001985088 -> 2185001986048
	2185001985088 [label=ReluBackward0]
	2185001983984 -> 2185001985088
	2185001983984 [label=CudnnBatchNormBackward0]
	2185001982112 -> 2185001983984
	2185001982112 [label=ConvolutionBackward0]
	2185001983120 -> 2185001982112
	2185001983120 [label=ReluBackward0]
	2185001991856 -> 2185001983120
	2185001991856 [label=CudnnBatchNormBackward0]
	2185001992096 -> 2185001991856
	2185001992096 [label=ConvolutionBackward0]
	2185001987008 -> 2185001992096
	2185001987008 [label=ReluBackward0]
	2185001991376 -> 2185001987008
	2185001991376 [label=AddBackward0]
	2185001991616 -> 2185001991376
	2185001991616 [label=CudnnBatchNormBackward0]
	2185001991280 -> 2185001991616
	2185001991280 [label=ConvolutionBackward0]
	2185001990992 -> 2185001991280
	2185001990992 [label=ReluBackward0]
	2185001991040 -> 2185001990992
	2185001991040 [label=CudnnBatchNormBackward0]
	2185001990752 -> 2185001991040
	2185001990752 [label=ConvolutionBackward0]
	2185001990656 -> 2185001990752
	2185001990656 [label=ReluBackward0]
	2185001990320 -> 2185001990656
	2185001990320 [label=CudnnBatchNormBackward0]
	2185001990128 -> 2185001990320
	2185001990128 [label=ConvolutionBackward0]
	2185001991472 -> 2185001990128
	2185001991472 [label=ReluBackward0]
	2185001989840 -> 2185001991472
	2185001989840 [label=AddBackward0]
	2185001989648 -> 2185001989840
	2185001989648 [label=CudnnBatchNormBackward0]
	2185001989696 -> 2185001989648
	2185001989696 [label=ConvolutionBackward0]
	2185001989312 -> 2185001989696
	2185001989312 [label=ReluBackward0]
	2185001989072 -> 2185001989312
	2185001989072 [label=CudnnBatchNormBackward0]
	2185001989024 -> 2185001989072
	2185001989024 [label=ConvolutionBackward0]
	2185001988688 -> 2185001989024
	2185001988688 [label=ReluBackward0]
	2185001988736 -> 2185001988688
	2185001988736 [label=CudnnBatchNormBackward0]
	2185001988640 -> 2185001988736
	2185001988640 [label=ConvolutionBackward0]
	2185001988016 -> 2185001988640
	2185001988016 [label=ReluBackward0]
	2185001988064 -> 2185001988016
	2185001988064 [label=AddBackward0]
	2185001987920 -> 2185001988064
	2185001987920 [label=CudnnBatchNormBackward0]
	2185001987536 -> 2185001987920
	2185001987536 [label=ConvolutionBackward0]
	2185001987680 -> 2185001987536
	2185001987680 [label=ReluBackward0]
	2185001987248 -> 2185001987680
	2185001987248 [label=CudnnBatchNormBackward0]
	2185001987152 -> 2185001987248
	2185001987152 [label=ConvolutionBackward0]
	2185001986960 -> 2185001987152
	2185001986960 [label=ReluBackward0]
	2185001986576 -> 2185001986960
	2185001986576 [label=CudnnBatchNormBackward0]
	2185001986816 -> 2185001986576
	2185001986816 [label=ConvolutionBackward0]
	2185001988160 -> 2185001986816
	2185001988160 [label=ReluBackward0]
	2185001986096 -> 2185001988160
	2185001986096 [label=AddBackward0]
	2185001986336 -> 2185001986096
	2185001986336 [label=CudnnBatchNormBackward0]
	2185001986000 -> 2185001986336
	2185001986000 [label=ConvolutionBackward0]
	2185001985712 -> 2185001986000
	2185001985712 [label=ReluBackward0]
	2185001985760 -> 2185001985712
	2185001985760 [label=CudnnBatchNormBackward0]
	2185001985472 -> 2185001985760
	2185001985472 [label=ConvolutionBackward0]
	2185001985376 -> 2185001985472
	2185001985376 [label=ReluBackward0]
	2185001985040 -> 2185001985376
	2185001985040 [label=CudnnBatchNormBackward0]
	2185001984848 -> 2185001985040
	2185001984848 [label=ConvolutionBackward0]
	2185001986192 -> 2185001984848
	2185001986192 [label=ReluBackward0]
	2185001984560 -> 2185001986192
	2185001984560 [label=AddBackward0]
	2185001984368 -> 2185001984560
	2185001984368 [label=CudnnBatchNormBackward0]
	2185001984416 -> 2185001984368
	2185001984416 [label=ConvolutionBackward0]
	2185001984032 -> 2185001984416
	2185001984032 [label=ReluBackward0]
	2185001983792 -> 2185001984032
	2185001983792 [label=CudnnBatchNormBackward0]
	2185001983744 -> 2185001983792
	2185001983744 [label=ConvolutionBackward0]
	2185001980816 -> 2185001983744
	2185001980816 [label=ReluBackward0]
	2185001980768 -> 2185001980816
	2185001980768 [label=CudnnBatchNormBackward0]
	2185001983408 -> 2185001980768
	2185001983408 [label=ConvolutionBackward0]
	2185001984512 -> 2185001983408
	2185001984512 [label=ReluBackward0]
	2185001983216 -> 2185001984512
	2185001983216 [label=AddBackward0]
	2185001979232 -> 2185001983216
	2185001979232 [label=CudnnBatchNormBackward0]
	2185001977600 -> 2185001979232
	2185001977600 [label=ConvolutionBackward0]
	2185001982880 -> 2185001977600
	2185001982880 [label=ReluBackward0]
	2185001981776 -> 2185001982880
	2185001981776 [label=CudnnBatchNormBackward0]
	2185001981296 -> 2185001981776
	2185001981296 [label=ConvolutionBackward0]
	2185001980384 -> 2185001981296
	2185001980384 [label=ReluBackward0]
	2185001980480 -> 2185001980384
	2185001980480 [label=CudnnBatchNormBackward0]
	2185001978080 -> 2185001980480
	2185001978080 [label=ConvolutionBackward0]
	2185001979520 -> 2185001978080
	2185001979520 [label=ReluBackward0]
	2185001978176 -> 2185001979520
	2185001978176 [label=AddBackward0]
	2185001977840 -> 2185001978176
	2185001977840 [label=CudnnBatchNormBackward0]
	2185001983504 -> 2185001977840
	2185001983504 [label=ConvolutionBackward0]
	2185001981584 -> 2185001983504
	2185001981584 [label=ReluBackward0]
	2185001978464 -> 2185001981584
	2185001978464 [label=CudnnBatchNormBackward0]
	2185001976016 -> 2185001978464
	2185001976016 [label=ConvolutionBackward0]
	2185001982736 -> 2185001976016
	2185001982736 [label=ReluBackward0]
	2185001977168 -> 2185001982736
	2185001977168 [label=CudnnBatchNormBackward0]
	2185001983312 -> 2185001977168
	2185001983312 [label=ConvolutionBackward0]
	2185001977216 -> 2185001983312
	2185001977216 [label=ReluBackward0]
	2185001983264 -> 2185001977216
	2185001983264 [label=AddBackward0]
	2185001976928 -> 2185001983264
	2185001976928 [label=CudnnBatchNormBackward0]
	2185001982400 -> 2185001976928
	2185001982400 [label=ConvolutionBackward0]
	2185001978368 -> 2185001982400
	2185001978368 [label=ReluBackward0]
	2185001980048 -> 2185001978368
	2185001980048 [label=CudnnBatchNormBackward0]
	2185001983360 -> 2185001980048
	2185001983360 [label=ConvolutionBackward0]
	2185001980912 -> 2185001983360
	2185001980912 [label=ReluBackward0]
	2185001980624 -> 2185001980912
	2185001980624 [label=CudnnBatchNormBackward0]
	2185001982832 -> 2185001980624
	2185001982832 [label=ConvolutionBackward0]
	2185001975872 -> 2185001982832
	2185001975872 [label=ReluBackward0]
	2185002071760 -> 2185001975872
	2185002071760 [label=AddBackward0]
	2185002072048 -> 2185002071760
	2185002072048 [label=CudnnBatchNormBackward0]
	2185002063648 -> 2185002072048
	2185002063648 [label=ConvolutionBackward0]
	2185002072768 -> 2185002063648
	2185002072768 [label=ReluBackward0]
	2185002073056 -> 2185002072768
	2185002073056 [label=CudnnBatchNormBackward0]
	2185002057792 -> 2185002073056
	2185002057792 [label=ConvolutionBackward0]
	2185002073680 -> 2185002057792
	2185002073680 [label=ReluBackward0]
	2184741124944 -> 2185002073680
	2184741124944 [label=CudnnBatchNormBackward0]
	2185061227712 -> 2184741124944
	2185061227712 [label=ConvolutionBackward0]
	2185002070896 -> 2185061227712
	2185002070896 [label=ReluBackward0]
	2185061226272 -> 2185002070896
	2185061226272 [label=AddBackward0]
	2185061225792 -> 2185061226272
	2185061225792 [label=CudnnBatchNormBackward0]
	2185061224688 -> 2185061225792
	2185061224688 [label=ConvolutionBackward0]
	2185061223728 -> 2185061224688
	2185061223728 [label=ReluBackward0]
	2185061223392 -> 2185061223728
	2185061223392 [label=CudnnBatchNormBackward0]
	2185061222912 -> 2185061223392
	2185061222912 [label=ConvolutionBackward0]
	2185061221952 -> 2185061222912
	2185061221952 [label=ReluBackward0]
	2185061220848 -> 2185061221952
	2185061220848 [label=CudnnBatchNormBackward0]
	2185061220512 -> 2185061220848
	2185061220512 [label=ConvolutionBackward0]
	2185061225648 -> 2185061220512
	2185061225648 [label=ReluBackward0]
	2185061228000 -> 2185061225648
	2185061228000 [label=AddBackward0]
	2185061227904 -> 2185061228000
	2185061227904 [label=CudnnBatchNormBackward0]
	2185061227472 -> 2185061227904
	2185061227472 [label=ConvolutionBackward0]
	2185061227328 -> 2185061227472
	2185061227328 [label=ReluBackward0]
	2185061227136 -> 2185061227328
	2185061227136 [label=CudnnBatchNormBackward0]
	2185061226800 -> 2185061227136
	2185061226800 [label=ConvolutionBackward0]
	2185061226944 -> 2185061226800
	2185061226944 [label=ReluBackward0]
	2185061226512 -> 2185061226944
	2185061226512 [label=CudnnBatchNormBackward0]
	2185061226416 -> 2185061226512
	2185061226416 [label=ConvolutionBackward0]
	2185061227808 -> 2185061226416
	2185061227808 [label=ReluBackward0]
	2185061226032 -> 2185061227808
	2185061226032 [label=AddBackward0]
	2185061225936 -> 2185061226032
	2185061225936 [label=CudnnBatchNormBackward0]
	2185061225984 -> 2185061225936
	2185061225984 [label=ConvolutionBackward0]
	2185061225360 -> 2185061225984
	2185061225360 [label=ReluBackward0]
	2185061225408 -> 2185061225360
	2185061225408 [label=CudnnBatchNormBackward0]
	2185061225264 -> 2185061225408
	2185061225264 [label=ConvolutionBackward0]
	2185061224976 -> 2185061225264
	2185061224976 [label=ReluBackward0]
	2185061225024 -> 2185061224976
	2185061225024 [label=CudnnBatchNormBackward0]
	2185061224736 -> 2185061225024
	2185061224736 [label=ConvolutionBackward0]
	2185061224640 -> 2185061224736
	2185061224640 [label=ReluBackward0]
	2185061224304 -> 2185061224640
	2185061224304 [label=AddBackward0]
	2185061224112 -> 2185061224304
	2185061224112 [label=CudnnBatchNormBackward0]
	2185061224160 -> 2185061224112
	2185061224160 [label=ConvolutionBackward0]
	2185061223776 -> 2185061224160
	2185061223776 [label=ReluBackward0]
	2185061223536 -> 2185061223776
	2185061223536 [label=CudnnBatchNormBackward0]
	2185061223488 -> 2185061223536
	2185061223488 [label=ConvolutionBackward0]
	2185061223152 -> 2185061223488
	2185061223152 [label=ReluBackward0]
	2185061223200 -> 2185061223152
	2185061223200 [label=CudnnBatchNormBackward0]
	2185061223104 -> 2185061223200
	2185061223104 [label=ConvolutionBackward0]
	2185061224256 -> 2185061223104
	2185061224256 [label=ReluBackward0]
	2185061222720 -> 2185061224256
	2185061222720 [label=AddBackward0]
	2185061222624 -> 2185061222720
	2185061222624 [label=CudnnBatchNormBackward0]
	2185061222192 -> 2185061222624
	2185061222192 [label=ConvolutionBackward0]
	2185061222048 -> 2185061222192
	2185061222048 [label=ReluBackward0]
	2185061221856 -> 2185061222048
	2185061221856 [label=CudnnBatchNormBackward0]
	2185061221520 -> 2185061221856
	2185061221520 [label=ConvolutionBackward0]
	2185061221664 -> 2185061221520
	2185061221664 [label=ReluBackward0]
	2185061221232 -> 2185061221664
	2185061221232 [label=CudnnBatchNormBackward0]
	2185061221136 -> 2185061221232
	2185061221136 [label=ConvolutionBackward0]
	2185061222528 -> 2185061221136
	2185061222528 [label=ReluBackward0]
	2185061220752 -> 2185061222528
	2185061220752 [label=AddBackward0]
	2185061220656 -> 2185061220752
	2185061220656 [label=CudnnBatchNormBackward0]
	2185061220704 -> 2185061220656
	2185061220704 [label=ConvolutionBackward0]
	2185061228624 -> 2185061220704
	2185061228624 [label=ReluBackward0]
	2185061228960 -> 2185061228624
	2185061228960 [label=CudnnBatchNormBackward0]
	2185061229056 -> 2185061228960
	2185061229056 [label=ConvolutionBackward0]
	2185061229248 -> 2185061229056
	2185061229248 [label=ReluBackward0]
	2185061229392 -> 2185061229248
	2185061229392 [label=CudnnBatchNormBackward0]
	2185061229488 -> 2185061229392
	2185061229488 [label=ConvolutionBackward0]
	2185061229680 -> 2185061229488
	2185061229680 [label=MaxPool2DWithIndicesBackward0]
	2185061229824 -> 2185061229680
	2185061229824 [label=ReluBackward0]
	2185061229920 -> 2185061229824
	2185061229920 [label=CudnnBatchNormBackward0]
	2185061230016 -> 2185061229920
	2185061230016 [label=ConvolutionBackward0]
	2185061230208 -> 2185061230016
	2185086090704 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2185086090704 -> 2185061230208
	2185061230208 [label=AccumulateGrad]
	2185061229968 -> 2185061229920
	2185064157392 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2185064157392 -> 2185061229968
	2185061229968 [label=AccumulateGrad]
	2185061229728 -> 2185061229920
	2185064157104 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2185064157104 -> 2185061229728
	2185061229728 [label=AccumulateGrad]
	2185061229632 -> 2185061229488
	2185064157008 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2185064157008 -> 2185061229632
	2185061229632 [label=AccumulateGrad]
	2185061229440 -> 2185061229392
	2185064156912 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2185064156912 -> 2185061229440
	2185061229440 [label=AccumulateGrad]
	2185061229296 -> 2185061229392
	2185064156144 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2185064156144 -> 2185061229296
	2185061229296 [label=AccumulateGrad]
	2185061229200 -> 2185061229056
	2185064156240 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2185064156240 -> 2185061229200
	2185061229200 [label=AccumulateGrad]
	2185061229008 -> 2185061228960
	2185064156336 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2185064156336 -> 2185061229008
	2185061229008 [label=AccumulateGrad]
	2185061228864 -> 2185061228960
	2185064156432 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2185064156432 -> 2185061228864
	2185061228864 [label=AccumulateGrad]
	2185061228768 -> 2185061220704
	2185064156720 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2185064156720 -> 2185061228768
	2185061228768 [label=AccumulateGrad]
	2185061220608 -> 2185061220656
	2185064155760 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2185064155760 -> 2185061220608
	2185061220608 [label=AccumulateGrad]
	2185061220800 -> 2185061220656
	2185064155664 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2185064155664 -> 2185061220800
	2185061220800 [label=AccumulateGrad]
	2185061220560 -> 2185061220752
	2185061220560 [label=CudnnBatchNormBackward0]
	2185061229152 -> 2185061220560
	2185061229152 [label=ConvolutionBackward0]
	2185061229680 -> 2185061229152
	2185061229536 -> 2185061229152
	2185064157584 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2185064157584 -> 2185061229536
	2185061229536 [label=AccumulateGrad]
	2185061220416 -> 2185061220560
	2185064157680 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2185064157680 -> 2185061220416
	2185061220416 [label=AccumulateGrad]
	2185061220464 -> 2185061220560
	2185064157776 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2185064157776 -> 2185061220464
	2185061220464 [label=AccumulateGrad]
	2185061220944 -> 2185061221136
	2185064155472 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2185064155472 -> 2185061220944
	2185061220944 [label=AccumulateGrad]
	2185061221040 -> 2185061221232
	2185064154992 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2185064154992 -> 2185061221040
	2185061221040 [label=AccumulateGrad]
	2185061221424 -> 2185061221232
	2185064153648 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2185064153648 -> 2185061221424
	2185061221424 [label=AccumulateGrad]
	2185061221568 -> 2185061221520
	2185064155568 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2185064155568 -> 2185061221568
	2185061221568 [label=AccumulateGrad]
	2185061221712 -> 2185061221856
	2185064155376 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2185064155376 -> 2185061221712
	2185061221712 [label=AccumulateGrad]
	2185061222144 -> 2185061221856
	2185086391248 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2185086391248 -> 2185061222144
	2185061222144 [label=AccumulateGrad]
	2185061222240 -> 2185061222192
	2185086391632 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2185086391632 -> 2185061222240
	2185061222240 [label=AccumulateGrad]
	2185061222336 -> 2185061222624
	2185086391728 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2185086391728 -> 2185061222336
	2185061222336 [label=AccumulateGrad]
	2185061222384 -> 2185061222624
	2185086391824 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2185086391824 -> 2185061222384
	2185061222384 [label=AccumulateGrad]
	2185061222528 -> 2185061222720
	2185061222480 -> 2185061223104
	2185086392208 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2185086392208 -> 2185061222480
	2185061222480 [label=AccumulateGrad]
	2185061223008 -> 2185061223200
	2185086392304 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2185086392304 -> 2185061223008
	2185061223008 [label=AccumulateGrad]
	2185061222960 -> 2185061223200
	2185086392400 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2185086392400 -> 2185061222960
	2185061222960 [label=AccumulateGrad]
	2185061223296 -> 2185061223488
	2185086392784 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2185086392784 -> 2185061223296
	2185061223296 [label=AccumulateGrad]
	2185061223680 -> 2185061223536
	2185086392880 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2185086392880 -> 2185061223680
	2185061223680 [label=AccumulateGrad]
	2185061223632 -> 2185061223536
	2185086392976 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2185086392976 -> 2185061223632
	2185061223632 [label=AccumulateGrad]
	2185061223824 -> 2185061224160
	2185086393360 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2185086393360 -> 2185061223824
	2185061223824 [label=AccumulateGrad]
	2185061224016 -> 2185061224112
	2185086393456 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2185086393456 -> 2185061224016
	2185061224016 [label=AccumulateGrad]
	2185061223920 -> 2185061224112
	2185086393552 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2185086393552 -> 2185061223920
	2185061223920 [label=AccumulateGrad]
	2185061224256 -> 2185061224304
	2185061224496 -> 2185061224736
	2185086394512 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2185086394512 -> 2185061224496
	2185061224496 [label=AccumulateGrad]
	2185061224784 -> 2185061225024
	2185086394608 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2185086394608 -> 2185061224784
	2185061224784 [label=AccumulateGrad]
	2185061225120 -> 2185061225024
	2185086394704 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2185086394704 -> 2185061225120
	2185061225120 [label=AccumulateGrad]
	2185061224880 -> 2185061225264
	2185086395088 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2185086395088 -> 2185061224880
	2185061224880 [label=AccumulateGrad]
	2185061225504 -> 2185061225408
	2185086395184 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2185086395184 -> 2185061225504
	2185061225504 [label=AccumulateGrad]
	2185061225456 -> 2185061225408
	2185086395280 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2185086395280 -> 2185061225456
	2185061225456 [label=AccumulateGrad]
	2185061225552 -> 2185061225984
	2185086395664 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2185086395664 -> 2185061225552
	2185061225552 [label=AccumulateGrad]
	2185061225888 -> 2185061225936
	2185086395760 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2185086395760 -> 2185061225888
	2185061225888 [label=AccumulateGrad]
	2185061226080 -> 2185061225936
	2185086395856 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2185086395856 -> 2185061226080
	2185061226080 [label=AccumulateGrad]
	2185061225840 -> 2185061226032
	2185061225840 [label=CudnnBatchNormBackward0]
	2185061225072 -> 2185061225840
	2185061225072 [label=ConvolutionBackward0]
	2185061224640 -> 2185061225072
	2185061224592 -> 2185061225072
	2185086393936 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2185086393936 -> 2185061224592
	2185061224592 [label=AccumulateGrad]
	2185061225696 -> 2185061225840
	2185086394032 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2185086394032 -> 2185061225696
	2185061225696 [label=AccumulateGrad]
	2185061225744 -> 2185061225840
	2185086394128 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2185086394128 -> 2185061225744
	2185061225744 [label=AccumulateGrad]
	2185061226224 -> 2185061226416
	2185086396240 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2185086396240 -> 2185061226224
	2185061226224 [label=AccumulateGrad]
	2185061226320 -> 2185061226512
	2185086396336 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2185086396336 -> 2185061226320
	2185061226320 [label=AccumulateGrad]
	2185061226704 -> 2185061226512
	2185086396432 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2185086396432 -> 2185061226704
	2185061226704 [label=AccumulateGrad]
	2185061226848 -> 2185061226800
	2185086396816 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2185086396816 -> 2185061226848
	2185061226848 [label=AccumulateGrad]
	2185061226992 -> 2185061227136
	2185086396912 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2185086396912 -> 2185061226992
	2185061226992 [label=AccumulateGrad]
	2185061227424 -> 2185061227136
	2185086397008 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2185086397008 -> 2185061227424
	2185061227424 [label=AccumulateGrad]
	2185061227520 -> 2185061227472
	2185086397392 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2185086397392 -> 2185061227520
	2185061227520 [label=AccumulateGrad]
	2185061227616 -> 2185061227904
	2185086397488 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2185086397488 -> 2185061227616
	2185061227616 [label=AccumulateGrad]
	2185061227664 -> 2185061227904
	2185086397584 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2185086397584 -> 2185061227664
	2185061227664 [label=AccumulateGrad]
	2185061227808 -> 2185061228000
	2185061227760 -> 2185061220512
	2185086397968 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2185086397968 -> 2185061227760
	2185061227760 [label=AccumulateGrad]
	2185061220992 -> 2185061220848
	2185086398064 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2185086398064 -> 2185061220992
	2185061220992 [label=AccumulateGrad]
	2185061221328 -> 2185061220848
	2185086398160 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2185086398160 -> 2185061221328
	2185061221328 [label=AccumulateGrad]
	2185061221808 -> 2185061222912
	2185086398544 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2185086398544 -> 2185061221808
	2185061221808 [label=AccumulateGrad]
	2185061222768 -> 2185061223392
	2185086398640 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2185086398640 -> 2185061222768
	2185061222768 [label=AccumulateGrad]
	2185061223872 -> 2185061223392
	2185086398736 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2185086398736 -> 2185061223872
	2185061223872 [label=AccumulateGrad]
	2185061224352 -> 2185061224688
	2185086399120 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2185086399120 -> 2185061224352
	2185061224352 [label=AccumulateGrad]
	2185061225312 -> 2185061225792
	2185086399216 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2185086399216 -> 2185061225312
	2185061225312 [label=AccumulateGrad]
	2185061225168 -> 2185061225792
	2185086399312 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2185086399312 -> 2185061225168
	2185061225168 [label=AccumulateGrad]
	2185061225648 -> 2185061226272
	2185061226752 -> 2185061227712
	2185086399696 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2185086399696 -> 2185061226752
	2185061226752 [label=AccumulateGrad]
	2185061227568 -> 2184741124944
	2185086399792 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2185086399792 -> 2185061227568
	2185061227568 [label=AccumulateGrad]
	2185061228192 -> 2184741124944
	2185086399888 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2185086399888 -> 2185061228192
	2185061228192 [label=AccumulateGrad]
	2185002073920 -> 2185002057792
	2185086400272 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2185086400272 -> 2185002073920
	2185002073920 [label=AccumulateGrad]
	2185002069360 -> 2185002073056
	2185086400368 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2185086400368 -> 2185002069360
	2185002069360 [label=AccumulateGrad]
	2185002071520 -> 2185002073056
	2185086400464 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2185086400464 -> 2185002071520
	2185002071520 [label=AccumulateGrad]
	2185002071280 -> 2185002063648
	2185086400848 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2185086400848 -> 2185002071280
	2185002071280 [label=AccumulateGrad]
	2185002068880 -> 2185002072048
	2185086400944 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2185086400944 -> 2185002068880
	2185002068880 [label=AccumulateGrad]
	2185002069984 -> 2185002072048
	2185086401040 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2185086401040 -> 2185002069984
	2185002069984 [label=AccumulateGrad]
	2185002070896 -> 2185002071760
	2185002060480 -> 2185001982832
	2185086402000 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2185086402000 -> 2185002060480
	2185002060480 [label=AccumulateGrad]
	2185001976112 -> 2185001980624
	2185086402096 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2185086402096 -> 2185001976112
	2185001976112 [label=AccumulateGrad]
	2185001979328 -> 2185001980624
	2185086402192 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2185086402192 -> 2185001979328
	2185001979328 [label=AccumulateGrad]
	2185001983168 -> 2185001983360
	2185086074960 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2185086074960 -> 2185001983168
	2185001983168 [label=AccumulateGrad]
	2185001979856 -> 2185001980048
	2185086075056 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2185086075056 -> 2185001979856
	2185001979856 [label=AccumulateGrad]
	2185001982592 -> 2185001980048
	2185086075152 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2185086075152 -> 2185001982592
	2185001982592 [label=AccumulateGrad]
	2185001983024 -> 2185001982400
	2185086075536 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2185086075536 -> 2185001983024
	2185001983024 [label=AccumulateGrad]
	2185001980528 -> 2185001976928
	2185086075632 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2185086075632 -> 2185001980528
	2185001980528 [label=AccumulateGrad]
	2185001981152 -> 2185001976928
	2185086075728 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2185086075728 -> 2185001981152
	2185001981152 [label=AccumulateGrad]
	2185001982496 -> 2185001983264
	2185001982496 [label=CudnnBatchNormBackward0]
	2185001976544 -> 2185001982496
	2185001976544 [label=ConvolutionBackward0]
	2185001975872 -> 2185001976544
	2185001981056 -> 2185001976544
	2185086401424 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2185086401424 -> 2185001981056
	2185001981056 [label=AccumulateGrad]
	2185001978848 -> 2185001982496
	2185086401520 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2185086401520 -> 2185001978848
	2185001978848 [label=AccumulateGrad]
	2185001977984 -> 2185001982496
	2185086401616 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2185086401616 -> 2185001977984
	2185001977984 [label=AccumulateGrad]
	2185001982784 -> 2185001983312
	2185086076112 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2185086076112 -> 2185001982784
	2185001982784 [label=AccumulateGrad]
	2185001976736 -> 2185001977168
	2185086076208 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2185086076208 -> 2185001976736
	2185001976736 [label=AccumulateGrad]
	2185001977696 -> 2185001977168
	2185086076304 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2185086076304 -> 2185001977696
	2185001977696 [label=AccumulateGrad]
	2185001983456 -> 2185001976016
	2185086076688 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2185086076688 -> 2185001983456
	2185001983456 [label=AccumulateGrad]
	2185001977072 -> 2185001978464
	2185086076784 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2185086076784 -> 2185001977072
	2185001977072 [label=AccumulateGrad]
	2185001979952 -> 2185001978464
	2185086076880 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2185086076880 -> 2185001979952
	2185001979952 [label=AccumulateGrad]
	2185001979616 -> 2185001983504
	2185086077264 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2185086077264 -> 2185001979616
	2185001979616 [label=AccumulateGrad]
	2185001978272 -> 2185001977840
	2185086077360 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2185086077360 -> 2185001978272
	2185001978272 [label=AccumulateGrad]
	2185001976976 -> 2185001977840
	2185086077456 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2185086077456 -> 2185001976976
	2185001976976 [label=AccumulateGrad]
	2185001977216 -> 2185001978176
	2185001981392 -> 2185001978080
	2185086077840 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2185086077840 -> 2185001981392
	2185001981392 [label=AccumulateGrad]
	2185001977744 -> 2185001980480
	2185086077936 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2185086077936 -> 2185001977744
	2185001977744 [label=AccumulateGrad]
	2185001980240 -> 2185001980480
	2185086078032 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2185086078032 -> 2185001980240
	2185001980240 [label=AccumulateGrad]
	2185001980288 -> 2185001981296
	2185086078416 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2185086078416 -> 2185001980288
	2185001980288 [label=AccumulateGrad]
	2185001978560 -> 2185001981776
	2185086078512 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2185086078512 -> 2185001978560
	2185001978560 [label=AccumulateGrad]
	2185001977648 -> 2185001981776
	2185086078608 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2185086078608 -> 2185001977648
	2185001977648 [label=AccumulateGrad]
	2185001982256 -> 2185001977600
	2185086078992 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2185086078992 -> 2185001982256
	2185001982256 [label=AccumulateGrad]
	2185001982928 -> 2185001979232
	2185086079088 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2185086079088 -> 2185001982928
	2185001982928 [label=AccumulateGrad]
	2185001980720 -> 2185001979232
	2185086079184 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2185086079184 -> 2185001980720
	2185001980720 [label=AccumulateGrad]
	2185001979520 -> 2185001983216
	2185001977936 -> 2185001983408
	2185086079568 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2185086079568 -> 2185001977936
	2185001977936 [label=AccumulateGrad]
	2185001977312 -> 2185001980768
	2185086079664 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2185086079664 -> 2185001977312
	2185001977312 [label=AccumulateGrad]
	2185001978944 -> 2185001980768
	2185086079760 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2185086079760 -> 2185001978944
	2185001978944 [label=AccumulateGrad]
	2185001977024 -> 2185001983744
	2185086080144 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2185086080144 -> 2185001977024
	2185001977024 [label=AccumulateGrad]
	2185001983936 -> 2185001983792
	2185086080240 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2185086080240 -> 2185001983936
	2185001983936 [label=AccumulateGrad]
	2185001983888 -> 2185001983792
	2185086080336 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2185086080336 -> 2185001983888
	2185001983888 [label=AccumulateGrad]
	2185001984080 -> 2185001984416
	2185086080720 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2185086080720 -> 2185001984080
	2185001984080 [label=AccumulateGrad]
	2185001984272 -> 2185001984368
	2185086080816 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2185086080816 -> 2185001984272
	2185001984272 [label=AccumulateGrad]
	2185001984176 -> 2185001984368
	2185086080912 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2185086080912 -> 2185001984176
	2185001984176 [label=AccumulateGrad]
	2185001984512 -> 2185001984560
	2185001984704 -> 2185001984848
	2185086081296 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2185086081296 -> 2185001984704
	2185001984704 [label=AccumulateGrad]
	2185001984992 -> 2185001985040
	2185086081392 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2185086081392 -> 2185001984992
	2185001984992 [label=AccumulateGrad]
	2185001985184 -> 2185001985040
	2185086081488 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2185086081488 -> 2185001985184
	2185001985184 [label=AccumulateGrad]
	2185001985232 -> 2185001985472
	2185086081872 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2185086081872 -> 2185001985232
	2185001985232 [label=AccumulateGrad]
	2185001985520 -> 2185001985760
	2185086081968 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2185086081968 -> 2185001985520
	2185001985520 [label=AccumulateGrad]
	2185001985856 -> 2185001985760
	2185086082064 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2185086082064 -> 2185001985856
	2185001985856 [label=AccumulateGrad]
	2185001985616 -> 2185001986000
	2185086082448 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2185086082448 -> 2185001985616
	2185001985616 [label=AccumulateGrad]
	2185001986240 -> 2185001986336
	2185086082544 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2185086082544 -> 2185001986240
	2185001986240 [label=AccumulateGrad]
	2185001986144 -> 2185001986336
	2185086082640 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2185086082640 -> 2185001986144
	2185001986144 [label=AccumulateGrad]
	2185001986192 -> 2185001986096
	2185001986432 -> 2185001986816
	2185086083024 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2185086083024 -> 2185001986432
	2185001986432 [label=AccumulateGrad]
	2185001986672 -> 2185001986576
	2185086083120 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2185086083120 -> 2185001986672
	2185001986672 [label=AccumulateGrad]
	2185001986912 -> 2185001986576
	2185086083216 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2185086083216 -> 2185001986912
	2185001986912 [label=AccumulateGrad]
	2185001987200 -> 2185001987152
	2185086083600 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2185086083600 -> 2185001987200
	2185001987200 [label=AccumulateGrad]
	2185001987056 -> 2185001987248
	2185086083696 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2185086083696 -> 2185001987056
	2185001987056 [label=AccumulateGrad]
	2185001987440 -> 2185001987248
	2185086083792 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2185086083792 -> 2185001987440
	2185001987440 [label=AccumulateGrad]
	2185001987584 -> 2185001987536
	2185086084176 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2185086084176 -> 2185001987584
	2185001987584 [label=AccumulateGrad]
	2185001987728 -> 2185001987920
	2185086084272 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2185086084272 -> 2185001987728
	2185001987728 [label=AccumulateGrad]
	2185001987872 -> 2185001987920
	2185086084368 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2185086084368 -> 2185001987872
	2185001987872 [label=AccumulateGrad]
	2185001988160 -> 2185001988064
	2185001988208 -> 2185001988640
	2185086085328 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2185086085328 -> 2185001988208
	2185001988208 [label=AccumulateGrad]
	2185001988544 -> 2185001988736
	2185086085424 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2185086085424 -> 2185001988544
	2185001988544 [label=AccumulateGrad]
	2185001988496 -> 2185001988736
	2185086085520 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2185086085520 -> 2185001988496
	2185001988496 [label=AccumulateGrad]
	2185001988832 -> 2185001989024
	2185086085904 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2185086085904 -> 2185001988832
	2185001988832 [label=AccumulateGrad]
	2185001989216 -> 2185001989072
	2185086086000 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2185086086000 -> 2185001989216
	2185001989216 [label=AccumulateGrad]
	2185001989168 -> 2185001989072
	2185086086096 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2185086086096 -> 2185001989168
	2185001989168 [label=AccumulateGrad]
	2185001989360 -> 2185001989696
	2185086086480 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2185086086480 -> 2185001989360
	2185001989360 [label=AccumulateGrad]
	2185001989552 -> 2185001989648
	2185086086576 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2185086086576 -> 2185001989552
	2185001989552 [label=AccumulateGrad]
	2185001989456 -> 2185001989648
	2185086086672 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2185086086672 -> 2185001989456
	2185001989456 [label=AccumulateGrad]
	2185001989792 -> 2185001989840
	2185001989792 [label=CudnnBatchNormBackward0]
	2185001988880 -> 2185001989792
	2185001988880 [label=ConvolutionBackward0]
	2185001988016 -> 2185001988880
	2185001988400 -> 2185001988880
	2185086084752 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2185086084752 -> 2185001988400
	2185001988400 [label=AccumulateGrad]
	2185001989600 -> 2185001989792
	2185086084848 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2185086084848 -> 2185001989600
	2185001989600 [label=AccumulateGrad]
	2185001989504 -> 2185001989792
	2185086084944 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2185086084944 -> 2185001989504
	2185001989504 [label=AccumulateGrad]
	2185001989984 -> 2185001990128
	2185086087056 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2185086087056 -> 2185001989984
	2185001989984 [label=AccumulateGrad]
	2185001990272 -> 2185001990320
	2185086087152 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2185086087152 -> 2185001990272
	2185001990272 [label=AccumulateGrad]
	2185001990464 -> 2185001990320
	2185086087248 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2185086087248 -> 2185001990464
	2185001990464 [label=AccumulateGrad]
	2185001990512 -> 2185001990752
	2185086087632 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2185086087632 -> 2185001990512
	2185001990512 [label=AccumulateGrad]
	2185001990800 -> 2185001991040
	2185086087728 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2185086087728 -> 2185001990800
	2185001990800 [label=AccumulateGrad]
	2185001991136 -> 2185001991040
	2185086087824 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2185086087824 -> 2185001991136
	2185001991136 [label=AccumulateGrad]
	2185001990896 -> 2185001991280
	2185086088208 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2185086088208 -> 2185001990896
	2185001990896 [label=AccumulateGrad]
	2185001991520 -> 2185001991616
	2185086088304 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2185086088304 -> 2185001991520
	2185001991520 [label=AccumulateGrad]
	2185001991424 -> 2185001991616
	2185086088400 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2185086088400 -> 2185001991424
	2185001991424 [label=AccumulateGrad]
	2185001991472 -> 2185001991376
	2185001991712 -> 2185001992096
	2185086088784 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2185086088784 -> 2185001991712
	2185001991712 [label=AccumulateGrad]
	2185001991952 -> 2185001991856
	2185086088880 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2185086088880 -> 2185001991952
	2185001991952 [label=AccumulateGrad]
	2185001976400 -> 2185001991856
	2185086088976 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2185086088976 -> 2185001976400
	2185001976400 [label=AccumulateGrad]
	2185001979136 -> 2185001982112
	2185086089360 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2185086089360 -> 2185001979136
	2185001979136 [label=AccumulateGrad]
	2185001984128 -> 2185001983984
	2185086089456 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2185086089456 -> 2185001984128
	2185001984128 [label=AccumulateGrad]
	2185001984464 -> 2185001983984
	2185086089552 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2185086089552 -> 2185001984464
	2185001984464 [label=AccumulateGrad]
	2185001984944 -> 2185001986048
	2185086089936 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2185086089936 -> 2185001984944
	2185001984944 [label=AccumulateGrad]
	2185001985904 -> 2185001986384
	2185086090032 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2185086090032 -> 2185001985904
	2185001985904 [label=AccumulateGrad]
	2185001986528 -> 2185001986384
	2185086090128 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2185086090128 -> 2185001986528
	2185001986528 [label=AccumulateGrad]
	2185001987008 -> 2185001986864
	2185001988304 -> 2185001989888
	2185001988304 [label=TBackward0]
	2185001987488 -> 2185001988304
	2185086090800 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2185086090800 -> 2185001987488
	2185001987488 [label=AccumulateGrad]
	2185001989888 -> 2185060875248
}
