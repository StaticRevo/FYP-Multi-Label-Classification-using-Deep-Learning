digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1506893043984 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1506868140672 [label=AddmmBackward0]
	1506868140816 -> 1506868140672
	1506870659984 [label="fc.bias
 (19)" fillcolor=lightblue]
	1506870659984 -> 1506868140816
	1506868140816 [label=AccumulateGrad]
	1506868140864 -> 1506868140672
	1506868140864 [label=ViewBackward0]
	1506868140960 -> 1506868140864
	1506868140960 [label=MeanBackward1]
	1506868141104 -> 1506868140960
	1506868141104 [label=ReluBackward0]
	1506868141200 -> 1506868141104
	1506868141200 [label=AddBackward0]
	1506868141296 -> 1506868141200
	1506868141296 [label=CudnnBatchNormBackward0]
	1506868141440 -> 1506868141296
	1506868141440 [label=ConvolutionBackward0]
	1506868141632 -> 1506868141440
	1506868141632 [label=ReluBackward0]
	1506868141776 -> 1506868141632
	1506868141776 [label=CudnnBatchNormBackward0]
	1506868141872 -> 1506868141776
	1506868141872 [label=ConvolutionBackward0]
	1506868142064 -> 1506868141872
	1506868142064 [label=ReluBackward0]
	1506868142208 -> 1506868142064
	1506868142208 [label=CudnnBatchNormBackward0]
	1506868142304 -> 1506868142208
	1506868142304 [label=ConvolutionBackward0]
	1506868141248 -> 1506868142304
	1506868141248 [label=ReluBackward0]
	1506868142592 -> 1506868141248
	1506868142592 [label=AddBackward0]
	1506868142688 -> 1506868142592
	1506868142688 [label=CudnnBatchNormBackward0]
	1506868142832 -> 1506868142688
	1506868142832 [label=ConvolutionBackward0]
	1506868143024 -> 1506868142832
	1506868143024 [label=ReluBackward0]
	1506868143168 -> 1506868143024
	1506868143168 [label=CudnnBatchNormBackward0]
	1506868134480 -> 1506868143168
	1506868134480 [label=ConvolutionBackward0]
	1506868134864 -> 1506868134480
	1506868134864 [label=ReluBackward0]
	1506868135008 -> 1506868134864
	1506868135008 [label=CudnnBatchNormBackward0]
	1506868135104 -> 1506868135008
	1506868135104 [label=ConvolutionBackward0]
	1506868142640 -> 1506868135104
	1506868142640 [label=ReluBackward0]
	1506868135392 -> 1506868142640
	1506868135392 [label=AddBackward0]
	1506868135488 -> 1506868135392
	1506868135488 [label=CudnnBatchNormBackward0]
	1506868135632 -> 1506868135488
	1506868135632 [label=ConvolutionBackward0]
	1506868135824 -> 1506868135632
	1506868135824 [label=ReluBackward0]
	1506868135968 -> 1506868135824
	1506868135968 [label=CudnnBatchNormBackward0]
	1506868136064 -> 1506868135968
	1506868136064 [label=ConvolutionBackward0]
	1506868136256 -> 1506868136064
	1506868136256 [label=ReluBackward0]
	1506868136400 -> 1506868136256
	1506868136400 [label=CudnnBatchNormBackward0]
	1506868136496 -> 1506868136400
	1506868136496 [label=ConvolutionBackward0]
	1506868136688 -> 1506868136496
	1506868136688 [label=ReluBackward0]
	1506868136832 -> 1506868136688
	1506868136832 [label=AddBackward0]
	1506868136928 -> 1506868136832
	1506868136928 [label=CudnnBatchNormBackward0]
	1506868137072 -> 1506868136928
	1506868137072 [label=ConvolutionBackward0]
	1506868137264 -> 1506868137072
	1506868137264 [label=ReluBackward0]
	1506868137408 -> 1506868137264
	1506868137408 [label=CudnnBatchNormBackward0]
	1506868137504 -> 1506868137408
	1506868137504 [label=ConvolutionBackward0]
	1506868137696 -> 1506868137504
	1506868137696 [label=ReluBackward0]
	1506868137840 -> 1506868137696
	1506868137840 [label=CudnnBatchNormBackward0]
	1506868137936 -> 1506868137840
	1506868137936 [label=ConvolutionBackward0]
	1506868136880 -> 1506868137936
	1506868136880 [label=ReluBackward0]
	1506868138224 -> 1506868136880
	1506868138224 [label=AddBackward0]
	1506868138320 -> 1506868138224
	1506868138320 [label=CudnnBatchNormBackward0]
	1506868138464 -> 1506868138320
	1506868138464 [label=ConvolutionBackward0]
	1506868138656 -> 1506868138464
	1506868138656 [label=ReluBackward0]
	1506868138800 -> 1506868138656
	1506868138800 [label=CudnnBatchNormBackward0]
	1506868138896 -> 1506868138800
	1506868138896 [label=ConvolutionBackward0]
	1506868139088 -> 1506868138896
	1506868139088 [label=ReluBackward0]
	1506868139232 -> 1506868139088
	1506868139232 [label=CudnnBatchNormBackward0]
	1506868139328 -> 1506868139232
	1506868139328 [label=ConvolutionBackward0]
	1506868138272 -> 1506868139328
	1506868138272 [label=ReluBackward0]
	1506868139616 -> 1506868138272
	1506868139616 [label=AddBackward0]
	1506868139712 -> 1506868139616
	1506868139712 [label=CudnnBatchNormBackward0]
	1506868139856 -> 1506868139712
	1506868139856 [label=ConvolutionBackward0]
	1506868140048 -> 1506868139856
	1506868140048 [label=ReluBackward0]
	1506868140192 -> 1506868140048
	1506868140192 [label=CudnnBatchNormBackward0]
	1506868134432 -> 1506868140192
	1506868134432 [label=ConvolutionBackward0]
	1506868143264 -> 1506868134432
	1506868143264 [label=ReluBackward0]
	1506868143408 -> 1506868143264
	1506868143408 [label=CudnnBatchNormBackward0]
	1506868143504 -> 1506868143408
	1506868143504 [label=ConvolutionBackward0]
	1506868139664 -> 1506868143504
	1506868139664 [label=ReluBackward0]
	1506868143792 -> 1506868139664
	1506868143792 [label=AddBackward0]
	1506868143888 -> 1506868143792
	1506868143888 [label=CudnnBatchNormBackward0]
	1506868144032 -> 1506868143888
	1506868144032 [label=ConvolutionBackward0]
	1506868144224 -> 1506868144032
	1506868144224 [label=ReluBackward0]
	1506868144368 -> 1506868144224
	1506868144368 [label=CudnnBatchNormBackward0]
	1506868144464 -> 1506868144368
	1506868144464 [label=ConvolutionBackward0]
	1506868144656 -> 1506868144464
	1506868144656 [label=ReluBackward0]
	1506868144800 -> 1506868144656
	1506868144800 [label=CudnnBatchNormBackward0]
	1506868144896 -> 1506868144800
	1506868144896 [label=ConvolutionBackward0]
	1506868143840 -> 1506868144896
	1506868143840 [label=ReluBackward0]
	1506868145184 -> 1506868143840
	1506868145184 [label=AddBackward0]
	1506868145280 -> 1506868145184
	1506868145280 [label=CudnnBatchNormBackward0]
	1506868145424 -> 1506868145280
	1506868145424 [label=ConvolutionBackward0]
	1506868145616 -> 1506868145424
	1506868145616 [label=ReluBackward0]
	1506868145760 -> 1506868145616
	1506868145760 [label=CudnnBatchNormBackward0]
	1506868145856 -> 1506868145760
	1506868145856 [label=ConvolutionBackward0]
	1506868146048 -> 1506868145856
	1506868146048 [label=ReluBackward0]
	1506868146192 -> 1506868146048
	1506868146192 [label=CudnnBatchNormBackward0]
	1506868146288 -> 1506868146192
	1506868146288 [label=ConvolutionBackward0]
	1506868145232 -> 1506868146288
	1506868145232 [label=ReluBackward0]
	1506868146576 -> 1506868145232
	1506868146576 [label=AddBackward0]
	1506868146672 -> 1506868146576
	1506868146672 [label=CudnnBatchNormBackward0]
	1506868146816 -> 1506868146672
	1506868146816 [label=ConvolutionBackward0]
	1506868147008 -> 1506868146816
	1506868147008 [label=ReluBackward0]
	1506868147152 -> 1506868147008
	1506868147152 [label=CudnnBatchNormBackward0]
	1506868147248 -> 1506868147152
	1506868147248 [label=ConvolutionBackward0]
	1506868147440 -> 1506868147248
	1506868147440 [label=ReluBackward0]
	1506868147584 -> 1506868147440
	1506868147584 [label=CudnnBatchNormBackward0]
	1506868147680 -> 1506868147584
	1506868147680 [label=ConvolutionBackward0]
	1506868147872 -> 1506868147680
	1506868147872 [label=ReluBackward0]
	1506868148016 -> 1506868147872
	1506868148016 [label=AddBackward0]
	1506868148112 -> 1506868148016
	1506868148112 [label=CudnnBatchNormBackward0]
	1506868148256 -> 1506868148112
	1506868148256 [label=ConvolutionBackward0]
	1506868148448 -> 1506868148256
	1506868148448 [label=ReluBackward0]
	1506868148592 -> 1506868148448
	1506868148592 [label=CudnnBatchNormBackward0]
	1506868148688 -> 1506868148592
	1506868148688 [label=ConvolutionBackward0]
	1506868148880 -> 1506868148688
	1506868148880 [label=ReluBackward0]
	1506868149024 -> 1506868148880
	1506868149024 [label=CudnnBatchNormBackward0]
	1506868149120 -> 1506868149024
	1506868149120 [label=ConvolutionBackward0]
	1506868148064 -> 1506868149120
	1506868148064 [label=ReluBackward0]
	1506868149408 -> 1506868148064
	1506868149408 [label=AddBackward0]
	1506868149504 -> 1506868149408
	1506868149504 [label=CudnnBatchNormBackward0]
	1506868149648 -> 1506868149504
	1506868149648 [label=ConvolutionBackward0]
	1506868149840 -> 1506868149648
	1506868149840 [label=ReluBackward0]
	1506868149984 -> 1506868149840
	1506868149984 [label=CudnnBatchNormBackward0]
	1506868150080 -> 1506868149984
	1506868150080 [label=ConvolutionBackward0]
	1506868134048 -> 1506868150080
	1506868134048 [label=ReluBackward0]
	1506881243840 -> 1506868134048
	1506881243840 [label=CudnnBatchNormBackward0]
	1506881243936 -> 1506881243840
	1506881243936 [label=ConvolutionBackward0]
	1506868149456 -> 1506881243936
	1506868149456 [label=ReluBackward0]
	1506881244224 -> 1506868149456
	1506881244224 [label=AddBackward0]
	1506881244320 -> 1506881244224
	1506881244320 [label=CudnnBatchNormBackward0]
	1506881244464 -> 1506881244320
	1506881244464 [label=ConvolutionBackward0]
	1506881244656 -> 1506881244464
	1506881244656 [label=ReluBackward0]
	1506881244800 -> 1506881244656
	1506881244800 [label=CudnnBatchNormBackward0]
	1506881244896 -> 1506881244800
	1506881244896 [label=ConvolutionBackward0]
	1506881245088 -> 1506881244896
	1506881245088 [label=ReluBackward0]
	1506881245232 -> 1506881245088
	1506881245232 [label=CudnnBatchNormBackward0]
	1506881245328 -> 1506881245232
	1506881245328 [label=ConvolutionBackward0]
	1506881244272 -> 1506881245328
	1506881244272 [label=ReluBackward0]
	1506881245616 -> 1506881244272
	1506881245616 [label=AddBackward0]
	1506881245712 -> 1506881245616
	1506881245712 [label=CudnnBatchNormBackward0]
	1506881245856 -> 1506881245712
	1506881245856 [label=ConvolutionBackward0]
	1506881246048 -> 1506881245856
	1506881246048 [label=ReluBackward0]
	1506881246192 -> 1506881246048
	1506881246192 [label=CudnnBatchNormBackward0]
	1506881246288 -> 1506881246192
	1506881246288 [label=ConvolutionBackward0]
	1506881246480 -> 1506881246288
	1506881246480 [label=ReluBackward0]
	1506881246624 -> 1506881246480
	1506881246624 [label=CudnnBatchNormBackward0]
	1506881246720 -> 1506881246624
	1506881246720 [label=ConvolutionBackward0]
	1506881246912 -> 1506881246720
	1506881246912 [label=ReluBackward0]
	1506881247056 -> 1506881246912
	1506881247056 [label=AddBackward0]
	1506881247152 -> 1506881247056
	1506881247152 [label=CudnnBatchNormBackward0]
	1506881247296 -> 1506881247152
	1506881247296 [label=ConvolutionBackward0]
	1506881247488 -> 1506881247296
	1506881247488 [label=ReluBackward0]
	1506881247632 -> 1506881247488
	1506881247632 [label=CudnnBatchNormBackward0]
	1506881247728 -> 1506881247632
	1506881247728 [label=ConvolutionBackward0]
	1506881247920 -> 1506881247728
	1506881247920 [label=ReluBackward0]
	1506881248064 -> 1506881247920
	1506881248064 [label=CudnnBatchNormBackward0]
	1506881248160 -> 1506881248064
	1506881248160 [label=ConvolutionBackward0]
	1506881247104 -> 1506881248160
	1506881247104 [label=ReluBackward0]
	1506881248448 -> 1506881247104
	1506881248448 [label=AddBackward0]
	1506881248544 -> 1506881248448
	1506881248544 [label=CudnnBatchNormBackward0]
	1506881248688 -> 1506881248544
	1506881248688 [label=ConvolutionBackward0]
	1506881248880 -> 1506881248688
	1506881248880 [label=ReluBackward0]
	1506881249024 -> 1506881248880
	1506881249024 [label=CudnnBatchNormBackward0]
	1506881249120 -> 1506881249024
	1506881249120 [label=ConvolutionBackward0]
	1506881249312 -> 1506881249120
	1506881249312 [label=ReluBackward0]
	1506881249456 -> 1506881249312
	1506881249456 [label=CudnnBatchNormBackward0]
	1506881249552 -> 1506881249456
	1506881249552 [label=ConvolutionBackward0]
	1506881248496 -> 1506881249552
	1506881248496 [label=ReluBackward0]
	1506881249840 -> 1506881248496
	1506881249840 [label=AddBackward0]
	1506881249936 -> 1506881249840
	1506881249936 [label=CudnnBatchNormBackward0]
	1506881250080 -> 1506881249936
	1506881250080 [label=ConvolutionBackward0]
	1506881250272 -> 1506881250080
	1506881250272 [label=ReluBackward0]
	1506881250416 -> 1506881250272
	1506881250416 [label=CudnnBatchNormBackward0]
	1506881250512 -> 1506881250416
	1506881250512 [label=ConvolutionBackward0]
	1506881250704 -> 1506881250512
	1506881250704 [label=ReluBackward0]
	1506881250848 -> 1506881250704
	1506881250848 [label=CudnnBatchNormBackward0]
	1506881250944 -> 1506881250848
	1506881250944 [label=ConvolutionBackward0]
	1506881251136 -> 1506881250944
	1506881251136 [label=MaxPool2DWithIndicesBackward0]
	1506881251280 -> 1506881251136
	1506881251280 [label=ReluBackward0]
	1506881251376 -> 1506881251280
	1506881251376 [label=CudnnBatchNormBackward0]
	1506881251472 -> 1506881251376
	1506881251472 [label=ConvolutionBackward0]
	1506881251664 -> 1506881251472
	1506870659888 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1506870659888 -> 1506881251664
	1506881251664 [label=AccumulateGrad]
	1506881251424 -> 1506881251376
	1506875606320 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1506875606320 -> 1506881251424
	1506881251424 [label=AccumulateGrad]
	1506881251184 -> 1506881251376
	1506875606416 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1506875606416 -> 1506881251184
	1506881251184 [label=AccumulateGrad]
	1506881251088 -> 1506881250944
	1506875607280 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1506875607280 -> 1506881251088
	1506881251088 [label=AccumulateGrad]
	1506881250896 -> 1506881250848
	1506875607376 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1506875607376 -> 1506881250896
	1506881250896 [label=AccumulateGrad]
	1506881250752 -> 1506881250848
	1506875607472 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1506875607472 -> 1506881250752
	1506881250752 [label=AccumulateGrad]
	1506881250656 -> 1506881250512
	1506875607952 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1506875607952 -> 1506881250656
	1506881250656 [label=AccumulateGrad]
	1506881250464 -> 1506881250416
	1506875608048 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1506875608048 -> 1506881250464
	1506881250464 [label=AccumulateGrad]
	1506881250320 -> 1506881250416
	1506875608144 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1506875608144 -> 1506881250320
	1506881250320 [label=AccumulateGrad]
	1506881250224 -> 1506881250080
	1506875608528 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1506875608528 -> 1506881250224
	1506881250224 [label=AccumulateGrad]
	1506881250032 -> 1506881249936
	1506875608624 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1506875608624 -> 1506881250032
	1506881250032 [label=AccumulateGrad]
	1506881249984 -> 1506881249936
	1506875608720 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1506875608720 -> 1506881249984
	1506881249984 [label=AccumulateGrad]
	1506881249888 -> 1506881249840
	1506881249888 [label=CudnnBatchNormBackward0]
	1506881250608 -> 1506881249888
	1506881250608 [label=ConvolutionBackward0]
	1506881251136 -> 1506881250608
	1506881250992 -> 1506881250608
	1506875606704 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1506875606704 -> 1506881250992
	1506881250992 [label=AccumulateGrad]
	1506881250176 -> 1506881249888
	1506875606800 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1506875606800 -> 1506881250176
	1506881250176 [label=AccumulateGrad]
	1506881250128 -> 1506881249888
	1506875606896 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1506875606896 -> 1506881250128
	1506881250128 [label=AccumulateGrad]
	1506881249744 -> 1506881249552
	1506875609104 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1506875609104 -> 1506881249744
	1506881249744 [label=AccumulateGrad]
	1506881249504 -> 1506881249456
	1506875609200 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1506875609200 -> 1506881249504
	1506881249504 [label=AccumulateGrad]
	1506881249360 -> 1506881249456
	1506875609296 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1506875609296 -> 1506881249360
	1506881249360 [label=AccumulateGrad]
	1506881249264 -> 1506881249120
	1506875609680 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1506875609680 -> 1506881249264
	1506881249264 [label=AccumulateGrad]
	1506881249072 -> 1506881249024
	1506875609776 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1506875609776 -> 1506881249072
	1506881249072 [label=AccumulateGrad]
	1506881248928 -> 1506881249024
	1506875609872 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1506875609872 -> 1506881248928
	1506881248928 [label=AccumulateGrad]
	1506881248832 -> 1506881248688
	1506875610256 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1506875610256 -> 1506881248832
	1506881248832 [label=AccumulateGrad]
	1506881248640 -> 1506881248544
	1506875610352 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1506875610352 -> 1506881248640
	1506881248640 [label=AccumulateGrad]
	1506881248592 -> 1506881248544
	1506875610448 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1506875610448 -> 1506881248592
	1506881248592 [label=AccumulateGrad]
	1506881248496 -> 1506881248448
	1506881248352 -> 1506881248160
	1506875610832 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1506875610832 -> 1506881248352
	1506881248352 [label=AccumulateGrad]
	1506881248112 -> 1506881248064
	1506875610928 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1506875610928 -> 1506881248112
	1506881248112 [label=AccumulateGrad]
	1506881247968 -> 1506881248064
	1506875611024 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1506875611024 -> 1506881247968
	1506881247968 [label=AccumulateGrad]
	1506881247872 -> 1506881247728
	1506875611408 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1506875611408 -> 1506881247872
	1506881247872 [label=AccumulateGrad]
	1506881247680 -> 1506881247632
	1506875611504 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1506875611504 -> 1506881247680
	1506881247680 [label=AccumulateGrad]
	1506881247536 -> 1506881247632
	1506875611600 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1506875611600 -> 1506881247536
	1506881247536 [label=AccumulateGrad]
	1506881247440 -> 1506881247296
	1506875611984 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1506875611984 -> 1506881247440
	1506881247440 [label=AccumulateGrad]
	1506881247248 -> 1506881247152
	1506875612080 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1506875612080 -> 1506881247248
	1506881247248 [label=AccumulateGrad]
	1506881247200 -> 1506881247152
	1506875612176 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1506875612176 -> 1506881247200
	1506881247200 [label=AccumulateGrad]
	1506881247104 -> 1506881247056
	1506881246864 -> 1506881246720
	1506875613136 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1506875613136 -> 1506881246864
	1506881246864 [label=AccumulateGrad]
	1506881246672 -> 1506881246624
	1506875613232 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1506875613232 -> 1506881246672
	1506881246672 [label=AccumulateGrad]
	1506881246528 -> 1506881246624
	1506875613328 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1506875613328 -> 1506881246528
	1506881246528 [label=AccumulateGrad]
	1506881246432 -> 1506881246288
	1506875613712 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1506875613712 -> 1506881246432
	1506881246432 [label=AccumulateGrad]
	1506881246240 -> 1506881246192
	1506875613808 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1506875613808 -> 1506881246240
	1506881246240 [label=AccumulateGrad]
	1506881246096 -> 1506881246192
	1506875613904 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1506875613904 -> 1506881246096
	1506881246096 [label=AccumulateGrad]
	1506881246000 -> 1506881245856
	1506875614288 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1506875614288 -> 1506881246000
	1506881246000 [label=AccumulateGrad]
	1506881245808 -> 1506881245712
	1506875614384 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1506875614384 -> 1506881245808
	1506881245808 [label=AccumulateGrad]
	1506881245760 -> 1506881245712
	1506875614480 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1506875614480 -> 1506881245760
	1506881245760 [label=AccumulateGrad]
	1506881245664 -> 1506881245616
	1506881245664 [label=CudnnBatchNormBackward0]
	1506881246384 -> 1506881245664
	1506881246384 [label=ConvolutionBackward0]
	1506881246912 -> 1506881246384
	1506881246768 -> 1506881246384
	1506875612560 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1506875612560 -> 1506881246768
	1506881246768 [label=AccumulateGrad]
	1506881245952 -> 1506881245664
	1506875612656 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1506875612656 -> 1506881245952
	1506881245952 [label=AccumulateGrad]
	1506881245904 -> 1506881245664
	1506875612752 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1506875612752 -> 1506881245904
	1506881245904 [label=AccumulateGrad]
	1506881245520 -> 1506881245328
	1506875614864 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1506875614864 -> 1506881245520
	1506881245520 [label=AccumulateGrad]
	1506881245280 -> 1506881245232
	1506875614960 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1506875614960 -> 1506881245280
	1506881245280 [label=AccumulateGrad]
	1506881245136 -> 1506881245232
	1506875615056 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1506875615056 -> 1506881245136
	1506881245136 [label=AccumulateGrad]
	1506881245040 -> 1506881244896
	1506875615440 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1506875615440 -> 1506881245040
	1506881245040 [label=AccumulateGrad]
	1506881244848 -> 1506881244800
	1506875615536 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1506875615536 -> 1506881244848
	1506881244848 [label=AccumulateGrad]
	1506881244704 -> 1506881244800
	1506875615632 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1506875615632 -> 1506881244704
	1506881244704 [label=AccumulateGrad]
	1506881244608 -> 1506881244464
	1506875616016 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1506875616016 -> 1506881244608
	1506881244608 [label=AccumulateGrad]
	1506881244416 -> 1506881244320
	1506875616112 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1506875616112 -> 1506881244416
	1506881244416 [label=AccumulateGrad]
	1506881244368 -> 1506881244320
	1506875616208 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1506875616208 -> 1506881244368
	1506881244368 [label=AccumulateGrad]
	1506881244272 -> 1506881244224
	1506881244128 -> 1506881243936
	1506875616592 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1506875616592 -> 1506881244128
	1506881244128 [label=AccumulateGrad]
	1506881243888 -> 1506881243840
	1506875616688 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1506875616688 -> 1506881243888
	1506881243888 [label=AccumulateGrad]
	1506881243744 -> 1506881243840
	1506875616784 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1506875616784 -> 1506881243744
	1506881243744 [label=AccumulateGrad]
	1506868150224 -> 1506868150080
	1506875617168 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1506875617168 -> 1506868150224
	1506868150224 [label=AccumulateGrad]
	1506868150032 -> 1506868149984
	1506875617264 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1506875617264 -> 1506868150032
	1506868150032 [label=AccumulateGrad]
	1506868149888 -> 1506868149984
	1506875617360 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1506875617360 -> 1506868149888
	1506868149888 [label=AccumulateGrad]
	1506868149792 -> 1506868149648
	1506875617744 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1506875617744 -> 1506868149792
	1506868149792 [label=AccumulateGrad]
	1506868149600 -> 1506868149504
	1506875617840 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1506875617840 -> 1506868149600
	1506868149600 [label=AccumulateGrad]
	1506868149552 -> 1506868149504
	1506875617936 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1506875617936 -> 1506868149552
	1506868149552 [label=AccumulateGrad]
	1506868149456 -> 1506868149408
	1506868149312 -> 1506868149120
	1506875618320 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1506875618320 -> 1506868149312
	1506868149312 [label=AccumulateGrad]
	1506868149072 -> 1506868149024
	1506875618416 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1506875618416 -> 1506868149072
	1506868149072 [label=AccumulateGrad]
	1506868148928 -> 1506868149024
	1506875618512 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1506875618512 -> 1506868148928
	1506868148928 [label=AccumulateGrad]
	1506868148832 -> 1506868148688
	1506875618896 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1506875618896 -> 1506868148832
	1506868148832 [label=AccumulateGrad]
	1506868148640 -> 1506868148592
	1506875618992 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1506875618992 -> 1506868148640
	1506868148640 [label=AccumulateGrad]
	1506868148496 -> 1506868148592
	1506875619088 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1506875619088 -> 1506868148496
	1506868148496 [label=AccumulateGrad]
	1506868148400 -> 1506868148256
	1506875619472 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1506875619472 -> 1506868148400
	1506868148400 [label=AccumulateGrad]
	1506868148208 -> 1506868148112
	1506875619568 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1506875619568 -> 1506868148208
	1506868148208 [label=AccumulateGrad]
	1506868148160 -> 1506868148112
	1506875619664 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1506875619664 -> 1506868148160
	1506868148160 [label=AccumulateGrad]
	1506868148064 -> 1506868148016
	1506868147824 -> 1506868147680
	1506875620624 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1506875620624 -> 1506868147824
	1506868147824 [label=AccumulateGrad]
	1506868147632 -> 1506868147584
	1506875620720 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1506875620720 -> 1506868147632
	1506868147632 [label=AccumulateGrad]
	1506868147488 -> 1506868147584
	1506875620816 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1506875620816 -> 1506868147488
	1506868147488 [label=AccumulateGrad]
	1506868147392 -> 1506868147248
	1506875621200 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1506875621200 -> 1506868147392
	1506868147392 [label=AccumulateGrad]
	1506868147200 -> 1506868147152
	1506875621296 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1506875621296 -> 1506868147200
	1506868147200 [label=AccumulateGrad]
	1506868147056 -> 1506868147152
	1506871410768 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1506871410768 -> 1506868147056
	1506868147056 [label=AccumulateGrad]
	1506868146960 -> 1506868146816
	1506871411152 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1506871411152 -> 1506868146960
	1506868146960 [label=AccumulateGrad]
	1506868146768 -> 1506868146672
	1506871411248 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1506871411248 -> 1506868146768
	1506868146768 [label=AccumulateGrad]
	1506868146720 -> 1506868146672
	1506871411344 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1506871411344 -> 1506868146720
	1506868146720 [label=AccumulateGrad]
	1506868146624 -> 1506868146576
	1506868146624 [label=CudnnBatchNormBackward0]
	1506868147344 -> 1506868146624
	1506868147344 [label=ConvolutionBackward0]
	1506868147872 -> 1506868147344
	1506868147728 -> 1506868147344
	1506875620048 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1506875620048 -> 1506868147728
	1506868147728 [label=AccumulateGrad]
	1506868146912 -> 1506868146624
	1506875620144 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1506875620144 -> 1506868146912
	1506868146912 [label=AccumulateGrad]
	1506868146864 -> 1506868146624
	1506875620240 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1506875620240 -> 1506868146864
	1506868146864 [label=AccumulateGrad]
	1506868146480 -> 1506868146288
	1506871411728 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1506871411728 -> 1506868146480
	1506868146480 [label=AccumulateGrad]
	1506868146240 -> 1506868146192
	1506868053488 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1506868053488 -> 1506868146240
	1506868146240 [label=AccumulateGrad]
	1506868146096 -> 1506868146192
	1506868054640 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1506868054640 -> 1506868146096
	1506868146096 [label=AccumulateGrad]
	1506868146000 -> 1506868145856
	1506868058768 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1506868058768 -> 1506868146000
	1506868146000 [label=AccumulateGrad]
	1506868145808 -> 1506868145760
	1506868058864 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1506868058864 -> 1506868145808
	1506868145808 [label=AccumulateGrad]
	1506868145664 -> 1506868145760
	1506868058960 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1506868058960 -> 1506868145664
	1506868145664 [label=AccumulateGrad]
	1506868145568 -> 1506868145424
	1506868059344 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1506868059344 -> 1506868145568
	1506868145568 [label=AccumulateGrad]
	1506868145376 -> 1506868145280
	1506868059440 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1506868059440 -> 1506868145376
	1506868145376 [label=AccumulateGrad]
	1506868145328 -> 1506868145280
	1506868059536 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1506868059536 -> 1506868145328
	1506868145328 [label=AccumulateGrad]
	1506868145232 -> 1506868145184
	1506868145088 -> 1506868144896
	1506868059920 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1506868059920 -> 1506868145088
	1506868145088 [label=AccumulateGrad]
	1506868144848 -> 1506868144800
	1506868060016 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1506868060016 -> 1506868144848
	1506868144848 [label=AccumulateGrad]
	1506868144704 -> 1506868144800
	1506868060112 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1506868060112 -> 1506868144704
	1506868144704 [label=AccumulateGrad]
	1506868144608 -> 1506868144464
	1506868060496 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1506868060496 -> 1506868144608
	1506868144608 [label=AccumulateGrad]
	1506868144416 -> 1506868144368
	1506868060592 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1506868060592 -> 1506868144416
	1506868144416 [label=AccumulateGrad]
	1506868144272 -> 1506868144368
	1506868060688 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1506868060688 -> 1506868144272
	1506868144272 [label=AccumulateGrad]
	1506868144176 -> 1506868144032
	1506868061072 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1506868061072 -> 1506868144176
	1506868144176 [label=AccumulateGrad]
	1506868143984 -> 1506868143888
	1506868061168 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1506868061168 -> 1506868143984
	1506868143984 [label=AccumulateGrad]
	1506868143936 -> 1506868143888
	1506868061264 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1506868061264 -> 1506868143936
	1506868143936 [label=AccumulateGrad]
	1506868143840 -> 1506868143792
	1506868143696 -> 1506868143504
	1506868061648 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1506868061648 -> 1506868143696
	1506868143696 [label=AccumulateGrad]
	1506868143456 -> 1506868143408
	1506868061744 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1506868061744 -> 1506868143456
	1506868143456 [label=AccumulateGrad]
	1506868143312 -> 1506868143408
	1506868061840 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1506868061840 -> 1506868143312
	1506868143312 [label=AccumulateGrad]
	1506868143216 -> 1506868134432
	1506868062224 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1506868062224 -> 1506868143216
	1506868143216 [label=AccumulateGrad]
	1506868140240 -> 1506868140192
	1506868062320 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1506868062320 -> 1506868140240
	1506868140240 [label=AccumulateGrad]
	1506868140096 -> 1506868140192
	1506868062416 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1506868062416 -> 1506868140096
	1506868140096 [label=AccumulateGrad]
	1506868140000 -> 1506868139856
	1506868062800 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1506868062800 -> 1506868140000
	1506868140000 [label=AccumulateGrad]
	1506868139808 -> 1506868139712
	1506868062896 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1506868062896 -> 1506868139808
	1506868139808 [label=AccumulateGrad]
	1506868139760 -> 1506868139712
	1506868062992 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1506868062992 -> 1506868139760
	1506868139760 [label=AccumulateGrad]
	1506868139664 -> 1506868139616
	1506868139520 -> 1506868139328
	1506868063376 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1506868063376 -> 1506868139520
	1506868139520 [label=AccumulateGrad]
	1506868139280 -> 1506868139232
	1506868063472 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1506868063472 -> 1506868139280
	1506868139280 [label=AccumulateGrad]
	1506868139136 -> 1506868139232
	1506868063568 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1506868063568 -> 1506868139136
	1506868139136 [label=AccumulateGrad]
	1506868139040 -> 1506868138896
	1506868063952 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1506868063952 -> 1506868139040
	1506868139040 [label=AccumulateGrad]
	1506868138848 -> 1506868138800
	1506868064048 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1506868064048 -> 1506868138848
	1506868138848 [label=AccumulateGrad]
	1506868138704 -> 1506868138800
	1506868064144 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1506868064144 -> 1506868138704
	1506868138704 [label=AccumulateGrad]
	1506868138608 -> 1506868138464
	1506868064528 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1506868064528 -> 1506868138608
	1506868138608 [label=AccumulateGrad]
	1506868138416 -> 1506868138320
	1506868064624 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1506868064624 -> 1506868138416
	1506868138416 [label=AccumulateGrad]
	1506868138368 -> 1506868138320
	1506868064720 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1506868064720 -> 1506868138368
	1506868138368 [label=AccumulateGrad]
	1506868138272 -> 1506868138224
	1506868138128 -> 1506868137936
	1506868065104 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1506868065104 -> 1506868138128
	1506868138128 [label=AccumulateGrad]
	1506868137888 -> 1506868137840
	1506868065200 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1506868065200 -> 1506868137888
	1506868137888 [label=AccumulateGrad]
	1506868137744 -> 1506868137840
	1506868065296 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1506868065296 -> 1506868137744
	1506868137744 [label=AccumulateGrad]
	1506868137648 -> 1506868137504
	1506868065680 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1506868065680 -> 1506868137648
	1506868137648 [label=AccumulateGrad]
	1506868137456 -> 1506868137408
	1506868065776 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1506868065776 -> 1506868137456
	1506868137456 [label=AccumulateGrad]
	1506868137312 -> 1506868137408
	1506868065872 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1506868065872 -> 1506868137312
	1506868137312 [label=AccumulateGrad]
	1506868137216 -> 1506868137072
	1506868066256 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1506868066256 -> 1506868137216
	1506868137216 [label=AccumulateGrad]
	1506868137024 -> 1506868136928
	1506868066352 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1506868066352 -> 1506868137024
	1506868137024 [label=AccumulateGrad]
	1506868136976 -> 1506868136928
	1506868066448 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1506868066448 -> 1506868136976
	1506868136976 [label=AccumulateGrad]
	1506868136880 -> 1506868136832
	1506868136640 -> 1506868136496
	1506868067504 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1506868067504 -> 1506868136640
	1506868136640 [label=AccumulateGrad]
	1506868136448 -> 1506868136400
	1506868067600 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1506868067600 -> 1506868136448
	1506868136448 [label=AccumulateGrad]
	1506868136304 -> 1506868136400
	1506868067696 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1506868067696 -> 1506868136304
	1506868136304 [label=AccumulateGrad]
	1506868136208 -> 1506868136064
	1506868068080 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1506868068080 -> 1506868136208
	1506868136208 [label=AccumulateGrad]
	1506868136016 -> 1506868135968
	1506868068176 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1506868068176 -> 1506868136016
	1506868136016 [label=AccumulateGrad]
	1506868135872 -> 1506868135968
	1506868068272 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1506868068272 -> 1506868135872
	1506868135872 [label=AccumulateGrad]
	1506868135776 -> 1506868135632
	1506881174128 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1506881174128 -> 1506868135776
	1506868135776 [label=AccumulateGrad]
	1506868135584 -> 1506868135488
	1506881174224 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1506881174224 -> 1506868135584
	1506868135584 [label=AccumulateGrad]
	1506868135536 -> 1506868135488
	1506881174320 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1506881174320 -> 1506868135536
	1506868135536 [label=AccumulateGrad]
	1506868135440 -> 1506868135392
	1506868135440 [label=CudnnBatchNormBackward0]
	1506868136160 -> 1506868135440
	1506868136160 [label=ConvolutionBackward0]
	1506868136688 -> 1506868136160
	1506868136544 -> 1506868136160
	1506868066928 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1506868066928 -> 1506868136544
	1506868136544 [label=AccumulateGrad]
	1506868135728 -> 1506868135440
	1506868067024 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1506868067024 -> 1506868135728
	1506868135728 [label=AccumulateGrad]
	1506868135680 -> 1506868135440
	1506868067120 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1506868067120 -> 1506868135680
	1506868135680 [label=AccumulateGrad]
	1506868135296 -> 1506868135104
	1506881174704 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1506881174704 -> 1506868135296
	1506868135296 [label=AccumulateGrad]
	1506868135056 -> 1506868135008
	1506881174800 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1506881174800 -> 1506868135056
	1506868135056 [label=AccumulateGrad]
	1506868134912 -> 1506868135008
	1506881174896 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1506881174896 -> 1506868134912
	1506868134912 [label=AccumulateGrad]
	1506868134960 -> 1506868134480
	1506881175280 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1506881175280 -> 1506868134960
	1506868134960 [label=AccumulateGrad]
	1506868134672 -> 1506868143168
	1506881175376 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1506881175376 -> 1506868134672
	1506868134672 [label=AccumulateGrad]
	1506868143072 -> 1506868143168
	1506881175472 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1506881175472 -> 1506868143072
	1506868143072 [label=AccumulateGrad]
	1506868142976 -> 1506868142832
	1506870657392 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1506870657392 -> 1506868142976
	1506868142976 [label=AccumulateGrad]
	1506868142784 -> 1506868142688
	1506870657488 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1506870657488 -> 1506868142784
	1506868142784 [label=AccumulateGrad]
	1506868142736 -> 1506868142688
	1506870657584 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1506870657584 -> 1506868142736
	1506868142736 [label=AccumulateGrad]
	1506868142640 -> 1506868142592
	1506868142496 -> 1506868142304
	1506870657968 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1506870657968 -> 1506868142496
	1506868142496 [label=AccumulateGrad]
	1506868142256 -> 1506868142208
	1506870658064 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1506870658064 -> 1506868142256
	1506868142256 [label=AccumulateGrad]
	1506868142112 -> 1506868142208
	1506870658160 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1506870658160 -> 1506868142112
	1506868142112 [label=AccumulateGrad]
	1506868142016 -> 1506868141872
	1506870658544 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1506870658544 -> 1506868142016
	1506868142016 [label=AccumulateGrad]
	1506868141824 -> 1506868141776
	1506870658640 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1506870658640 -> 1506868141824
	1506868141824 [label=AccumulateGrad]
	1506868141680 -> 1506868141776
	1506870658736 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1506870658736 -> 1506868141680
	1506868141680 [label=AccumulateGrad]
	1506868141584 -> 1506868141440
	1506870659120 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1506870659120 -> 1506868141584
	1506868141584 [label=AccumulateGrad]
	1506868141392 -> 1506868141296
	1506870659216 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1506870659216 -> 1506868141392
	1506868141392 [label=AccumulateGrad]
	1506868141344 -> 1506868141296
	1506870659312 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1506870659312 -> 1506868141344
	1506868141344 [label=AccumulateGrad]
	1506868141248 -> 1506868141200
	1506868140912 -> 1506868140672
	1506868140912 [label=TBackward0]
	1506868141152 -> 1506868140912
	1506870660176 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	1506870660176 -> 1506868141152
	1506868141152 [label=AccumulateGrad]
	1506868140672 -> 1506893043984
}
