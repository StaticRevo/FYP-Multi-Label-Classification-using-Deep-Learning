digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1607856563088 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1607852842464 [label=AddmmBackward0]
	1607852842608 -> 1607852842464
	1607854587824 [label="fc.bias
 (19)" fillcolor=lightblue]
	1607854587824 -> 1607852842608
	1607852842608 [label=AccumulateGrad]
	1607852842656 -> 1607852842464
	1607852842656 [label=ViewBackward0]
	1607852842752 -> 1607852842656
	1607852842752 [label=MeanBackward1]
	1607852842896 -> 1607852842752
	1607852842896 [label=ReluBackward0]
	1607852842992 -> 1607852842896
	1607852842992 [label=AddBackward0]
	1607852843088 -> 1607852842992
	1607852843088 [label=CudnnBatchNormBackward0]
	1607852843232 -> 1607852843088
	1607852843232 [label=ConvolutionBackward0]
	1607852843424 -> 1607852843232
	1607852843424 [label=ReluBackward0]
	1607852843568 -> 1607852843424
	1607852843568 [label=CudnnBatchNormBackward0]
	1607852843664 -> 1607852843568
	1607852843664 [label=ConvolutionBackward0]
	1607852843856 -> 1607852843664
	1607852843856 [label=ReluBackward0]
	1607852844000 -> 1607852843856
	1607852844000 [label=CudnnBatchNormBackward0]
	1607852844096 -> 1607852844000
	1607852844096 [label=ConvolutionBackward0]
	1607852843040 -> 1607852844096
	1607852843040 [label=ReluBackward0]
	1607852844384 -> 1607852843040
	1607852844384 [label=AddBackward0]
	1607852844480 -> 1607852844384
	1607852844480 [label=CudnnBatchNormBackward0]
	1607852844624 -> 1607852844480
	1607852844624 [label=ConvolutionBackward0]
	1607852844816 -> 1607852844624
	1607852844816 [label=ReluBackward0]
	1607852845104 -> 1607852844816
	1607852845104 [label=CudnnBatchNormBackward0]
	1607852841888 -> 1607852845104
	1607852841888 [label=ConvolutionBackward0]
	1607866250432 -> 1607852841888
	1607866250432 [label=ReluBackward0]
	1607866250576 -> 1607866250432
	1607866250576 [label=CudnnBatchNormBackward0]
	1607866250672 -> 1607866250576
	1607866250672 [label=ConvolutionBackward0]
	1607852844432 -> 1607866250672
	1607852844432 [label=ReluBackward0]
	1607866250960 -> 1607852844432
	1607866250960 [label=AddBackward0]
	1607866251056 -> 1607866250960
	1607866251056 [label=CudnnBatchNormBackward0]
	1607866251200 -> 1607866251056
	1607866251200 [label=ConvolutionBackward0]
	1607866251392 -> 1607866251200
	1607866251392 [label=ReluBackward0]
	1607866251536 -> 1607866251392
	1607866251536 [label=CudnnBatchNormBackward0]
	1607866251632 -> 1607866251536
	1607866251632 [label=ConvolutionBackward0]
	1607866251824 -> 1607866251632
	1607866251824 [label=ReluBackward0]
	1607866251968 -> 1607866251824
	1607866251968 [label=CudnnBatchNormBackward0]
	1607866252064 -> 1607866251968
	1607866252064 [label=ConvolutionBackward0]
	1607866252256 -> 1607866252064
	1607866252256 [label=ReluBackward0]
	1607866252400 -> 1607866252256
	1607866252400 [label=AddBackward0]
	1607866252496 -> 1607866252400
	1607866252496 [label=CudnnBatchNormBackward0]
	1607866252640 -> 1607866252496
	1607866252640 [label=ConvolutionBackward0]
	1607866252832 -> 1607866252640
	1607866252832 [label=ReluBackward0]
	1607866252976 -> 1607866252832
	1607866252976 [label=CudnnBatchNormBackward0]
	1607866253072 -> 1607866252976
	1607866253072 [label=ConvolutionBackward0]
	1607866253264 -> 1607866253072
	1607866253264 [label=ReluBackward0]
	1607867302048 -> 1607866253264
	1607867302048 [label=CudnnBatchNormBackward0]
	1607867302144 -> 1607867302048
	1607867302144 [label=ConvolutionBackward0]
	1607866252448 -> 1607867302144
	1607866252448 [label=ReluBackward0]
	1607867302432 -> 1607866252448
	1607867302432 [label=AddBackward0]
	1607867302528 -> 1607867302432
	1607867302528 [label=CudnnBatchNormBackward0]
	1607867302672 -> 1607867302528
	1607867302672 [label=ConvolutionBackward0]
	1607867302864 -> 1607867302672
	1607867302864 [label=ReluBackward0]
	1607867303008 -> 1607867302864
	1607867303008 [label=CudnnBatchNormBackward0]
	1607867303104 -> 1607867303008
	1607867303104 [label=ConvolutionBackward0]
	1607867303296 -> 1607867303104
	1607867303296 [label=ReluBackward0]
	1607867303440 -> 1607867303296
	1607867303440 [label=CudnnBatchNormBackward0]
	1607867303536 -> 1607867303440
	1607867303536 [label=ConvolutionBackward0]
	1607867302480 -> 1607867303536
	1607867302480 [label=ReluBackward0]
	1607867303824 -> 1607867302480
	1607867303824 [label=AddBackward0]
	1607867303920 -> 1607867303824
	1607867303920 [label=CudnnBatchNormBackward0]
	1607867304064 -> 1607867303920
	1607867304064 [label=ConvolutionBackward0]
	1607867304256 -> 1607867304064
	1607867304256 [label=ReluBackward0]
	1607867304400 -> 1607867304256
	1607867304400 [label=CudnnBatchNormBackward0]
	1607867304496 -> 1607867304400
	1607867304496 [label=ConvolutionBackward0]
	1607867304688 -> 1607867304496
	1607867304688 [label=ReluBackward0]
	1607867304832 -> 1607867304688
	1607867304832 [label=CudnnBatchNormBackward0]
	1607867304928 -> 1607867304832
	1607867304928 [label=ConvolutionBackward0]
	1607867303872 -> 1607867304928
	1607867303872 [label=ReluBackward0]
	1607867305216 -> 1607867303872
	1607867305216 [label=AddBackward0]
	1607867305312 -> 1607867305216
	1607867305312 [label=CudnnBatchNormBackward0]
	1607867305456 -> 1607867305312
	1607867305456 [label=ConvolutionBackward0]
	1607867305648 -> 1607867305456
	1607867305648 [label=ReluBackward0]
	1607867305792 -> 1607867305648
	1607867305792 [label=CudnnBatchNormBackward0]
	1607867305888 -> 1607867305792
	1607867305888 [label=ConvolutionBackward0]
	1607867306080 -> 1607867305888
	1607867306080 [label=ReluBackward0]
	1607867306224 -> 1607867306080
	1607867306224 [label=CudnnBatchNormBackward0]
	1607867306320 -> 1607867306224
	1607867306320 [label=ConvolutionBackward0]
	1607867305264 -> 1607867306320
	1607867305264 [label=ReluBackward0]
	1607867306608 -> 1607867305264
	1607867306608 [label=AddBackward0]
	1607867306704 -> 1607867306608
	1607867306704 [label=CudnnBatchNormBackward0]
	1607867306848 -> 1607867306704
	1607867306848 [label=ConvolutionBackward0]
	1607867307040 -> 1607867306848
	1607867307040 [label=ReluBackward0]
	1607867307184 -> 1607867307040
	1607867307184 [label=CudnnBatchNormBackward0]
	1607867307280 -> 1607867307184
	1607867307280 [label=ConvolutionBackward0]
	1607867307472 -> 1607867307280
	1607867307472 [label=ReluBackward0]
	1607867307616 -> 1607867307472
	1607867307616 [label=CudnnBatchNormBackward0]
	1607867307712 -> 1607867307616
	1607867307712 [label=ConvolutionBackward0]
	1607867306656 -> 1607867307712
	1607867306656 [label=ReluBackward0]
	1607867308000 -> 1607867306656
	1607867308000 [label=AddBackward0]
	1607867308096 -> 1607867308000
	1607867308096 [label=CudnnBatchNormBackward0]
	1607867308240 -> 1607867308096
	1607867308240 [label=ConvolutionBackward0]
	1607867308432 -> 1607867308240
	1607867308432 [label=ReluBackward0]
	1607867308576 -> 1607867308432
	1607867308576 [label=CudnnBatchNormBackward0]
	1607867308672 -> 1607867308576
	1607867308672 [label=ConvolutionBackward0]
	1607867308864 -> 1607867308672
	1607867308864 [label=ReluBackward0]
	1607867309008 -> 1607867308864
	1607867309008 [label=CudnnBatchNormBackward0]
	1607867309104 -> 1607867309008
	1607867309104 [label=ConvolutionBackward0]
	1607867309296 -> 1607867309104
	1607867309296 [label=ReluBackward0]
	1607867309440 -> 1607867309296
	1607867309440 [label=AddBackward0]
	1607867309536 -> 1607867309440
	1607867309536 [label=CudnnBatchNormBackward0]
	1607867309680 -> 1607867309536
	1607867309680 [label=ConvolutionBackward0]
	1607867309872 -> 1607867309680
	1607867309872 [label=ReluBackward0]
	1607867310016 -> 1607867309872
	1607867310016 [label=CudnnBatchNormBackward0]
	1607867310112 -> 1607867310016
	1607867310112 [label=ConvolutionBackward0]
	1607867310304 -> 1607867310112
	1607867310304 [label=ReluBackward0]
	1607867310448 -> 1607867310304
	1607867310448 [label=CudnnBatchNormBackward0]
	1607867310544 -> 1607867310448
	1607867310544 [label=ConvolutionBackward0]
	1607867309488 -> 1607867310544
	1607867309488 [label=ReluBackward0]
	1607867310832 -> 1607867309488
	1607867310832 [label=AddBackward0]
	1607867310928 -> 1607867310832
	1607867310928 [label=CudnnBatchNormBackward0]
	1607867311072 -> 1607867310928
	1607867311072 [label=ConvolutionBackward0]
	1607867311264 -> 1607867311072
	1607867311264 [label=ReluBackward0]
	1607867311408 -> 1607867311264
	1607867311408 [label=CudnnBatchNormBackward0]
	1607867311504 -> 1607867311408
	1607867311504 [label=ConvolutionBackward0]
	1607867311696 -> 1607867311504
	1607867311696 [label=ReluBackward0]
	1607867311840 -> 1607867311696
	1607867311840 [label=CudnnBatchNormBackward0]
	1607867311936 -> 1607867311840
	1607867311936 [label=ConvolutionBackward0]
	1607867310880 -> 1607867311936
	1607867310880 [label=ReluBackward0]
	1607867312224 -> 1607867310880
	1607867312224 [label=AddBackward0]
	1607867312320 -> 1607867312224
	1607867312320 [label=CudnnBatchNormBackward0]
	1607867312464 -> 1607867312320
	1607867312464 [label=ConvolutionBackward0]
	1607867312656 -> 1607867312464
	1607867312656 [label=ReluBackward0]
	1607867312800 -> 1607867312656
	1607867312800 [label=CudnnBatchNormBackward0]
	1607867312896 -> 1607867312800
	1607867312896 [label=ConvolutionBackward0]
	1607867313088 -> 1607867312896
	1607867313088 [label=ReluBackward0]
	1607867313232 -> 1607867313088
	1607867313232 [label=CudnnBatchNormBackward0]
	1607867313328 -> 1607867313232
	1607867313328 [label=ConvolutionBackward0]
	1607867312272 -> 1607867313328
	1607867312272 [label=ReluBackward0]
	1607867313616 -> 1607867312272
	1607867313616 [label=AddBackward0]
	1607867313712 -> 1607867313616
	1607867313712 [label=CudnnBatchNormBackward0]
	1607867313856 -> 1607867313712
	1607867313856 [label=ConvolutionBackward0]
	1607867314048 -> 1607867313856
	1607867314048 [label=ReluBackward0]
	1607867314192 -> 1607867314048
	1607867314192 [label=CudnnBatchNormBackward0]
	1607867314288 -> 1607867314192
	1607867314288 [label=ConvolutionBackward0]
	1607867314480 -> 1607867314288
	1607867314480 [label=ReluBackward0]
	1607867314624 -> 1607867314480
	1607867314624 [label=CudnnBatchNormBackward0]
	1607867314720 -> 1607867314624
	1607867314720 [label=ConvolutionBackward0]
	1607867314912 -> 1607867314720
	1607867314912 [label=ReluBackward0]
	1607867315056 -> 1607867314912
	1607867315056 [label=AddBackward0]
	1607867315152 -> 1607867315056
	1607867315152 [label=CudnnBatchNormBackward0]
	1607867315248 -> 1607867315152
	1607867315248 [label=ConvolutionBackward0]
	1607856161120 -> 1607867315248
	1607856161120 [label=ReluBackward0]
	1607856161264 -> 1607856161120
	1607856161264 [label=CudnnBatchNormBackward0]
	1607856161360 -> 1607856161264
	1607856161360 [label=ConvolutionBackward0]
	1607856161552 -> 1607856161360
	1607856161552 [label=ReluBackward0]
	1607856161696 -> 1607856161552
	1607856161696 [label=CudnnBatchNormBackward0]
	1607856161792 -> 1607856161696
	1607856161792 [label=ConvolutionBackward0]
	1607867315104 -> 1607856161792
	1607867315104 [label=ReluBackward0]
	1607856162080 -> 1607867315104
	1607856162080 [label=AddBackward0]
	1607856162176 -> 1607856162080
	1607856162176 [label=CudnnBatchNormBackward0]
	1607861203104 -> 1607856162176
	1607861203104 [label=ConvolutionBackward0]
	1607856162416 -> 1607861203104
	1607856162416 [label=ReluBackward0]
	1607856162560 -> 1607856162416
	1607856162560 [label=CudnnBatchNormBackward0]
	1607856162656 -> 1607856162560
	1607856162656 [label=ConvolutionBackward0]
	1607856162848 -> 1607856162656
	1607856162848 [label=ReluBackward0]
	1607856162992 -> 1607856162848
	1607856162992 [label=CudnnBatchNormBackward0]
	1607856163088 -> 1607856162992
	1607856163088 [label=ConvolutionBackward0]
	1607856162128 -> 1607856163088
	1607856162128 [label=ReluBackward0]
	1607856163376 -> 1607856162128
	1607856163376 [label=AddBackward0]
	1607856163472 -> 1607856163376
	1607856163472 [label=CudnnBatchNormBackward0]
	1607856163616 -> 1607856163472
	1607856163616 [label=ConvolutionBackward0]
	1607856163808 -> 1607856163616
	1607856163808 [label=ReluBackward0]
	1607856163952 -> 1607856163808
	1607856163952 [label=CudnnBatchNormBackward0]
	1607856164048 -> 1607856163952
	1607856164048 [label=ConvolutionBackward0]
	1607856164240 -> 1607856164048
	1607856164240 [label=ReluBackward0]
	1607856164384 -> 1607856164240
	1607856164384 [label=CudnnBatchNormBackward0]
	1607856164480 -> 1607856164384
	1607856164480 [label=ConvolutionBackward0]
	1607856164672 -> 1607856164480
	1607856164672 [label=MaxPool2DWithIndicesBackward0]
	1607856164816 -> 1607856164672
	1607856164816 [label=ReluBackward0]
	1607856164912 -> 1607856164816
	1607856164912 [label=CudnnBatchNormBackward0]
	1607856165008 -> 1607856164912
	1607856165008 [label=ConvolutionBackward0]
	1607856165200 -> 1607856165008
	1607867536144 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1607867536144 -> 1607856165200
	1607856165200 [label=AccumulateGrad]
	1607856164960 -> 1607856164912
	1607861366576 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1607861366576 -> 1607856164960
	1607856164960 [label=AccumulateGrad]
	1607856164720 -> 1607856164912
	1607861366672 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1607861366672 -> 1607856164720
	1607856164720 [label=AccumulateGrad]
	1607856164624 -> 1607856164480
	1607861367632 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1607861367632 -> 1607856164624
	1607856164624 [label=AccumulateGrad]
	1607856164432 -> 1607856164384
	1607861367728 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1607861367728 -> 1607856164432
	1607856164432 [label=AccumulateGrad]
	1607856164288 -> 1607856164384
	1607861367824 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1607861367824 -> 1607856164288
	1607856164288 [label=AccumulateGrad]
	1607856164192 -> 1607856164048
	1607861368304 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1607861368304 -> 1607856164192
	1607856164192 [label=AccumulateGrad]
	1607856164000 -> 1607856163952
	1607861368400 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1607861368400 -> 1607856164000
	1607856164000 [label=AccumulateGrad]
	1607856163856 -> 1607856163952
	1607861368496 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1607861368496 -> 1607856163856
	1607856163856 [label=AccumulateGrad]
	1607856163760 -> 1607856163616
	1607861368880 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1607861368880 -> 1607856163760
	1607856163760 [label=AccumulateGrad]
	1607856163568 -> 1607856163472
	1607861368976 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1607861368976 -> 1607856163568
	1607856163568 [label=AccumulateGrad]
	1607856163520 -> 1607856163472
	1607861369072 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1607861369072 -> 1607856163520
	1607856163520 [label=AccumulateGrad]
	1607856163424 -> 1607856163376
	1607856163424 [label=CudnnBatchNormBackward0]
	1607856164144 -> 1607856163424
	1607856164144 [label=ConvolutionBackward0]
	1607856164672 -> 1607856164144
	1607856164528 -> 1607856164144
	1607861367056 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1607861367056 -> 1607856164528
	1607856164528 [label=AccumulateGrad]
	1607856163712 -> 1607856163424
	1607861367152 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1607861367152 -> 1607856163712
	1607856163712 [label=AccumulateGrad]
	1607856163664 -> 1607856163424
	1607861367248 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1607861367248 -> 1607856163664
	1607856163664 [label=AccumulateGrad]
	1607856163280 -> 1607856163088
	1607861369456 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1607861369456 -> 1607856163280
	1607856163280 [label=AccumulateGrad]
	1607856163040 -> 1607856162992
	1607861369552 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1607861369552 -> 1607856163040
	1607856163040 [label=AccumulateGrad]
	1607856162896 -> 1607856162992
	1607861369648 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1607861369648 -> 1607856162896
	1607856162896 [label=AccumulateGrad]
	1607856162800 -> 1607856162656
	1607861370032 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1607861370032 -> 1607856162800
	1607856162800 [label=AccumulateGrad]
	1607856162608 -> 1607856162560
	1607861370128 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1607861370128 -> 1607856162608
	1607856162608 [label=AccumulateGrad]
	1607856162464 -> 1607856162560
	1607861370224 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1607861370224 -> 1607856162464
	1607856162464 [label=AccumulateGrad]
	1607856162368 -> 1607861203104
	1607861370608 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1607861370608 -> 1607856162368
	1607856162368 [label=AccumulateGrad]
	1607861202912 -> 1607856162176
	1607861370704 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1607861370704 -> 1607861202912
	1607861202912 [label=AccumulateGrad]
	1607856162320 -> 1607856162176
	1607861370800 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1607861370800 -> 1607856162320
	1607856162320 [label=AccumulateGrad]
	1607856162128 -> 1607856162080
	1607856161984 -> 1607856161792
	1607544252624 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1607544252624 -> 1607856161984
	1607856161984 [label=AccumulateGrad]
	1607856161744 -> 1607856161696
	1607529337808 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1607529337808 -> 1607856161744
	1607856161744 [label=AccumulateGrad]
	1607856161600 -> 1607856161696
	1607529337712 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1607529337712 -> 1607856161600
	1607856161600 [label=AccumulateGrad]
	1607856161504 -> 1607856161360
	1607520075120 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1607520075120 -> 1607856161504
	1607856161504 [label=AccumulateGrad]
	1607856161312 -> 1607856161264
	1607517035152 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1607517035152 -> 1607856161312
	1607856161312 [label=AccumulateGrad]
	1607856161168 -> 1607856161264
	1607517034960 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1607517034960 -> 1607856161168
	1607856161168 [label=AccumulateGrad]
	1607856161072 -> 1607867315248
	1607852966064 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1607852966064 -> 1607856161072
	1607856161072 [label=AccumulateGrad]
	1607867315200 -> 1607867315152
	1607852966160 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1607852966160 -> 1607867315200
	1607867315200 [label=AccumulateGrad]
	1607856160832 -> 1607867315152
	1607852966256 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1607852966256 -> 1607856160832
	1607856160832 [label=AccumulateGrad]
	1607867315104 -> 1607867315056
	1607867314864 -> 1607867314720
	1607852967216 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1607852967216 -> 1607867314864
	1607867314864 [label=AccumulateGrad]
	1607867314672 -> 1607867314624
	1607852967312 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1607852967312 -> 1607867314672
	1607867314672 [label=AccumulateGrad]
	1607867314528 -> 1607867314624
	1607852967408 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1607852967408 -> 1607867314528
	1607867314528 [label=AccumulateGrad]
	1607867314432 -> 1607867314288
	1607852967792 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607852967792 -> 1607867314432
	1607867314432 [label=AccumulateGrad]
	1607867314240 -> 1607867314192
	1607852967888 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1607852967888 -> 1607867314240
	1607867314240 [label=AccumulateGrad]
	1607867314096 -> 1607867314192
	1607852967984 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1607852967984 -> 1607867314096
	1607867314096 [label=AccumulateGrad]
	1607867314000 -> 1607867313856
	1607852968368 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607852968368 -> 1607867314000
	1607867314000 [label=AccumulateGrad]
	1607867313808 -> 1607867313712
	1607852968464 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1607852968464 -> 1607867313808
	1607867313808 [label=AccumulateGrad]
	1607867313760 -> 1607867313712
	1607852968560 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1607852968560 -> 1607867313760
	1607867313760 [label=AccumulateGrad]
	1607867313664 -> 1607867313616
	1607867313664 [label=CudnnBatchNormBackward0]
	1607867314384 -> 1607867313664
	1607867314384 [label=ConvolutionBackward0]
	1607867314912 -> 1607867314384
	1607867314768 -> 1607867314384
	1607852966640 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1607852966640 -> 1607867314768
	1607867314768 [label=AccumulateGrad]
	1607867313952 -> 1607867313664
	1607852966736 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1607852966736 -> 1607867313952
	1607867313952 [label=AccumulateGrad]
	1607867313904 -> 1607867313664
	1607852966832 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1607852966832 -> 1607867313904
	1607867313904 [label=AccumulateGrad]
	1607867313520 -> 1607867313328
	1607852968944 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607852968944 -> 1607867313520
	1607867313520 [label=AccumulateGrad]
	1607867313280 -> 1607867313232
	1607852969040 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1607852969040 -> 1607867313280
	1607867313280 [label=AccumulateGrad]
	1607867313136 -> 1607867313232
	1607852969136 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1607852969136 -> 1607867313136
	1607867313136 [label=AccumulateGrad]
	1607867313040 -> 1607867312896
	1607852969520 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607852969520 -> 1607867313040
	1607867313040 [label=AccumulateGrad]
	1607867312848 -> 1607867312800
	1607852969616 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1607852969616 -> 1607867312848
	1607867312848 [label=AccumulateGrad]
	1607867312704 -> 1607867312800
	1607852969712 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1607852969712 -> 1607867312704
	1607867312704 [label=AccumulateGrad]
	1607867312608 -> 1607867312464
	1607852970096 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607852970096 -> 1607867312608
	1607867312608 [label=AccumulateGrad]
	1607867312416 -> 1607867312320
	1607852970192 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1607852970192 -> 1607867312416
	1607867312416 [label=AccumulateGrad]
	1607867312368 -> 1607867312320
	1607852970288 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1607852970288 -> 1607867312368
	1607867312368 [label=AccumulateGrad]
	1607867312272 -> 1607867312224
	1607867312128 -> 1607867311936
	1607852970672 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607852970672 -> 1607867312128
	1607867312128 [label=AccumulateGrad]
	1607867311888 -> 1607867311840
	1607852970768 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1607852970768 -> 1607867311888
	1607867311888 [label=AccumulateGrad]
	1607867311744 -> 1607867311840
	1607852970864 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1607852970864 -> 1607867311744
	1607867311744 [label=AccumulateGrad]
	1607867311648 -> 1607867311504
	1607852971248 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607852971248 -> 1607867311648
	1607867311648 [label=AccumulateGrad]
	1607867311456 -> 1607867311408
	1607852971344 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1607852971344 -> 1607867311456
	1607867311456 [label=AccumulateGrad]
	1607867311312 -> 1607867311408
	1607852971440 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1607852971440 -> 1607867311312
	1607867311312 [label=AccumulateGrad]
	1607867311216 -> 1607867311072
	1607852971824 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607852971824 -> 1607867311216
	1607867311216 [label=AccumulateGrad]
	1607867311024 -> 1607867310928
	1607852971920 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1607852971920 -> 1607867311024
	1607867311024 [label=AccumulateGrad]
	1607867310976 -> 1607867310928
	1607852972016 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1607852972016 -> 1607867310976
	1607867310976 [label=AccumulateGrad]
	1607867310880 -> 1607867310832
	1607867310736 -> 1607867310544
	1607852972400 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607852972400 -> 1607867310736
	1607867310736 [label=AccumulateGrad]
	1607867310496 -> 1607867310448
	1607852972496 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1607852972496 -> 1607867310496
	1607867310496 [label=AccumulateGrad]
	1607867310352 -> 1607867310448
	1607852972592 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1607852972592 -> 1607867310352
	1607867310352 [label=AccumulateGrad]
	1607867310256 -> 1607867310112
	1607852972976 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607852972976 -> 1607867310256
	1607867310256 [label=AccumulateGrad]
	1607867310064 -> 1607867310016
	1607852973072 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1607852973072 -> 1607867310064
	1607867310064 [label=AccumulateGrad]
	1607867309920 -> 1607867310016
	1607852973168 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1607852973168 -> 1607867309920
	1607867309920 [label=AccumulateGrad]
	1607867309824 -> 1607867309680
	1607852973552 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607852973552 -> 1607867309824
	1607867309824 [label=AccumulateGrad]
	1607867309632 -> 1607867309536
	1607852973648 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1607852973648 -> 1607867309632
	1607867309632 [label=AccumulateGrad]
	1607867309584 -> 1607867309536
	1607852973744 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1607852973744 -> 1607867309584
	1607867309584 [label=AccumulateGrad]
	1607867309488 -> 1607867309440
	1607867309248 -> 1607867309104
	1607852974704 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1607852974704 -> 1607867309248
	1607867309248 [label=AccumulateGrad]
	1607867309056 -> 1607867309008
	1607852974800 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1607852974800 -> 1607867309056
	1607867309056 [label=AccumulateGrad]
	1607867308912 -> 1607867309008
	1607852974896 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1607852974896 -> 1607867308912
	1607867308912 [label=AccumulateGrad]
	1607867308816 -> 1607867308672
	1607852975280 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607852975280 -> 1607867308816
	1607867308816 [label=AccumulateGrad]
	1607867308624 -> 1607867308576
	1607852975376 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1607852975376 -> 1607867308624
	1607867308624 [label=AccumulateGrad]
	1607867308480 -> 1607867308576
	1607852975472 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1607852975472 -> 1607867308480
	1607867308480 [label=AccumulateGrad]
	1607867308384 -> 1607867308240
	1607852975856 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607852975856 -> 1607867308384
	1607867308384 [label=AccumulateGrad]
	1607867308192 -> 1607867308096
	1607852975952 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1607852975952 -> 1607867308192
	1607867308192 [label=AccumulateGrad]
	1607867308144 -> 1607867308096
	1607852976048 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1607852976048 -> 1607867308144
	1607867308144 [label=AccumulateGrad]
	1607867308048 -> 1607867308000
	1607867308048 [label=CudnnBatchNormBackward0]
	1607867308768 -> 1607867308048
	1607867308768 [label=ConvolutionBackward0]
	1607867309296 -> 1607867308768
	1607867309152 -> 1607867308768
	1607852974128 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1607852974128 -> 1607867309152
	1607867309152 [label=AccumulateGrad]
	1607867308336 -> 1607867308048
	1607852974224 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1607852974224 -> 1607867308336
	1607867308336 [label=AccumulateGrad]
	1607867308288 -> 1607867308048
	1607852974320 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1607852974320 -> 1607867308288
	1607867308288 [label=AccumulateGrad]
	1607867307904 -> 1607867307712
	1607849503312 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607849503312 -> 1607867307904
	1607867307904 [label=AccumulateGrad]
	1607867307664 -> 1607867307616
	1607849506096 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1607849506096 -> 1607867307664
	1607867307664 [label=AccumulateGrad]
	1607867307520 -> 1607867307616
	1607849506192 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1607849506192 -> 1607867307520
	1607867307520 [label=AccumulateGrad]
	1607867307424 -> 1607867307280
	1607849506576 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607849506576 -> 1607867307424
	1607867307424 [label=AccumulateGrad]
	1607867307232 -> 1607867307184
	1607849506672 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1607849506672 -> 1607867307232
	1607867307232 [label=AccumulateGrad]
	1607867307088 -> 1607867307184
	1607849506768 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1607849506768 -> 1607867307088
	1607867307088 [label=AccumulateGrad]
	1607867306992 -> 1607867306848
	1607849507152 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607849507152 -> 1607867306992
	1607867306992 [label=AccumulateGrad]
	1607867306800 -> 1607867306704
	1607849507248 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1607849507248 -> 1607867306800
	1607867306800 [label=AccumulateGrad]
	1607867306752 -> 1607867306704
	1607849507344 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1607849507344 -> 1607867306752
	1607867306752 [label=AccumulateGrad]
	1607867306656 -> 1607867306608
	1607867306512 -> 1607867306320
	1607849507728 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607849507728 -> 1607867306512
	1607867306512 [label=AccumulateGrad]
	1607867306272 -> 1607867306224
	1607849507824 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1607849507824 -> 1607867306272
	1607867306272 [label=AccumulateGrad]
	1607867306128 -> 1607867306224
	1607849507920 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1607849507920 -> 1607867306128
	1607867306128 [label=AccumulateGrad]
	1607867306032 -> 1607867305888
	1607849508304 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607849508304 -> 1607867306032
	1607867306032 [label=AccumulateGrad]
	1607867305840 -> 1607867305792
	1607849508400 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1607849508400 -> 1607867305840
	1607867305840 [label=AccumulateGrad]
	1607867305696 -> 1607867305792
	1607849508496 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1607849508496 -> 1607867305696
	1607867305696 [label=AccumulateGrad]
	1607867305600 -> 1607867305456
	1607849504368 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607849504368 -> 1607867305600
	1607867305600 [label=AccumulateGrad]
	1607867305408 -> 1607867305312
	1607849504080 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1607849504080 -> 1607867305408
	1607867305408 [label=AccumulateGrad]
	1607867305360 -> 1607867305312
	1607849503408 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1607849503408 -> 1607867305360
	1607867305360 [label=AccumulateGrad]
	1607867305264 -> 1607867305216
	1607867305120 -> 1607867304928
	1607866492368 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607866492368 -> 1607867305120
	1607867305120 [label=AccumulateGrad]
	1607867304880 -> 1607867304832
	1607866492464 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1607866492464 -> 1607867304880
	1607867304880 [label=AccumulateGrad]
	1607867304736 -> 1607867304832
	1607866492560 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1607866492560 -> 1607867304736
	1607867304736 [label=AccumulateGrad]
	1607867304640 -> 1607867304496
	1607866492944 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607866492944 -> 1607867304640
	1607867304640 [label=AccumulateGrad]
	1607867304448 -> 1607867304400
	1607866493040 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1607866493040 -> 1607867304448
	1607867304448 [label=AccumulateGrad]
	1607867304304 -> 1607867304400
	1607866493136 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1607866493136 -> 1607867304304
	1607867304304 [label=AccumulateGrad]
	1607867304208 -> 1607867304064
	1607866493520 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607866493520 -> 1607867304208
	1607867304208 [label=AccumulateGrad]
	1607867304016 -> 1607867303920
	1607866493616 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1607866493616 -> 1607867304016
	1607867304016 [label=AccumulateGrad]
	1607867303968 -> 1607867303920
	1607866493712 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1607866493712 -> 1607867303968
	1607867303968 [label=AccumulateGrad]
	1607867303872 -> 1607867303824
	1607867303728 -> 1607867303536
	1607866494096 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607866494096 -> 1607867303728
	1607867303728 [label=AccumulateGrad]
	1607867303488 -> 1607867303440
	1607866494192 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1607866494192 -> 1607867303488
	1607867303488 [label=AccumulateGrad]
	1607867303344 -> 1607867303440
	1607866494288 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1607866494288 -> 1607867303344
	1607867303344 [label=AccumulateGrad]
	1607867303248 -> 1607867303104
	1607866494672 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607866494672 -> 1607867303248
	1607867303248 [label=AccumulateGrad]
	1607867303056 -> 1607867303008
	1607866494768 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1607866494768 -> 1607867303056
	1607867303056 [label=AccumulateGrad]
	1607867302912 -> 1607867303008
	1607866494864 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1607866494864 -> 1607867302912
	1607867302912 [label=AccumulateGrad]
	1607867302816 -> 1607867302672
	1607866495248 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607866495248 -> 1607867302816
	1607867302816 [label=AccumulateGrad]
	1607867302624 -> 1607867302528
	1607866495344 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1607866495344 -> 1607867302624
	1607867302624 [label=AccumulateGrad]
	1607867302576 -> 1607867302528
	1607866495440 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1607866495440 -> 1607867302576
	1607867302576 [label=AccumulateGrad]
	1607867302480 -> 1607867302432
	1607867302336 -> 1607867302144
	1607866495824 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607866495824 -> 1607867302336
	1607867302336 [label=AccumulateGrad]
	1607867302096 -> 1607867302048
	1607866495920 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1607866495920 -> 1607867302096
	1607867302096 [label=AccumulateGrad]
	1607867301952 -> 1607867302048
	1607866496016 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1607866496016 -> 1607867301952
	1607867301952 [label=AccumulateGrad]
	1607866253216 -> 1607866253072
	1607866496400 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607866496400 -> 1607866253216
	1607866253216 [label=AccumulateGrad]
	1607866253024 -> 1607866252976
	1607866496496 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1607866496496 -> 1607866253024
	1607866253024 [label=AccumulateGrad]
	1607866252880 -> 1607866252976
	1607866496592 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1607866496592 -> 1607866252880
	1607866252880 [label=AccumulateGrad]
	1607866252784 -> 1607866252640
	1607866496976 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607866496976 -> 1607866252784
	1607866252784 [label=AccumulateGrad]
	1607866252592 -> 1607866252496
	1607866497072 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1607866497072 -> 1607866252592
	1607866252592 [label=AccumulateGrad]
	1607866252544 -> 1607866252496
	1607866497168 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1607866497168 -> 1607866252544
	1607866252544 [label=AccumulateGrad]
	1607866252448 -> 1607866252400
	1607866252208 -> 1607866252064
	1607866498224 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1607866498224 -> 1607866252208
	1607866252208 [label=AccumulateGrad]
	1607866252016 -> 1607866251968
	1607866498320 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1607866498320 -> 1607866252016
	1607866252016 [label=AccumulateGrad]
	1607866251872 -> 1607866251968
	1607866498416 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1607866498416 -> 1607866251872
	1607866251872 [label=AccumulateGrad]
	1607866251776 -> 1607866251632
	1607866498800 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1607866498800 -> 1607866251776
	1607866251776 [label=AccumulateGrad]
	1607866251584 -> 1607866251536
	1607866498896 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1607866498896 -> 1607866251584
	1607866251584 [label=AccumulateGrad]
	1607866251440 -> 1607866251536
	1607866498992 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1607866498992 -> 1607866251440
	1607866251440 [label=AccumulateGrad]
	1607866251344 -> 1607866251200
	1607867531632 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1607867531632 -> 1607866251344
	1607866251344 [label=AccumulateGrad]
	1607866251152 -> 1607866251056
	1607867531728 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1607867531728 -> 1607866251152
	1607866251152 [label=AccumulateGrad]
	1607866251104 -> 1607866251056
	1607867531824 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1607867531824 -> 1607866251104
	1607866251104 [label=AccumulateGrad]
	1607866251008 -> 1607866250960
	1607866251008 [label=CudnnBatchNormBackward0]
	1607866251728 -> 1607866251008
	1607866251728 [label=ConvolutionBackward0]
	1607866252256 -> 1607866251728
	1607866252112 -> 1607866251728
	1607866497648 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1607866497648 -> 1607866252112
	1607866252112 [label=AccumulateGrad]
	1607866251296 -> 1607866251008
	1607866497744 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1607866497744 -> 1607866251296
	1607866251296 [label=AccumulateGrad]
	1607866251248 -> 1607866251008
	1607866497840 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1607866497840 -> 1607866251248
	1607866251248 [label=AccumulateGrad]
	1607866250864 -> 1607866250672
	1607867532208 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1607867532208 -> 1607866250864
	1607866250864 [label=AccumulateGrad]
	1607866250624 -> 1607866250576
	1607867532304 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1607867532304 -> 1607866250624
	1607866250624 [label=AccumulateGrad]
	1607866250480 -> 1607866250576
	1607867532400 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1607867532400 -> 1607866250480
	1607866250480 [label=AccumulateGrad]
	1607866250384 -> 1607852841888
	1607867532784 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1607867532784 -> 1607866250384
	1607866250384 [label=AccumulateGrad]
	1607852841792 -> 1607852845104
	1607867532880 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1607867532880 -> 1607852841792
	1607852841792 [label=AccumulateGrad]
	1607852844864 -> 1607852845104
	1607867532976 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1607867532976 -> 1607852844864
	1607852844864 [label=AccumulateGrad]
	1607852844768 -> 1607852844624
	1607867533360 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1607867533360 -> 1607852844768
	1607852844768 [label=AccumulateGrad]
	1607852844576 -> 1607852844480
	1607867533456 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1607867533456 -> 1607852844576
	1607852844576 [label=AccumulateGrad]
	1607852844528 -> 1607852844480
	1607867533552 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1607867533552 -> 1607852844528
	1607852844528 [label=AccumulateGrad]
	1607852844432 -> 1607852844384
	1607852844288 -> 1607852844096
	1607867533936 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1607867533936 -> 1607852844288
	1607852844288 [label=AccumulateGrad]
	1607852844048 -> 1607852844000
	1607867534032 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1607867534032 -> 1607852844048
	1607852844048 [label=AccumulateGrad]
	1607852843904 -> 1607852844000
	1607867534128 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1607867534128 -> 1607852843904
	1607852843904 [label=AccumulateGrad]
	1607852843808 -> 1607852843664
	1607867534512 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1607867534512 -> 1607852843808
	1607852843808 [label=AccumulateGrad]
	1607852843616 -> 1607852843568
	1607867534608 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1607867534608 -> 1607852843616
	1607852843616 [label=AccumulateGrad]
	1607852843472 -> 1607852843568
	1607867534704 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1607867534704 -> 1607852843472
	1607852843472 [label=AccumulateGrad]
	1607852843376 -> 1607852843232
	1607867535088 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1607867535088 -> 1607852843376
	1607852843376 [label=AccumulateGrad]
	1607852843184 -> 1607852843088
	1607867535184 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1607867535184 -> 1607852843184
	1607852843184 [label=AccumulateGrad]
	1607852843136 -> 1607852843088
	1607867535280 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1607867535280 -> 1607852843136
	1607852843136 [label=AccumulateGrad]
	1607852843040 -> 1607852842992
	1607852842704 -> 1607852842464
	1607852842704 [label=TBackward0]
	1607852842944 -> 1607852842704
	1607867535952 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	1607867535952 -> 1607852842944
	1607852842944 [label=AccumulateGrad]
	1607852842464 -> 1607856563088
}
