digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2040066919024 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2040041731408 [label=AddmmBackward0]
	2040041731072 -> 2040041731408
	2040041371408 [label="fc.bias
 (19)" fillcolor=lightblue]
	2040041371408 -> 2040041731072
	2040041731072 [label=AccumulateGrad]
	2040041730448 -> 2040041731408
	2040041730448 [label=ViewBackward0]
	2040041729968 -> 2040041730448
	2040041729968 [label=MeanBackward1]
	2040041729632 -> 2040041729968
	2040041729632 [label=ReluBackward0]
	2040041729152 -> 2040041729632
	2040041729152 [label=AddBackward0]
	2040041728672 -> 2040041729152
	2040041728672 [label=CudnnBatchNormBackward0]
	2040041727568 -> 2040041728672
	2040041727568 [label=ConvolutionBackward0]
	2040041726608 -> 2040041727568
	2040041726608 [label=ReluBackward0]
	2040041726272 -> 2040041726608
	2040041726272 [label=CudnnBatchNormBackward0]
	2040041725792 -> 2040041726272
	2040041725792 [label=ConvolutionBackward0]
	2040041728528 -> 2040041725792
	2040041728528 [label=ReluBackward0]
	2040041724352 -> 2040041728528
	2040041724352 [label=AddBackward0]
	2040041723872 -> 2040041724352
	2040041723872 [label=CudnnBatchNormBackward0]
	2040041722768 -> 2040041723872
	2040041722768 [label=ConvolutionBackward0]
	2040041721808 -> 2040041722768
	2040041721808 [label=ReluBackward0]
	2040041721472 -> 2040041721808
	2040041721472 [label=CudnnBatchNormBackward0]
	2040041720992 -> 2040041721472
	2040041720992 [label=ConvolutionBackward0]
	2040041719792 -> 2040041720992
	2040041719792 [label=ReluBackward0]
	2040041720128 -> 2040041719792
	2040041720128 [label=AddBackward0]
	2040041734000 -> 2040041720128
	2040041734000 [label=CudnnBatchNormBackward0]
	2040041733760 -> 2040041734000
	2040041733760 [label=ConvolutionBackward0]
	2040041733376 -> 2040041733760
	2040041733376 [label=ReluBackward0]
	2040041733136 -> 2040041733376
	2040041733136 [label=CudnnBatchNormBackward0]
	2040041733088 -> 2040041733136
	2040041733088 [label=ConvolutionBackward0]
	2040041718352 -> 2040041733088
	2040041718352 [label=ReluBackward0]
	2040041732656 -> 2040041718352
	2040041732656 [label=AddBackward0]
	2040041732608 -> 2040041732656
	2040041732608 [label=CudnnBatchNormBackward0]
	2040041732416 -> 2040041732608
	2040041732416 [label=ConvolutionBackward0]
	2040041732320 -> 2040041732416
	2040041732320 [label=ReluBackward0]
	2040041731984 -> 2040041732320
	2040041731984 [label=CudnnBatchNormBackward0]
	2040041731792 -> 2040041731984
	2040041731792 [label=ConvolutionBackward0]
	2040041731648 -> 2040041731792
	2040041731648 [label=ReluBackward0]
	2040041731456 -> 2040041731648
	2040041731456 [label=AddBackward0]
	2040041731120 -> 2040041731456
	2040041731120 [label=CudnnBatchNormBackward0]
	2040041731168 -> 2040041731120
	2040041731168 [label=ConvolutionBackward0]
	2040041730832 -> 2040041731168
	2040041730832 [label=ReluBackward0]
	2040041730880 -> 2040041730832
	2040041730880 [label=CudnnBatchNormBackward0]
	2040041730784 -> 2040041730880
	2040041730784 [label=ConvolutionBackward0]
	2040041731312 -> 2040041730784
	2040041731312 [label=ReluBackward0]
	2040041730400 -> 2040041731312
	2040041730400 [label=AddBackward0]
	2040041730304 -> 2040041730400
	2040041730304 [label=CudnnBatchNormBackward0]
	2040041729872 -> 2040041730304
	2040041729872 [label=ConvolutionBackward0]
	2040041729728 -> 2040041729872
	2040041729728 [label=ReluBackward0]
	2040041729536 -> 2040041729728
	2040041729536 [label=CudnnBatchNormBackward0]
	2040041729200 -> 2040041729536
	2040041729200 [label=ConvolutionBackward0]
	2040041729344 -> 2040041729200
	2040041729344 [label=ReluBackward0]
	2040041728912 -> 2040041729344
	2040041728912 [label=AddBackward0]
	2040041728816 -> 2040041728912
	2040041728816 [label=CudnnBatchNormBackward0]
	2040041728864 -> 2040041728816
	2040041728864 [label=ConvolutionBackward0]
	2040041728240 -> 2040041728864
	2040041728240 [label=ReluBackward0]
	2040041728288 -> 2040041728240
	2040041728288 [label=CudnnBatchNormBackward0]
	2040041728144 -> 2040041728288
	2040041728144 [label=ConvolutionBackward0]
	2040041728720 -> 2040041728144
	2040041728720 [label=ReluBackward0]
	2040041727808 -> 2040041728720
	2040041727808 [label=AddBackward0]
	2040041727664 -> 2040041727808
	2040041727664 [label=CudnnBatchNormBackward0]
	2040041727280 -> 2040041727664
	2040041727280 [label=ConvolutionBackward0]
	2040041727424 -> 2040041727280
	2040041727424 [label=ReluBackward0]
	2040041726992 -> 2040041727424
	2040041726992 [label=CudnnBatchNormBackward0]
	2040041726896 -> 2040041726992
	2040041726896 [label=ConvolutionBackward0]
	2040041727904 -> 2040041726896
	2040041727904 [label=MaxPool2DWithIndicesBackward0]
	2040041726512 -> 2040041727904
	2040041726512 [label=ReluBackward0]
	2040041726416 -> 2040041726512
	2040041726416 [label=CudnnBatchNormBackward0]
	2040041726368 -> 2040041726416
	2040041726368 [label=ConvolutionBackward0]
	2040041726032 -> 2040041726368
	2040041371216 [label="conv1.weight
 (64, 4, 7, 7)" fillcolor=lightblue]
	2040041371216 -> 2040041726032
	2040041726032 [label=AccumulateGrad]
	2040041726560 -> 2040041726416
	2040041329616 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2040041329616 -> 2040041726560
	2040041726560 [label=AccumulateGrad]
	2040041726848 -> 2040041726416
	2040041329328 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2040041329328 -> 2040041726848
	2040041726848 [label=AccumulateGrad]
	2040041726704 -> 2040041726896
	2040041329808 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2040041329808 -> 2040041726704
	2040041726704 [label=AccumulateGrad]
	2040041726800 -> 2040041726992
	2040041329904 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2040041329904 -> 2040041726800
	2040041726800 [label=AccumulateGrad]
	2040041727184 -> 2040041726992
	2040041330000 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2040041330000 -> 2040041727184
	2040041727184 [label=AccumulateGrad]
	2040041727328 -> 2040041727280
	2040041329232 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2040041329232 -> 2040041727328
	2040041727328 [label=AccumulateGrad]
	2040041727472 -> 2040041727664
	2040041329136 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2040041329136 -> 2040041727472
	2040041727472 [label=AccumulateGrad]
	2040041727616 -> 2040041727664
	2040041328368 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2040041328368 -> 2040041727616
	2040041727616 [label=AccumulateGrad]
	2040041727904 -> 2040041727808
	2040041727856 -> 2040041728144
	2040041328464 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2040041328464 -> 2040041727856
	2040041727856 [label=AccumulateGrad]
	2040041728384 -> 2040041728288
	2040041328560 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2040041328560 -> 2040041728384
	2040041728384 [label=AccumulateGrad]
	2040041728336 -> 2040041728288
	2040041328656 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2040041328656 -> 2040041728336
	2040041728336 [label=AccumulateGrad]
	2040041728432 -> 2040041728864
	2040041328944 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2040041328944 -> 2040041728432
	2040041728432 [label=AccumulateGrad]
	2040041728768 -> 2040041728816
	2040041327984 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2040041327984 -> 2040041728768
	2040041728768 [label=AccumulateGrad]
	2040041728960 -> 2040041728816
	2040041327888 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2040041327888 -> 2040041728960
	2040041728960 [label=AccumulateGrad]
	2040041728720 -> 2040041728912
	2040041729248 -> 2040041729200
	2040041327792 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2040041327792 -> 2040041729248
	2040041729248 [label=AccumulateGrad]
	2040041729392 -> 2040041729536
	2040041327696 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2040041327696 -> 2040041729392
	2040041729392 [label=AccumulateGrad]
	2040041729824 -> 2040041729536
	2040041363152 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2040041363152 -> 2040041729824
	2040041729824 [label=AccumulateGrad]
	2040041729920 -> 2040041729872
	2040041363536 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2040041363536 -> 2040041729920
	2040041729920 [label=AccumulateGrad]
	2040041730016 -> 2040041730304
	2040041363632 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2040041363632 -> 2040041730016
	2040041730016 [label=AccumulateGrad]
	2040041730064 -> 2040041730304
	2040041363728 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2040041363728 -> 2040041730064
	2040041730064 [label=AccumulateGrad]
	2040041730208 -> 2040041730400
	2040041730208 [label=CudnnBatchNormBackward0]
	2040041729440 -> 2040041730208
	2040041729440 [label=ConvolutionBackward0]
	2040041729344 -> 2040041729440
	2040041729104 -> 2040041729440
	2040041325200 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2040041325200 -> 2040041729104
	2040041729104 [label=AccumulateGrad]
	2040041729776 -> 2040041730208
	2040041325392 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2040041325392 -> 2040041729776
	2040041729776 [label=AccumulateGrad]
	2040041729680 -> 2040041730208
	2040041327024 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2040041327024 -> 2040041729680
	2040041729680 [label=AccumulateGrad]
	2040041730160 -> 2040041730784
	2040041364112 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2040041364112 -> 2040041730160
	2040041730160 [label=AccumulateGrad]
	2040041730688 -> 2040041730880
	2040041364208 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2040041364208 -> 2040041730688
	2040041730688 [label=AccumulateGrad]
	2040041730640 -> 2040041730880
	2040041364304 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2040041364304 -> 2040041730640
	2040041730640 [label=AccumulateGrad]
	2040041730976 -> 2040041731168
	2040041364688 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2040041364688 -> 2040041730976
	2040041730976 [label=AccumulateGrad]
	2040041731360 -> 2040041731120
	2040041364784 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2040041364784 -> 2040041731360
	2040041731360 [label=AccumulateGrad]
	2040041731216 -> 2040041731120
	2040041364880 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2040041364880 -> 2040041731216
	2040041731216 [label=AccumulateGrad]
	2040041731312 -> 2040041731456
	2040041731840 -> 2040041731792
	2040041365840 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2040041365840 -> 2040041731840
	2040041731840 [label=AccumulateGrad]
	2040041731936 -> 2040041731984
	2040041365936 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2040041365936 -> 2040041731936
	2040041731936 [label=AccumulateGrad]
	2040041732128 -> 2040041731984
	2040041366032 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2040041366032 -> 2040041732128
	2040041732128 [label=AccumulateGrad]
	2040041732176 -> 2040041732416
	2040041366416 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2040041366416 -> 2040041732176
	2040041732176 [label=AccumulateGrad]
	2040041732464 -> 2040041732608
	2040041366512 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2040041366512 -> 2040041732464
	2040041732464 [label=AccumulateGrad]
	2040041732704 -> 2040041732608
	2040041366608 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2040041366608 -> 2040041732704
	2040041732704 [label=AccumulateGrad]
	2040041732800 -> 2040041732656
	2040041732800 [label=CudnnBatchNormBackward0]
	2040041731696 -> 2040041732800
	2040041731696 [label=ConvolutionBackward0]
	2040041731648 -> 2040041731696
	2040041731744 -> 2040041731696
	2040041365264 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2040041365264 -> 2040041731744
	2040041731744 [label=AccumulateGrad]
	2040041732080 -> 2040041732800
	2040041365360 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2040041365360 -> 2040041732080
	2040041732080 [label=AccumulateGrad]
	2040041732272 -> 2040041732800
	2040041365456 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2040041365456 -> 2040041732272
	2040041732272 [label=AccumulateGrad]
	2040041732752 -> 2040041733088
	2040041366992 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2040041366992 -> 2040041732752
	2040041732752 [label=AccumulateGrad]
	2040041733280 -> 2040041733136
	2040041367088 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2040041367088 -> 2040041733280
	2040041733280 [label=AccumulateGrad]
	2040041733232 -> 2040041733136
	2040041367184 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2040041367184 -> 2040041733232
	2040041733232 [label=AccumulateGrad]
	2040041733424 -> 2040041733760
	2040041367568 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2040041367568 -> 2040041733424
	2040041733424 [label=AccumulateGrad]
	2040041733616 -> 2040041734000
	2040041367664 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2040041367664 -> 2040041733616
	2040041733616 [label=AccumulateGrad]
	2040041733520 -> 2040041734000
	2040041367760 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2040041367760 -> 2040041733520
	2040041733520 [label=AccumulateGrad]
	2040041718352 -> 2040041720128
	2040041719840 -> 2040041720992
	2040041368720 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2040041368720 -> 2040041719840
	2040041719840 [label=AccumulateGrad]
	2040041720848 -> 2040041721472
	2040041368816 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2040041368816 -> 2040041720848
	2040041720848 [label=AccumulateGrad]
	2040041721952 -> 2040041721472
	2040041368912 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2040041368912 -> 2040041721952
	2040041721952 [label=AccumulateGrad]
	2040041722432 -> 2040041722768
	2040041369296 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2040041369296 -> 2040041722432
	2040041722432 [label=AccumulateGrad]
	2040041723392 -> 2040041723872
	2040041369392 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2040041369392 -> 2040041723392
	2040041723392 [label=AccumulateGrad]
	2040041723248 -> 2040041723872
	2040041369488 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2040041369488 -> 2040041723248
	2040041723248 [label=AccumulateGrad]
	2040041723728 -> 2040041724352
	2040041723728 [label=CudnnBatchNormBackward0]
	2040041720512 -> 2040041723728
	2040041720512 [label=ConvolutionBackward0]
	2040041719792 -> 2040041720512
	2040041718496 -> 2040041720512
	2040041368144 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2040041368144 -> 2040041718496
	2040041718496 [label=AccumulateGrad]
	2040041722288 -> 2040041723728
	2040041368240 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2040041368240 -> 2040041722288
	2040041722288 [label=AccumulateGrad]
	2040041722912 -> 2040041723728
	2040041368336 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2040041368336 -> 2040041722912
	2040041722912 [label=AccumulateGrad]
	2040041724832 -> 2040041725792
	2040041369872 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2040041369872 -> 2040041724832
	2040041724832 [label=AccumulateGrad]
	2040041725648 -> 2040041726272
	2040041369968 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2040041369968 -> 2040041725648
	2040041725648 [label=AccumulateGrad]
	2040041726752 -> 2040041726272
	2040041370064 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2040041370064 -> 2040041726752
	2040041726752 [label=AccumulateGrad]
	2040041727232 -> 2040041727568
	2040041370448 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2040041370448 -> 2040041727232
	2040041727232 [label=AccumulateGrad]
	2040041728192 -> 2040041728672
	2040041370544 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2040041370544 -> 2040041728192
	2040041728192 [label=AccumulateGrad]
	2040041728048 -> 2040041728672
	2040041370640 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2040041370640 -> 2040041728048
	2040041728048 [label=AccumulateGrad]
	2040041728528 -> 2040041729152
	2040041730592 -> 2040041731408
	2040041730592 [label=TBackward0]
	2040041729008 -> 2040041730592
	2040041371312 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2040041371312 -> 2040041729008
	2040041729008 [label=AccumulateGrad]
	2040041731408 -> 2040066919024
}
