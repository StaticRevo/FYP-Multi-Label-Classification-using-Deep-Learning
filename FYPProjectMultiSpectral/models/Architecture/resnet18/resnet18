digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2116522928624 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2116525391200 [label=AddmmBackward0]
	2116525391344 -> 2116525391200
	2116528191728 [label="fc.bias
 (19)" fillcolor=lightblue]
	2116528191728 -> 2116525391344
	2116525391344 [label=AccumulateGrad]
	2116525391392 -> 2116525391200
	2116525391392 [label=ViewBackward0]
	2116525391488 -> 2116525391392
	2116525391488 [label=MeanBackward1]
	2116525391632 -> 2116525391488
	2116525391632 [label=ReluBackward0]
	2116525391728 -> 2116525391632
	2116525391728 [label=AddBackward0]
	2116525391824 -> 2116525391728
	2116525391824 [label=CudnnBatchNormBackward0]
	2116516670016 -> 2116525391824
	2116516670016 [label=ConvolutionBackward0]
	2116516665120 -> 2116516670016
	2116516665120 [label=ReluBackward0]
	2116516671456 -> 2116516665120
	2116516671456 [label=CudnnBatchNormBackward0]
	2116516665456 -> 2116516671456
	2116516665456 [label=ConvolutionBackward0]
	2116525391776 -> 2116516665456
	2116525391776 [label=ReluBackward0]
	2116516668000 -> 2116525391776
	2116516668000 [label=AddBackward0]
	2116516667760 -> 2116516668000
	2116516667760 [label=CudnnBatchNormBackward0]
	2116516672752 -> 2116516667760
	2116516672752 [label=ConvolutionBackward0]
	2116516668192 -> 2116516672752
	2116516668192 [label=ReluBackward0]
	2116516668816 -> 2116516668192
	2116516668816 [label=CudnnBatchNormBackward0]
	2116516667712 -> 2116516668816
	2116516667712 [label=ConvolutionBackward0]
	2116516668672 -> 2116516667712
	2116516668672 [label=ReluBackward0]
	2116516672896 -> 2116516668672
	2116516672896 [label=AddBackward0]
	2116516667568 -> 2116516672896
	2116516667568 [label=CudnnBatchNormBackward0]
	2116516671984 -> 2116516667568
	2116516671984 [label=ConvolutionBackward0]
	2116516668960 -> 2116516671984
	2116516668960 [label=ReluBackward0]
	2116516672800 -> 2116516668960
	2116516672800 [label=CudnnBatchNormBackward0]
	2116516669296 -> 2116516672800
	2116516669296 [label=ConvolutionBackward0]
	2116516668240 -> 2116516669296
	2116516668240 [label=ReluBackward0]
	2116516668576 -> 2116516668240
	2116516668576 [label=AddBackward0]
	2116521164960 -> 2116516668576
	2116521164960 [label=CudnnBatchNormBackward0]
	2116521165104 -> 2116521164960
	2116521165104 [label=ConvolutionBackward0]
	2116521165296 -> 2116521165104
	2116521165296 [label=ReluBackward0]
	2116521165440 -> 2116521165296
	2116521165440 [label=CudnnBatchNormBackward0]
	2116521165536 -> 2116521165440
	2116521165536 [label=ConvolutionBackward0]
	2116521165728 -> 2116521165536
	2116521165728 [label=ReluBackward0]
	2116521165872 -> 2116521165728
	2116521165872 [label=AddBackward0]
	2116521165968 -> 2116521165872
	2116521165968 [label=CudnnBatchNormBackward0]
	2116521166112 -> 2116521165968
	2116521166112 [label=ConvolutionBackward0]
	2116521166304 -> 2116521166112
	2116521166304 [label=ReluBackward0]
	2116521166448 -> 2116521166304
	2116521166448 [label=CudnnBatchNormBackward0]
	2116521166544 -> 2116521166448
	2116521166544 [label=ConvolutionBackward0]
	2116521165920 -> 2116521166544
	2116521165920 [label=ReluBackward0]
	2116521166832 -> 2116521165920
	2116521166832 [label=AddBackward0]
	2116521166928 -> 2116521166832
	2116521166928 [label=CudnnBatchNormBackward0]
	2116521167072 -> 2116521166928
	2116521167072 [label=ConvolutionBackward0]
	2116521167264 -> 2116521167072
	2116521167264 [label=ReluBackward0]
	2116521167408 -> 2116521167264
	2116521167408 [label=CudnnBatchNormBackward0]
	2116521167504 -> 2116521167408
	2116521167504 [label=ConvolutionBackward0]
	2116521167696 -> 2116521167504
	2116521167696 [label=ReluBackward0]
	2116521167840 -> 2116521167696
	2116521167840 [label=AddBackward0]
	2116521167936 -> 2116521167840
	2116521167936 [label=CudnnBatchNormBackward0]
	2116521168080 -> 2116521167936
	2116521168080 [label=ConvolutionBackward0]
	2116521168272 -> 2116521168080
	2116521168272 [label=ReluBackward0]
	2116521168416 -> 2116521168272
	2116521168416 [label=CudnnBatchNormBackward0]
	2116521168464 -> 2116521168416
	2116521168464 [label=ConvolutionBackward0]
	2116521167888 -> 2116521168464
	2116521167888 [label=ReluBackward0]
	2116521168848 -> 2116521167888
	2116521168848 [label=AddBackward0]
	2116521168896 -> 2116521168848
	2116521168896 [label=CudnnBatchNormBackward0]
	2116521169136 -> 2116521168896
	2116521169136 [label=ConvolutionBackward0]
	2116521169328 -> 2116521169136
	2116521169328 [label=ReluBackward0]
	2116521169472 -> 2116521169328
	2116521169472 [label=CudnnBatchNormBackward0]
	2116521169520 -> 2116521169472
	2116521169520 [label=ConvolutionBackward0]
	2116521168656 -> 2116521169520
	2116521168656 [label=MaxPool2DWithIndicesBackward0]
	2116521169904 -> 2116521168656
	2116521169904 [label=ReluBackward0]
	2116521169952 -> 2116521169904
	2116521169952 [label=CudnnBatchNormBackward0]
	2116521170096 -> 2116521169952
	2116521170096 [label=ConvolutionBackward0]
	2116521170384 -> 2116521170096
	2116528191536 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2116528191536 -> 2116521170384
	2116521170384 [label=AccumulateGrad]
	2116521170048 -> 2116521169952
	2116518641840 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2116518641840 -> 2116521170048
	2116521170048 [label=AccumulateGrad]
	2116521170192 -> 2116521169952
	2116518644720 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2116518644720 -> 2116521170192
	2116521170192 [label=AccumulateGrad]
	2116521169808 -> 2116521169520
	2116518647696 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2116518647696 -> 2116521169808
	2116521169808 [label=AccumulateGrad]
	2116521169376 -> 2116521169472
	2116518647792 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2116518647792 -> 2116521169376
	2116521169376 [label=AccumulateGrad]
	2116521169616 -> 2116521169472
	2116518647888 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2116518647888 -> 2116521169616
	2116521169616 [label=AccumulateGrad]
	2116521169280 -> 2116521169136
	2116518648272 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2116518648272 -> 2116521169280
	2116521169280 [label=AccumulateGrad]
	2116521169088 -> 2116521168896
	2116518648368 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2116518648368 -> 2116521169088
	2116521169088 [label=AccumulateGrad]
	2116521169040 -> 2116521168896
	2116518648464 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2116518648464 -> 2116521169040
	2116521169040 [label=AccumulateGrad]
	2116521168656 -> 2116521168848
	2116521168752 -> 2116521168464
	2116518648848 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2116518648848 -> 2116521168752
	2116521168752 [label=AccumulateGrad]
	2116521168320 -> 2116521168416
	2116518648944 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2116518648944 -> 2116521168320
	2116521168320 [label=AccumulateGrad]
	2116521168560 -> 2116521168416
	2116518649040 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2116518649040 -> 2116521168560
	2116521168560 [label=AccumulateGrad]
	2116521168224 -> 2116521168080
	2116518649424 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2116518649424 -> 2116521168224
	2116521168224 [label=AccumulateGrad]
	2116521168032 -> 2116521167936
	2116518649520 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2116518649520 -> 2116521168032
	2116521168032 [label=AccumulateGrad]
	2116521167984 -> 2116521167936
	2116518649616 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2116518649616 -> 2116521167984
	2116521167984 [label=AccumulateGrad]
	2116521167888 -> 2116521167840
	2116521167648 -> 2116521167504
	2116518650576 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2116518650576 -> 2116521167648
	2116521167648 [label=AccumulateGrad]
	2116521167456 -> 2116521167408
	2116518650672 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2116518650672 -> 2116521167456
	2116521167456 [label=AccumulateGrad]
	2116521167312 -> 2116521167408
	2116518650768 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2116518650768 -> 2116521167312
	2116521167312 [label=AccumulateGrad]
	2116521167216 -> 2116521167072
	2116518651152 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2116518651152 -> 2116521167216
	2116521167216 [label=AccumulateGrad]
	2116521167024 -> 2116521166928
	2116518651248 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2116518651248 -> 2116521167024
	2116521167024 [label=AccumulateGrad]
	2116521166976 -> 2116521166928
	2116518651344 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2116518651344 -> 2116521166976
	2116521166976 [label=AccumulateGrad]
	2116521166880 -> 2116521166832
	2116521166880 [label=CudnnBatchNormBackward0]
	2116521167600 -> 2116521166880
	2116521167600 [label=ConvolutionBackward0]
	2116521167696 -> 2116521167600
	2116521167744 -> 2116521167600
	2116518650000 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2116518650000 -> 2116521167744
	2116521167744 [label=AccumulateGrad]
	2116521167168 -> 2116521166880
	2116518650096 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2116518650096 -> 2116521167168
	2116521167168 [label=AccumulateGrad]
	2116521167120 -> 2116521166880
	2116518650192 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2116518650192 -> 2116521167120
	2116521167120 [label=AccumulateGrad]
	2116521166736 -> 2116521166544
	2116518651728 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2116518651728 -> 2116521166736
	2116521166736 [label=AccumulateGrad]
	2116521166496 -> 2116521166448
	2116518651824 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2116518651824 -> 2116521166496
	2116521166496 [label=AccumulateGrad]
	2116521166352 -> 2116521166448
	2116518651920 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2116518651920 -> 2116521166352
	2116521166352 [label=AccumulateGrad]
	2116521166256 -> 2116521166112
	2116518652304 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2116518652304 -> 2116521166256
	2116521166256 [label=AccumulateGrad]
	2116521166064 -> 2116521165968
	2116518652400 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2116518652400 -> 2116521166064
	2116521166064 [label=AccumulateGrad]
	2116521166016 -> 2116521165968
	2116518652496 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2116518652496 -> 2116521166016
	2116521166016 [label=AccumulateGrad]
	2116521165920 -> 2116521165872
	2116521165680 -> 2116521165536
	2116518653456 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2116518653456 -> 2116521165680
	2116521165680 [label=AccumulateGrad]
	2116521165488 -> 2116521165440
	2116518653552 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2116518653552 -> 2116521165488
	2116521165488 [label=AccumulateGrad]
	2116521165344 -> 2116521165440
	2116518653648 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2116518653648 -> 2116521165344
	2116521165344 [label=AccumulateGrad]
	2116521165248 -> 2116521165104
	2116518654032 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2116518654032 -> 2116521165248
	2116521165248 [label=AccumulateGrad]
	2116521165056 -> 2116521164960
	2116518654128 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2116518654128 -> 2116521165056
	2116521165056 [label=AccumulateGrad]
	2116521165008 -> 2116521164960
	2116518654224 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2116518654224 -> 2116521165008
	2116521165008 [label=AccumulateGrad]
	2116521164912 -> 2116516668576
	2116521164912 [label=CudnnBatchNormBackward0]
	2116521165632 -> 2116521164912
	2116521165632 [label=ConvolutionBackward0]
	2116521165728 -> 2116521165632
	2116521165776 -> 2116521165632
	2116518652880 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2116518652880 -> 2116521165776
	2116521165776 [label=AccumulateGrad]
	2116521165200 -> 2116521164912
	2116518652976 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2116518652976 -> 2116521165200
	2116521165200 [label=AccumulateGrad]
	2116521165152 -> 2116521164912
	2116518653072 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2116518653072 -> 2116521165152
	2116521165152 [label=AccumulateGrad]
	2116516671600 -> 2116516669296
	2116518654608 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2116518654608 -> 2116516671600
	2116516671600 [label=AccumulateGrad]
	2116516665792 -> 2116516672800
	2116518654704 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2116518654704 -> 2116516665792
	2116516665792 [label=AccumulateGrad]
	2116516669248 -> 2116516672800
	2116518654800 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2116518654800 -> 2116516669248
	2116516669248 [label=AccumulateGrad]
	2116516667136 -> 2116516671984
	2116518655184 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2116518655184 -> 2116516667136
	2116516667136 [label=AccumulateGrad]
	2116516666512 -> 2116516667568
	2116518655280 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2116518655280 -> 2116516666512
	2116516666512 [label=AccumulateGrad]
	2116516673280 -> 2116516667568
	2116518655376 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2116518655376 -> 2116516673280
	2116516673280 [label=AccumulateGrad]
	2116516668240 -> 2116516672896
	2116516668336 -> 2116516667712
	2116518656336 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2116518656336 -> 2116516668336
	2116516668336 [label=AccumulateGrad]
	2116516671648 -> 2116516668816
	2116518656432 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2116518656432 -> 2116516671648
	2116516671648 [label=AccumulateGrad]
	2116516671792 -> 2116516668816
	2116518656528 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2116518656528 -> 2116516671792
	2116516671792 [label=AccumulateGrad]
	2116516670928 -> 2116516672752
	2116518656912 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2116518656912 -> 2116516670928
	2116516670928 [label=AccumulateGrad]
	2116516665696 -> 2116516667760
	2116518657008 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2116518657008 -> 2116516665696
	2116516665696 [label=AccumulateGrad]
	2116516671888 -> 2116516667760
	2116518657104 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2116518657104 -> 2116516671888
	2116516671888 [label=AccumulateGrad]
	2116516671360 -> 2116516668000
	2116516671360 [label=CudnnBatchNormBackward0]
	2116516675008 -> 2116516671360
	2116516675008 [label=ConvolutionBackward0]
	2116516668672 -> 2116516675008
	2116516669632 -> 2116516675008
	2116518655760 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2116518655760 -> 2116516669632
	2116516669632 [label=AccumulateGrad]
	2116516672272 -> 2116516671360
	2116518655856 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2116518655856 -> 2116516672272
	2116516672272 [label=AccumulateGrad]
	2116516668288 -> 2116516671360
	2116518655952 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2116518655952 -> 2116516668288
	2116516668288 [label=AccumulateGrad]
	2116516665360 -> 2116516665456
	2116518657488 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2116518657488 -> 2116516665360
	2116516665360 [label=AccumulateGrad]
	2116516666416 -> 2116516671456
	2116518657584 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2116518657584 -> 2116516666416
	2116516666416 [label=AccumulateGrad]
	2116516666944 -> 2116516671456
	2116518657680 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2116518657680 -> 2116516666944
	2116516666944 [label=AccumulateGrad]
	2116516666128 -> 2116516670016
	2116518642032 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2116518642032 -> 2116516666128
	2116516666128 [label=AccumulateGrad]
	2116516669200 -> 2116525391824
	2116528190864 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2116528190864 -> 2116516669200
	2116516669200 [label=AccumulateGrad]
	2116516666032 -> 2116525391824
	2116528190960 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2116528190960 -> 2116516666032
	2116516666032 [label=AccumulateGrad]
	2116525391776 -> 2116525391728
	2116525391440 -> 2116525391200
	2116525391440 [label=TBackward0]
	2116525391680 -> 2116525391440
	2116528191632 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2116528191632 -> 2116525391680
	2116525391680 [label=AccumulateGrad]
	2116525391200 -> 2116522928624
}
