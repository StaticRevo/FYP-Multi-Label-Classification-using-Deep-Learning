digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2870560596592 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2870553850976 [label=AddmmBackward0]
	2870553851120 -> 2870553850976
	2870547187184 [label="fc.bias
 (19)" fillcolor=lightblue]
	2870547187184 -> 2870553851120
	2870553851120 [label=AccumulateGrad]
	2870553851168 -> 2870553850976
	2870553851168 [label=ViewBackward0]
	2870553851264 -> 2870553851168
	2870553851264 [label=MeanBackward1]
	2870553851408 -> 2870553851264
	2870553851408 [label=ReluBackward0]
	2870553851504 -> 2870553851408
	2870553851504 [label=AddBackward0]
	2870553851600 -> 2870553851504
	2870553851600 [label=CudnnBatchNormBackward0]
	2870553851744 -> 2870553851600
	2870553851744 [label=ConvolutionBackward0]
	2870553851936 -> 2870553851744
	2870553851936 [label=ReluBackward0]
	2870553852080 -> 2870553851936
	2870553852080 [label=CudnnBatchNormBackward0]
	2870553852176 -> 2870553852080
	2870553852176 [label=ConvolutionBackward0]
	2870553851552 -> 2870553852176
	2870553851552 [label=ReluBackward0]
	2870553852320 -> 2870553851552
	2870553852320 [label=AddBackward0]
	2870553852416 -> 2870553852320
	2870553852416 [label=CudnnBatchNormBackward0]
	2870553852560 -> 2870553852416
	2870553852560 [label=ConvolutionBackward0]
	2870553852752 -> 2870553852560
	2870553852752 [label=ReluBackward0]
	2870553852896 -> 2870553852752
	2870553852896 [label=CudnnBatchNormBackward0]
	2870553852992 -> 2870553852896
	2870553852992 [label=ConvolutionBackward0]
	2870553853184 -> 2870553852992
	2870553853184 [label=ReluBackward0]
	2870553853328 -> 2870553853184
	2870553853328 [label=AddBackward0]
	2870553853424 -> 2870553853328
	2870553853424 [label=CudnnBatchNormBackward0]
	2870553853568 -> 2870553853424
	2870553853568 [label=ConvolutionBackward0]
	2870553853760 -> 2870553853568
	2870553853760 [label=ReluBackward0]
	2870553853904 -> 2870553853760
	2870553853904 [label=CudnnBatchNormBackward0]
	2870553853808 -> 2870553853904
	2870553853808 [label=ConvolutionBackward0]
	2870553853376 -> 2870553853808
	2870553853376 [label=ReluBackward0]
	2870546497936 -> 2870553853376
	2870546497936 [label=AddBackward0]
	2870546498032 -> 2870546497936
	2870546498032 [label=CudnnBatchNormBackward0]
	2870546498176 -> 2870546498032
	2870546498176 [label=ConvolutionBackward0]
	2870546498368 -> 2870546498176
	2870546498368 [label=ReluBackward0]
	2870546498512 -> 2870546498368
	2870546498512 [label=CudnnBatchNormBackward0]
	2870546498608 -> 2870546498512
	2870546498608 [label=ConvolutionBackward0]
	2870546498800 -> 2870546498608
	2870546498800 [label=ReluBackward0]
	2870546498944 -> 2870546498800
	2870546498944 [label=AddBackward0]
	2870546499040 -> 2870546498944
	2870546499040 [label=CudnnBatchNormBackward0]
	2870546499184 -> 2870546499040
	2870546499184 [label=ConvolutionBackward0]
	2870546499376 -> 2870546499184
	2870546499376 [label=ReluBackward0]
	2870546499520 -> 2870546499376
	2870546499520 [label=CudnnBatchNormBackward0]
	2870546499616 -> 2870546499520
	2870546499616 [label=ConvolutionBackward0]
	2870546498992 -> 2870546499616
	2870546498992 [label=ReluBackward0]
	2870546499904 -> 2870546498992
	2870546499904 [label=AddBackward0]
	2870546500000 -> 2870546499904
	2870546500000 [label=CudnnBatchNormBackward0]
	2870546500144 -> 2870546500000
	2870546500144 [label=ConvolutionBackward0]
	2870546500336 -> 2870546500144
	2870546500336 [label=ReluBackward0]
	2870546500480 -> 2870546500336
	2870546500480 [label=CudnnBatchNormBackward0]
	2870546500576 -> 2870546500480
	2870546500576 [label=ConvolutionBackward0]
	2870546500768 -> 2870546500576
	2870546500768 [label=ReluBackward0]
	2870546500912 -> 2870546500768
	2870546500912 [label=AddBackward0]
	2870546501008 -> 2870546500912
	2870546501008 [label=CudnnBatchNormBackward0]
	2870546501152 -> 2870546501008
	2870546501152 [label=ConvolutionBackward0]
	2870546501344 -> 2870546501152
	2870546501344 [label=ReluBackward0]
	2870546501488 -> 2870546501344
	2870546501488 [label=CudnnBatchNormBackward0]
	2870546501584 -> 2870546501488
	2870546501584 [label=ConvolutionBackward0]
	2870546500960 -> 2870546501584
	2870546500960 [label=ReluBackward0]
	2870546501872 -> 2870546500960
	2870546501872 [label=AddBackward0]
	2870546501968 -> 2870546501872
	2870546501968 [label=CudnnBatchNormBackward0]
	2870546502112 -> 2870546501968
	2870546502112 [label=ConvolutionBackward0]
	2870546502544 -> 2870546502112
	2870546502544 [label=ReluBackward0]
	2870560465552 -> 2870546502544
	2870560465552 [label=CudnnBatchNormBackward0]
	2870560465648 -> 2870560465552
	2870560465648 [label=ConvolutionBackward0]
	2870546501920 -> 2870560465648
	2870546501920 [label=MaxPool2DWithIndicesBackward0]
	2870560465936 -> 2870546501920
	2870560465936 [label=ReluBackward0]
	2870560466032 -> 2870560465936
	2870560466032 [label=CudnnBatchNormBackward0]
	2870560466128 -> 2870560466032
	2870560466128 [label=ConvolutionBackward0]
	2870560466320 -> 2870560466128
	2870547187088 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2870547187088 -> 2870560466320
	2870560466320 [label=AccumulateGrad]
	2870560466080 -> 2870560466032
	2870540392432 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2870540392432 -> 2870560466080
	2870560466080 [label=AccumulateGrad]
	2870560465744 -> 2870560466032
	2870540392528 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2870540392528 -> 2870560465744
	2870560465744 [label=AccumulateGrad]
	2870560465840 -> 2870560465648
	2870540392912 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2870540392912 -> 2870560465840
	2870560465840 [label=AccumulateGrad]
	2870560465600 -> 2870560465552
	2870540393008 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2870540393008 -> 2870560465600
	2870560465600 [label=AccumulateGrad]
	2870560465456 -> 2870560465552
	2870540393104 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2870540393104 -> 2870560465456
	2870560465456 [label=AccumulateGrad]
	2870546502256 -> 2870546502112
	2870540393488 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2870540393488 -> 2870546502256
	2870546502256 [label=AccumulateGrad]
	2870546502064 -> 2870546501968
	2870540393584 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2870540393584 -> 2870546502064
	2870546502064 [label=AccumulateGrad]
	2870546502016 -> 2870546501968
	2870540393680 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2870540393680 -> 2870546502016
	2870546502016 [label=AccumulateGrad]
	2870546501920 -> 2870546501872
	2870546501776 -> 2870546501584
	2870540394064 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2870540394064 -> 2870546501776
	2870546501776 [label=AccumulateGrad]
	2870546501536 -> 2870546501488
	2870540394160 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2870540394160 -> 2870546501536
	2870546501536 [label=AccumulateGrad]
	2870546501392 -> 2870546501488
	2870540394256 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2870540394256 -> 2870546501392
	2870546501392 [label=AccumulateGrad]
	2870546501296 -> 2870546501152
	2870540394640 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2870540394640 -> 2870546501296
	2870546501296 [label=AccumulateGrad]
	2870546501104 -> 2870546501008
	2870540394736 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2870540394736 -> 2870546501104
	2870546501104 [label=AccumulateGrad]
	2870546501056 -> 2870546501008
	2870540394832 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2870540394832 -> 2870546501056
	2870546501056 [label=AccumulateGrad]
	2870546500960 -> 2870546500912
	2870546500720 -> 2870546500576
	2870540395792 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2870540395792 -> 2870546500720
	2870546500720 [label=AccumulateGrad]
	2870546500528 -> 2870546500480
	2870540395888 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2870540395888 -> 2870546500528
	2870546500528 [label=AccumulateGrad]
	2870546500384 -> 2870546500480
	2870540395984 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2870540395984 -> 2870546500384
	2870546500384 [label=AccumulateGrad]
	2870546500288 -> 2870546500144
	2870540396368 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2870540396368 -> 2870546500288
	2870546500288 [label=AccumulateGrad]
	2870546500096 -> 2870546500000
	2870540396464 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2870540396464 -> 2870546500096
	2870546500096 [label=AccumulateGrad]
	2870546500048 -> 2870546500000
	2870540396560 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2870540396560 -> 2870546500048
	2870546500048 [label=AccumulateGrad]
	2870546499952 -> 2870546499904
	2870546499952 [label=CudnnBatchNormBackward0]
	2870546500672 -> 2870546499952
	2870546500672 [label=ConvolutionBackward0]
	2870546500768 -> 2870546500672
	2870546500816 -> 2870546500672
	2870540395216 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2870540395216 -> 2870546500816
	2870546500816 [label=AccumulateGrad]
	2870546500240 -> 2870546499952
	2870540395312 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2870540395312 -> 2870546500240
	2870546500240 [label=AccumulateGrad]
	2870546500192 -> 2870546499952
	2870540395408 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2870540395408 -> 2870546500192
	2870546500192 [label=AccumulateGrad]
	2870546499808 -> 2870546499616
	2870540396944 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2870540396944 -> 2870546499808
	2870546499808 [label=AccumulateGrad]
	2870546499568 -> 2870546499520
	2870540397040 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2870540397040 -> 2870546499568
	2870546499568 [label=AccumulateGrad]
	2870546499424 -> 2870546499520
	2870540397136 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2870540397136 -> 2870546499424
	2870546499424 [label=AccumulateGrad]
	2870546499328 -> 2870546499184
	2870540397520 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2870540397520 -> 2870546499328
	2870546499328 [label=AccumulateGrad]
	2870546499136 -> 2870546499040
	2870540397616 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2870540397616 -> 2870546499136
	2870546499136 [label=AccumulateGrad]
	2870546499088 -> 2870546499040
	2870540397712 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2870540397712 -> 2870546499088
	2870546499088 [label=AccumulateGrad]
	2870546498992 -> 2870546498944
	2870546498752 -> 2870546498608
	2870540398672 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2870540398672 -> 2870546498752
	2870546498752 [label=AccumulateGrad]
	2870546498560 -> 2870546498512
	2870540398768 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2870540398768 -> 2870546498560
	2870546498560 [label=AccumulateGrad]
	2870546498416 -> 2870546498512
	2870540398864 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2870540398864 -> 2870546498416
	2870546498416 [label=AccumulateGrad]
	2870546498320 -> 2870546498176
	2870540399248 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2870540399248 -> 2870546498320
	2870546498320 [label=AccumulateGrad]
	2870546498128 -> 2870546498032
	2870540399344 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2870540399344 -> 2870546498128
	2870546498128 [label=AccumulateGrad]
	2870546498080 -> 2870546498032
	2870540399440 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2870540399440 -> 2870546498080
	2870546498080 [label=AccumulateGrad]
	2870546497984 -> 2870546497936
	2870546497984 [label=CudnnBatchNormBackward0]
	2870546498704 -> 2870546497984
	2870546498704 [label=ConvolutionBackward0]
	2870546498800 -> 2870546498704
	2870546498848 -> 2870546498704
	2870540398096 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2870540398096 -> 2870546498848
	2870546498848 [label=AccumulateGrad]
	2870546498272 -> 2870546497984
	2870540398192 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2870540398192 -> 2870546498272
	2870546498272 [label=AccumulateGrad]
	2870546498224 -> 2870546497984
	2870540398288 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2870540398288 -> 2870546498224
	2870546498224 [label=AccumulateGrad]
	2870546497840 -> 2870553853808
	2870540399824 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2870540399824 -> 2870546497840
	2870546497840 [label=AccumulateGrad]
	2870546497648 -> 2870553853904
	2870540399920 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2870540399920 -> 2870546497648
	2870546497648 [label=AccumulateGrad]
	2870546497600 -> 2870553853904
	2870540400016 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2870540400016 -> 2870546497600
	2870546497600 [label=AccumulateGrad]
	2870553853712 -> 2870553853568
	2870540400400 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2870540400400 -> 2870553853712
	2870553853712 [label=AccumulateGrad]
	2870553853520 -> 2870553853424
	2870540400496 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2870540400496 -> 2870553853520
	2870553853520 [label=AccumulateGrad]
	2870553853472 -> 2870553853424
	2870540400592 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2870540400592 -> 2870553853472
	2870553853472 [label=AccumulateGrad]
	2870553853376 -> 2870553853328
	2870553853136 -> 2870553852992
	2870540401552 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2870540401552 -> 2870553853136
	2870553853136 [label=AccumulateGrad]
	2870553852944 -> 2870553852896
	2870540401648 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2870540401648 -> 2870553852944
	2870553852944 [label=AccumulateGrad]
	2870553852800 -> 2870553852896
	2870540401744 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2870540401744 -> 2870553852800
	2870553852800 [label=AccumulateGrad]
	2870553852704 -> 2870553852560
	2870540402128 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2870540402128 -> 2870553852704
	2870553852704 [label=AccumulateGrad]
	2870553852512 -> 2870553852416
	2870540402224 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2870540402224 -> 2870553852512
	2870553852512 [label=AccumulateGrad]
	2870553852464 -> 2870553852416
	2870540402320 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2870540402320 -> 2870553852464
	2870553852464 [label=AccumulateGrad]
	2870553852368 -> 2870553852320
	2870553852368 [label=CudnnBatchNormBackward0]
	2870553853088 -> 2870553852368
	2870553853088 [label=ConvolutionBackward0]
	2870553853184 -> 2870553853088
	2870553853232 -> 2870553853088
	2870540400976 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2870540400976 -> 2870553853232
	2870553853232 [label=AccumulateGrad]
	2870553852656 -> 2870553852368
	2870540401072 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2870540401072 -> 2870553852656
	2870553852656 [label=AccumulateGrad]
	2870553852608 -> 2870553852368
	2870540401168 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2870540401168 -> 2870553852608
	2870553852608 [label=AccumulateGrad]
	2870553850400 -> 2870553852176
	2870547185744 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2870547185744 -> 2870553850400
	2870553850400 [label=AccumulateGrad]
	2870553852128 -> 2870553852080
	2870547185840 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2870547185840 -> 2870553852128
	2870553852128 [label=AccumulateGrad]
	2870553851984 -> 2870553852080
	2870547185936 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2870547185936 -> 2870553851984
	2870553851984 [label=AccumulateGrad]
	2870553851888 -> 2870553851744
	2870547186320 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2870547186320 -> 2870553851888
	2870553851888 [label=AccumulateGrad]
	2870553851696 -> 2870553851600
	2870547186416 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2870547186416 -> 2870553851696
	2870553851696 [label=AccumulateGrad]
	2870553851648 -> 2870553851600
	2870547186512 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2870547186512 -> 2870553851648
	2870553851648 [label=AccumulateGrad]
	2870553851552 -> 2870553851504
	2870553851216 -> 2870553850976
	2870553851216 [label=TBackward0]
	2870553851456 -> 2870553851216
	2870547187280 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2870547187280 -> 2870553851456
	2870553851456 [label=AccumulateGrad]
	2870553850976 -> 2870560596592
}
