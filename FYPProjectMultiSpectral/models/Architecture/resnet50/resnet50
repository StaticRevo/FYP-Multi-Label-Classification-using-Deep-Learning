digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2326608156944 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2326612777168 [label=AddmmBackward0]
	2326612774096 -> 2326612777168
	2326608574704 [label="fc.bias
 (19)" fillcolor=lightblue]
	2326608574704 -> 2326612774096
	2326612774096 [label=AccumulateGrad]
	2326612779232 -> 2326612777168
	2326612779232 [label=ViewBackward0]
	2326612777264 -> 2326612779232
	2326612777264 [label=MeanBackward1]
	2326612773472 -> 2326612777264
	2326612773472 [label=ReluBackward0]
	2326612777888 -> 2326612773472
	2326612777888 [label=AddBackward0]
	2326612779856 -> 2326612777888
	2326612779856 [label=CudnnBatchNormBackward0]
	2326612781296 -> 2326612779856
	2326612781296 [label=ConvolutionBackward0]
	2326612777216 -> 2326612781296
	2326612777216 [label=ReluBackward0]
	2326612785088 -> 2326612777216
	2326612785088 [label=CudnnBatchNormBackward0]
	2326612780816 -> 2326612785088
	2326612780816 [label=ConvolutionBackward0]
	2326611662240 -> 2326612780816
	2326611662240 [label=ReluBackward0]
	2326611664832 -> 2326611662240
	2326611664832 [label=CudnnBatchNormBackward0]
	2326611660752 -> 2326611664832
	2326611660752 [label=ConvolutionBackward0]
	2326612775008 -> 2326611660752
	2326612775008 [label=ReluBackward0]
	2326611665696 -> 2326612775008
	2326611665696 [label=AddBackward0]
	2326611657008 -> 2326611665696
	2326611657008 [label=CudnnBatchNormBackward0]
	2326611663104 -> 2326611657008
	2326611663104 [label=ConvolutionBackward0]
	2326611660320 -> 2326611663104
	2326611660320 [label=ReluBackward0]
	2326611657344 -> 2326611660320
	2326611657344 [label=CudnnBatchNormBackward0]
	2326611663488 -> 2326611657344
	2326611663488 [label=ConvolutionBackward0]
	2326611668384 -> 2326611663488
	2326611668384 [label=ReluBackward0]
	2326611661520 -> 2326611668384
	2326611661520 [label=CudnnBatchNormBackward0]
	2326611657920 -> 2326611661520
	2326611657920 [label=ConvolutionBackward0]
	2326611659888 -> 2326611657920
	2326611659888 [label=ReluBackward0]
	2326611667184 -> 2326611659888
	2326611667184 [label=AddBackward0]
	2326611664400 -> 2326611667184
	2326611664400 [label=CudnnBatchNormBackward0]
	2326611669152 -> 2326611664400
	2326611669152 [label=ConvolutionBackward0]
	2326611660608 -> 2326611669152
	2326611660608 [label=ReluBackward0]
	2326611666512 -> 2326611660608
	2326611666512 [label=CudnnBatchNormBackward0]
	2326611665792 -> 2326611666512
	2326611665792 [label=ConvolutionBackward0]
	2326611659648 -> 2326611665792
	2326611659648 [label=ReluBackward0]
	2326611664448 -> 2326611659648
	2326611664448 [label=CudnnBatchNormBackward0]
	2326611668240 -> 2326611664448
	2326611668240 [label=ConvolutionBackward0]
	2326611664064 -> 2326611668240
	2326611664064 [label=ReluBackward0]
	2326611657440 -> 2326611664064
	2326611657440 [label=AddBackward0]
	2326611668048 -> 2326611657440
	2326611668048 [label=CudnnBatchNormBackward0]
	2326611661376 -> 2326611668048
	2326611661376 [label=ConvolutionBackward0]
	2326611658016 -> 2326611661376
	2326611658016 [label=ReluBackward0]
	2326611669344 -> 2326611658016
	2326611669344 [label=CudnnBatchNormBackward0]
	2326611660176 -> 2326611669344
	2326611660176 [label=ConvolutionBackward0]
	2326611664208 -> 2326611660176
	2326611664208 [label=ReluBackward0]
	2326611668864 -> 2326611664208
	2326611668864 [label=CudnnBatchNormBackward0]
	2326611659408 -> 2326611668864
	2326611659408 [label=ConvolutionBackward0]
	2326611661328 -> 2326611659408
	2326611661328 [label=ReluBackward0]
	2326611666560 -> 2326611661328
	2326611666560 [label=AddBackward0]
	2326611666464 -> 2326611666560
	2326611666464 [label=CudnnBatchNormBackward0]
	2326611665168 -> 2326611666464
	2326611665168 [label=ConvolutionBackward0]
	2326611662864 -> 2326611665168
	2326611662864 [label=ReluBackward0]
	2326611667376 -> 2326611662864
	2326611667376 [label=CudnnBatchNormBackward0]
	2326611658832 -> 2326611667376
	2326611658832 [label=ConvolutionBackward0]
	2326611666224 -> 2326611658832
	2326611666224 [label=ReluBackward0]
	2326611658256 -> 2326611666224
	2326611658256 [label=CudnnBatchNormBackward0]
	2326611663872 -> 2326611658256
	2326611663872 [label=ConvolutionBackward0]
	2326611665600 -> 2326611663872
	2326611665600 [label=ReluBackward0]
	2326611666656 -> 2326611665600
	2326611666656 [label=AddBackward0]
	2326611667568 -> 2326611666656
	2326611667568 [label=CudnnBatchNormBackward0]
	2326611659600 -> 2326611667568
	2326611659600 [label=ConvolutionBackward0]
	2326611665072 -> 2326611659600
	2326611665072 [label=ReluBackward0]
	2326611659120 -> 2326611665072
	2326611659120 [label=CudnnBatchNormBackward0]
	2326611669440 -> 2326611659120
	2326611669440 [label=ConvolutionBackward0]
	2326611664304 -> 2326611669440
	2326611664304 [label=ReluBackward0]
	2326611668720 -> 2326611664304
	2326611668720 [label=CudnnBatchNormBackward0]
	2326611665360 -> 2326611668720
	2326611665360 [label=ConvolutionBackward0]
	2326611664736 -> 2326611665360
	2326611664736 [label=ReluBackward0]
	2326611668816 -> 2326611664736
	2326611668816 [label=AddBackward0]
	2326611661424 -> 2326611668816
	2326611661424 [label=CudnnBatchNormBackward0]
	2326611665504 -> 2326611661424
	2326611665504 [label=ConvolutionBackward0]
	2326611669632 -> 2326611665504
	2326611669632 [label=ReluBackward0]
	2326611669776 -> 2326611669632
	2326611669776 [label=CudnnBatchNormBackward0]
	2326611669872 -> 2326611669776
	2326611669872 [label=ConvolutionBackward0]
	2326611670064 -> 2326611669872
	2326611670064 [label=ReluBackward0]
	2326611670208 -> 2326611670064
	2326611670208 [label=CudnnBatchNormBackward0]
	2326611670304 -> 2326611670208
	2326611670304 [label=ConvolutionBackward0]
	2326611658640 -> 2326611670304
	2326611658640 [label=ReluBackward0]
	2326611670592 -> 2326611658640
	2326611670592 [label=AddBackward0]
	2326611670688 -> 2326611670592
	2326611670688 [label=CudnnBatchNormBackward0]
	2326611670832 -> 2326611670688
	2326611670832 [label=ConvolutionBackward0]
	2326611671024 -> 2326611670832
	2326611671024 [label=ReluBackward0]
	2326611671168 -> 2326611671024
	2326611671168 [label=CudnnBatchNormBackward0]
	2326611671264 -> 2326611671168
	2326611671264 [label=ConvolutionBackward0]
	2326611671456 -> 2326611671264
	2326611671456 [label=ReluBackward0]
	2326611671600 -> 2326611671456
	2326611671600 [label=CudnnBatchNormBackward0]
	2326611671696 -> 2326611671600
	2326611671696 [label=ConvolutionBackward0]
	2326611670640 -> 2326611671696
	2326611670640 [label=ReluBackward0]
	2326611671984 -> 2326611670640
	2326611671984 [label=AddBackward0]
	2326611672080 -> 2326611671984
	2326611672080 [label=CudnnBatchNormBackward0]
	2326611672224 -> 2326611672080
	2326611672224 [label=ConvolutionBackward0]
	2326611672416 -> 2326611672224
	2326611672416 [label=ReluBackward0]
	2326611672560 -> 2326611672416
	2326611672560 [label=CudnnBatchNormBackward0]
	2326611672656 -> 2326611672560
	2326611672656 [label=ConvolutionBackward0]
	2326611672848 -> 2326611672656
	2326611672848 [label=ReluBackward0]
	2326611672992 -> 2326611672848
	2326611672992 [label=CudnnBatchNormBackward0]
	2326611673040 -> 2326611672992
	2326611673040 [label=ConvolutionBackward0]
	2326605889792 -> 2326611673040
	2326605889792 [label=ReluBackward0]
	2326605889936 -> 2326605889792
	2326605889936 [label=AddBackward0]
	2326605890032 -> 2326605889936
	2326605890032 [label=CudnnBatchNormBackward0]
	2326605890176 -> 2326605890032
	2326605890176 [label=ConvolutionBackward0]
	2326605890368 -> 2326605890176
	2326605890368 [label=ReluBackward0]
	2326605890512 -> 2326605890368
	2326605890512 [label=CudnnBatchNormBackward0]
	2326605890608 -> 2326605890512
	2326605890608 [label=ConvolutionBackward0]
	2326605890800 -> 2326605890608
	2326605890800 [label=ReluBackward0]
	2326605890944 -> 2326605890800
	2326605890944 [label=CudnnBatchNormBackward0]
	2326605891040 -> 2326605890944
	2326605891040 [label=ConvolutionBackward0]
	2326605889984 -> 2326605891040
	2326605889984 [label=ReluBackward0]
	2326605891328 -> 2326605889984
	2326605891328 [label=AddBackward0]
	2326605891424 -> 2326605891328
	2326605891424 [label=CudnnBatchNormBackward0]
	2326605891568 -> 2326605891424
	2326605891568 [label=ConvolutionBackward0]
	2326605891760 -> 2326605891568
	2326605891760 [label=ReluBackward0]
	2326605891904 -> 2326605891760
	2326605891904 [label=CudnnBatchNormBackward0]
	2326605892000 -> 2326605891904
	2326605892000 [label=ConvolutionBackward0]
	2326605892192 -> 2326605892000
	2326605892192 [label=ReluBackward0]
	2326605892336 -> 2326605892192
	2326605892336 [label=CudnnBatchNormBackward0]
	2326605892432 -> 2326605892336
	2326605892432 [label=ConvolutionBackward0]
	2326605891376 -> 2326605892432
	2326605891376 [label=ReluBackward0]
	2326605892720 -> 2326605891376
	2326605892720 [label=AddBackward0]
	2326605892816 -> 2326605892720
	2326605892816 [label=CudnnBatchNormBackward0]
	2326605892960 -> 2326605892816
	2326605892960 [label=ConvolutionBackward0]
	2326605893152 -> 2326605892960
	2326605893152 [label=ReluBackward0]
	2326605893296 -> 2326605893152
	2326605893296 [label=CudnnBatchNormBackward0]
	2326605893392 -> 2326605893296
	2326605893392 [label=ConvolutionBackward0]
	2326605893584 -> 2326605893392
	2326605893584 [label=ReluBackward0]
	2326605893728 -> 2326605893584
	2326605893728 [label=CudnnBatchNormBackward0]
	2326605893824 -> 2326605893728
	2326605893824 [label=ConvolutionBackward0]
	2326605892768 -> 2326605893824
	2326605892768 [label=ReluBackward0]
	2326605894112 -> 2326605892768
	2326605894112 [label=AddBackward0]
	2326605894208 -> 2326605894112
	2326605894208 [label=CudnnBatchNormBackward0]
	2326605894352 -> 2326605894208
	2326605894352 [label=ConvolutionBackward0]
	2326605894544 -> 2326605894352
	2326605894544 [label=ReluBackward0]
	2326605894688 -> 2326605894544
	2326605894688 [label=CudnnBatchNormBackward0]
	2326605894784 -> 2326605894688
	2326605894784 [label=ConvolutionBackward0]
	2326605894976 -> 2326605894784
	2326605894976 [label=ReluBackward0]
	2326605895120 -> 2326605894976
	2326605895120 [label=CudnnBatchNormBackward0]
	2326605895216 -> 2326605895120
	2326605895216 [label=ConvolutionBackward0]
	2326605895408 -> 2326605895216
	2326605895408 [label=ReluBackward0]
	2326605895552 -> 2326605895408
	2326605895552 [label=AddBackward0]
	2326605895648 -> 2326605895552
	2326605895648 [label=CudnnBatchNormBackward0]
	2326605895792 -> 2326605895648
	2326605895792 [label=ConvolutionBackward0]
	2326605895984 -> 2326605895792
	2326605895984 [label=ReluBackward0]
	2326605896128 -> 2326605895984
	2326605896128 [label=CudnnBatchNormBackward0]
	2326605896224 -> 2326605896128
	2326605896224 [label=ConvolutionBackward0]
	2326605896416 -> 2326605896224
	2326605896416 [label=ReluBackward0]
	2326605896560 -> 2326605896416
	2326605896560 [label=CudnnBatchNormBackward0]
	2326605896656 -> 2326605896560
	2326605896656 [label=ConvolutionBackward0]
	2326605895600 -> 2326605896656
	2326605895600 [label=ReluBackward0]
	2326605896944 -> 2326605895600
	2326605896944 [label=AddBackward0]
	2326605897040 -> 2326605896944
	2326605897040 [label=CudnnBatchNormBackward0]
	2326605897184 -> 2326605897040
	2326605897184 [label=ConvolutionBackward0]
	2326605897376 -> 2326605897184
	2326605897376 [label=ReluBackward0]
	2326605897520 -> 2326605897376
	2326605897520 [label=CudnnBatchNormBackward0]
	2326605897616 -> 2326605897520
	2326605897616 [label=ConvolutionBackward0]
	2326605897808 -> 2326605897616
	2326605897808 [label=ReluBackward0]
	2326605897952 -> 2326605897808
	2326605897952 [label=CudnnBatchNormBackward0]
	2326605898048 -> 2326605897952
	2326605898048 [label=ConvolutionBackward0]
	2326605896992 -> 2326605898048
	2326605896992 [label=ReluBackward0]
	2326605898336 -> 2326605896992
	2326605898336 [label=AddBackward0]
	2326605898432 -> 2326605898336
	2326605898432 [label=CudnnBatchNormBackward0]
	2326605898576 -> 2326605898432
	2326605898576 [label=ConvolutionBackward0]
	2326605898768 -> 2326605898576
	2326605898768 [label=ReluBackward0]
	2326605898912 -> 2326605898768
	2326605898912 [label=CudnnBatchNormBackward0]
	2326605899008 -> 2326605898912
	2326605899008 [label=ConvolutionBackward0]
	2326605899200 -> 2326605899008
	2326605899200 [label=ReluBackward0]
	2326605899344 -> 2326605899200
	2326605899344 [label=CudnnBatchNormBackward0]
	2326605899440 -> 2326605899344
	2326605899440 [label=ConvolutionBackward0]
	2326605899632 -> 2326605899440
	2326605899632 [label=MaxPool2DWithIndicesBackward0]
	2326605899776 -> 2326605899632
	2326605899776 [label=ReluBackward0]
	2326605899872 -> 2326605899776
	2326605899872 [label=CudnnBatchNormBackward0]
	2326605899968 -> 2326605899872
	2326605899968 [label=ConvolutionBackward0]
	2326605900160 -> 2326605899968
	2326608574512 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2326608574512 -> 2326605900160
	2326605900160 [label=AccumulateGrad]
	2326605899920 -> 2326605899872
	2326610774832 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2326610774832 -> 2326605899920
	2326605899920 [label=AccumulateGrad]
	2326605899680 -> 2326605899872
	2326610774544 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2326610774544 -> 2326605899680
	2326605899680 [label=AccumulateGrad]
	2326605899584 -> 2326605899440
	2326610774448 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2326610774448 -> 2326605899584
	2326605899584 [label=AccumulateGrad]
	2326605899392 -> 2326605899344
	2326610774352 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2326610774352 -> 2326605899392
	2326605899392 [label=AccumulateGrad]
	2326605899248 -> 2326605899344
	2326610773584 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2326610773584 -> 2326605899248
	2326605899248 [label=AccumulateGrad]
	2326605899152 -> 2326605899008
	2326610773680 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2326610773680 -> 2326605899152
	2326605899152 [label=AccumulateGrad]
	2326605898960 -> 2326605898912
	2326610773776 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2326610773776 -> 2326605898960
	2326605898960 [label=AccumulateGrad]
	2326605898816 -> 2326605898912
	2326610773872 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2326610773872 -> 2326605898816
	2326605898816 [label=AccumulateGrad]
	2326605898720 -> 2326605898576
	2326610774160 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2326610774160 -> 2326605898720
	2326605898720 [label=AccumulateGrad]
	2326605898528 -> 2326605898432
	2326610773200 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2326610773200 -> 2326605898528
	2326605898528 [label=AccumulateGrad]
	2326605898480 -> 2326605898432
	2326610773104 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2326610773104 -> 2326605898480
	2326605898480 [label=AccumulateGrad]
	2326605898384 -> 2326605898336
	2326605898384 [label=CudnnBatchNormBackward0]
	2326605899104 -> 2326605898384
	2326605899104 [label=ConvolutionBackward0]
	2326605899632 -> 2326605899104
	2326605899488 -> 2326605899104
	2326610775024 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2326610775024 -> 2326605899488
	2326605899488 [label=AccumulateGrad]
	2326605898672 -> 2326605898384
	2326610775120 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2326610775120 -> 2326605898672
	2326605898672 [label=AccumulateGrad]
	2326605898624 -> 2326605898384
	2326610775216 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2326610775216 -> 2326605898624
	2326605898624 [label=AccumulateGrad]
	2326605898240 -> 2326605898048
	2326610772432 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2326610772432 -> 2326605898240
	2326605898240 [label=AccumulateGrad]
	2326605898000 -> 2326605897952
	2326610772048 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2326610772048 -> 2326605898000
	2326605898000 [label=AccumulateGrad]
	2326605897856 -> 2326605897952
	2326610772336 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2326610772336 -> 2326605897856
	2326605897856 [label=AccumulateGrad]
	2326605897760 -> 2326605897616
	2326610773008 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2326610773008 -> 2326605897760
	2326605897760 [label=AccumulateGrad]
	2326605897568 -> 2326605897520
	2326610772912 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2326610772912 -> 2326605897568
	2326605897568 [label=AccumulateGrad]
	2326605897424 -> 2326605897520
	2326608809520 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2326608809520 -> 2326605897424
	2326605897424 [label=AccumulateGrad]
	2326605897328 -> 2326605897184
	2326608809904 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2326608809904 -> 2326605897328
	2326605897328 [label=AccumulateGrad]
	2326605897136 -> 2326605897040
	2326608810000 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2326608810000 -> 2326605897136
	2326605897136 [label=AccumulateGrad]
	2326605897088 -> 2326605897040
	2326608810096 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2326608810096 -> 2326605897088
	2326605897088 [label=AccumulateGrad]
	2326605896992 -> 2326605896944
	2326605896848 -> 2326605896656
	2326608810480 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2326608810480 -> 2326605896848
	2326605896848 [label=AccumulateGrad]
	2326605896608 -> 2326605896560
	2326608810576 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2326608810576 -> 2326605896608
	2326605896608 [label=AccumulateGrad]
	2326605896464 -> 2326605896560
	2326608810672 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2326608810672 -> 2326605896464
	2326605896464 [label=AccumulateGrad]
	2326605896368 -> 2326605896224
	2326608811056 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2326608811056 -> 2326605896368
	2326605896368 [label=AccumulateGrad]
	2326605896176 -> 2326605896128
	2326608811152 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2326608811152 -> 2326605896176
	2326605896176 [label=AccumulateGrad]
	2326605896032 -> 2326605896128
	2326608811248 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2326608811248 -> 2326605896032
	2326605896032 [label=AccumulateGrad]
	2326605895936 -> 2326605895792
	2326608811632 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2326608811632 -> 2326605895936
	2326605895936 [label=AccumulateGrad]
	2326605895744 -> 2326605895648
	2326608811728 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2326608811728 -> 2326605895744
	2326605895744 [label=AccumulateGrad]
	2326605895696 -> 2326605895648
	2326608811824 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2326608811824 -> 2326605895696
	2326605895696 [label=AccumulateGrad]
	2326605895600 -> 2326605895552
	2326605895360 -> 2326605895216
	2326608812784 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2326608812784 -> 2326605895360
	2326605895360 [label=AccumulateGrad]
	2326605895168 -> 2326605895120
	2326608812880 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2326608812880 -> 2326605895168
	2326605895168 [label=AccumulateGrad]
	2326605895024 -> 2326605895120
	2326608812976 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2326608812976 -> 2326605895024
	2326605895024 [label=AccumulateGrad]
	2326605894928 -> 2326605894784
	2326608813360 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2326608813360 -> 2326605894928
	2326605894928 [label=AccumulateGrad]
	2326605894736 -> 2326605894688
	2326608813456 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2326608813456 -> 2326605894736
	2326605894736 [label=AccumulateGrad]
	2326605894592 -> 2326605894688
	2326608813552 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2326608813552 -> 2326605894592
	2326605894592 [label=AccumulateGrad]
	2326605894496 -> 2326605894352
	2326608813936 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2326608813936 -> 2326605894496
	2326605894496 [label=AccumulateGrad]
	2326605894304 -> 2326605894208
	2326608814032 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2326608814032 -> 2326605894304
	2326605894304 [label=AccumulateGrad]
	2326605894256 -> 2326605894208
	2326608814128 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2326608814128 -> 2326605894256
	2326605894256 [label=AccumulateGrad]
	2326605894160 -> 2326605894112
	2326605894160 [label=CudnnBatchNormBackward0]
	2326605894880 -> 2326605894160
	2326605894880 [label=ConvolutionBackward0]
	2326605895408 -> 2326605894880
	2326605895264 -> 2326605894880
	2326608812208 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2326608812208 -> 2326605895264
	2326605895264 [label=AccumulateGrad]
	2326605894448 -> 2326605894160
	2326608812304 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2326608812304 -> 2326605894448
	2326605894448 [label=AccumulateGrad]
	2326605894400 -> 2326605894160
	2326608812400 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2326608812400 -> 2326605894400
	2326605894400 [label=AccumulateGrad]
	2326605894016 -> 2326605893824
	2326608814512 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2326608814512 -> 2326605894016
	2326605894016 [label=AccumulateGrad]
	2326605893776 -> 2326605893728
	2326608814608 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2326608814608 -> 2326605893776
	2326605893776 [label=AccumulateGrad]
	2326605893632 -> 2326605893728
	2326608814704 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2326608814704 -> 2326605893632
	2326605893632 [label=AccumulateGrad]
	2326605893536 -> 2326605893392
	2326608815088 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2326608815088 -> 2326605893536
	2326605893536 [label=AccumulateGrad]
	2326605893344 -> 2326605893296
	2326608815184 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2326608815184 -> 2326605893344
	2326605893344 [label=AccumulateGrad]
	2326605893200 -> 2326605893296
	2326608815280 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2326608815280 -> 2326605893200
	2326605893200 [label=AccumulateGrad]
	2326605893104 -> 2326605892960
	2326608815664 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2326608815664 -> 2326605893104
	2326605893104 [label=AccumulateGrad]
	2326605892912 -> 2326605892816
	2326608815760 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2326608815760 -> 2326605892912
	2326605892912 [label=AccumulateGrad]
	2326605892864 -> 2326605892816
	2326608815856 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2326608815856 -> 2326605892864
	2326605892864 [label=AccumulateGrad]
	2326605892768 -> 2326605892720
	2326605892624 -> 2326605892432
	2326608816240 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2326608816240 -> 2326605892624
	2326605892624 [label=AccumulateGrad]
	2326605892384 -> 2326605892336
	2326608816336 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2326608816336 -> 2326605892384
	2326605892384 [label=AccumulateGrad]
	2326605892240 -> 2326605892336
	2326608816432 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2326608816432 -> 2326605892240
	2326605892240 [label=AccumulateGrad]
	2326605892144 -> 2326605892000
	2326608816816 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2326608816816 -> 2326605892144
	2326605892144 [label=AccumulateGrad]
	2326605891952 -> 2326605891904
	2326608816912 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2326608816912 -> 2326605891952
	2326605891952 [label=AccumulateGrad]
	2326605891808 -> 2326605891904
	2326608817008 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2326608817008 -> 2326605891808
	2326605891808 [label=AccumulateGrad]
	2326605891712 -> 2326605891568
	2326608817392 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2326608817392 -> 2326605891712
	2326605891712 [label=AccumulateGrad]
	2326605891520 -> 2326605891424
	2326608817488 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2326608817488 -> 2326605891520
	2326605891520 [label=AccumulateGrad]
	2326605891472 -> 2326605891424
	2326608817584 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2326608817584 -> 2326605891472
	2326605891472 [label=AccumulateGrad]
	2326605891376 -> 2326605891328
	2326605891232 -> 2326605891040
	2326608817968 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2326608817968 -> 2326605891232
	2326605891232 [label=AccumulateGrad]
	2326605890992 -> 2326605890944
	2326608818064 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2326608818064 -> 2326605890992
	2326605890992 [label=AccumulateGrad]
	2326605890848 -> 2326605890944
	2326608818160 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2326608818160 -> 2326605890848
	2326605890848 [label=AccumulateGrad]
	2326605890752 -> 2326605890608
	2326608818544 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2326608818544 -> 2326605890752
	2326605890752 [label=AccumulateGrad]
	2326605890560 -> 2326605890512
	2326608818640 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2326608818640 -> 2326605890560
	2326605890560 [label=AccumulateGrad]
	2326605890416 -> 2326605890512
	2326608818736 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2326608818736 -> 2326605890416
	2326605890416 [label=AccumulateGrad]
	2326605890320 -> 2326605890176
	2326608819120 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2326608819120 -> 2326605890320
	2326605890320 [label=AccumulateGrad]
	2326605890128 -> 2326605890032
	2326608819216 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2326608819216 -> 2326605890128
	2326605890128 [label=AccumulateGrad]
	2326605890080 -> 2326605890032
	2326608819312 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2326608819312 -> 2326605890080
	2326605890080 [label=AccumulateGrad]
	2326605889984 -> 2326605889936
	2326605889744 -> 2326611673040
	2326608820272 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2326608820272 -> 2326605889744
	2326605889744 [label=AccumulateGrad]
	2326611672896 -> 2326611672992
	2326608820368 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2326608820368 -> 2326611672896
	2326611672896 [label=AccumulateGrad]
	2326605889600 -> 2326611672992
	2326608820464 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2326608820464 -> 2326605889600
	2326605889600 [label=AccumulateGrad]
	2326611672800 -> 2326611672656
	2326608820848 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2326608820848 -> 2326611672800
	2326611672800 [label=AccumulateGrad]
	2326611672608 -> 2326611672560
	2326608820944 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2326608820944 -> 2326611672608
	2326611672608 [label=AccumulateGrad]
	2326611672464 -> 2326611672560
	2326608821040 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2326608821040 -> 2326611672464
	2326611672464 [label=AccumulateGrad]
	2326611672368 -> 2326611672224
	2326608821424 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2326608821424 -> 2326611672368
	2326611672368 [label=AccumulateGrad]
	2326611672176 -> 2326611672080
	2326608821520 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2326608821520 -> 2326611672176
	2326611672176 [label=AccumulateGrad]
	2326611672128 -> 2326611672080
	2326608821616 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2326608821616 -> 2326611672128
	2326611672128 [label=AccumulateGrad]
	2326611672032 -> 2326611671984
	2326611672032 [label=CudnnBatchNormBackward0]
	2326611672752 -> 2326611672032
	2326611672752 [label=ConvolutionBackward0]
	2326605889792 -> 2326611672752
	2326611672944 -> 2326611672752
	2326608819696 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2326608819696 -> 2326611672944
	2326611672944 [label=AccumulateGrad]
	2326611672320 -> 2326611672032
	2326608819792 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2326608819792 -> 2326611672320
	2326611672320 [label=AccumulateGrad]
	2326611672272 -> 2326611672032
	2326608819888 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2326608819888 -> 2326611672272
	2326611672272 [label=AccumulateGrad]
	2326611671888 -> 2326611671696
	2326608822000 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2326608822000 -> 2326611671888
	2326611671888 [label=AccumulateGrad]
	2326611671648 -> 2326611671600
	2326608822096 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2326608822096 -> 2326611671648
	2326611671648 [label=AccumulateGrad]
	2326611671504 -> 2326611671600
	2326608822192 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2326608822192 -> 2326611671504
	2326611671504 [label=AccumulateGrad]
	2326611671408 -> 2326611671264
	2326608560496 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2326608560496 -> 2326611671408
	2326611671408 [label=AccumulateGrad]
	2326611671216 -> 2326611671168
	2326608560592 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2326608560592 -> 2326611671216
	2326611671216 [label=AccumulateGrad]
	2326611671072 -> 2326611671168
	2326608560688 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2326608560688 -> 2326611671072
	2326611671072 [label=AccumulateGrad]
	2326611670976 -> 2326611670832
	2326608561072 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2326608561072 -> 2326611670976
	2326611670976 [label=AccumulateGrad]
	2326611670784 -> 2326611670688
	2326608561168 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2326608561168 -> 2326611670784
	2326611670784 [label=AccumulateGrad]
	2326611670736 -> 2326611670688
	2326608561264 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2326608561264 -> 2326611670736
	2326611670736 [label=AccumulateGrad]
	2326611670640 -> 2326611670592
	2326611670496 -> 2326611670304
	2326608561648 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2326608561648 -> 2326611670496
	2326611670496 [label=AccumulateGrad]
	2326611670256 -> 2326611670208
	2326608561744 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2326608561744 -> 2326611670256
	2326611670256 [label=AccumulateGrad]
	2326611670112 -> 2326611670208
	2326608561840 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2326608561840 -> 2326611670112
	2326611670112 [label=AccumulateGrad]
	2326611670016 -> 2326611669872
	2326608562224 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2326608562224 -> 2326611670016
	2326611670016 [label=AccumulateGrad]
	2326611669824 -> 2326611669776
	2326608562320 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2326608562320 -> 2326611669824
	2326611669824 [label=AccumulateGrad]
	2326611669680 -> 2326611669776
	2326608562416 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2326608562416 -> 2326611669680
	2326611669680 [label=AccumulateGrad]
	2326611669584 -> 2326611665504
	2326608562800 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2326608562800 -> 2326611669584
	2326611669584 [label=AccumulateGrad]
	2326611658736 -> 2326611661424
	2326608562896 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2326608562896 -> 2326611658736
	2326611658736 [label=AccumulateGrad]
	2326611664352 -> 2326611661424
	2326608562992 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2326608562992 -> 2326611664352
	2326611664352 [label=AccumulateGrad]
	2326611658640 -> 2326611668816
	2326611667040 -> 2326611665360
	2326608563376 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2326608563376 -> 2326611667040
	2326611667040 [label=AccumulateGrad]
	2326611662288 -> 2326611668720
	2326608563472 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2326608563472 -> 2326611662288
	2326611662288 [label=AccumulateGrad]
	2326611663440 -> 2326611668720
	2326608563568 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2326608563568 -> 2326611663440
	2326611663440 [label=AccumulateGrad]
	2326611667712 -> 2326611669440
	2326608563952 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2326608563952 -> 2326611667712
	2326611667712 [label=AccumulateGrad]
	2326611659360 -> 2326611659120
	2326608564048 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2326608564048 -> 2326611659360
	2326611659360 [label=AccumulateGrad]
	2326611667424 -> 2326611659120
	2326608564144 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2326608564144 -> 2326611667424
	2326611667424 [label=AccumulateGrad]
	2326611663008 -> 2326611659600
	2326608564528 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2326608564528 -> 2326611663008
	2326611663008 [label=AccumulateGrad]
	2326611663200 -> 2326611667568
	2326608564624 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2326608564624 -> 2326611663200
	2326611663200 [label=AccumulateGrad]
	2326611660512 -> 2326611667568
	2326608564720 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2326608564720 -> 2326611660512
	2326611660512 [label=AccumulateGrad]
	2326611664736 -> 2326611666656
	2326611659312 -> 2326611663872
	2326608565104 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2326608565104 -> 2326611659312
	2326611659312 [label=AccumulateGrad]
	2326611668576 -> 2326611658256
	2326608565200 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2326608565200 -> 2326611668576
	2326611668576 [label=AccumulateGrad]
	2326611666032 -> 2326611658256
	2326608565296 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2326608565296 -> 2326611666032
	2326611666032 [label=AccumulateGrad]
	2326611668480 -> 2326611658832
	2326608565680 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2326608565680 -> 2326611668480
	2326611668480 [label=AccumulateGrad]
	2326611659744 -> 2326611667376
	2326608565776 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2326608565776 -> 2326611659744
	2326611659744 [label=AccumulateGrad]
	2326611662480 -> 2326611667376
	2326608565872 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2326608565872 -> 2326611662480
	2326611662480 [label=AccumulateGrad]
	2326611660272 -> 2326611665168
	2326608566256 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2326608566256 -> 2326611660272
	2326611660272 [label=AccumulateGrad]
	2326611659552 -> 2326611666464
	2326608566352 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2326608566352 -> 2326611659552
	2326611659552 [label=AccumulateGrad]
	2326611659456 -> 2326611666464
	2326608566448 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2326608566448 -> 2326611659456
	2326611659456 [label=AccumulateGrad]
	2326611665600 -> 2326611666560
	2326611660080 -> 2326611659408
	2326608566832 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2326608566832 -> 2326611660080
	2326611660080 [label=AccumulateGrad]
	2326611664592 -> 2326611668864
	2326608566928 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2326608566928 -> 2326611664592
	2326611664592 [label=AccumulateGrad]
	2326611666272 -> 2326611668864
	2326608567024 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2326608567024 -> 2326611666272
	2326611666272 [label=AccumulateGrad]
	2326611667520 -> 2326611660176
	2326608567408 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2326608567408 -> 2326611667520
	2326611667520 [label=AccumulateGrad]
	2326611665936 -> 2326611669344
	2326608567504 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2326608567504 -> 2326611665936
	2326611665936 [label=AccumulateGrad]
	2326611668432 -> 2326611669344
	2326608567600 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2326608567600 -> 2326611668432
	2326611668432 [label=AccumulateGrad]
	2326611669392 -> 2326611661376
	2326608567984 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2326608567984 -> 2326611669392
	2326611669392 [label=AccumulateGrad]
	2326611667136 -> 2326611668048
	2326608568080 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2326608568080 -> 2326611667136
	2326611667136 [label=AccumulateGrad]
	2326611669488 -> 2326611668048
	2326608568176 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2326608568176 -> 2326611669488
	2326611669488 [label=AccumulateGrad]
	2326611661328 -> 2326611657440
	2326611662720 -> 2326611668240
	2326608569136 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2326608569136 -> 2326611662720
	2326611662720 [label=AccumulateGrad]
	2326611660464 -> 2326611664448
	2326608569232 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2326608569232 -> 2326611660464
	2326611660464 [label=AccumulateGrad]
	2326611660896 -> 2326611664448
	2326608569328 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2326608569328 -> 2326611660896
	2326611660896 [label=AccumulateGrad]
	2326611658400 -> 2326611665792
	2326608569712 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2326608569712 -> 2326611658400
	2326611658400 [label=AccumulateGrad]
	2326611668624 -> 2326611666512
	2326608569808 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2326608569808 -> 2326611668624
	2326611668624 [label=AccumulateGrad]
	2326611660800 -> 2326611666512
	2326608569904 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2326608569904 -> 2326611660800
	2326611660800 [label=AccumulateGrad]
	2326611669200 -> 2326611669152
	2326608570288 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2326608570288 -> 2326611669200
	2326611669200 [label=AccumulateGrad]
	2326611666752 -> 2326611664400
	2326608570384 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2326608570384 -> 2326611666752
	2326611666752 [label=AccumulateGrad]
	2326611668672 -> 2326611664400
	2326608570480 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2326608570480 -> 2326611668672
	2326611668672 [label=AccumulateGrad]
	2326611666608 -> 2326611667184
	2326611666608 [label=CudnnBatchNormBackward0]
	2326611664688 -> 2326611666608
	2326611664688 [label=ConvolutionBackward0]
	2326611664064 -> 2326611664688
	2326611667616 -> 2326611664688
	2326608568560 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2326608568560 -> 2326611667616
	2326611667616 [label=AccumulateGrad]
	2326611666944 -> 2326611666608
	2326608568656 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2326608568656 -> 2326611666944
	2326611666944 [label=AccumulateGrad]
	2326611666704 -> 2326611666608
	2326608568752 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2326608568752 -> 2326611666704
	2326611666704 [label=AccumulateGrad]
	2326611664256 -> 2326611657920
	2326608570864 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2326608570864 -> 2326611664256
	2326611664256 [label=AccumulateGrad]
	2326611657680 -> 2326611661520
	2326608570960 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2326608570960 -> 2326611657680
	2326611657680 [label=AccumulateGrad]
	2326611658976 -> 2326611661520
	2326608571056 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2326608571056 -> 2326611658976
	2326611658976 [label=AccumulateGrad]
	2326611658304 -> 2326611663488
	2326608571440 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2326608571440 -> 2326611658304
	2326611658304 [label=AccumulateGrad]
	2326611664496 -> 2326611657344
	2326608571536 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2326608571536 -> 2326611664496
	2326611664496 [label=AccumulateGrad]
	2326611657824 -> 2326611657344
	2326608571632 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2326608571632 -> 2326611657824
	2326611657824 [label=AccumulateGrad]
	2326611657056 -> 2326611663104
	2326608572016 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2326608572016 -> 2326611657056
	2326611657056 [label=AccumulateGrad]
	2326611667760 -> 2326611657008
	2326608572112 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2326608572112 -> 2326611667760
	2326611667760 [label=AccumulateGrad]
	2326611669104 -> 2326611657008
	2326608572208 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2326608572208 -> 2326611669104
	2326611669104 [label=AccumulateGrad]
	2326611659888 -> 2326611665696
	2326611666800 -> 2326611660752
	2326608572592 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2326608572592 -> 2326611666800
	2326611666800 [label=AccumulateGrad]
	2326611657296 -> 2326611664832
	2326608572688 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2326608572688 -> 2326611657296
	2326611657296 [label=AccumulateGrad]
	2326611668192 -> 2326611664832
	2326608572784 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2326608572784 -> 2326611668192
	2326611668192 [label=AccumulateGrad]
	2326611661952 -> 2326612780816
	2326608573168 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2326608573168 -> 2326611661952
	2326611661952 [label=AccumulateGrad]
	2326612777552 -> 2326612785088
	2326608573264 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2326608573264 -> 2326612777552
	2326612777552 [label=AccumulateGrad]
	2326611662336 -> 2326612785088
	2326608573360 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2326608573360 -> 2326611662336
	2326611662336 [label=AccumulateGrad]
	2326612779136 -> 2326612781296
	2326608573744 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2326608573744 -> 2326612779136
	2326612779136 [label=AccumulateGrad]
	2326612780864 -> 2326612779856
	2326608573840 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2326608573840 -> 2326612780864
	2326612780864 [label=AccumulateGrad]
	2326612784080 -> 2326612779856
	2326608573936 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2326608573936 -> 2326612784080
	2326612784080 [label=AccumulateGrad]
	2326612775008 -> 2326612777888
	2326612774192 -> 2326612777168
	2326612774192 [label=TBackward0]
	2326612786288 -> 2326612774192
	2326608574608 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2326608574608 -> 2326612786288
	2326612786288 [label=AccumulateGrad]
	2326612777168 -> 2326608156944
}
