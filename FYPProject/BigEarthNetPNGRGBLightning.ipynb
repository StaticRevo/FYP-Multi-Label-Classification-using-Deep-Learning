{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Library modules\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platformâ€™s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch and Deep Learning Libaries\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "from torch.nn import functional as F  # Functional module for defining functions and loss functions.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Data handling and batching\n",
    "import torchvision  # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Image datasets and transformations.\n",
    "import torchvision.datasets as datasets  # Specific datasets for vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "from torchvision.datasets import MNIST, EuroSAT  # Standard datasets.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations.\n",
    "from torchvision.models import ResNet18_Weights  # ResNet-18 model with pretrained weights.\n",
    "from torchsummary import summary  # Model summary.\n",
    "import torchmetrics  # Model evaluation metrics.\n",
    "from torchmetrics import MeanMetric, Accuracy  # Accuracy metrics.\n",
    "from torchmetrics.classification import (\n",
    "    MultilabelF1Score, MultilabelRecall, MultilabelPrecision, MultilabelAccuracy\n",
    ")  # Classification metrics.\n",
    "from torchviz import make_dot  # Model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for class imbalance.\n",
    "from torchcam.methods import GradCAM  # Grad-CAM for model interpretability.\n",
    "from torchcam.utils import overlay_mask  # Overlay mask for visualizations.\n",
    "import pytorch_lightning as pl  # Training management.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, Callback  # Callbacks.\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Logger for TensorBoard.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation.\n",
    "from rasterio.enums import Resampling  # Resampling for raster resizing.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation, Analysis and Visualization Libraries\n",
    "import pandas as pd  # Data analysis and manipulation.\n",
    "import numpy as np  # Array operations and computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics.\n",
    "import matplotlib.pyplot as plt  # Static and interactive plotting.\n",
    "import seaborn as sns  # High-level interface for statistical graphics.\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm  # Progress bar for loops.\n",
    "from PIL import Image  # Image handling and manipulation.\n",
    "import ast  # Parsing Python code.\n",
    "import requests  # HTTP requests.\n",
    "import zstandard as zstd  # Compression and decompression.\n",
    "from collections import Counter  # Counting hashable objects.\n",
    "import certifi  # Certificates for HTTPS.\n",
    "import ssl  # Secure connections.\n",
    "import urllib.request  # URL handling.\n",
    "import kaggle  # Kaggle API for datasets.\n",
    "from IPython.display import Image  # Display images in notebooks.\n",
    "from pathlib import Path # File system path handling.\n",
    "from typing import Dict, List, Tuple  # Type hints.\n",
    "import sys  # System-specific parameters and functions.\n",
    "import time # Time access and conversions.\n",
    "import logging # Logging facility for Python.\n",
    "\n",
    "# Custom Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda (GPU: NVIDIA GeForce RTX 3050)\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42  \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Render plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device} {'(GPU: ' + torch.cuda.get_device_name(0) + ')' if device.type == 'cuda' else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    dataset_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%'\n",
    "    combined_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%\\CombinedRGBImagesJPG'\n",
    "    metadata_path: str =r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\metadata_50_percent.csv'\n",
    "    metadata_csv = pd.read_csv(metadata_path)\n",
    "    img_size: int = 120\n",
    "    img_mean, img_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    num_classes: int = 19\n",
    "    band_channels: int = 3 #13\n",
    "    valid_pct: float = 0.1\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 10\n",
    "    model_name: str = 'resnet18'\n",
    "    num_workers: int = 2 #os.cpu_count() // 2\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arable land': 0, 'Broad-leaved forest': 1, 'Land principally occupied by agriculture, with significant areas of natural vegetation': 2, 'Pastures': 3, 'Urban fabric': 4, 'Complex cultivation patterns': 5, 'Mixed forest': 6, 'Industrial or commercial units': 7, 'Coniferous forest': 8, 'Transitional woodland, shrub': 9, 'Natural grassland and sparsely vegetated areas': 10, 'Inland waters': 11, 'Marine waters': 12, 'Inland wetlands': 13, 'Moors, heathland and sclerophyllous vegetation': 14, 'Permanent crops': 15, 'Agro-forestry areas': 16, 'Beaches, dunes, sands': 17, 'Coastal wetlands': 18}\n",
      "{0: 'Arable land', 1: 'Broad-leaved forest', 2: 'Land principally occupied by agriculture, with significant areas of natural vegetation', 3: 'Pastures', 4: 'Urban fabric', 5: 'Complex cultivation patterns', 6: 'Mixed forest', 7: 'Industrial or commercial units', 8: 'Coniferous forest', 9: 'Transitional woodland, shrub', 10: 'Natural grassland and sparsely vegetated areas', 11: 'Inland waters', 12: 'Marine waters', 13: 'Inland wetlands', 14: 'Moors, heathland and sclerophyllous vegetation', 15: 'Permanent crops', 16: 'Agro-forestry areas', 17: 'Beaches, dunes, sands', 18: 'Coastal wetlands'}\n"
     ]
    }
   ],
   "source": [
    "# Check if the labels are strings and need to be converted\n",
    "if isinstance(DatasetConfig.metadata_csv['labels'].iloc[0], str):\n",
    "    DatasetConfig.metadata_csv['labels'] = DatasetConfig.metadata_csv['labels'].apply(ast.literal_eval)\n",
    "\n",
    "# Get unique class labels\n",
    "class_labels = DatasetConfig.metadata_csv['labels'].explode().unique()\n",
    "\n",
    "# Create a dictionary mapping class labels to indices\n",
    "class_labels_dict = {label: idx for idx, label in enumerate(class_labels)}\n",
    "\n",
    "# Create a reversed dictionary mapping indices to class labels\n",
    "reversed_class_labels_dict = {idx: label for label, idx in class_labels_dict.items()}\n",
    "\n",
    "print(class_labels_dict)\n",
    "print(reversed_class_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label: list, num_classes=DatasetConfig.num_classes):\n",
    "    target = torch.zeros(num_classes)\n",
    "    for l in label:\n",
    "        if l in class_labels_dict:\n",
    "            target[class_labels_dict[l]] = 1.0\n",
    "    return target\n",
    "\n",
    "def decode_target(\n",
    "    target: list,\n",
    "    text_labels: bool = False,\n",
    "    threshold: float = 0.4,\n",
    "    cls_labels: dict = None,\n",
    "):\n",
    "    result = []\n",
    "    for i, x in enumerate(target):\n",
    "        if x >= threshold:\n",
    "            if text_labels:\n",
    "                result.append(cls_labels[i] + \"(\" + str(i) + \")\")\n",
    "            else:\n",
    "                result.append(str(i))\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubset(Dataset):\n",
    "    \"\"\"\n",
    "    Parse raw data to form a Dataset of (X, y).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, df, root_dir, transforms=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "\n",
    "        self.metadata = pd.read_csv(DatasetConfig.metadata_path)\n",
    "        self.patch_to_labels = dict(zip(self.metadata['patch_id'], self.metadata['labels']))\n",
    "        self.image_paths = list(Path(root_dir).rglob(\"*.tif\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        img_id = row[\"patch_id\"]\n",
    "        img_path = self.root_dir + os.sep + str(img_id) + \".jpg\"\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        label = self.get_label(img_path)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return img, img_id\n",
    "    \n",
    "        return img, label\n",
    "    \n",
    "    def get_label(self, img_path):\n",
    "        img_path = Path(img_path) \n",
    "        patch_id = img_path.stem\n",
    "        labels = self.patch_to_labels.get(patch_id, None)\n",
    "        print(labels)\n",
    "        if labels is None:\n",
    "            return torch.zeros(DatasetConfig.num_classes)  # Return an all-zero tensor as a fallback\n",
    "    \n",
    "        # Convert the labels string to an actual list if needed\n",
    "        if isinstance(labels, str):\n",
    "            labels = ast.literal_eval(labels) \n",
    "    \n",
    "        encoded = encode_label(labels)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1195\n"
     ]
    }
   ],
   "source": [
    "train_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'train']\n",
    "\n",
    "train_dataset = BigEarthNetSubset(df=train_df, root_dir=DatasetConfig.combined_path, transforms=ModelConfig.train_transforms)\n",
    "print(f\"Dataset length: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Broad-leaved forest', 'Coniferous forest', 'Mixed forest', 'Transitional woodland, shrub']\n",
      "Image shape: <built-in method size of Tensor object at 0x000001FA3B02C170>, Label: tensor([0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image, label = train_dataset[5]\n",
    "print(f\"Image shape: {image.size}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BigEarthNetSubset object at 0x000001FA3B22D1D0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8268, -1.8268, -1.8439,  ..., -1.7925, -1.7925, -1.7925],\n",
       "         [-1.8268, -1.8268, -1.8439,  ..., -1.7925, -1.7925, -1.7925],\n",
       "         [-1.8439, -1.8439, -1.8610,  ..., -1.7925, -1.7925, -1.7925],\n",
       "         ...,\n",
       "         [-1.8953, -1.8953, -1.8953,  ..., -1.8953, -1.8782, -1.8782],\n",
       "         [-1.8953, -1.8953, -1.8953,  ..., -1.8953, -1.8782, -1.8782],\n",
       "         [-1.8953, -1.8953, -1.8953,  ..., -1.8953, -1.8782, -1.8782]],\n",
       "\n",
       "        [[-1.5455, -1.5455, -1.5630,  ..., -1.3529, -1.3529, -1.3529],\n",
       "         [-1.5455, -1.5455, -1.5630,  ..., -1.3529, -1.3529, -1.3529],\n",
       "         [-1.5455, -1.5455, -1.5630,  ..., -1.3529, -1.3529, -1.3529],\n",
       "         ...,\n",
       "         [-1.5805, -1.5805, -1.5980,  ..., -1.5455, -1.5280, -1.5280],\n",
       "         [-1.5630, -1.5630, -1.5805,  ..., -1.5455, -1.5280, -1.5280],\n",
       "         [-1.5630, -1.5630, -1.5805,  ..., -1.5455, -1.5280, -1.5280]],\n",
       "\n",
       "        [[-1.5953, -1.5953, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
       "         [-1.5953, -1.5953, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
       "         [-1.6127, -1.6127, -1.6302,  ..., -1.5604, -1.5604, -1.5604],\n",
       "         ...,\n",
       "         [-1.6302, -1.6302, -1.6127,  ..., -1.6476, -1.6302, -1.6302],\n",
       "         [-1.6302, -1.6302, -1.6127,  ..., -1.6476, -1.6302, -1.6302],\n",
       "         [-1.6302, -1.6302, -1.6127,  ..., -1.6476, -1.6302, -1.6302]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubsetDataModule(pl.LightningDataModule):    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_transform = ModelConfig.train_transforms\n",
    "        self.val_transform = ModelConfig.val_transforms\n",
    "        self.test_transform = ModelConfig.test_transforms\n",
    "\n",
    "        train_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'train']\n",
    "        val_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'validation']\n",
    "        test_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'test']\n",
    "\n",
    "        self.train_dataset = BigEarthNetSubset(df=train_df, root_dir=DatasetConfig.combined_path, transforms=self.train_transform)\n",
    "        self.val_dataset = BigEarthNetSubset(df=val_df, root_dir=DatasetConfig.combined_path, transforms=self.val_transform)\n",
    "        self.test_dataset = BigEarthNetSubset(df=test_df, root_dir=DatasetConfig.combined_path, transforms=self.test_transform)\n",
    "        print(f\"Number of samples in train set: {len(self.train_dataset)}, val set: {len(self.val_dataset)}, test set: {len(self.test_dataset)}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=ModelConfig.batch_size, num_workers=ModelConfig.num_workers, pin_memory=True, shuffle=True, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=ModelConfig.batch_size, num_workers=0, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=ModelConfig.batch_size, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 1195, val set: 606, test set: 551\n"
     ]
    }
   ],
   "source": [
    "data_module = BigEarthNetSubsetDataModule()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensors, *, mean, std):\n",
    "    for c in range(DatasetConfig.band_channels):\n",
    "        tensors[:, c, :, :].mul_(std[c]).add_(mean[c])\n",
    "\n",
    "    return torch.clamp(tensors, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples: 606\n"
     ]
    }
   ],
   "source": [
    "# Load the first batch\n",
    "validation_loader = data_module.val_dataloader()\n",
    "batch = next(iter(validation_loader))\n",
    "\n",
    "images = denormalize(batch[0], mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "\n",
    "plt.figure(figsize=(32, 32))\n",
    "\n",
    "grid_img = make_grid(images, nrow=8, padding=5, pad_value=1.0)\n",
    "\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "#plt.savefig(\"bigEarthNet.png\", bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
