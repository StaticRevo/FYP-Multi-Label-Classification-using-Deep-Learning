digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	3099866143376 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	3099566683424 [label=AddmmBackward0]
	3099566683088 -> 3099566683424
	3099882506096 [label="fc.bias
 (19)" fillcolor=lightblue]
	3099882506096 -> 3099566683088
	3099566683088 [label=AccumulateGrad]
	3099566682464 -> 3099566683424
	3099566682464 [label=ViewBackward0]
	3099566681984 -> 3099566682464
	3099566681984 [label=MeanBackward1]
	3099566681648 -> 3099566681984
	3099566681648 [label=ReluBackward0]
	3099566681168 -> 3099566681648
	3099566681168 [label=AddBackward0]
	3099566680688 -> 3099566681168
	3099566680688 [label=CudnnBatchNormBackward0]
	3099566679584 -> 3099566680688
	3099566679584 [label=ConvolutionBackward0]
	3099566678624 -> 3099566679584
	3099566678624 [label=ReluBackward0]
	3099566678288 -> 3099566678624
	3099566678288 [label=CudnnBatchNormBackward0]
	3099566677808 -> 3099566678288
	3099566677808 [label=ConvolutionBackward0]
	3099566680544 -> 3099566677808
	3099566680544 [label=ReluBackward0]
	3099566676368 -> 3099566680544
	3099566676368 [label=AddBackward0]
	3099566675888 -> 3099566676368
	3099566675888 [label=CudnnBatchNormBackward0]
	3099566674784 -> 3099566675888
	3099566674784 [label=ConvolutionBackward0]
	3099566672816 -> 3099566674784
	3099566672816 [label=ReluBackward0]
	3099566672432 -> 3099566672816
	3099566672432 [label=CudnnBatchNormBackward0]
	3099566671040 -> 3099566672432
	3099566671040 [label=ConvolutionBackward0]
	3099566685632 -> 3099566671040
	3099566685632 [label=ReluBackward0]
	3099566685680 -> 3099566685632
	3099566685680 [label=AddBackward0]
	3099566685392 -> 3099566685680
	3099566685392 [label=CudnnBatchNormBackward0]
	3099566685152 -> 3099566685392
	3099566685152 [label=ConvolutionBackward0]
	3099566684960 -> 3099566685152
	3099566684960 [label=ReluBackward0]
	3099566684576 -> 3099566684960
	3099566684576 [label=CudnnBatchNormBackward0]
	3099566684816 -> 3099566684576
	3099566684816 [label=ConvolutionBackward0]
	3099566685440 -> 3099566684816
	3099566685440 [label=ReluBackward0]
	3099566684096 -> 3099566685440
	3099566684096 [label=AddBackward0]
	3099566684336 -> 3099566684096
	3099566684336 [label=CudnnBatchNormBackward0]
	3099566684000 -> 3099566684336
	3099566684000 [label=ConvolutionBackward0]
	3099566683712 -> 3099566684000
	3099566683712 [label=ReluBackward0]
	3099566683760 -> 3099566683712
	3099566683760 [label=CudnnBatchNormBackward0]
	3099566683472 -> 3099566683760
	3099566683472 [label=ConvolutionBackward0]
	3099566683376 -> 3099566683472
	3099566683376 [label=ReluBackward0]
	3099566683040 -> 3099566683376
	3099566683040 [label=AddBackward0]
	3099566682848 -> 3099566683040
	3099566682848 [label=CudnnBatchNormBackward0]
	3099566682896 -> 3099566682848
	3099566682896 [label=ConvolutionBackward0]
	3099566682512 -> 3099566682896
	3099566682512 [label=ReluBackward0]
	3099566682272 -> 3099566682512
	3099566682272 [label=CudnnBatchNormBackward0]
	3099566682224 -> 3099566682272
	3099566682224 [label=ConvolutionBackward0]
	3099566682992 -> 3099566682224
	3099566682992 [label=ReluBackward0]
	3099566681792 -> 3099566682992
	3099566681792 [label=AddBackward0]
	3099566681744 -> 3099566681792
	3099566681744 [label=CudnnBatchNormBackward0]
	3099566681552 -> 3099566681744
	3099566681552 [label=ConvolutionBackward0]
	3099566681456 -> 3099566681552
	3099566681456 [label=ReluBackward0]
	3099566681120 -> 3099566681456
	3099566681120 [label=CudnnBatchNormBackward0]
	3099566680928 -> 3099566681120
	3099566680928 [label=ConvolutionBackward0]
	3099566680784 -> 3099566680928
	3099566680784 [label=ReluBackward0]
	3099566680592 -> 3099566680784
	3099566680592 [label=AddBackward0]
	3099566680256 -> 3099566680592
	3099566680256 [label=CudnnBatchNormBackward0]
	3099566680304 -> 3099566680256
	3099566680304 [label=ConvolutionBackward0]
	3099566679968 -> 3099566680304
	3099566679968 [label=ReluBackward0]
	3099566680016 -> 3099566679968
	3099566680016 [label=CudnnBatchNormBackward0]
	3099566679920 -> 3099566680016
	3099566679920 [label=ConvolutionBackward0]
	3099566680448 -> 3099566679920
	3099566680448 [label=ReluBackward0]
	3099566679536 -> 3099566680448
	3099566679536 [label=AddBackward0]
	3099566679440 -> 3099566679536
	3099566679440 [label=CudnnBatchNormBackward0]
	3099566679008 -> 3099566679440
	3099566679008 [label=ConvolutionBackward0]
	3099566678864 -> 3099566679008
	3099566678864 [label=ReluBackward0]
	3099566678672 -> 3099566678864
	3099566678672 [label=CudnnBatchNormBackward0]
	3099566678336 -> 3099566678672
	3099566678336 [label=ConvolutionBackward0]
	3099566679344 -> 3099566678336
	3099566679344 [label=MaxPool2DWithIndicesBackward0]
	3099566678192 -> 3099566679344
	3099566678192 [label=ReluBackward0]
	3099566677856 -> 3099566678192
	3099566677856 [label=CudnnBatchNormBackward0]
	3099566678096 -> 3099566677856
	3099566678096 [label=ConvolutionBackward0]
	3099566677712 -> 3099566678096
	3099882505904 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	3099882505904 -> 3099566677712
	3099566677712 [label=AccumulateGrad]
	3099566677952 -> 3099566677856
	3099740955632 [label="bn1.weight
 (64)" fillcolor=lightblue]
	3099740955632 -> 3099566677952
	3099566677952 [label=AccumulateGrad]
	3099566678576 -> 3099566677856
	3099740955344 [label="bn1.bias
 (64)" fillcolor=lightblue]
	3099740955344 -> 3099566678576
	3099566678576 [label=AccumulateGrad]
	3099566678480 -> 3099566678336
	3099740955824 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3099740955824 -> 3099566678480
	3099566678480 [label=AccumulateGrad]
	3099566678528 -> 3099566678672
	3099740955920 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	3099740955920 -> 3099566678528
	3099566678528 [label=AccumulateGrad]
	3099566678960 -> 3099566678672
	3099740956016 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	3099740956016 -> 3099566678960
	3099566678960 [label=AccumulateGrad]
	3099566679056 -> 3099566679008
	3099740955248 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3099740955248 -> 3099566679056
	3099566679056 [label=AccumulateGrad]
	3099566679152 -> 3099566679440
	3099740955152 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	3099740955152 -> 3099566679152
	3099566679152 [label=AccumulateGrad]
	3099566679200 -> 3099566679440
	3099740954384 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	3099740954384 -> 3099566679200
	3099566679200 [label=AccumulateGrad]
	3099566679344 -> 3099566679536
	3099566679296 -> 3099566679920
	3099740954480 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3099740954480 -> 3099566679296
	3099566679296 [label=AccumulateGrad]
	3099566679824 -> 3099566680016
	3099740954576 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	3099740954576 -> 3099566679824
	3099566679824 [label=AccumulateGrad]
	3099566679776 -> 3099566680016
	3099740954672 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	3099740954672 -> 3099566679776
	3099566679776 [label=AccumulateGrad]
	3099566680112 -> 3099566680304
	3099740954960 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3099740954960 -> 3099566680112
	3099566680112 [label=AccumulateGrad]
	3099566680496 -> 3099566680256
	3099740954000 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	3099740954000 -> 3099566680496
	3099566680496 [label=AccumulateGrad]
	3099566680352 -> 3099566680256
	3099740953904 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	3099740953904 -> 3099566680352
	3099566680352 [label=AccumulateGrad]
	3099566680448 -> 3099566680592
	3099566680976 -> 3099566680928
	3099740953808 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	3099740953808 -> 3099566680976
	3099566680976 [label=AccumulateGrad]
	3099566681072 -> 3099566681120
	3099740953712 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	3099740953712 -> 3099566681072
	3099566681072 [label=AccumulateGrad]
	3099566681264 -> 3099566681120
	3099882481392 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	3099882481392 -> 3099566681264
	3099566681264 [label=AccumulateGrad]
	3099566681312 -> 3099566681552
	3099882481776 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3099882481776 -> 3099566681312
	3099566681312 [label=AccumulateGrad]
	3099566681600 -> 3099566681744
	3099882481872 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	3099882481872 -> 3099566681600
	3099566681600 [label=AccumulateGrad]
	3099566681840 -> 3099566681744
	3099882481968 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	3099882481968 -> 3099566681840
	3099566681840 [label=AccumulateGrad]
	3099566681936 -> 3099566681792
	3099566681936 [label=CudnnBatchNormBackward0]
	3099566680832 -> 3099566681936
	3099566680832 [label=ConvolutionBackward0]
	3099566680784 -> 3099566680832
	3099566680880 -> 3099566680832
	3099740951216 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	3099740951216 -> 3099566680880
	3099566680880 [label=AccumulateGrad]
	3099566681216 -> 3099566681936
	3099740951408 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	3099740951408 -> 3099566681216
	3099566681216 [label=AccumulateGrad]
	3099566681408 -> 3099566681936
	3099740953040 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	3099740953040 -> 3099566681408
	3099566681408 [label=AccumulateGrad]
	3099566681888 -> 3099566682224
	3099882482352 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3099882482352 -> 3099566681888
	3099566681888 [label=AccumulateGrad]
	3099566682416 -> 3099566682272
	3099882482448 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	3099882482448 -> 3099566682416
	3099566682416 [label=AccumulateGrad]
	3099566682368 -> 3099566682272
	3099882482544 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	3099882482544 -> 3099566682368
	3099566682368 [label=AccumulateGrad]
	3099566682560 -> 3099566682896
	3099882482928 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3099882482928 -> 3099566682560
	3099566682560 [label=AccumulateGrad]
	3099566682752 -> 3099566682848
	3099882483024 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	3099882483024 -> 3099566682752
	3099566682752 [label=AccumulateGrad]
	3099566682656 -> 3099566682848
	3099882483120 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	3099882483120 -> 3099566682656
	3099566682656 [label=AccumulateGrad]
	3099566682992 -> 3099566683040
	3099566683232 -> 3099566683472
	3099882484080 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	3099882484080 -> 3099566683232
	3099566683232 [label=AccumulateGrad]
	3099566683520 -> 3099566683760
	3099882484176 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	3099882484176 -> 3099566683520
	3099566683520 [label=AccumulateGrad]
	3099566683856 -> 3099566683760
	3099882484272 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	3099882484272 -> 3099566683856
	3099566683856 [label=AccumulateGrad]
	3099566683616 -> 3099566684000
	3099882484656 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3099882484656 -> 3099566683616
	3099566683616 [label=AccumulateGrad]
	3099566684240 -> 3099566684336
	3099882484752 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	3099882484752 -> 3099566684240
	3099566684240 [label=AccumulateGrad]
	3099566684144 -> 3099566684336
	3099882484848 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	3099882484848 -> 3099566684144
	3099566684144 [label=AccumulateGrad]
	3099566684192 -> 3099566684096
	3099566684192 [label=CudnnBatchNormBackward0]
	3099566683136 -> 3099566684192
	3099566683136 [label=ConvolutionBackward0]
	3099566683376 -> 3099566683136
	3099566683184 -> 3099566683136
	3099882483504 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	3099882483504 -> 3099566683184
	3099566683184 [label=AccumulateGrad]
	3099566683808 -> 3099566684192
	3099882483600 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	3099882483600 -> 3099566683808
	3099566683808 [label=AccumulateGrad]
	3099566683952 -> 3099566684192
	3099882483696 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	3099882483696 -> 3099566683952
	3099566683952 [label=AccumulateGrad]
	3099566684432 -> 3099566684816
	3099882485232 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3099882485232 -> 3099566684432
	3099566684432 [label=AccumulateGrad]
	3099566684672 -> 3099566684576
	3099882485328 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	3099882485328 -> 3099566684672
	3099566684672 [label=AccumulateGrad]
	3099566684912 -> 3099566684576
	3099882485424 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	3099882485424 -> 3099566684912
	3099566684912 [label=AccumulateGrad]
	3099566685200 -> 3099566685152
	3099882485808 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3099882485808 -> 3099566685200
	3099566685200 [label=AccumulateGrad]
	3099566685056 -> 3099566685392
	3099882485904 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	3099882485904 -> 3099566685056
	3099566685056 [label=AccumulateGrad]
	3099566685248 -> 3099566685392
	3099882486000 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	3099882486000 -> 3099566685248
	3099566685248 [label=AccumulateGrad]
	3099566685440 -> 3099566685680
	3099566685536 -> 3099566671040
	3099882486960 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	3099882486960 -> 3099566685536
	3099566685536 [label=AccumulateGrad]
	3099566669984 -> 3099566672432
	3099882487056 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	3099882487056 -> 3099566669984
	3099566669984 [label=AccumulateGrad]
	3099566673968 -> 3099566672432
	3099882487152 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	3099882487152 -> 3099566673968
	3099566673968 [label=AccumulateGrad]
	3099566674448 -> 3099566674784
	3099882487536 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3099882487536 -> 3099566674448
	3099566674448 [label=AccumulateGrad]
	3099566675408 -> 3099566675888
	3099882487632 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	3099882487632 -> 3099566675408
	3099566675408 [label=AccumulateGrad]
	3099566675264 -> 3099566675888
	3099882487728 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	3099882487728 -> 3099566675264
	3099566675264 [label=AccumulateGrad]
	3099566675744 -> 3099566676368
	3099566675744 [label=CudnnBatchNormBackward0]
	3099566685728 -> 3099566675744
	3099566685728 [label=ConvolutionBackward0]
	3099566685632 -> 3099566685728
	3099566685776 -> 3099566685728
	3099882486384 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	3099882486384 -> 3099566685776
	3099566685776 [label=AccumulateGrad]
	3099566674304 -> 3099566675744
	3099882486480 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	3099882486480 -> 3099566674304
	3099566674304 [label=AccumulateGrad]
	3099566674928 -> 3099566675744
	3099882486576 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	3099882486576 -> 3099566674928
	3099566674928 [label=AccumulateGrad]
	3099566676848 -> 3099566677808
	3099882504560 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3099882504560 -> 3099566676848
	3099566676848 [label=AccumulateGrad]
	3099566677664 -> 3099566678288
	3099882504656 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	3099882504656 -> 3099566677664
	3099566677664 [label=AccumulateGrad]
	3099566678768 -> 3099566678288
	3099882504752 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	3099882504752 -> 3099566678768
	3099566678768 [label=AccumulateGrad]
	3099566679248 -> 3099566679584
	3099882505136 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3099882505136 -> 3099566679248
	3099566679248 [label=AccumulateGrad]
	3099566680208 -> 3099566680688
	3099882505232 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	3099882505232 -> 3099566680208
	3099566680208 [label=AccumulateGrad]
	3099566680064 -> 3099566680688
	3099882505328 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	3099882505328 -> 3099566680064
	3099566680064 [label=AccumulateGrad]
	3099566680544 -> 3099566681168
	3099566682608 -> 3099566683424
	3099566682608 [label=TBackward0]
	3099566681024 -> 3099566682608
	3099882506000 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	3099882506000 -> 3099566681024
	3099566681024 [label=AccumulateGrad]
	3099566683424 -> 3099866143376
}
