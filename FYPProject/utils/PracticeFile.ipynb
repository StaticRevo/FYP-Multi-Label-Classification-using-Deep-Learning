{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zstandard as zstd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\OnePBigEarthNetCopy'\n",
    "subset_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets'\n",
    "metadata_file = r'C:\\Users\\isaac\\Downloads\\metadata.parquet'\n",
    "unwanted_metadata_file = r'C:\\Users\\isaac\\Downloads\\metadata_for_patches_with_snow_cloud_or_shadow.parquet'\n",
    "    \n",
    "metadata_df = pd.read_parquet(metadata_file)\n",
    "snow_cloud_metadata_df = pd.read_parquet(unwanted_metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeTiffFiles(input_tiff, output_tiff, new_width, new_height):\n",
    "    with rasterio.open(input_tiff) as src:\n",
    "        transform, width, height = calculate_default_transform(src.crs, src.crs, new_width, new_height, *src.bounds)\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': src.crs,\n",
    "            'transform': transform,\n",
    "            'width': new_width,\n",
    "            'height': new_height\n",
    "        })\n",
    "        with rasterio.open(output_tiff, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=src.crs,\n",
    "                    resampling=Resampling.nearest\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tiff_file(input_tiff, output_tiff, new_width=120, new_height=120):\n",
    "    \"\"\"Resize a single TIFF file.\"\"\"\n",
    "    with rasterio.open(input_tiff) as src:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, src.crs, new_width, new_height, *src.bounds)\n",
    "        \n",
    "        # Update metadata for new width and height\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': src.crs,\n",
    "            'transform': transform,\n",
    "            'width': new_width,\n",
    "            'height': new_height\n",
    "        })\n",
    "        \n",
    "        # Write directly to the output file\n",
    "        with rasterio.open(output_tiff, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=src.crs,\n",
    "                    resampling=Resampling.nearest\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubsets(dataset_dir, subsets, metadata_df, subset_dir):\n",
    "    metadata_50 = pd.DataFrame(columns=metadata_df.columns)\n",
    "    metadata_10 = pd.DataFrame(columns=metadata_df.columns)\n",
    "    metadata_1 = pd.DataFrame(columns=metadata_df.columns)\n",
    "\n",
    "    for subset in subsets.values():\n",
    "        if not os.path.exists(subset):\n",
    "            os.makedirs(subset)\n",
    "            print(f\"Created subset: {subset}\")\n",
    "        else:\n",
    "            print(f\"Subset already exists: {subset}\")\n",
    "    \n",
    "    for folder in tqdm(os.listdir(dataset_dir), desc='Creating subsets'):\n",
    "        folder_path = os.path.join(dataset_dir, folder)\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            # List all subfolders\n",
    "            subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "\n",
    "            # Calculate number of folders in each subset\n",
    "            num_subfolders = len(subfolders)\n",
    "            num_50_percent = min(max(1, num_subfolders // 2), num_subfolders)\n",
    "            num_10_percent = min(max(1, num_subfolders // 10), num_subfolders)\n",
    "            num_1_percent = min(max(1, num_subfolders // 100), num_subfolders)\n",
    "\n",
    "            # Select random subfolders for each subset\n",
    "            selected_50 = random.sample(subfolders, num_50_percent)\n",
    "            selected_10 = random.sample(subfolders, num_10_percent)\n",
    "            selected_1 = random.sample(subfolders, num_1_percent)\n",
    "\n",
    "            # Filter metadata for each subset\n",
    "            metadata_50 = pd.concat([metadata_50, metadata_df[metadata_df['patch_id'].isin(selected_50)]])\n",
    "            metadata_10 = pd.concat([metadata_10, metadata_df[metadata_df['patch_id'].isin(selected_10)]])\n",
    "            metadata_1 = pd.concat([metadata_1, metadata_df[metadata_df['patch_id'].isin(selected_1)]])\n",
    "\n",
    "            # Copy selected subfolders to each subset directory\n",
    "            for selected in selected_50:\n",
    "                dest_path = os.path.join(subsets['50%'], folder, selected)\n",
    "                if not os.path.exists(dest_path):  # Check if the destination folder exists\n",
    "                    try:\n",
    "                        shutil.copytree(os.path.join(folder_path, selected), dest_path)\n",
    "                    except FileExistsError:\n",
    "                        print(f\"Directory already exists, skipping: {dest_path}\")\n",
    "\n",
    "            for selected in selected_10:\n",
    "                dest_path = os.path.join(subsets['10%'], folder, selected)\n",
    "                if not os.path.exists(dest_path):  # Check if the destination folder exists\n",
    "                    try:\n",
    "                        shutil.copytree(os.path.join(folder_path, selected), dest_path)\n",
    "                    except FileExistsError:\n",
    "                        print(f\"Directory already exists, skipping: {dest_path}\")\n",
    "            \n",
    "            for selected in selected_1:\n",
    "                dest_path = os.path.join(subsets['1%'], folder, selected)\n",
    "                if not os.path.exists(dest_path):  # Check if the destination folder exists\n",
    "                    try:\n",
    "                        shutil.copytree(os.path.join(folder_path, selected), dest_path)\n",
    "                    except FileExistsError:\n",
    "                        print(f\"Directory already exists, skipping: {dest_path}\")\n",
    "\n",
    "    # Save metadata for each subset\n",
    "    metadata_50.to_csv(os.path.join(subset_dir, 'metadata_50_percent.csv'), index=False)\n",
    "    metadata_10.to_csv(os.path.join(subset_dir, 'metadata_10_percent.csv'), index=False)\n",
    "    metadata_1.to_csv(os.path.join(subset_dir, 'metadata_1_percent.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_subfolders(base_dir, folder):\n",
    "    # Dictionary to hold folder counts\n",
    "    folder_counts = {}\n",
    "    total_subfolders = 0  # Initialize total subfolder counter\n",
    "    \n",
    "    # Iterate through all folders in the base directory\n",
    "    for folder in tqdm(os.listdir(base_dir), desc=\"Processing folders\"):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        \n",
    "        # Check if the current path is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Count subdirectories within this folder\n",
    "            subfolder_count = sum(os.path.isdir(os.path.join(folder_path, subfolder)) for subfolder in os.listdir(folder_path))\n",
    "            folder_counts[folder] = subfolder_count\n",
    "        \n",
    "            # Update total subfolder count\n",
    "            total_subfolders += subfolder_count\n",
    "\n",
    "    # Print total subfolders\n",
    "    return total_subfolders, folder\n",
    "\n",
    "# Function to calculate and display subfolder count and percentage\n",
    "def display_percentage(partial_count, full_count, folder_name):\n",
    "    percentage = (partial_count / full_count) * 100\n",
    "    print(f\"Folder: {folder_name} | Subfolder Count: {partial_count} | Percentage: {percentage:.2f}%\")\n",
    "\n",
    "def labels_to_binary_vector(labels, unique_labels):\n",
    "    binary_vector = [1 if label in labels else 0 for label in unique_labels]\n",
    "    return binary_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the BigEarthNet dataset\n",
    "def BigEarthNetDataPreprocessing(dataset_dir, subset_dir, metadata_df, snow_cloud_metadata_df):\n",
    "    # Stage 3: Create subsets of the dataset and filter metadata\n",
    "    subsets = {\n",
    "        '50%': os.path.join(subset_dir, '50%'),\n",
    "        '10%': os.path.join(subset_dir, '10%'),\n",
    "        '1%': os.path.join(subset_dir, '1%')\n",
    "    }\n",
    "    metadata_df = pd.read_parquet(metadata_file)\n",
    "\n",
    "    if os.path.exists(subsets['50%']) and os.path.exists(subsets['10%']) and os.path.exists(subsets['1%']):\n",
    "        print('Subsets already exist')\n",
    "    else:\n",
    "        createSubsets(dataset_dir, subsets, metadata_df, subset_dir)\n",
    "\n",
    "    full_subfolder_count, folder = count_subfolders(dataset_dir, '100%BigEarthNet')\n",
    "    half_subfolder_count, folder = count_subfolders(subsets['50%'], '50%BigEarthNet' )\n",
    "    tenth_subfolder_count, folder = count_subfolders(subsets['10%'], '10%BigEarthNet' )\n",
    "    hundredth_subfolder_count, folder = count_subfolders(subsets['1%'], '1%BigEarthNet')\n",
    "\n",
    "    # Display the counts and percentages for each folder\n",
    "    print(f\"Total subfolder count in full dataset: {full_subfolder_count}\\n\")\n",
    "    display_percentage(half_subfolder_count, full_subfolder_count, '50%BigEarthNet')\n",
    "    display_percentage(tenth_subfolder_count, full_subfolder_count, '10%BigEarthNet')\n",
    "    display_percentage(hundredth_subfolder_count, full_subfolder_count, '1%BigEarthNet')\n",
    "\n",
    "    # Stage 4: Add a binary vector to the metadata files to indicate the presence of a specific land cover class\n",
    "    unique_labels = metadata_df['labels'].explode().unique()\n",
    "    print(unique_labels)\n",
    "\n",
    "    metadata_df['binary_vector'] = metadata_df['labels'].apply(lambda x: labels_to_binary_vector(x, unique_labels))\n",
    "\n",
    "    print(metadata_df.columns, \"\\n\")\n",
    "\n",
    "    # Save the updated DataFrame to a new Parquet file\n",
    "    updated_metadata_file = r'C:\\Users\\isaac\\Downloads\\updated_metadata.parquet'\n",
    "    metadata_df.to_parquet(updated_metadata_file)\n",
    "\n",
    "    updated_metadata_df = pd.read_parquet(updated_metadata_file)\n",
    "    print(updated_metadata_df.columns, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created subset: C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%\n",
      "Created subset: C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\10%\n",
      "Created subset: C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating subsets: 100%|██████████| 115/115 [02:48<00:00,  1.46s/it]\n",
      "Processing folders: 100%|██████████| 115/115 [00:00<00:00, 265.85it/s]\n",
      "Processing folders: 100%|██████████| 115/115 [00:00<00:00, 462.62it/s]\n",
      "Processing folders: 100%|██████████| 115/115 [00:00<00:00, 1922.18it/s]\n",
      "Processing folders: 100%|██████████| 115/115 [00:00<00:00, 5148.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subfolder count in full dataset: 4750\n",
      "\n",
      "Folder: 50%BigEarthNet | Subfolder Count: 2352 | Percentage: 49.52%\n",
      "Folder: 10%BigEarthNet | Subfolder Count: 441 | Percentage: 9.28%\n",
      "Folder: 1%BigEarthNet | Subfolder Count: 115 | Percentage: 2.42%\n",
      "['Arable land' 'Broad-leaved forest' 'Mixed forest' 'Pastures'\n",
      " 'Inland waters' 'Coniferous forest' 'Complex cultivation patterns'\n",
      " 'Land principally occupied by agriculture, with significant areas of natural vegetation'\n",
      " 'Urban fabric' 'Industrial or commercial units' 'Inland wetlands'\n",
      " 'Transitional woodland, shrub'\n",
      " 'Natural grassland and sparsely vegetated areas'\n",
      " 'Moors, heathland and sclerophyllous vegetation' 'Marine waters'\n",
      " 'Coastal wetlands' 'Permanent crops' 'Beaches, dunes, sands'\n",
      " 'Agro-forestry areas']\n",
      "Index(['patch_id', 'labels', 'split', 'country', 's1_name', 's2v1_name',\n",
      "       'contains_seasonal_snow', 'contains_cloud_or_shadow', 'binary_vector'],\n",
      "      dtype='object') \n",
      "\n",
      "Index(['patch_id', 'labels', 'split', 'country', 's1_name', 's2v1_name',\n",
      "       'contains_seasonal_snow', 'contains_cloud_or_shadow', 'binary_vector'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BigEarthNetDataPreprocessing(dataset_dir, subset_dir, metadata_file, unwanted_metadata_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "Resizing TIFF files: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 133.3486819267273 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\1%Copy'\n",
    "\n",
    "bands_of_interest = ['B01', 'B05', 'B06', 'B07', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "for folder in os.listdir(dataset_dir):\n",
    "    folder_path = os.path.join(dataset_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for subfolder in tqdm(os.listdir(folder_path), desc='Resizing TIFF files'):\n",
    "            subfolder_path = os.path.join(folder_path, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                for band in bands_of_interest:\n",
    "                    band_source = subfolder_path + \"/\" + subfolder + \"_\" + band + \".tif\"\n",
    "                    temp_tif = subfolder_path + \"/\" + subfolder + \"_\" + band + \"_resized.tif\"\n",
    "                    new_width = 120\n",
    "                    new_height = 120\n",
    "\n",
    "                    resizeTiffFiles(band_source, temp_tif, new_width, new_height)\n",
    "\n",
    "                    os.remove(band_source)  # Delete the original\n",
    "                    os.rename(temp_tif, band_source)  # Rename the temporary file\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "execution_time = end_time - start_time  # Calculate the execution time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Arable land', 'Land principally occupied by agriculture, with significant areas of natural vegetation']\n"
     ]
    }
   ],
   "source": [
    "row = metadata_df.loc[metadata_df['patch_id'] == 'S2A_MSIL2A_20171002T112111_N9999_R037_T29SNB_71_20']\n",
    "\n",
    "labels = row['labels'].values[0]\n",
    "\n",
    "print(f\"Labels: {labels}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
