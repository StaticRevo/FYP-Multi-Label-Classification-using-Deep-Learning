digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2717640380528 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2717626169088 [label=AddmmBackward0]
	2717626167984 -> 2717626169088
	2717631452016 [label="fc.bias
 (19)" fillcolor=lightblue]
	2717631452016 -> 2717626167984
	2717626167984 [label=AccumulateGrad]
	2717626168128 -> 2717626169088
	2717626168128 [label=ViewBackward0]
	2717626167648 -> 2717626168128
	2717626167648 [label=MeanBackward1]
	2717626166544 -> 2717626167648
	2717626166544 [label=ReluBackward0]
	2717626166064 -> 2717626166544
	2717626166064 [label=AddBackward0]
	2717626165584 -> 2717626166064
	2717626165584 [label=CudnnBatchNormBackward0]
	2717626165248 -> 2717626165584
	2717626165248 [label=ConvolutionBackward0]
	2717626164288 -> 2717626165248
	2717626164288 [label=ReluBackward0]
	2717626163184 -> 2717626164288
	2717626163184 [label=CudnnBatchNormBackward0]
	2717626162704 -> 2717626163184
	2717626162704 [label=ConvolutionBackward0]
	2717626161744 -> 2717626162704
	2717626161744 [label=ReluBackward0]
	2717626161408 -> 2717626161744
	2717626161408 [label=CudnnBatchNormBackward0]
	2717626160928 -> 2717626161408
	2717626160928 [label=ConvolutionBackward0]
	2717626166208 -> 2717626160928
	2717626166208 [label=ReluBackward0]
	2717626159488 -> 2717626166208
	2717626159488 [label=AddBackward0]
	2717626159008 -> 2717626159488
	2717626159008 [label=CudnnBatchNormBackward0]
	2717626155312 -> 2717626159008
	2717626155312 [label=ConvolutionBackward0]
	2717626171152 -> 2717626155312
	2717626171152 [label=ReluBackward0]
	2717626171200 -> 2717626171152
	2717626171200 [label=CudnnBatchNormBackward0]
	2717626170912 -> 2717626171200
	2717626170912 [label=ConvolutionBackward0]
	2717626170816 -> 2717626170912
	2717626170816 [label=ReluBackward0]
	2717626170480 -> 2717626170816
	2717626170480 [label=CudnnBatchNormBackward0]
	2717626170288 -> 2717626170480
	2717626170288 [label=ConvolutionBackward0]
	2717626157280 -> 2717626170288
	2717626157280 [label=ReluBackward0]
	2717626170000 -> 2717626157280
	2717626170000 [label=AddBackward0]
	2717626169808 -> 2717626170000
	2717626169808 [label=CudnnBatchNormBackward0]
	2717626169856 -> 2717626169808
	2717626169856 [label=ConvolutionBackward0]
	2717626169472 -> 2717626169856
	2717626169472 [label=ReluBackward0]
	2717626169232 -> 2717626169472
	2717626169232 [label=CudnnBatchNormBackward0]
	2717626169184 -> 2717626169232
	2717626169184 [label=ConvolutionBackward0]
	2717626168848 -> 2717626169184
	2717626168848 [label=ReluBackward0]
	2717626168896 -> 2717626168848
	2717626168896 [label=CudnnBatchNormBackward0]
	2717626168800 -> 2717626168896
	2717626168800 [label=ConvolutionBackward0]
	2717626168176 -> 2717626168800
	2717626168176 [label=ReluBackward0]
	2717626168224 -> 2717626168176
	2717626168224 [label=AddBackward0]
	2717626168080 -> 2717626168224
	2717626168080 [label=CudnnBatchNormBackward0]
	2717626167696 -> 2717626168080
	2717626167696 [label=ConvolutionBackward0]
	2717626167840 -> 2717626167696
	2717626167840 [label=ReluBackward0]
	2717626167408 -> 2717626167840
	2717626167408 [label=CudnnBatchNormBackward0]
	2717626167312 -> 2717626167408
	2717626167312 [label=ConvolutionBackward0]
	2717626167120 -> 2717626167312
	2717626167120 [label=ReluBackward0]
	2717626166736 -> 2717626167120
	2717626166736 [label=CudnnBatchNormBackward0]
	2717626166976 -> 2717626166736
	2717626166976 [label=ConvolutionBackward0]
	2717626168320 -> 2717626166976
	2717626168320 [label=ReluBackward0]
	2717626166256 -> 2717626168320
	2717626166256 [label=AddBackward0]
	2717626166496 -> 2717626166256
	2717626166496 [label=CudnnBatchNormBackward0]
	2717626166160 -> 2717626166496
	2717626166160 [label=ConvolutionBackward0]
	2717626165872 -> 2717626166160
	2717626165872 [label=ReluBackward0]
	2717626165920 -> 2717626165872
	2717626165920 [label=CudnnBatchNormBackward0]
	2717626165632 -> 2717626165920
	2717626165632 [label=ConvolutionBackward0]
	2717626165536 -> 2717626165632
	2717626165536 [label=ReluBackward0]
	2717626165200 -> 2717626165536
	2717626165200 [label=CudnnBatchNormBackward0]
	2717626165008 -> 2717626165200
	2717626165008 [label=ConvolutionBackward0]
	2717626166352 -> 2717626165008
	2717626166352 [label=ReluBackward0]
	2717626164720 -> 2717626166352
	2717626164720 [label=AddBackward0]
	2717626164528 -> 2717626164720
	2717626164528 [label=CudnnBatchNormBackward0]
	2717626164576 -> 2717626164528
	2717626164576 [label=ConvolutionBackward0]
	2717626164192 -> 2717626164576
	2717626164192 [label=ReluBackward0]
	2717626163952 -> 2717626164192
	2717626163952 [label=CudnnBatchNormBackward0]
	2717626163904 -> 2717626163952
	2717626163904 [label=ConvolutionBackward0]
	2717626163568 -> 2717626163904
	2717626163568 [label=ReluBackward0]
	2717626163616 -> 2717626163568
	2717626163616 [label=CudnnBatchNormBackward0]
	2717626163520 -> 2717626163616
	2717626163520 [label=ConvolutionBackward0]
	2717626164672 -> 2717626163520
	2717626164672 [label=ReluBackward0]
	2717626163136 -> 2717626164672
	2717626163136 [label=AddBackward0]
	2717626163040 -> 2717626163136
	2717626163040 [label=CudnnBatchNormBackward0]
	2717626162608 -> 2717626163040
	2717626162608 [label=ConvolutionBackward0]
	2717626162464 -> 2717626162608
	2717626162464 [label=ReluBackward0]
	2717626162272 -> 2717626162464
	2717626162272 [label=CudnnBatchNormBackward0]
	2717626161936 -> 2717626162272
	2717626161936 [label=ConvolutionBackward0]
	2717626162080 -> 2717626161936
	2717626162080 [label=ReluBackward0]
	2717626161648 -> 2717626162080
	2717626161648 [label=CudnnBatchNormBackward0]
	2717626161552 -> 2717626161648
	2717626161552 [label=ConvolutionBackward0]
	2717626162944 -> 2717626161552
	2717626162944 [label=ReluBackward0]
	2717626161168 -> 2717626162944
	2717626161168 [label=AddBackward0]
	2717626161072 -> 2717626161168
	2717626161072 [label=CudnnBatchNormBackward0]
	2717626161120 -> 2717626161072
	2717626161120 [label=ConvolutionBackward0]
	2717626160496 -> 2717626161120
	2717626160496 [label=ReluBackward0]
	2717626160544 -> 2717626160496
	2717626160544 [label=CudnnBatchNormBackward0]
	2717626160400 -> 2717626160544
	2717626160400 [label=ConvolutionBackward0]
	2717626160112 -> 2717626160400
	2717626160112 [label=ReluBackward0]
	2717626160160 -> 2717626160112
	2717626160160 [label=CudnnBatchNormBackward0]
	2717626159872 -> 2717626160160
	2717626159872 [label=ConvolutionBackward0]
	2717626160976 -> 2717626159872
	2717626160976 [label=ReluBackward0]
	2717626159680 -> 2717626160976
	2717626159680 [label=AddBackward0]
	2717626159392 -> 2717626159680
	2717626159392 [label=CudnnBatchNormBackward0]
	2717626155744 -> 2717626159392
	2717626155744 [label=ConvolutionBackward0]
	2717626158192 -> 2717626155744
	2717626158192 [label=ReluBackward0]
	2717626157520 -> 2717626158192
	2717626157520 [label=CudnnBatchNormBackward0]
	2717626158000 -> 2717626157520
	2717626158000 [label=ConvolutionBackward0]
	2717626157040 -> 2717626158000
	2717626157040 [label=ReluBackward0]
	2717626156272 -> 2717626157040
	2717626156272 [label=CudnnBatchNormBackward0]
	2717626158528 -> 2717626156272
	2717626158528 [label=ConvolutionBackward0]
	2717626156800 -> 2717626158528
	2717626156800 [label=ReluBackward0]
	2717626155456 -> 2717626156800
	2717626155456 [label=AddBackward0]
	2717626157328 -> 2717626155456
	2717626157328 [label=CudnnBatchNormBackward0]
	2717626155120 -> 2717626157328
	2717626155120 [label=ConvolutionBackward0]
	2717626156416 -> 2717626155120
	2717626156416 [label=ReluBackward0]
	2717626157472 -> 2717626156416
	2717626157472 [label=CudnnBatchNormBackward0]
	2717626156128 -> 2717626157472
	2717626156128 [label=ConvolutionBackward0]
	2717626157376 -> 2717626156128
	2717626157376 [label=ReluBackward0]
	2717626155648 -> 2717626157376
	2717626155648 [label=CudnnBatchNormBackward0]
	2717626158480 -> 2717626155648
	2717626158480 [label=ConvolutionBackward0]
	2717626155168 -> 2717626158480
	2717626155168 [label=ReluBackward0]
	2717626324912 -> 2717626155168
	2717626324912 [label=AddBackward0]
	2717626330576 -> 2717626324912
	2717626330576 [label=CudnnBatchNormBackward0]
	2717626334992 -> 2717626330576
	2717626334992 [label=ConvolutionBackward0]
	2717626328656 -> 2717626334992
	2717626328656 [label=ReluBackward0]
	2717626334464 -> 2717626328656
	2717626334464 [label=CudnnBatchNormBackward0]
	2717626331344 -> 2717626334464
	2717626331344 [label=ConvolutionBackward0]
	2717626325632 -> 2717626331344
	2717626325632 [label=ReluBackward0]
	2717626322800 -> 2717626325632
	2717626322800 [label=CudnnBatchNormBackward0]
	2717626326736 -> 2717626322800
	2717626326736 [label=ConvolutionBackward0]
	2717626334320 -> 2717626326736
	2717626334320 [label=ReluBackward0]
	2717626327936 -> 2717626334320
	2717626327936 [label=AddBackward0]
	2717626330912 -> 2717626327936
	2717626330912 [label=CudnnBatchNormBackward0]
	2717626321456 -> 2717626330912
	2717626321456 [label=ConvolutionBackward0]
	2717626335184 -> 2717626321456
	2717626335184 [label=ReluBackward0]
	2717626324432 -> 2717626335184
	2717626324432 [label=CudnnBatchNormBackward0]
	2717626325536 -> 2717626324432
	2717626325536 [label=ConvolutionBackward0]
	2717626319296 -> 2717626325536
	2717626319296 [label=ReluBackward0]
	2717626328704 -> 2717626319296
	2717626328704 [label=CudnnBatchNormBackward0]
	2717626326256 -> 2717626328704
	2717626326256 [label=ConvolutionBackward0]
	2717626334656 -> 2717626326256
	2717626334656 [label=ReluBackward0]
	2717626321936 -> 2717626334656
	2717626321936 [label=AddBackward0]
	2717626319488 -> 2717626321936
	2717626319488 [label=CudnnBatchNormBackward0]
	2717626327120 -> 2717626319488
	2717626327120 [label=ConvolutionBackward0]
	2717626327072 -> 2717626327120
	2717626327072 [label=ReluBackward0]
	2717488539552 -> 2717626327072
	2717488539552 [label=CudnnBatchNormBackward0]
	2717626320352 -> 2717488539552
	2717626320352 [label=ConvolutionBackward0]
	2717640281056 -> 2717626320352
	2717640281056 [label=ReluBackward0]
	2717640279952 -> 2717640281056
	2717640279952 [label=CudnnBatchNormBackward0]
	2717640279472 -> 2717640279952
	2717640279472 [label=ConvolutionBackward0]
	2717640278512 -> 2717640279472
	2717640278512 [label=ReluBackward0]
	2717640281968 -> 2717640278512
	2717640281968 [label=AddBackward0]
	2717640281776 -> 2717640281968
	2717640281776 [label=CudnnBatchNormBackward0]
	2717640281824 -> 2717640281776
	2717640281824 [label=ConvolutionBackward0]
	2717640281440 -> 2717640281824
	2717640281440 [label=ReluBackward0]
	2717640281200 -> 2717640281440
	2717640281200 [label=CudnnBatchNormBackward0]
	2717640281152 -> 2717640281200
	2717640281152 [label=ConvolutionBackward0]
	2717640280816 -> 2717640281152
	2717640280816 [label=ReluBackward0]
	2717640280864 -> 2717640280816
	2717640280864 [label=CudnnBatchNormBackward0]
	2717640280768 -> 2717640280864
	2717640280768 [label=ConvolutionBackward0]
	2717640281920 -> 2717640280768
	2717640281920 [label=ReluBackward0]
	2717640280384 -> 2717640281920
	2717640280384 [label=AddBackward0]
	2717640280288 -> 2717640280384
	2717640280288 [label=CudnnBatchNormBackward0]
	2717640279856 -> 2717640280288
	2717640279856 [label=ConvolutionBackward0]
	2717640279712 -> 2717640279856
	2717640279712 [label=ReluBackward0]
	2717640279520 -> 2717640279712
	2717640279520 [label=CudnnBatchNormBackward0]
	2717640279184 -> 2717640279520
	2717640279184 [label=ConvolutionBackward0]
	2717640279328 -> 2717640279184
	2717640279328 [label=ReluBackward0]
	2717640278896 -> 2717640279328
	2717640278896 [label=CudnnBatchNormBackward0]
	2717640278800 -> 2717640278896
	2717640278800 [label=ConvolutionBackward0]
	2717640280192 -> 2717640278800
	2717640280192 [label=ReluBackward0]
	2717640278416 -> 2717640280192
	2717640278416 [label=AddBackward0]
	2717640278320 -> 2717640278416
	2717640278320 [label=CudnnBatchNormBackward0]
	2717640278368 -> 2717640278320
	2717640278368 [label=ConvolutionBackward0]
	2717640282448 -> 2717640278368
	2717640282448 [label=ReluBackward0]
	2717640282784 -> 2717640282448
	2717640282784 [label=CudnnBatchNormBackward0]
	2717640282880 -> 2717640282784
	2717640282880 [label=ConvolutionBackward0]
	2717640283072 -> 2717640282880
	2717640283072 [label=ReluBackward0]
	2717640283216 -> 2717640283072
	2717640283216 [label=CudnnBatchNormBackward0]
	2717640283312 -> 2717640283216
	2717640283312 [label=ConvolutionBackward0]
	2717640283504 -> 2717640283312
	2717640283504 [label=MaxPool2DWithIndicesBackward0]
	2717640283648 -> 2717640283504
	2717640283648 [label=ReluBackward0]
	2717640283744 -> 2717640283648
	2717640283744 [label=CudnnBatchNormBackward0]
	2717640283840 -> 2717640283744
	2717640283840 [label=ConvolutionBackward0]
	2717640284032 -> 2717640283840
	2717631451824 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2717631451824 -> 2717640284032
	2717640284032 [label=AccumulateGrad]
	2717640283792 -> 2717640283744
	2717615384016 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2717615384016 -> 2717640283792
	2717640283792 [label=AccumulateGrad]
	2717640283552 -> 2717640283744
	2717615383728 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2717615383728 -> 2717640283552
	2717640283552 [label=AccumulateGrad]
	2717640283456 -> 2717640283312
	2717615383632 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2717615383632 -> 2717640283456
	2717640283456 [label=AccumulateGrad]
	2717640283264 -> 2717640283216
	2717615383536 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2717615383536 -> 2717640283264
	2717640283264 [label=AccumulateGrad]
	2717640283120 -> 2717640283216
	2717615382768 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2717615382768 -> 2717640283120
	2717640283120 [label=AccumulateGrad]
	2717640283024 -> 2717640282880
	2717615382864 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2717615382864 -> 2717640283024
	2717640283024 [label=AccumulateGrad]
	2717640282832 -> 2717640282784
	2717615382960 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2717615382960 -> 2717640282832
	2717640282832 [label=AccumulateGrad]
	2717640282688 -> 2717640282784
	2717615383056 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2717615383056 -> 2717640282688
	2717640282688 [label=AccumulateGrad]
	2717640282592 -> 2717640278368
	2717615383344 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2717615383344 -> 2717640282592
	2717640282592 [label=AccumulateGrad]
	2717640278272 -> 2717640278320
	2717615382384 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2717615382384 -> 2717640278272
	2717640278272 [label=AccumulateGrad]
	2717640278464 -> 2717640278320
	2717615382288 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2717615382288 -> 2717640278464
	2717640278464 [label=AccumulateGrad]
	2717640278224 -> 2717640278416
	2717640278224 [label=CudnnBatchNormBackward0]
	2717640282976 -> 2717640278224
	2717640282976 [label=ConvolutionBackward0]
	2717640283504 -> 2717640282976
	2717640283360 -> 2717640282976
	2717615384208 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2717615384208 -> 2717640283360
	2717640283360 [label=AccumulateGrad]
	2717640278080 -> 2717640278224
	2717615384304 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2717615384304 -> 2717640278080
	2717640278080 [label=AccumulateGrad]
	2717640278128 -> 2717640278224
	2717615384400 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2717615384400 -> 2717640278128
	2717640278128 [label=AccumulateGrad]
	2717640278608 -> 2717640278800
	2717615379600 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2717615379600 -> 2717640278608
	2717640278608 [label=AccumulateGrad]
	2717640278704 -> 2717640278896
	2717615379792 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2717615379792 -> 2717640278704
	2717640278704 [label=AccumulateGrad]
	2717640279088 -> 2717640278896
	2717615381424 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2717615381424 -> 2717640279088
	2717640279088 [label=AccumulateGrad]
	2717640279232 -> 2717640279184
	2717615382192 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2717615382192 -> 2717640279232
	2717640279232 [label=AccumulateGrad]
	2717640279376 -> 2717640279520
	2717615382096 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2717615382096 -> 2717640279376
	2717640279376 [label=AccumulateGrad]
	2717640279808 -> 2717640279520
	2717631932624 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2717631932624 -> 2717640279808
	2717640279808 [label=AccumulateGrad]
	2717640279904 -> 2717640279856
	2717631933008 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2717631933008 -> 2717640279904
	2717640279904 [label=AccumulateGrad]
	2717640280000 -> 2717640280288
	2717631933104 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2717631933104 -> 2717640280000
	2717640280000 [label=AccumulateGrad]
	2717640280048 -> 2717640280288
	2717631933200 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2717631933200 -> 2717640280048
	2717640280048 [label=AccumulateGrad]
	2717640280192 -> 2717640280384
	2717640280144 -> 2717640280768
	2717631933584 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2717631933584 -> 2717640280144
	2717640280144 [label=AccumulateGrad]
	2717640280672 -> 2717640280864
	2717631933680 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2717631933680 -> 2717640280672
	2717640280672 [label=AccumulateGrad]
	2717640280624 -> 2717640280864
	2717631933776 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2717631933776 -> 2717640280624
	2717640280624 [label=AccumulateGrad]
	2717640280960 -> 2717640281152
	2717631934160 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2717631934160 -> 2717640280960
	2717640280960 [label=AccumulateGrad]
	2717640281344 -> 2717640281200
	2717631934256 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2717631934256 -> 2717640281344
	2717640281344 [label=AccumulateGrad]
	2717640281296 -> 2717640281200
	2717631934352 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2717631934352 -> 2717640281296
	2717640281296 [label=AccumulateGrad]
	2717640281488 -> 2717640281824
	2717631934736 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2717631934736 -> 2717640281488
	2717640281488 [label=AccumulateGrad]
	2717640281680 -> 2717640281776
	2717631934832 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2717631934832 -> 2717640281680
	2717640281680 [label=AccumulateGrad]
	2717640281584 -> 2717640281776
	2717631934928 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2717631934928 -> 2717640281584
	2717640281584 [label=AccumulateGrad]
	2717640281920 -> 2717640281968
	2717640279136 -> 2717640279472
	2717631935888 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2717631935888 -> 2717640279136
	2717640279136 [label=AccumulateGrad]
	2717640280096 -> 2717640279952
	2717631935984 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2717631935984 -> 2717640280096
	2717640280096 [label=AccumulateGrad]
	2717640280432 -> 2717640279952
	2717631936080 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2717631936080 -> 2717640280432
	2717640280432 [label=AccumulateGrad]
	2717640280912 -> 2717626320352
	2717631936464 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2717631936464 -> 2717640280912
	2717640280912 [label=AccumulateGrad]
	2717640281872 -> 2717488539552
	2717631936560 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2717631936560 -> 2717640281872
	2717640281872 [label=AccumulateGrad]
	2717640282016 -> 2717488539552
	2717631936656 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2717631936656 -> 2717640282016
	2717640282016 [label=AccumulateGrad]
	2717626329280 -> 2717626327120
	2717631937040 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2717631937040 -> 2717626329280
	2717626329280 [label=AccumulateGrad]
	2717626329952 -> 2717626319488
	2717631937136 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2717631937136 -> 2717626329952
	2717626329952 [label=AccumulateGrad]
	2717626331536 -> 2717626319488
	2717631937232 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2717631937232 -> 2717626331536
	2717626331536 [label=AccumulateGrad]
	2717626321360 -> 2717626321936
	2717626321360 [label=CudnnBatchNormBackward0]
	2717626325728 -> 2717626321360
	2717626325728 [label=ConvolutionBackward0]
	2717640278512 -> 2717626325728
	2717640279616 -> 2717626325728
	2717631935312 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2717631935312 -> 2717640279616
	2717640279616 [label=AccumulateGrad]
	2717626334608 -> 2717626321360
	2717631935408 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2717631935408 -> 2717626334608
	2717626334608 [label=AccumulateGrad]
	2717626323136 -> 2717626321360
	2717631935504 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2717631935504 -> 2717626323136
	2717626323136 [label=AccumulateGrad]
	2717626333408 -> 2717626326256
	2717631937616 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2717631937616 -> 2717626333408
	2717626333408 [label=AccumulateGrad]
	2717626319584 -> 2717626328704
	2717631937712 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2717631937712 -> 2717626319584
	2717626319584 [label=AccumulateGrad]
	2717626318960 -> 2717626328704
	2717631937808 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2717631937808 -> 2717626318960
	2717626318960 [label=AccumulateGrad]
	2717626329856 -> 2717626325536
	2717631938192 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2717631938192 -> 2717626329856
	2717626329856 [label=AccumulateGrad]
	2717626322608 -> 2717626324432
	2717631938288 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2717631938288 -> 2717626322608
	2717626322608 [label=AccumulateGrad]
	2717626330816 -> 2717626324432
	2717631938384 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2717631938384 -> 2717626330816
	2717626330816 [label=AccumulateGrad]
	2717626320592 -> 2717626321456
	2717631742224 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2717631742224 -> 2717626320592
	2717626320592 [label=AccumulateGrad]
	2717626329760 -> 2717626330912
	2717631742320 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2717631742320 -> 2717626329760
	2717626329760 [label=AccumulateGrad]
	2717626328992 -> 2717626330912
	2717631742416 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2717631742416 -> 2717626328992
	2717626328992 [label=AccumulateGrad]
	2717626334656 -> 2717626327936
	2717626327216 -> 2717626326736
	2717631742800 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2717631742800 -> 2717626327216
	2717626327216 [label=AccumulateGrad]
	2717626327696 -> 2717626322800
	2717631742896 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2717631742896 -> 2717626327696
	2717626327696 [label=AccumulateGrad]
	2717626325968 -> 2717626322800
	2717631742992 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2717631742992 -> 2717626325968
	2717626325968 [label=AccumulateGrad]
	2717626331008 -> 2717626331344
	2717631743376 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2717631743376 -> 2717626331008
	2717626331008 [label=AccumulateGrad]
	2717626322848 -> 2717626334464
	2717631743472 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2717631743472 -> 2717626322848
	2717626322848 [label=AccumulateGrad]
	2717626319344 -> 2717626334464
	2717631743568 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2717631743568 -> 2717626319344
	2717626319344 [label=AccumulateGrad]
	2717626330192 -> 2717626334992
	2717631743952 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2717631743952 -> 2717626330192
	2717626330192 [label=AccumulateGrad]
	2717626327744 -> 2717626330576
	2717631744048 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2717631744048 -> 2717626327744
	2717626327744 [label=AccumulateGrad]
	2717626323568 -> 2717626330576
	2717631744144 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2717631744144 -> 2717626323568
	2717626323568 [label=AccumulateGrad]
	2717626334320 -> 2717626324912
	2717626326880 -> 2717626158480
	2717631744528 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2717631744528 -> 2717626326880
	2717626326880 [label=AccumulateGrad]
	2717626158576 -> 2717626155648
	2717631744624 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2717631744624 -> 2717626158576
	2717626158576 [label=AccumulateGrad]
	2717626329328 -> 2717626155648
	2717631744720 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2717631744720 -> 2717626329328
	2717626329328 [label=AccumulateGrad]
	2717626158816 -> 2717626156128
	2717631745104 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2717631745104 -> 2717626158816
	2717626158816 [label=AccumulateGrad]
	2717626157856 -> 2717626157472
	2717631745200 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2717631745200 -> 2717626157856
	2717626157856 [label=AccumulateGrad]
	2717626155504 -> 2717626157472
	2717631745296 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2717631745296 -> 2717626155504
	2717626155504 [label=AccumulateGrad]
	2717626156032 -> 2717626155120
	2717631745680 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2717631745680 -> 2717626156032
	2717626156032 [label=AccumulateGrad]
	2717626156656 -> 2717626157328
	2717631745776 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2717631745776 -> 2717626156656
	2717626156656 [label=AccumulateGrad]
	2717626156944 -> 2717626157328
	2717631745872 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2717631745872 -> 2717626156944
	2717626156944 [label=AccumulateGrad]
	2717626155168 -> 2717626155456
	2717626155936 -> 2717626158528
	2717631746832 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2717631746832 -> 2717626155936
	2717626155936 [label=AccumulateGrad]
	2717626156992 -> 2717626156272
	2717631746928 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2717631746928 -> 2717626156992
	2717626156992 [label=AccumulateGrad]
	2717626157904 -> 2717626156272
	2717631747024 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2717631747024 -> 2717626157904
	2717626157904 [label=AccumulateGrad]
	2717626157424 -> 2717626158000
	2717631747408 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2717631747408 -> 2717626157424
	2717626157424 [label=AccumulateGrad]
	2717626158768 -> 2717626157520
	2717631747504 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2717631747504 -> 2717626158768
	2717626158768 [label=AccumulateGrad]
	2717626159200 -> 2717626157520
	2717631747600 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2717631747600 -> 2717626159200
	2717626159200 [label=AccumulateGrad]
	2717626158960 -> 2717626155744
	2717631747984 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2717631747984 -> 2717626158960
	2717626158960 [label=AccumulateGrad]
	2717626156512 -> 2717626159392
	2717631748080 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2717631748080 -> 2717626156512
	2717626156512 [label=AccumulateGrad]
	2717626159248 -> 2717626159392
	2717631748176 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2717631748176 -> 2717626159248
	2717626159248 [label=AccumulateGrad]
	2717626159440 -> 2717626159680
	2717626159440 [label=CudnnBatchNormBackward0]
	2717626156080 -> 2717626159440
	2717626156080 [label=ConvolutionBackward0]
	2717626156800 -> 2717626156080
	2717626156848 -> 2717626156080
	2717631746256 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2717631746256 -> 2717626156848
	2717626156848 [label=AccumulateGrad]
	2717626159056 -> 2717626159440
	2717631746352 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2717631746352 -> 2717626159056
	2717626159056 [label=AccumulateGrad]
	2717626159296 -> 2717626159440
	2717631746448 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2717631746448 -> 2717626159296
	2717626159296 [label=AccumulateGrad]
	2717626159776 -> 2717626159872
	2717631748560 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2717631748560 -> 2717626159776
	2717626159776 [label=AccumulateGrad]
	2717626159920 -> 2717626160160
	2717631748656 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2717631748656 -> 2717626159920
	2717626159920 [label=AccumulateGrad]
	2717626160256 -> 2717626160160
	2717631748752 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2717631748752 -> 2717626160256
	2717626160256 [label=AccumulateGrad]
	2717626160016 -> 2717626160400
	2717631749136 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2717631749136 -> 2717626160016
	2717626160016 [label=AccumulateGrad]
	2717626160640 -> 2717626160544
	2717631749232 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2717631749232 -> 2717626160640
	2717626160640 [label=AccumulateGrad]
	2717626160592 -> 2717626160544
	2717631749328 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2717631749328 -> 2717626160592
	2717626160592 [label=AccumulateGrad]
	2717626160688 -> 2717626161120
	2717631749712 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2717631749712 -> 2717626160688
	2717626160688 [label=AccumulateGrad]
	2717626161024 -> 2717626161072
	2717631749808 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2717631749808 -> 2717626161024
	2717626161024 [label=AccumulateGrad]
	2717626161216 -> 2717626161072
	2717631749904 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2717631749904 -> 2717626161216
	2717626161216 [label=AccumulateGrad]
	2717626160976 -> 2717626161168
	2717626161360 -> 2717626161552
	2717631750288 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2717631750288 -> 2717626161360
	2717626161360 [label=AccumulateGrad]
	2717626161456 -> 2717626161648
	2717631750384 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2717631750384 -> 2717626161456
	2717626161456 [label=AccumulateGrad]
	2717626161840 -> 2717626161648
	2717631750480 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2717631750480 -> 2717626161840
	2717626161840 [label=AccumulateGrad]
	2717626161984 -> 2717626161936
	2717631750864 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2717631750864 -> 2717626161984
	2717626161984 [label=AccumulateGrad]
	2717626162128 -> 2717626162272
	2717631750960 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2717631750960 -> 2717626162128
	2717626162128 [label=AccumulateGrad]
	2717626162560 -> 2717626162272
	2717631751056 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2717631751056 -> 2717626162560
	2717626162560 [label=AccumulateGrad]
	2717626162656 -> 2717626162608
	2717631751440 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2717631751440 -> 2717626162656
	2717626162656 [label=AccumulateGrad]
	2717626162752 -> 2717626163040
	2717631751536 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2717631751536 -> 2717626162752
	2717626162752 [label=AccumulateGrad]
	2717626162800 -> 2717626163040
	2717631751632 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2717631751632 -> 2717626162800
	2717626162800 [label=AccumulateGrad]
	2717626162944 -> 2717626163136
	2717626162896 -> 2717626163520
	2717631752016 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2717631752016 -> 2717626162896
	2717626162896 [label=AccumulateGrad]
	2717626163424 -> 2717626163616
	2717631752112 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2717631752112 -> 2717626163424
	2717626163424 [label=AccumulateGrad]
	2717626163376 -> 2717626163616
	2717631752208 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2717631752208 -> 2717626163376
	2717626163376 [label=AccumulateGrad]
	2717626163712 -> 2717626163904
	2717631752592 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2717631752592 -> 2717626163712
	2717626163712 [label=AccumulateGrad]
	2717626164096 -> 2717626163952
	2717631752688 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2717631752688 -> 2717626164096
	2717626164096 [label=AccumulateGrad]
	2717626164048 -> 2717626163952
	2717631752784 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2717631752784 -> 2717626164048
	2717626164048 [label=AccumulateGrad]
	2717626164240 -> 2717626164576
	2717631753168 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2717631753168 -> 2717626164240
	2717626164240 [label=AccumulateGrad]
	2717626164432 -> 2717626164528
	2717631753264 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2717631753264 -> 2717626164432
	2717626164432 [label=AccumulateGrad]
	2717626164336 -> 2717626164528
	2717631753360 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2717631753360 -> 2717626164336
	2717626164336 [label=AccumulateGrad]
	2717626164672 -> 2717626164720
	2717626164864 -> 2717626165008
	2717631753744 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2717631753744 -> 2717626164864
	2717626164864 [label=AccumulateGrad]
	2717626165152 -> 2717626165200
	2717631753840 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2717631753840 -> 2717626165152
	2717626165152 [label=AccumulateGrad]
	2717626165344 -> 2717626165200
	2717631753936 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2717631753936 -> 2717626165344
	2717626165344 [label=AccumulateGrad]
	2717626165392 -> 2717626165632
	2717631754320 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2717631754320 -> 2717626165392
	2717626165392 [label=AccumulateGrad]
	2717626165680 -> 2717626165920
	2717631754416 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2717631754416 -> 2717626165680
	2717626165680 [label=AccumulateGrad]
	2717626166016 -> 2717626165920
	2717631754512 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2717631754512 -> 2717626166016
	2717626166016 [label=AccumulateGrad]
	2717626165776 -> 2717626166160
	2717631754896 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2717631754896 -> 2717626165776
	2717626165776 [label=AccumulateGrad]
	2717626166400 -> 2717626166496
	2717631754992 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2717631754992 -> 2717626166400
	2717626166400 [label=AccumulateGrad]
	2717626166304 -> 2717626166496
	2717631755088 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2717631755088 -> 2717626166304
	2717626166304 [label=AccumulateGrad]
	2717626166352 -> 2717626166256
	2717626166592 -> 2717626166976
	2717631755472 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2717631755472 -> 2717626166592
	2717626166592 [label=AccumulateGrad]
	2717626166832 -> 2717626166736
	2717631755568 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2717631755568 -> 2717626166832
	2717626166832 [label=AccumulateGrad]
	2717626167072 -> 2717626166736
	2717631755664 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2717631755664 -> 2717626167072
	2717626167072 [label=AccumulateGrad]
	2717626167360 -> 2717626167312
	2717631756048 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2717631756048 -> 2717626167360
	2717626167360 [label=AccumulateGrad]
	2717626167216 -> 2717626167408
	2717631756144 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2717631756144 -> 2717626167216
	2717626167216 [label=AccumulateGrad]
	2717626167600 -> 2717626167408
	2717631756240 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2717631756240 -> 2717626167600
	2717626167600 [label=AccumulateGrad]
	2717626167744 -> 2717626167696
	2717631756624 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2717631756624 -> 2717626167744
	2717626167744 [label=AccumulateGrad]
	2717626167888 -> 2717626168080
	2717631756720 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2717631756720 -> 2717626167888
	2717626167888 [label=AccumulateGrad]
	2717626168032 -> 2717626168080
	2717631756816 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2717631756816 -> 2717626168032
	2717626168032 [label=AccumulateGrad]
	2717626168320 -> 2717626168224
	2717626168368 -> 2717626168800
	2717631757776 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2717631757776 -> 2717626168368
	2717626168368 [label=AccumulateGrad]
	2717626168704 -> 2717626168896
	2717631757872 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2717631757872 -> 2717626168704
	2717626168704 [label=AccumulateGrad]
	2717626168656 -> 2717626168896
	2717631757968 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2717631757968 -> 2717626168656
	2717626168656 [label=AccumulateGrad]
	2717626168992 -> 2717626169184
	2717631447120 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2717631447120 -> 2717626168992
	2717626168992 [label=AccumulateGrad]
	2717626169376 -> 2717626169232
	2717631447216 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2717631447216 -> 2717626169376
	2717626169376 [label=AccumulateGrad]
	2717626169328 -> 2717626169232
	2717631447312 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2717631447312 -> 2717626169328
	2717626169328 [label=AccumulateGrad]
	2717626169520 -> 2717626169856
	2717631447696 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2717631447696 -> 2717626169520
	2717626169520 [label=AccumulateGrad]
	2717626169712 -> 2717626169808
	2717631447792 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2717631447792 -> 2717626169712
	2717626169712 [label=AccumulateGrad]
	2717626169616 -> 2717626169808
	2717631447888 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2717631447888 -> 2717626169616
	2717626169616 [label=AccumulateGrad]
	2717626169952 -> 2717626170000
	2717626169952 [label=CudnnBatchNormBackward0]
	2717626169040 -> 2717626169952
	2717626169040 [label=ConvolutionBackward0]
	2717626168176 -> 2717626169040
	2717626168560 -> 2717626169040
	2717631757200 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2717631757200 -> 2717626168560
	2717626168560 [label=AccumulateGrad]
	2717626169760 -> 2717626169952
	2717631757296 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2717631757296 -> 2717626169760
	2717626169760 [label=AccumulateGrad]
	2717626169664 -> 2717626169952
	2717631757392 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2717631757392 -> 2717626169664
	2717626169664 [label=AccumulateGrad]
	2717626170144 -> 2717626170288
	2717631448272 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2717631448272 -> 2717626170144
	2717626170144 [label=AccumulateGrad]
	2717626170432 -> 2717626170480
	2717631448368 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2717631448368 -> 2717626170432
	2717626170432 [label=AccumulateGrad]
	2717626170624 -> 2717626170480
	2717631448464 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2717631448464 -> 2717626170624
	2717626170624 [label=AccumulateGrad]
	2717626170672 -> 2717626170912
	2717631448848 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2717631448848 -> 2717626170672
	2717626170672 [label=AccumulateGrad]
	2717626170960 -> 2717626171200
	2717631448944 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2717631448944 -> 2717626170960
	2717626170960 [label=AccumulateGrad]
	2717626171296 -> 2717626171200
	2717631449040 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2717631449040 -> 2717626171296
	2717626171296 [label=AccumulateGrad]
	2717626171056 -> 2717626155312
	2717631449424 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2717631449424 -> 2717626171056
	2717626171056 [label=AccumulateGrad]
	2717626157616 -> 2717626159008
	2717631449520 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2717631449520 -> 2717626157616
	2717626157616 [label=AccumulateGrad]
	2717626157184 -> 2717626159008
	2717631449616 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2717631449616 -> 2717626157184
	2717626157184 [label=AccumulateGrad]
	2717626157280 -> 2717626159488
	2717626159968 -> 2717626160928
	2717631450000 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2717631450000 -> 2717626159968
	2717626159968 [label=AccumulateGrad]
	2717626160784 -> 2717626161408
	2717631450096 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2717631450096 -> 2717626160784
	2717626160784 [label=AccumulateGrad]
	2717626161888 -> 2717626161408
	2717631450192 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2717631450192 -> 2717626161888
	2717626161888 [label=AccumulateGrad]
	2717626162368 -> 2717626162704
	2717631450576 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2717631450576 -> 2717626162368
	2717626162368 [label=AccumulateGrad]
	2717626163328 -> 2717626163184
	2717631450672 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2717631450672 -> 2717626163328
	2717626163328 [label=AccumulateGrad]
	2717626163664 -> 2717626163184
	2717631450768 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2717631450768 -> 2717626163664
	2717626163664 [label=AccumulateGrad]
	2717626164144 -> 2717626165248
	2717631451152 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2717631451152 -> 2717626164144
	2717626164144 [label=AccumulateGrad]
	2717626165104 -> 2717626165584
	2717631451248 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2717631451248 -> 2717626165104
	2717626165104 [label=AccumulateGrad]
	2717626165728 -> 2717626165584
	2717631451344 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2717631451344 -> 2717626165728
	2717626165728 [label=AccumulateGrad]
	2717626166208 -> 2717626166064
	2717626167504 -> 2717626169088
	2717626167504 [label=TBackward0]
	2717626166688 -> 2717626167504
	2717631451920 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2717631451920 -> 2717626166688
	2717626166688 [label=AccumulateGrad]
	2717626169088 -> 2717640380528
}
