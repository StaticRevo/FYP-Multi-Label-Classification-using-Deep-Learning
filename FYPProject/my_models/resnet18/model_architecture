digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2149783427632 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2149766284048 [label=SigmoidBackward0]
	2149766292592 -> 2149766284048
	2149766292592 [label=AddmmBackward0]
	2149766285824 -> 2149766292592
	2149786662096 [label="model.fc.bias
 (19)" fillcolor=lightblue]
	2149786662096 -> 2149766285824
	2149766285824 [label=AccumulateGrad]
	2149766284912 -> 2149766292592
	2149766284912 [label=ViewBackward0]
	2149766296624 -> 2149766284912
	2149766296624 [label=MeanBackward1]
	2149766285344 -> 2149766296624
	2149766285344 [label=ReluBackward0]
	2149766286688 -> 2149766285344
	2149766286688 [label=AddBackward0]
	2149766293600 -> 2149766286688
	2149766293600 [label=CudnnBatchNormBackward0]
	2149766290720 -> 2149766293600
	2149766290720 [label=ConvolutionBackward0]
	2149766288032 -> 2149766290720
	2149766288032 [label=ReluBackward0]
	2149766292064 -> 2149766288032
	2149766292064 [label=CudnnBatchNormBackward0]
	2149766283568 -> 2149766292064
	2149766283568 [label=ConvolutionBackward0]
	2149766288752 -> 2149766283568
	2149766288752 [label=ReluBackward0]
	2149766294512 -> 2149766288752
	2149766294512 [label=AddBackward0]
	2149766293792 -> 2149766294512
	2149766293792 [label=CudnnBatchNormBackward0]
	2149766296816 -> 2149766293792
	2149766296816 [label=ConvolutionBackward0]
	2149766284576 -> 2149766296816
	2149766284576 [label=ReluBackward0]
	2149766296912 -> 2149766284576
	2149766296912 [label=CudnnBatchNormBackward0]
	2149766292640 -> 2149766296912
	2149766292640 [label=ConvolutionBackward0]
	2149766296480 -> 2149766292640
	2149766296480 [label=ReluBackward0]
	2149766298400 -> 2149766296480
	2149766298400 [label=AddBackward0]
	2149766296432 -> 2149766298400
	2149766296432 [label=CudnnBatchNormBackward0]
	2149766292976 -> 2149766296432
	2149766292976 [label=ConvolutionBackward0]
	2149766298448 -> 2149766292976
	2149766298448 [label=ReluBackward0]
	2149766296384 -> 2149766298448
	2149766296384 [label=CudnnBatchNormBackward0]
	2149766286544 -> 2149766296384
	2149766286544 [label=ConvolutionBackward0]
	2149766296144 -> 2149766286544
	2149766296144 [label=ReluBackward0]
	2149766293648 -> 2149766296144
	2149766293648 [label=AddBackward0]
	2149766292496 -> 2149766293648
	2149766292496 [label=CudnnBatchNormBackward0]
	2149766284096 -> 2149766292496
	2149766284096 [label=ConvolutionBackward0]
	2149766297056 -> 2149766284096
	2149766297056 [label=ReluBackward0]
	2149766286448 -> 2149766297056
	2149766286448 [label=CudnnBatchNormBackward0]
	2149766285632 -> 2149766286448
	2149766285632 [label=ConvolutionBackward0]
	2149766287696 -> 2149766285632
	2149766287696 [label=ReluBackward0]
	2149766292016 -> 2149766287696
	2149766292016 [label=AddBackward0]
	2149766296336 -> 2149766292016
	2149766296336 [label=CudnnBatchNormBackward0]
	2149766290048 -> 2149766296336
	2149766290048 [label=ConvolutionBackward0]
	2149766294128 -> 2149766290048
	2149766294128 [label=ReluBackward0]
	2149766292448 -> 2149766294128
	2149766292448 [label=CudnnBatchNormBackward0]
	2149766298496 -> 2149766292448
	2149766298496 [label=ConvolutionBackward0]
	2149766297152 -> 2149766298496
	2149766297152 [label=ReluBackward0]
	2149766298160 -> 2149766297152
	2149766298160 [label=AddBackward0]
	2149766298880 -> 2149766298160
	2149766298880 [label=CudnnBatchNormBackward0]
	2149766286016 -> 2149766298880
	2149766286016 [label=ConvolutionBackward0]
	2149766289520 -> 2149766286016
	2149766289520 [label=ReluBackward0]
	2149766291440 -> 2149766289520
	2149766291440 [label=CudnnBatchNormBackward0]
	2149766295232 -> 2149766291440
	2149766295232 [label=ConvolutionBackward0]
	2149766293888 -> 2149766295232
	2149766293888 [label=ReluBackward0]
	2149766283328 -> 2149766293888
	2149766283328 [label=AddBackward0]
	2149766296960 -> 2149766283328
	2149766296960 [label=CudnnBatchNormBackward0]
	2149766297728 -> 2149766296960
	2149766297728 [label=ConvolutionBackward0]
	2149766295520 -> 2149766297728
	2149766295520 [label=ReluBackward0]
	2149766293504 -> 2149766295520
	2149766293504 [label=CudnnBatchNormBackward0]
	2149766295952 -> 2149766293504
	2149766295952 [label=ConvolutionBackward0]
	2149766294848 -> 2149766295952
	2149766294848 [label=ReluBackward0]
	2149766287648 -> 2149766294848
	2149766287648 [label=AddBackward0]
	2149766036736 -> 2149766287648
	2149766036736 [label=CudnnBatchNormBackward0]
	2149766036928 -> 2149766036736
	2149766036928 [label=ConvolutionBackward0]
	2149784950800 -> 2149766036928
	2149784950800 [label=ReluBackward0]
	2149784948688 -> 2149784950800
	2149784948688 [label=CudnnBatchNormBackward0]
	2149784950320 -> 2149784948688
	2149784950320 [label=ConvolutionBackward0]
	2149766037168 -> 2149784950320
	2149766037168 [label=MaxPool2DWithIndicesBackward0]
	2149784946048 -> 2149766037168
	2149784946048 [label=ReluBackward0]
	2149784945664 -> 2149784946048
	2149784945664 [label=CudnnBatchNormBackward0]
	2149784952000 -> 2149784945664
	2149784952000 [label=ConvolutionBackward0]
	2149784948400 -> 2149784952000
	2149783461648 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2149783461648 -> 2149784948400
	2149784948400 [label=AccumulateGrad]
	2149784950704 -> 2149784945664
	2149786649616 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	2149786649616 -> 2149784950704
	2149784950704 [label=AccumulateGrad]
	2149784948976 -> 2149784945664
	2149786650384 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	2149786650384 -> 2149784948976
	2149784948976 [label=AccumulateGrad]
	2149784944944 -> 2149784950320
	2149786650288 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2149786650288 -> 2149784944944
	2149784944944 [label=AccumulateGrad]
	2149784948496 -> 2149784948688
	2149786650768 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2149786650768 -> 2149784948496
	2149784948496 [label=AccumulateGrad]
	2149784947776 -> 2149784948688
	2149786651056 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2149786651056 -> 2149784947776
	2149784947776 [label=AccumulateGrad]
	2149784949936 -> 2149766036928
	2149786651440 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2149786651440 -> 2149784949936
	2149784949936 [label=AccumulateGrad]
	2149784945472 -> 2149766036736
	2149786651536 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2149786651536 -> 2149784945472
	2149784945472 [label=AccumulateGrad]
	2149784944896 -> 2149766036736
	2149786651632 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2149786651632 -> 2149784944896
	2149784944896 [label=AccumulateGrad]
	2149766037168 -> 2149766287648
	2149766297248 -> 2149766295952
	2149786652016 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2149786652016 -> 2149766297248
	2149766297248 [label=AccumulateGrad]
	2149766293936 -> 2149766293504
	2149786652112 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2149786652112 -> 2149766293936
	2149766293936 [label=AccumulateGrad]
	2149766297200 -> 2149766293504
	2149786652208 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2149786652208 -> 2149766297200
	2149766297200 [label=AccumulateGrad]
	2149766288512 -> 2149766297728
	2149786652592 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2149786652592 -> 2149766288512
	2149766288512 [label=AccumulateGrad]
	2149766290528 -> 2149766296960
	2149786652688 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2149786652688 -> 2149766290528
	2149766290528 [label=AccumulateGrad]
	2149766297440 -> 2149766296960
	2149786652784 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2149786652784 -> 2149766297440
	2149766297440 [label=AccumulateGrad]
	2149766294848 -> 2149766283328
	2149766290576 -> 2149766295232
	2149786653744 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2149786653744 -> 2149766290576
	2149766290576 [label=AccumulateGrad]
	2149766299264 -> 2149766291440
	2149786653840 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2149786653840 -> 2149766299264
	2149766299264 [label=AccumulateGrad]
	2149766286160 -> 2149766291440
	2149786653936 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2149786653936 -> 2149766286160
	2149766286160 [label=AccumulateGrad]
	2149766287600 -> 2149766286016
	2149786654320 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2149786654320 -> 2149766287600
	2149766287600 [label=AccumulateGrad]
	2149766297488 -> 2149766298880
	2149786654416 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2149786654416 -> 2149766297488
	2149766297488 [label=AccumulateGrad]
	2149766291488 -> 2149766298880
	2149786654512 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2149786654512 -> 2149766291488
	2149766291488 [label=AccumulateGrad]
	2149766290912 -> 2149766298160
	2149766290912 [label=CudnnBatchNormBackward0]
	2149766291104 -> 2149766290912
	2149766291104 [label=ConvolutionBackward0]
	2149766293888 -> 2149766291104
	2149766295568 -> 2149766291104
	2149786653168 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2149786653168 -> 2149766295568
	2149766295568 [label=AccumulateGrad]
	2149766285056 -> 2149766290912
	2149786653264 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2149786653264 -> 2149766285056
	2149766285056 [label=AccumulateGrad]
	2149766291584 -> 2149766290912
	2149786653360 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2149786653360 -> 2149766291584
	2149766291584 [label=AccumulateGrad]
	2149766296576 -> 2149766298496
	2149786654896 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2149786654896 -> 2149766296576
	2149766296576 [label=AccumulateGrad]
	2149766287024 -> 2149766292448
	2149786654992 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2149786654992 -> 2149766287024
	2149766287024 [label=AccumulateGrad]
	2149766293408 -> 2149766292448
	2149786655088 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2149786655088 -> 2149766293408
	2149766293408 [label=AccumulateGrad]
	2149766291632 -> 2149766290048
	2149786655472 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2149786655472 -> 2149766291632
	2149766291632 [label=AccumulateGrad]
	2149766297344 -> 2149766296336
	2149786655568 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2149786655568 -> 2149766297344
	2149766297344 [label=AccumulateGrad]
	2149766289472 -> 2149766296336
	2149786655664 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2149786655664 -> 2149766289472
	2149766289472 [label=AccumulateGrad]
	2149766297152 -> 2149766292016
	2149766299360 -> 2149766285632
	2149786656624 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2149786656624 -> 2149766299360
	2149766299360 [label=AccumulateGrad]
	2149766298592 -> 2149766286448
	2149786656720 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2149786656720 -> 2149766298592
	2149766298592 [label=AccumulateGrad]
	2149766295808 -> 2149766286448
	2149786656816 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2149786656816 -> 2149766295808
	2149766295808 [label=AccumulateGrad]
	2149766298016 -> 2149766284096
	2149786657200 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2149786657200 -> 2149766298016
	2149766298016 [label=AccumulateGrad]
	2149766286592 -> 2149766292496
	2149786657296 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2149786657296 -> 2149766286592
	2149766286592 [label=AccumulateGrad]
	2149766286832 -> 2149766292496
	2149786657392 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2149786657392 -> 2149766286832
	2149766286832 [label=AccumulateGrad]
	2149766284864 -> 2149766293648
	2149766284864 [label=CudnnBatchNormBackward0]
	2149766285872 -> 2149766284864
	2149766285872 [label=ConvolutionBackward0]
	2149766287696 -> 2149766285872
	2149766291968 -> 2149766285872
	2149786656048 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2149786656048 -> 2149766291968
	2149766291968 [label=AccumulateGrad]
	2149766286352 -> 2149766284864
	2149786656144 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2149786656144 -> 2149766286352
	2149766286352 [label=AccumulateGrad]
	2149766286880 -> 2149766284864
	2149786656240 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2149786656240 -> 2149766286880
	2149766286880 [label=AccumulateGrad]
	2149766291392 -> 2149766286544
	2149786657776 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2149786657776 -> 2149766291392
	2149766291392 [label=AccumulateGrad]
	2149766297392 -> 2149766296384
	2149786657872 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2149786657872 -> 2149766297392
	2149766297392 [label=AccumulateGrad]
	2149766285248 -> 2149766296384
	2149786657968 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2149786657968 -> 2149766285248
	2149766285248 [label=AccumulateGrad]
	2149766297920 -> 2149766292976
	2149786658352 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2149786658352 -> 2149766297920
	2149766297920 [label=AccumulateGrad]
	2149766295040 -> 2149766296432
	2149786658448 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2149786658448 -> 2149766295040
	2149766295040 [label=AccumulateGrad]
	2149766289712 -> 2149766296432
	2149786658544 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2149786658544 -> 2149766289712
	2149766289712 [label=AccumulateGrad]
	2149766296144 -> 2149766298400
	2149766299312 -> 2149766292640
	2149786659504 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2149786659504 -> 2149766299312
	2149766299312 [label=AccumulateGrad]
	2149766293552 -> 2149766296912
	2149786659600 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2149786659600 -> 2149766293552
	2149766293552 [label=AccumulateGrad]
	2149766290432 -> 2149766296912
	2149786659696 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2149786659696 -> 2149766290432
	2149766290432 [label=AccumulateGrad]
	2149766298688 -> 2149766296816
	2149786660080 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2149786660080 -> 2149766298688
	2149766298688 [label=AccumulateGrad]
	2149766289184 -> 2149766293792
	2149786660176 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2149786660176 -> 2149766289184
	2149766289184 [label=AccumulateGrad]
	2149766289760 -> 2149766293792
	2149786660272 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2149786660272 -> 2149766289760
	2149766289760 [label=AccumulateGrad]
	2149766291296 -> 2149766294512
	2149766291296 [label=CudnnBatchNormBackward0]
	2149766284960 -> 2149766291296
	2149766284960 [label=ConvolutionBackward0]
	2149766296480 -> 2149766284960
	2149766284768 -> 2149766284960
	2149786658928 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2149786658928 -> 2149766284768
	2149766284768 [label=AccumulateGrad]
	2149766294368 -> 2149766291296
	2149786659024 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2149786659024 -> 2149766294368
	2149766294368 [label=AccumulateGrad]
	2149766294704 -> 2149766291296
	2149786659120 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2149786659120 -> 2149766294704
	2149766294704 [label=AccumulateGrad]
	2149766295760 -> 2149766283568
	2149786660656 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2149786660656 -> 2149766295760
	2149766295760 [label=AccumulateGrad]
	2149766290336 -> 2149766292064
	2149786660752 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2149786660752 -> 2149766290336
	2149766290336 [label=AccumulateGrad]
	2149766289616 -> 2149766292064
	2149786660848 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2149786660848 -> 2149766289616
	2149766289616 [label=AccumulateGrad]
	2149766284432 -> 2149766290720
	2149786661232 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2149786661232 -> 2149766284432
	2149766284432 [label=AccumulateGrad]
	2149766289856 -> 2149766293600
	2149786661328 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2149786661328 -> 2149766289856
	2149766289856 [label=AccumulateGrad]
	2149766295664 -> 2149766293600
	2149786661424 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2149786661424 -> 2149766295664
	2149766295664 [label=AccumulateGrad]
	2149766288752 -> 2149766286688
	2149766287504 -> 2149766292592
	2149766287504 [label=TBackward0]
	2149766295184 -> 2149766287504
	2149786662000 [label="model.fc.weight
 (19, 512)" fillcolor=lightblue]
	2149786662000 -> 2149766295184
	2149766295184 [label=AccumulateGrad]
	2149766284048 -> 2149783427632
}
