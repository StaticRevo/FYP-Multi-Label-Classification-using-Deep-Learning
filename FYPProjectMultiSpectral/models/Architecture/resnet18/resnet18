digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2311213833040 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2311247590832 [label=AddmmBackward0]
	2311247590496 -> 2311247590832
	2311208028304 [label="fc.bias
 (19)" fillcolor=lightblue]
	2311208028304 -> 2311247590496
	2311247590496 [label=AccumulateGrad]
	2311247589872 -> 2311247590832
	2311247589872 [label=ViewBackward0]
	2311247589392 -> 2311247589872
	2311247589392 [label=MeanBackward1]
	2311247589056 -> 2311247589392
	2311247589056 [label=ReluBackward0]
	2311247588576 -> 2311247589056
	2311247588576 [label=AddBackward0]
	2311247588096 -> 2311247588576
	2311247588096 [label=CudnnBatchNormBackward0]
	2311247586992 -> 2311247588096
	2311247586992 [label=ConvolutionBackward0]
	2311247586032 -> 2311247586992
	2311247586032 [label=ReluBackward0]
	2311247585696 -> 2311247586032
	2311247585696 [label=CudnnBatchNormBackward0]
	2311247585216 -> 2311247585696
	2311247585216 [label=ConvolutionBackward0]
	2311247587952 -> 2311247585216
	2311247587952 [label=ReluBackward0]
	2311247583776 -> 2311247587952
	2311247583776 [label=AddBackward0]
	2311247583296 -> 2311247583776
	2311247583296 [label=CudnnBatchNormBackward0]
	2311247582192 -> 2311247583296
	2311247582192 [label=ConvolutionBackward0]
	2311247581232 -> 2311247582192
	2311247581232 [label=ReluBackward0]
	2311247580896 -> 2311247581232
	2311247580896 [label=CudnnBatchNormBackward0]
	2311247579408 -> 2311247580896
	2311247579408 [label=ConvolutionBackward0]
	2311247593184 -> 2311247579408
	2311247593184 [label=ReluBackward0]
	2311247592848 -> 2311247593184
	2311247592848 [label=AddBackward0]
	2311247592656 -> 2311247592848
	2311247592656 [label=CudnnBatchNormBackward0]
	2311247592704 -> 2311247592656
	2311247592704 [label=ConvolutionBackward0]
	2311247592320 -> 2311247592704
	2311247592320 [label=ReluBackward0]
	2311247592080 -> 2311247592320
	2311247592080 [label=CudnnBatchNormBackward0]
	2311247592032 -> 2311247592080
	2311247592032 [label=ConvolutionBackward0]
	2311247592800 -> 2311247592032
	2311247592800 [label=ReluBackward0]
	2311247591600 -> 2311247592800
	2311247591600 [label=AddBackward0]
	2311247591552 -> 2311247591600
	2311247591552 [label=CudnnBatchNormBackward0]
	2311247591360 -> 2311247591552
	2311247591360 [label=ConvolutionBackward0]
	2311247591264 -> 2311247591360
	2311247591264 [label=ReluBackward0]
	2311247590928 -> 2311247591264
	2311247590928 [label=CudnnBatchNormBackward0]
	2311247590736 -> 2311247590928
	2311247590736 [label=ConvolutionBackward0]
	2311247590592 -> 2311247590736
	2311247590592 [label=ReluBackward0]
	2311247590400 -> 2311247590592
	2311247590400 [label=AddBackward0]
	2311247590064 -> 2311247590400
	2311247590064 [label=CudnnBatchNormBackward0]
	2311247590112 -> 2311247590064
	2311247590112 [label=ConvolutionBackward0]
	2311247589776 -> 2311247590112
	2311247589776 [label=ReluBackward0]
	2311247589824 -> 2311247589776
	2311247589824 [label=CudnnBatchNormBackward0]
	2311247589728 -> 2311247589824
	2311247589728 [label=ConvolutionBackward0]
	2311247590256 -> 2311247589728
	2311247590256 [label=ReluBackward0]
	2311247589344 -> 2311247590256
	2311247589344 [label=AddBackward0]
	2311247589248 -> 2311247589344
	2311247589248 [label=CudnnBatchNormBackward0]
	2311247588816 -> 2311247589248
	2311247588816 [label=ConvolutionBackward0]
	2311247588672 -> 2311247588816
	2311247588672 [label=ReluBackward0]
	2311247588480 -> 2311247588672
	2311247588480 [label=CudnnBatchNormBackward0]
	2311247588144 -> 2311247588480
	2311247588144 [label=ConvolutionBackward0]
	2311247588288 -> 2311247588144
	2311247588288 [label=ReluBackward0]
	2311247587856 -> 2311247588288
	2311247587856 [label=AddBackward0]
	2311247587760 -> 2311247587856
	2311247587760 [label=CudnnBatchNormBackward0]
	2311247587808 -> 2311247587760
	2311247587808 [label=ConvolutionBackward0]
	2311247587184 -> 2311247587808
	2311247587184 [label=ReluBackward0]
	2311247587232 -> 2311247587184
	2311247587232 [label=CudnnBatchNormBackward0]
	2311247587088 -> 2311247587232
	2311247587088 [label=ConvolutionBackward0]
	2311247587664 -> 2311247587088
	2311247587664 [label=ReluBackward0]
	2311247586752 -> 2311247587664
	2311247586752 [label=AddBackward0]
	2311247586608 -> 2311247586752
	2311247586608 [label=CudnnBatchNormBackward0]
	2311247586224 -> 2311247586608
	2311247586224 [label=ConvolutionBackward0]
	2311247586368 -> 2311247586224
	2311247586368 [label=ReluBackward0]
	2311247585936 -> 2311247586368
	2311247585936 [label=CudnnBatchNormBackward0]
	2311247585840 -> 2311247585936
	2311247585840 [label=ConvolutionBackward0]
	2311247586848 -> 2311247585840
	2311247586848 [label=MaxPool2DWithIndicesBackward0]
	2311247585456 -> 2311247586848
	2311247585456 [label=ReluBackward0]
	2311247585360 -> 2311247585456
	2311247585360 [label=CudnnBatchNormBackward0]
	2311247585312 -> 2311247585360
	2311247585312 [label=ConvolutionBackward0]
	2311247584976 -> 2311247585312
	2311208028112 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2311208028112 -> 2311247584976
	2311247584976 [label=AccumulateGrad]
	2311247585504 -> 2311247585360
	2311231530256 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2311231530256 -> 2311247585504
	2311247585504 [label=AccumulateGrad]
	2311247585792 -> 2311247585360
	2311231529968 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2311231529968 -> 2311247585792
	2311247585792 [label=AccumulateGrad]
	2311247585648 -> 2311247585840
	2311231530448 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2311231530448 -> 2311247585648
	2311247585648 [label=AccumulateGrad]
	2311247585744 -> 2311247585936
	2311231530544 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2311231530544 -> 2311247585744
	2311247585744 [label=AccumulateGrad]
	2311247586128 -> 2311247585936
	2311231530640 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2311231530640 -> 2311247586128
	2311247586128 [label=AccumulateGrad]
	2311247586272 -> 2311247586224
	2311231529872 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2311231529872 -> 2311247586272
	2311247586272 [label=AccumulateGrad]
	2311247586416 -> 2311247586608
	2311231529776 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2311231529776 -> 2311247586416
	2311247586416 [label=AccumulateGrad]
	2311247586560 -> 2311247586608
	2311231529008 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2311231529008 -> 2311247586560
	2311247586560 [label=AccumulateGrad]
	2311247586848 -> 2311247586752
	2311247586800 -> 2311247587088
	2311231529104 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2311231529104 -> 2311247586800
	2311247586800 [label=AccumulateGrad]
	2311247587328 -> 2311247587232
	2311231529200 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2311231529200 -> 2311247587328
	2311247587328 [label=AccumulateGrad]
	2311247587280 -> 2311247587232
	2311231529296 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2311231529296 -> 2311247587280
	2311247587280 [label=AccumulateGrad]
	2311247587376 -> 2311247587808
	2311231529584 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2311231529584 -> 2311247587376
	2311247587376 [label=AccumulateGrad]
	2311247587712 -> 2311247587760
	2311231528624 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2311231528624 -> 2311247587712
	2311247587712 [label=AccumulateGrad]
	2311247587904 -> 2311247587760
	2311231528528 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2311231528528 -> 2311247587904
	2311247587904 [label=AccumulateGrad]
	2311247587664 -> 2311247587856
	2311247588192 -> 2311247588144
	2311231528432 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2311231528432 -> 2311247588192
	2311247588192 [label=AccumulateGrad]
	2311247588336 -> 2311247588480
	2311231528336 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2311231528336 -> 2311247588336
	2311247588336 [label=AccumulateGrad]
	2311247588768 -> 2311247588480
	2311208232976 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2311208232976 -> 2311247588768
	2311247588768 [label=AccumulateGrad]
	2311247588864 -> 2311247588816
	2311208233360 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2311208233360 -> 2311247588864
	2311247588864 [label=AccumulateGrad]
	2311247588960 -> 2311247589248
	2311208233456 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2311208233456 -> 2311247588960
	2311247588960 [label=AccumulateGrad]
	2311247589008 -> 2311247589248
	2311208233552 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2311208233552 -> 2311247589008
	2311247589008 [label=AccumulateGrad]
	2311247589152 -> 2311247589344
	2311247589152 [label=CudnnBatchNormBackward0]
	2311247588384 -> 2311247589152
	2311247588384 [label=ConvolutionBackward0]
	2311247588288 -> 2311247588384
	2311247588048 -> 2311247588384
	2311231525840 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2311231525840 -> 2311247588048
	2311247588048 [label=AccumulateGrad]
	2311247588720 -> 2311247589152
	2311231526032 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2311231526032 -> 2311247588720
	2311247588720 [label=AccumulateGrad]
	2311247588624 -> 2311247589152
	2311231527664 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2311231527664 -> 2311247588624
	2311247588624 [label=AccumulateGrad]
	2311247589104 -> 2311247589728
	2311208233936 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2311208233936 -> 2311247589104
	2311247589104 [label=AccumulateGrad]
	2311247589632 -> 2311247589824
	2311208234032 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2311208234032 -> 2311247589632
	2311247589632 [label=AccumulateGrad]
	2311247589584 -> 2311247589824
	2311208234128 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2311208234128 -> 2311247589584
	2311247589584 [label=AccumulateGrad]
	2311247589920 -> 2311247590112
	2311208234512 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2311208234512 -> 2311247589920
	2311247589920 [label=AccumulateGrad]
	2311247590304 -> 2311247590064
	2311208234608 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2311208234608 -> 2311247590304
	2311247590304 [label=AccumulateGrad]
	2311247590160 -> 2311247590064
	2311208234704 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2311208234704 -> 2311247590160
	2311247590160 [label=AccumulateGrad]
	2311247590256 -> 2311247590400
	2311247590784 -> 2311247590736
	2311208235664 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2311208235664 -> 2311247590784
	2311247590784 [label=AccumulateGrad]
	2311247590880 -> 2311247590928
	2311208235760 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2311208235760 -> 2311247590880
	2311247590880 [label=AccumulateGrad]
	2311247591072 -> 2311247590928
	2311208235856 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2311208235856 -> 2311247591072
	2311247591072 [label=AccumulateGrad]
	2311247591120 -> 2311247591360
	2311208236240 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2311208236240 -> 2311247591120
	2311247591120 [label=AccumulateGrad]
	2311247591408 -> 2311247591552
	2311208236336 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2311208236336 -> 2311247591408
	2311247591408 [label=AccumulateGrad]
	2311247591648 -> 2311247591552
	2311208236432 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2311208236432 -> 2311247591648
	2311247591648 [label=AccumulateGrad]
	2311247591744 -> 2311247591600
	2311247591744 [label=CudnnBatchNormBackward0]
	2311247590640 -> 2311247591744
	2311247590640 [label=ConvolutionBackward0]
	2311247590592 -> 2311247590640
	2311247590688 -> 2311247590640
	2311208235088 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2311208235088 -> 2311247590688
	2311247590688 [label=AccumulateGrad]
	2311247591024 -> 2311247591744
	2311208235184 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2311208235184 -> 2311247591024
	2311247591024 [label=AccumulateGrad]
	2311247591216 -> 2311247591744
	2311208235280 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2311208235280 -> 2311247591216
	2311247591216 [label=AccumulateGrad]
	2311247591696 -> 2311247592032
	2311208236816 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2311208236816 -> 2311247591696
	2311247591696 [label=AccumulateGrad]
	2311247592224 -> 2311247592080
	2311208236912 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2311208236912 -> 2311247592224
	2311247592224 [label=AccumulateGrad]
	2311247592176 -> 2311247592080
	2311208237008 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2311208237008 -> 2311247592176
	2311247592176 [label=AccumulateGrad]
	2311247592368 -> 2311247592704
	2311208237392 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2311208237392 -> 2311247592368
	2311247592368 [label=AccumulateGrad]
	2311247592560 -> 2311247592656
	2311208237488 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2311208237488 -> 2311247592560
	2311247592560 [label=AccumulateGrad]
	2311247592464 -> 2311247592656
	2311208237584 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2311208237584 -> 2311247592464
	2311247592464 [label=AccumulateGrad]
	2311247592800 -> 2311247592848
	2311247593424 -> 2311247579408
	2311208238544 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2311208238544 -> 2311247593424
	2311247593424 [label=AccumulateGrad]
	2311247578736 -> 2311247580896
	2311208238640 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2311208238640 -> 2311247578736
	2311247578736 [label=AccumulateGrad]
	2311247581376 -> 2311247580896
	2311208238736 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2311208238736 -> 2311247581376
	2311247581376 [label=AccumulateGrad]
	2311247581856 -> 2311247582192
	2311208026192 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2311208026192 -> 2311247581856
	2311247581856 [label=AccumulateGrad]
	2311247582816 -> 2311247583296
	2311208026288 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2311208026288 -> 2311247582816
	2311247582816 [label=AccumulateGrad]
	2311247582672 -> 2311247583296
	2311208026384 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2311208026384 -> 2311247582672
	2311247582672 [label=AccumulateGrad]
	2311247583152 -> 2311247583776
	2311247583152 [label=CudnnBatchNormBackward0]
	2311247579168 -> 2311247583152
	2311247579168 [label=ConvolutionBackward0]
	2311247593184 -> 2311247579168
	2311247592992 -> 2311247579168
	2311208237968 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2311208237968 -> 2311247592992
	2311247592992 [label=AccumulateGrad]
	2311247581712 -> 2311247583152
	2311208238064 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2311208238064 -> 2311247581712
	2311247581712 [label=AccumulateGrad]
	2311247582336 -> 2311247583152
	2311208238160 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2311208238160 -> 2311247582336
	2311247582336 [label=AccumulateGrad]
	2311247584256 -> 2311247585216
	2311208026768 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2311208026768 -> 2311247584256
	2311247584256 [label=AccumulateGrad]
	2311247585072 -> 2311247585696
	2311208026864 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2311208026864 -> 2311247585072
	2311247585072 [label=AccumulateGrad]
	2311247586176 -> 2311247585696
	2311208026960 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2311208026960 -> 2311247586176
	2311247586176 [label=AccumulateGrad]
	2311247586656 -> 2311247586992
	2311208027344 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2311208027344 -> 2311247586656
	2311247586656 [label=AccumulateGrad]
	2311247587616 -> 2311247588096
	2311208027440 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2311208027440 -> 2311247587616
	2311247587616 [label=AccumulateGrad]
	2311247587472 -> 2311247588096
	2311208027536 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2311208027536 -> 2311247587472
	2311247587472 [label=AccumulateGrad]
	2311247587952 -> 2311247588576
	2311247590016 -> 2311247590832
	2311247590016 [label=TBackward0]
	2311247588432 -> 2311247590016
	2311208028208 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2311208028208 -> 2311247588432
	2311247588432 [label=AccumulateGrad]
	2311247590832 -> 2311213833040
}
