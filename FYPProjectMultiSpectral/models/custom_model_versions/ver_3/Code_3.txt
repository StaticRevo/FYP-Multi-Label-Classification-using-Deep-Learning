# Custom Model Version 3
class CustomModelV3(BaseModel):
    def __init__(self, class_weights, num_classes, in_channels, model_weights, main_path):
        dummy_model = nn.Identity()
        super(CustomModelV3, self).__init__(dummy_model, num_classes, class_weights, in_channels, main_path)
    
        # Spectral Mixing & Initial Feature Extraction
        self.spectral_mixer = nn.Sequential(
            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1, bias=False),
            nn.BatchNorm2d(32),
            nn.GELU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # -- Block 1 --
        self.block1 = nn.Sequential(
            DepthwiseSeparableConv(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, 
                                   dilation=1, bias=False, padding_mode='zeros'), 
            nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
            nn.GELU(),
            ResidualBlock(in_channels=64, out_channels=64, stride=1), 
            SpectralAttention(in_channels=64), 
            CoordinateAttention(in_channels=64, reduction=16), 
        )
        # -- Block 2 --
        self.block2 = nn.Sequential(
            DepthwiseSeparableConv(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1, 
                                   dilation=1, bias=False, padding_mode='zeros'), 
            nn.BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
            nn.GELU(),
            MultiScaleBlock(in_channels=128, out_channels=128, kernel_size=3, stride=1, groups=1, 
                            bias=True, padding_mode='zeros'), 
            ResidualBlock(in_channels=128, out_channels=128, stride=1), 
            ECA(in_channels=128), 
        )
        # -- BLock 3 --
        self.block3 = nn.Sequential(
            DepthwiseSeparableConv(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1, 
                                  dilation=1, bias=False, padding_mode='zeros'), 
            nn.BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
            nn.GELU(),
            MultiScaleBlock(in_channels=256, out_channels=256, kernel_size=3, stride=1, groups=1, bias=True, padding_mode='zeros'), 
            ResidualBlock(in_channels=256, out_channels=256, stride=1), 
            SE(in_channels=256, kernel_size=1), 
        )
        self.transformer_block = TransformerModule(d_model=256, nhead=8, num_layers=1, dropout=0.2, return_mode="reshape")
        self.skip_adapter = nn.Conv2d(64, 256, kernel_size=1, bias=False)

        # -- Block 4 -- 
        self.block4 = nn.Sequential(
            DepthwiseSeparableConv(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1, 
                                   dilation=1, bias=False, padding_mode='zeros'), 
            nn.BatchNorm2d(num_features=512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
            nn.GELU(),
            ResidualBlock(in_channels=512, out_channels=512, stride=1),
            DualAttention(in_channels=512, kernel_size=7, stride=1), 
        )
        # -- Block 5 -- 
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(output_size=(1, 1)), 
            nn.Flatten(), 
            nn.Dropout(p=ModuleConfig.dropout_rt), 
            nn.Linear(in_features=512, out_features=num_classes) 
        )
    # Override forward function for CustomModel
    def forward(self, x):
        x = self.spectral_mixer(x) # Step 1: Spectral mixing
        features_low = self.block1(x) # Step 2: Extract low-level features.
        features_mid = self.block2(features_low) # Step 3: Compute mid-level features.
        features_deep = self.block3(features_mid) # Step 4: Compute deep features.
        features_deep = self.transformer_block(features_deep)  # Apply transformer block for global context
        adapted_features_low = self.skip_adapter(features_low) # Adjust low-level features
    
        # If spatial dimensions differ interpolate adapted_features_low to match features_deep
        adapted_features_low = torch.nn.functional.interpolate(
            adapted_features_low,
            size=(15, 15),  
            mode='bilinear',
            align_corners=False
        )
    
        fused_features = features_deep + adapted_features_low  # Step 5: Fuse features.
        features_high = self.block4(fused_features)      # Step 6: Refine high-level representations.
        out = self.classifier(features_high)             # Step 7: Final classification.
        return out

    # Override optimizer configuration for CustomModel
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=ModelConfig.learning_rate, weight_decay=ModelConfig.weight_decay)                
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=ModelConfig.lr_factor,  patience=ModelConfig.lr_patience)                                                  
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val_loss',
                'interval': 'epoch',
                'frequency': 1
            }
        }