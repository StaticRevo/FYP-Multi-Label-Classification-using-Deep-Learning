digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2163412240432 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2163377129312 [label=AddmmBackward0]
	2163377141360 -> 2163377129312
	2163405157584 [label="fc.bias
 (19)" fillcolor=lightblue]
	2163405157584 -> 2163377141360
	2163377141360 [label=AccumulateGrad]
	2163377137424 -> 2163377129312
	2163377137424 [label=ViewBackward0]
	2163377131760 -> 2163377137424
	2163377131760 [label=MeanBackward1]
	2163377138624 -> 2163377131760
	2163377138624 [label=ReluBackward0]
	2163377128544 -> 2163377138624
	2163377128544 [label=AddBackward0]
	2163377135264 -> 2163377128544
	2163377135264 [label=CudnnBatchNormBackward0]
	2163377127968 -> 2163377135264
	2163377127968 [label=ConvolutionBackward0]
	2163377132384 -> 2163377127968
	2163377132384 [label=ReluBackward0]
	2163377135024 -> 2163377132384
	2163377135024 [label=CudnnBatchNormBackward0]
	2163377129168 -> 2163377135024
	2163377129168 [label=ConvolutionBackward0]
	2163377135360 -> 2163377129168
	2163377135360 [label=ReluBackward0]
	2163377128640 -> 2163377135360
	2163377128640 [label=AddBackward0]
	2163377130128 -> 2163377128640
	2163377130128 [label=CudnnBatchNormBackward0]
	2163377135984 -> 2163377130128
	2163377135984 [label=ConvolutionBackward0]
	2163377129840 -> 2163377135984
	2163377129840 [label=ReluBackward0]
	2163377127872 -> 2163377129840
	2163377127872 [label=CudnnBatchNormBackward0]
	2163377129360 -> 2163377127872
	2163377129360 [label=ConvolutionBackward0]
	2163377141648 -> 2163377129360
	2163377141648 [label=ReluBackward0]
	2163377142704 -> 2163377141648
	2163377142704 [label=AddBackward0]
	2163377134352 -> 2163377142704
	2163377134352 [label=CudnnBatchNormBackward0]
	2163377136944 -> 2163377134352
	2163377136944 [label=ConvolutionBackward0]
	2163377136848 -> 2163377136944
	2163377136848 [label=ReluBackward0]
	2163377130032 -> 2163377136848
	2163377130032 [label=CudnnBatchNormBackward0]
	2163377133824 -> 2163377130032
	2163377133824 [label=ConvolutionBackward0]
	2163377130800 -> 2163377133824
	2163377130800 [label=ReluBackward0]
	2163377138528 -> 2163377130800
	2163377138528 [label=AddBackward0]
	2163377142848 -> 2163377138528
	2163377142848 [label=CudnnBatchNormBackward0]
	2163377137040 -> 2163377142848
	2163377137040 [label=ConvolutionBackward0]
	2163377128352 -> 2163377137040
	2163377128352 [label=ReluBackward0]
	2163377142992 -> 2163377128352
	2163377142992 [label=CudnnBatchNormBackward0]
	2163377138192 -> 2163377142992
	2163377138192 [label=ConvolutionBackward0]
	2163377130752 -> 2163377138192
	2163377130752 [label=ReluBackward0]
	2163377136464 -> 2163377130752
	2163377136464 [label=AddBackward0]
	2163377128736 -> 2163377136464
	2163377128736 [label=CudnnBatchNormBackward0]
	2163377138144 -> 2163377128736
	2163377138144 [label=ConvolutionBackward0]
	2163313847664 -> 2163377138144
	2163313847664 [label=ReluBackward0]
	2163313847808 -> 2163313847664
	2163313847808 [label=CudnnBatchNormBackward0]
	2163313847904 -> 2163313847808
	2163313847904 [label=ConvolutionBackward0]
	2163377129888 -> 2163313847904
	2163377129888 [label=ReluBackward0]
	2163313848192 -> 2163377129888
	2163313848192 [label=AddBackward0]
	2163313848288 -> 2163313848192
	2163313848288 [label=CudnnBatchNormBackward0]
	2163313848432 -> 2163313848288
	2163313848432 [label=ConvolutionBackward0]
	2163313848624 -> 2163313848432
	2163313848624 [label=ReluBackward0]
	2163313848768 -> 2163313848624
	2163313848768 [label=CudnnBatchNormBackward0]
	2163313848864 -> 2163313848768
	2163313848864 [label=ConvolutionBackward0]
	2163313849056 -> 2163313848864
	2163313849056 [label=ReluBackward0]
	2163313849200 -> 2163313849056
	2163313849200 [label=AddBackward0]
	2163313849296 -> 2163313849200
	2163313849296 [label=CudnnBatchNormBackward0]
	2163313849440 -> 2163313849296
	2163313849440 [label=ConvolutionBackward0]
	2163313849632 -> 2163313849440
	2163313849632 [label=ReluBackward0]
	2163313849776 -> 2163313849632
	2163313849776 [label=CudnnBatchNormBackward0]
	2163313849872 -> 2163313849776
	2163313849872 [label=ConvolutionBackward0]
	2163313849248 -> 2163313849872
	2163313849248 [label=ReluBackward0]
	2163313850160 -> 2163313849248
	2163313850160 [label=AddBackward0]
	2163313850256 -> 2163313850160
	2163313850256 [label=CudnnBatchNormBackward0]
	2163313850400 -> 2163313850256
	2163313850400 [label=ConvolutionBackward0]
	2163313850592 -> 2163313850400
	2163313850592 [label=ReluBackward0]
	2163313850736 -> 2163313850592
	2163313850736 [label=CudnnBatchNormBackward0]
	2163313850832 -> 2163313850736
	2163313850832 [label=ConvolutionBackward0]
	2163313850208 -> 2163313850832
	2163313850208 [label=MaxPool2DWithIndicesBackward0]
	2163313851120 -> 2163313850208
	2163313851120 [label=ReluBackward0]
	2163313851216 -> 2163313851120
	2163313851216 [label=CudnnBatchNormBackward0]
	2163313851312 -> 2163313851216
	2163313851312 [label=ConvolutionBackward0]
	2163313851504 -> 2163313851312
	2163381137200 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2163381137200 -> 2163313851504
	2163313851504 [label=AccumulateGrad]
	2163313851264 -> 2163313851216
	2163381125392 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2163381125392 -> 2163313851264
	2163313851264 [label=AccumulateGrad]
	2163313850928 -> 2163313851216
	2163381125488 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2163381125488 -> 2163313850928
	2163313850928 [label=AccumulateGrad]
	2163313851024 -> 2163313850832
	2163381125872 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2163381125872 -> 2163313851024
	2163313851024 [label=AccumulateGrad]
	2163313850784 -> 2163313850736
	2163381125968 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2163381125968 -> 2163313850784
	2163313850784 [label=AccumulateGrad]
	2163313850640 -> 2163313850736
	2163381126064 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2163381126064 -> 2163313850640
	2163313850640 [label=AccumulateGrad]
	2163313850544 -> 2163313850400
	2163381126448 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2163381126448 -> 2163313850544
	2163313850544 [label=AccumulateGrad]
	2163313850352 -> 2163313850256
	2163381126544 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2163381126544 -> 2163313850352
	2163313850352 [label=AccumulateGrad]
	2163313850304 -> 2163313850256
	2163381126640 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2163381126640 -> 2163313850304
	2163313850304 [label=AccumulateGrad]
	2163313850208 -> 2163313850160
	2163313850064 -> 2163313849872
	2163381127024 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2163381127024 -> 2163313850064
	2163313850064 [label=AccumulateGrad]
	2163313849824 -> 2163313849776
	2163381127120 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2163381127120 -> 2163313849824
	2163313849824 [label=AccumulateGrad]
	2163313849680 -> 2163313849776
	2163381127216 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2163381127216 -> 2163313849680
	2163313849680 [label=AccumulateGrad]
	2163313849584 -> 2163313849440
	2163381127600 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2163381127600 -> 2163313849584
	2163313849584 [label=AccumulateGrad]
	2163313849392 -> 2163313849296
	2163381127696 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2163381127696 -> 2163313849392
	2163313849392 [label=AccumulateGrad]
	2163313849344 -> 2163313849296
	2163381127792 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2163381127792 -> 2163313849344
	2163313849344 [label=AccumulateGrad]
	2163313849248 -> 2163313849200
	2163313849008 -> 2163313848864
	2163381128752 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2163381128752 -> 2163313849008
	2163313849008 [label=AccumulateGrad]
	2163313848816 -> 2163313848768
	2163381128848 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2163381128848 -> 2163313848816
	2163313848816 [label=AccumulateGrad]
	2163313848672 -> 2163313848768
	2163381128944 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2163381128944 -> 2163313848672
	2163313848672 [label=AccumulateGrad]
	2163313848576 -> 2163313848432
	2163381129328 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2163381129328 -> 2163313848576
	2163313848576 [label=AccumulateGrad]
	2163313848384 -> 2163313848288
	2163381129424 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2163381129424 -> 2163313848384
	2163313848384 [label=AccumulateGrad]
	2163313848336 -> 2163313848288
	2163381129520 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2163381129520 -> 2163313848336
	2163313848336 [label=AccumulateGrad]
	2163313848240 -> 2163313848192
	2163313848240 [label=CudnnBatchNormBackward0]
	2163313848960 -> 2163313848240
	2163313848960 [label=ConvolutionBackward0]
	2163313849056 -> 2163313848960
	2163313849104 -> 2163313848960
	2163381128176 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2163381128176 -> 2163313849104
	2163313849104 [label=AccumulateGrad]
	2163313848528 -> 2163313848240
	2163381128272 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2163381128272 -> 2163313848528
	2163313848528 [label=AccumulateGrad]
	2163313848480 -> 2163313848240
	2163381128368 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2163381128368 -> 2163313848480
	2163313848480 [label=AccumulateGrad]
	2163313848096 -> 2163313847904
	2163381129904 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2163381129904 -> 2163313848096
	2163313848096 [label=AccumulateGrad]
	2163313847856 -> 2163313847808
	2163381130000 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2163381130000 -> 2163313847856
	2163313847856 [label=AccumulateGrad]
	2163313847712 -> 2163313847808
	2163381130096 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2163381130096 -> 2163313847712
	2163313847712 [label=AccumulateGrad]
	2163313847616 -> 2163377138144
	2163381130480 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2163381130480 -> 2163313847616
	2163313847616 [label=AccumulateGrad]
	2163377133104 -> 2163377128736
	2163381130576 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2163381130576 -> 2163377133104
	2163377133104 [label=AccumulateGrad]
	2163313847472 -> 2163377128736
	2163381130672 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2163381130672 -> 2163313847472
	2163313847472 [label=AccumulateGrad]
	2163377129888 -> 2163377136464
	2163377134160 -> 2163377138192
	2163381131632 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2163381131632 -> 2163377134160
	2163377134160 [label=AccumulateGrad]
	2163377132048 -> 2163377142992
	2163381131728 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2163381131728 -> 2163377132048
	2163377132048 [label=AccumulateGrad]
	2163377129216 -> 2163377142992
	2163381131824 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2163381131824 -> 2163377129216
	2163377129216 [label=AccumulateGrad]
	2163377132912 -> 2163377137040
	2163381132208 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2163381132208 -> 2163377132912
	2163377132912 [label=AccumulateGrad]
	2163377130176 -> 2163377142848
	2163381132304 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2163381132304 -> 2163377130176
	2163377130176 [label=AccumulateGrad]
	2163377136128 -> 2163377142848
	2163381132400 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2163381132400 -> 2163377136128
	2163377136128 [label=AccumulateGrad]
	2163377134448 -> 2163377138528
	2163377134448 [label=CudnnBatchNormBackward0]
	2163377143184 -> 2163377134448
	2163377143184 [label=ConvolutionBackward0]
	2163377130752 -> 2163377143184
	2163377142608 -> 2163377143184
	2163381131056 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2163381131056 -> 2163377142608
	2163377142608 [label=AccumulateGrad]
	2163377135600 -> 2163377134448
	2163381131152 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2163381131152 -> 2163377135600
	2163377135600 [label=AccumulateGrad]
	2163377143376 -> 2163377134448
	2163381131248 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2163381131248 -> 2163377143376
	2163377143376 [label=AccumulateGrad]
	2163377129120 -> 2163377133824
	2163381132784 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2163381132784 -> 2163377129120
	2163377129120 [label=AccumulateGrad]
	2163377131376 -> 2163377130032
	2163381132880 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2163381132880 -> 2163377131376
	2163377131376 [label=AccumulateGrad]
	2163377136656 -> 2163377130032
	2163381132976 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2163381132976 -> 2163377136656
	2163377136656 [label=AccumulateGrad]
	2163377138864 -> 2163377136944
	2163381133360 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2163381133360 -> 2163377138864
	2163377138864 [label=AccumulateGrad]
	2163377141072 -> 2163377134352
	2163381133456 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2163381133456 -> 2163377141072
	2163377141072 [label=AccumulateGrad]
	2163377137712 -> 2163377134352
	2163381133552 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2163381133552 -> 2163377137712
	2163377137712 [label=AccumulateGrad]
	2163377130800 -> 2163377142704
	2163377130896 -> 2163377129360
	2163381134512 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2163381134512 -> 2163377130896
	2163377130896 [label=AccumulateGrad]
	2163377128592 -> 2163377127872
	2163381134608 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2163381134608 -> 2163377128592
	2163377128592 [label=AccumulateGrad]
	2163377129264 -> 2163377127872
	2163381134704 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2163381134704 -> 2163377129264
	2163377129264 [label=AccumulateGrad]
	2163377134400 -> 2163377135984
	2163381135088 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2163381135088 -> 2163377134400
	2163377134400 [label=AccumulateGrad]
	2163377143088 -> 2163377130128
	2163381135184 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2163381135184 -> 2163377143088
	2163377143088 [label=AccumulateGrad]
	2163377131568 -> 2163377130128
	2163381135280 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2163381135280 -> 2163377131568
	2163377131568 [label=AccumulateGrad]
	2163377132528 -> 2163377128640
	2163377132528 [label=CudnnBatchNormBackward0]
	2163377134016 -> 2163377132528
	2163377134016 [label=ConvolutionBackward0]
	2163377141648 -> 2163377134016
	2163377128976 -> 2163377134016
	2163381133936 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2163381133936 -> 2163377128976
	2163377128976 [label=AccumulateGrad]
	2163377128928 -> 2163377132528
	2163381134032 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2163381134032 -> 2163377128928
	2163377128928 [label=AccumulateGrad]
	2163377136704 -> 2163377132528
	2163381134128 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2163381134128 -> 2163377136704
	2163377136704 [label=AccumulateGrad]
	2163377128784 -> 2163377129168
	2163381135664 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2163381135664 -> 2163377128784
	2163377128784 [label=AccumulateGrad]
	2163377143328 -> 2163377135024
	2163381135760 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2163381135760 -> 2163377143328
	2163377143328 [label=AccumulateGrad]
	2163377133728 -> 2163377135024
	2163381135856 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2163381135856 -> 2163377133728
	2163377133728 [label=AccumulateGrad]
	2163377131664 -> 2163377127968
	2163381136240 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2163381136240 -> 2163377131664
	2163377131664 [label=AccumulateGrad]
	2163377136560 -> 2163377135264
	2163381136336 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2163381136336 -> 2163377136560
	2163377136560 [label=AccumulateGrad]
	2163377143712 -> 2163377135264
	2163381136432 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2163381136432 -> 2163377143712
	2163377143712 [label=AccumulateGrad]
	2163377135360 -> 2163377128544
	2163377142800 -> 2163377129312
	2163377142800 [label=TBackward0]
	2163377128064 -> 2163377142800
	2163381137104 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2163381137104 -> 2163377128064
	2163377128064 [label=AccumulateGrad]
	2163377129312 -> 2163412240432
}
