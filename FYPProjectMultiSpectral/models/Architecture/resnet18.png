digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1744555773776 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1744547868688 [label=AddmmBackward0]
	1744547868832 -> 1744547868688
	1744544792400 [label="fc.bias
 (19)" fillcolor=lightblue]
	1744544792400 -> 1744547868832
	1744547868832 [label=AccumulateGrad]
	1744547868880 -> 1744547868688
	1744547868880 [label=ViewBackward0]
	1744547868976 -> 1744547868880
	1744547868976 [label=MeanBackward1]
	1744547869120 -> 1744547868976
	1744547869120 [label=ReluBackward0]
	1744547869216 -> 1744547869120
	1744547869216 [label=AddBackward0]
	1744547869312 -> 1744547869216
	1744547869312 [label=CudnnBatchNormBackward0]
	1744547869456 -> 1744547869312
	1744547869456 [label=ConvolutionBackward0]
	1744547869648 -> 1744547869456
	1744547869648 [label=ReluBackward0]
	1744547869792 -> 1744547869648
	1744547869792 [label=CudnnBatchNormBackward0]
	1744547869888 -> 1744547869792
	1744547869888 [label=ConvolutionBackward0]
	1744547869264 -> 1744547869888
	1744547869264 [label=ReluBackward0]
	1744547870032 -> 1744547869264
	1744547870032 [label=AddBackward0]
	1744547870128 -> 1744547870032
	1744547870128 [label=CudnnBatchNormBackward0]
	1744547870272 -> 1744547870128
	1744547870272 [label=ConvolutionBackward0]
	1744547870464 -> 1744547870272
	1744547870464 [label=ReluBackward0]
	1744547870608 -> 1744547870464
	1744547870608 [label=CudnnBatchNormBackward0]
	1744547870704 -> 1744547870608
	1744547870704 [label=ConvolutionBackward0]
	1744547870896 -> 1744547870704
	1744547870896 [label=ReluBackward0]
	1744547871040 -> 1744547870896
	1744547871040 [label=AddBackward0]
	1744547871136 -> 1744547871040
	1744547871136 [label=CudnnBatchNormBackward0]
	1744547871280 -> 1744547871136
	1744547871280 [label=ConvolutionBackward0]
	1744547871472 -> 1744547871280
	1744547871472 [label=ReluBackward0]
	1744547871616 -> 1744547871472
	1744547871616 [label=CudnnBatchNormBackward0]
	1744547871712 -> 1744547871616
	1744547871712 [label=ConvolutionBackward0]
	1744547871088 -> 1744547871712
	1744547871088 [label=ReluBackward0]
	1744547872000 -> 1744547871088
	1744547872000 [label=AddBackward0]
	1744547872048 -> 1744547872000
	1744547872048 [label=CudnnBatchNormBackward0]
	1744544643776 -> 1744547872048
	1744544643776 [label=ConvolutionBackward0]
	1744544644640 -> 1744544643776
	1744544644640 [label=ReluBackward0]
	1744544644496 -> 1744544644640
	1744544644496 [label=CudnnBatchNormBackward0]
	1744544644400 -> 1744544644496
	1744544644400 [label=ConvolutionBackward0]
	1744544644064 -> 1744544644400
	1744544644064 [label=ReluBackward0]
	1744544643920 -> 1744544644064
	1744544643920 [label=AddBackward0]
	1744544643680 -> 1744544643920
	1744544643680 [label=CudnnBatchNormBackward0]
	1744544643536 -> 1744544643680
	1744544643536 [label=ConvolutionBackward0]
	1744544643440 -> 1744544643536
	1744544643440 [label=ReluBackward0]
	1744544644304 -> 1744544643440
	1744544644304 [label=CudnnBatchNormBackward0]
	1744551608768 -> 1744544644304
	1744551608768 [label=ConvolutionBackward0]
	1744544643872 -> 1744551608768
	1744544643872 [label=ReluBackward0]
	1744551609056 -> 1744544643872
	1744551609056 [label=AddBackward0]
	1744551609152 -> 1744551609056
	1744551609152 [label=CudnnBatchNormBackward0]
	1744551609296 -> 1744551609152
	1744551609296 [label=ConvolutionBackward0]
	1744551609488 -> 1744551609296
	1744551609488 [label=ReluBackward0]
	1744551609632 -> 1744551609488
	1744551609632 [label=CudnnBatchNormBackward0]
	1744551609728 -> 1744551609632
	1744551609728 [label=ConvolutionBackward0]
	1744551609920 -> 1744551609728
	1744551609920 [label=ReluBackward0]
	1744551610064 -> 1744551609920
	1744551610064 [label=AddBackward0]
	1744551610160 -> 1744551610064
	1744551610160 [label=CudnnBatchNormBackward0]
	1744551610304 -> 1744551610160
	1744551610304 [label=ConvolutionBackward0]
	1744551610496 -> 1744551610304
	1744551610496 [label=ReluBackward0]
	1744551610640 -> 1744551610496
	1744551610640 [label=CudnnBatchNormBackward0]
	1744551610736 -> 1744551610640
	1744551610736 [label=ConvolutionBackward0]
	1744551610112 -> 1744551610736
	1744551610112 [label=ReluBackward0]
	1744551611024 -> 1744551610112
	1744551611024 [label=AddBackward0]
	1744551611120 -> 1744551611024
	1744551611120 [label=CudnnBatchNormBackward0]
	1744551611264 -> 1744551611120
	1744551611264 [label=ConvolutionBackward0]
	1744551611456 -> 1744551611264
	1744551611456 [label=ReluBackward0]
	1744551611600 -> 1744551611456
	1744551611600 [label=CudnnBatchNormBackward0]
	1744551611696 -> 1744551611600
	1744551611696 [label=ConvolutionBackward0]
	1744551611072 -> 1744551611696
	1744551611072 [label=MaxPool2DWithIndicesBackward0]
	1744551611984 -> 1744551611072
	1744551611984 [label=ReluBackward0]
	1744551612080 -> 1744551611984
	1744551612080 [label=CudnnBatchNormBackward0]
	1744551612176 -> 1744551612080
	1744551612176 [label=ConvolutionBackward0]
	1744551612368 -> 1744551612176
	1744548216912 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1744548216912 -> 1744551612368
	1744551612368 [label=AccumulateGrad]
	1744551612128 -> 1744551612080
	1744544780784 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1744544780784 -> 1744551612128
	1744551612128 [label=AccumulateGrad]
	1744551611792 -> 1744551612080
	1744544780880 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1744544780880 -> 1744551611792
	1744551611792 [label=AccumulateGrad]
	1744551611888 -> 1744551611696
	1744544781264 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1744544781264 -> 1744551611888
	1744551611888 [label=AccumulateGrad]
	1744551611648 -> 1744551611600
	1744544781360 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1744544781360 -> 1744551611648
	1744551611648 [label=AccumulateGrad]
	1744551611504 -> 1744551611600
	1744544781456 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1744544781456 -> 1744551611504
	1744551611504 [label=AccumulateGrad]
	1744551611408 -> 1744551611264
	1744544781840 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1744544781840 -> 1744551611408
	1744551611408 [label=AccumulateGrad]
	1744551611216 -> 1744551611120
	1744544781936 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1744544781936 -> 1744551611216
	1744551611216 [label=AccumulateGrad]
	1744551611168 -> 1744551611120
	1744544782032 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1744544782032 -> 1744551611168
	1744551611168 [label=AccumulateGrad]
	1744551611072 -> 1744551611024
	1744551610928 -> 1744551610736
	1744544782416 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1744544782416 -> 1744551610928
	1744551610928 [label=AccumulateGrad]
	1744551610688 -> 1744551610640
	1744544782512 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1744544782512 -> 1744551610688
	1744551610688 [label=AccumulateGrad]
	1744551610544 -> 1744551610640
	1744544782608 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1744544782608 -> 1744551610544
	1744551610544 [label=AccumulateGrad]
	1744551610448 -> 1744551610304
	1744544782992 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1744544782992 -> 1744551610448
	1744551610448 [label=AccumulateGrad]
	1744551610256 -> 1744551610160
	1744544783088 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1744544783088 -> 1744551610256
	1744551610256 [label=AccumulateGrad]
	1744551610208 -> 1744551610160
	1744544783184 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1744544783184 -> 1744551610208
	1744551610208 [label=AccumulateGrad]
	1744551610112 -> 1744551610064
	1744551609872 -> 1744551609728
	1744544784144 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1744544784144 -> 1744551609872
	1744551609872 [label=AccumulateGrad]
	1744551609680 -> 1744551609632
	1744544784240 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1744544784240 -> 1744551609680
	1744551609680 [label=AccumulateGrad]
	1744551609536 -> 1744551609632
	1744544784336 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1744544784336 -> 1744551609536
	1744551609536 [label=AccumulateGrad]
	1744551609440 -> 1744551609296
	1744544784720 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1744544784720 -> 1744551609440
	1744551609440 [label=AccumulateGrad]
	1744551609248 -> 1744551609152
	1744544784816 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1744544784816 -> 1744551609248
	1744551609248 [label=AccumulateGrad]
	1744551609200 -> 1744551609152
	1744544784912 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1744544784912 -> 1744551609200
	1744551609200 [label=AccumulateGrad]
	1744551609104 -> 1744551609056
	1744551609104 [label=CudnnBatchNormBackward0]
	1744551609824 -> 1744551609104
	1744551609824 [label=ConvolutionBackward0]
	1744551609920 -> 1744551609824
	1744551609968 -> 1744551609824
	1744544783568 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1744544783568 -> 1744551609968
	1744551609968 [label=AccumulateGrad]
	1744551609392 -> 1744551609104
	1744544783664 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1744544783664 -> 1744551609392
	1744551609392 [label=AccumulateGrad]
	1744551609344 -> 1744551609104
	1744544783760 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1744544783760 -> 1744551609344
	1744551609344 [label=AccumulateGrad]
	1744551608960 -> 1744551608768
	1744544785296 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1744544785296 -> 1744551608960
	1744551608960 [label=AccumulateGrad]
	1744551608720 -> 1744544644304
	1744544785392 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1744544785392 -> 1744551608720
	1744551608720 [label=AccumulateGrad]
	1744551608624 -> 1744544644304
	1744544785488 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1744544785488 -> 1744551608624
	1744551608624 [label=AccumulateGrad]
	1744544643152 -> 1744544643536
	1744544785872 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1744544785872 -> 1744544643152
	1744544643152 [label=AccumulateGrad]
	1744544643584 -> 1744544643680
	1744544785968 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1744544785968 -> 1744544643584
	1744544643584 [label=AccumulateGrad]
	1744544643632 -> 1744544643680
	1744544786064 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1744544786064 -> 1744544643632
	1744544643632 [label=AccumulateGrad]
	1744544643872 -> 1744544643920
	1744544644112 -> 1744544644400
	1744544787024 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1744544787024 -> 1744544644112
	1744544644112 [label=AccumulateGrad]
	1744544644448 -> 1744544644496
	1744544787120 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1744544787120 -> 1744544644448
	1744544644448 [label=AccumulateGrad]
	1744544644592 -> 1744544644496
	1744544787216 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1744544787216 -> 1744544644592
	1744544644592 [label=AccumulateGrad]
	1744544644736 -> 1744544643776
	1744544787600 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1744544787600 -> 1744544644736
	1744544644736 [label=AccumulateGrad]
	1744544644688 -> 1744547872048
	1744544787696 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1744544787696 -> 1744544644688
	1744544644688 [label=AccumulateGrad]
	1744544643248 -> 1744547872048
	1744544787792 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1744544787792 -> 1744544643248
	1744544643248 [label=AccumulateGrad]
	1744547871808 -> 1744547872000
	1744547871808 [label=CudnnBatchNormBackward0]
	1744544644160 -> 1744547871808
	1744544644160 [label=ConvolutionBackward0]
	1744544644064 -> 1744544644160
	1744544644016 -> 1744544644160
	1744544786448 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1744544786448 -> 1744544644016
	1744544644016 [label=AccumulateGrad]
	1744544644784 -> 1744547871808
	1744544786544 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1744544786544 -> 1744544644784
	1744544644784 [label=AccumulateGrad]
	1744544644976 -> 1744547871808
	1744544786640 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1744544786640 -> 1744544644976
	1744544644976 [label=AccumulateGrad]
	1744547871904 -> 1744547871712
	1744544788176 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1744544788176 -> 1744547871904
	1744547871904 [label=AccumulateGrad]
	1744547871664 -> 1744547871616
	1744544788272 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1744544788272 -> 1744547871664
	1744547871664 [label=AccumulateGrad]
	1744547871520 -> 1744547871616
	1744544788368 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1744544788368 -> 1744547871520
	1744547871520 [label=AccumulateGrad]
	1744547871424 -> 1744547871280
	1744544788752 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1744544788752 -> 1744547871424
	1744547871424 [label=AccumulateGrad]
	1744547871232 -> 1744547871136
	1744544788848 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1744544788848 -> 1744547871232
	1744547871232 [label=AccumulateGrad]
	1744547871184 -> 1744547871136
	1744544788944 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1744544788944 -> 1744547871184
	1744547871184 [label=AccumulateGrad]
	1744547871088 -> 1744547871040
	1744547870848 -> 1744547870704
	1744544789904 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1744544789904 -> 1744547870848
	1744547870848 [label=AccumulateGrad]
	1744547870656 -> 1744547870608
	1744544790000 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1744544790000 -> 1744547870656
	1744547870656 [label=AccumulateGrad]
	1744547870512 -> 1744547870608
	1744544790096 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1744544790096 -> 1744547870512
	1744547870512 [label=AccumulateGrad]
	1744547870416 -> 1744547870272
	1744544790480 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1744544790480 -> 1744547870416
	1744547870416 [label=AccumulateGrad]
	1744547870224 -> 1744547870128
	1744544790576 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1744544790576 -> 1744547870224
	1744547870224 [label=AccumulateGrad]
	1744547870176 -> 1744547870128
	1744544790672 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1744544790672 -> 1744547870176
	1744547870176 [label=AccumulateGrad]
	1744547870080 -> 1744547870032
	1744547870080 [label=CudnnBatchNormBackward0]
	1744547870800 -> 1744547870080
	1744547870800 [label=ConvolutionBackward0]
	1744547870896 -> 1744547870800
	1744547870944 -> 1744547870800
	1744544789328 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1744544789328 -> 1744547870944
	1744547870944 [label=AccumulateGrad]
	1744547870368 -> 1744547870080
	1744544789424 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1744544789424 -> 1744547870368
	1744547870368 [label=AccumulateGrad]
	1744547870320 -> 1744547870080
	1744544789520 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1744544789520 -> 1744547870320
	1744547870320 [label=AccumulateGrad]
	1744547868112 -> 1744547869888
	1744544791056 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1744544791056 -> 1744547868112
	1744547868112 [label=AccumulateGrad]
	1744547869840 -> 1744547869792
	1744544791152 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1744544791152 -> 1744547869840
	1744547869840 [label=AccumulateGrad]
	1744547869696 -> 1744547869792
	1744544791248 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1744544791248 -> 1744547869696
	1744547869696 [label=AccumulateGrad]
	1744547869600 -> 1744547869456
	1744544791632 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1744544791632 -> 1744547869600
	1744547869600 [label=AccumulateGrad]
	1744547869408 -> 1744547869312
	1744544791728 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1744544791728 -> 1744547869408
	1744547869408 [label=AccumulateGrad]
	1744547869360 -> 1744547869312
	1744544791824 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1744544791824 -> 1744547869360
	1744547869360 [label=AccumulateGrad]
	1744547869264 -> 1744547869216
	1744547868928 -> 1744547868688
	1744547868928 [label=TBackward0]
	1744547869168 -> 1744547868928
	1744544792496 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1744544792496 -> 1744547869168
	1744547869168 [label=AccumulateGrad]
	1744547868688 -> 1744555773776
}
