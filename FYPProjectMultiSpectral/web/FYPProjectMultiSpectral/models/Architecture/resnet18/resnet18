digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2112155910896 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2112182236592 [label=AddmmBackward0]
	2112182236256 -> 2112182236592
	2112156528528 [label="fc.bias
 (19)" fillcolor=lightblue]
	2112156528528 -> 2112182236256
	2112182236256 [label=AccumulateGrad]
	2112182235632 -> 2112182236592
	2112182235632 [label=ViewBackward0]
	2112182235152 -> 2112182235632
	2112182235152 [label=MeanBackward1]
	2112182234816 -> 2112182235152
	2112182234816 [label=ReluBackward0]
	2112182234336 -> 2112182234816
	2112182234336 [label=AddBackward0]
	2112182233856 -> 2112182234336
	2112182233856 [label=CudnnBatchNormBackward0]
	2112182232752 -> 2112182233856
	2112182232752 [label=ConvolutionBackward0]
	2112182231792 -> 2112182232752
	2112182231792 [label=ReluBackward0]
	2112182231456 -> 2112182231792
	2112182231456 [label=CudnnBatchNormBackward0]
	2112182230976 -> 2112182231456
	2112182230976 [label=ConvolutionBackward0]
	2112182233712 -> 2112182230976
	2112182233712 [label=ReluBackward0]
	2112182228336 -> 2112182233712
	2112182228336 [label=AddBackward0]
	2112182225456 -> 2112182228336
	2112182225456 [label=CudnnBatchNormBackward0]
	2112182238752 -> 2112182225456
	2112182238752 [label=ConvolutionBackward0]
	2112182238416 -> 2112182238752
	2112182238416 [label=ReluBackward0]
	2112182238464 -> 2112182238416
	2112182238464 [label=CudnnBatchNormBackward0]
	2112182238368 -> 2112182238464
	2112182238368 [label=ConvolutionBackward0]
	2112182237744 -> 2112182238368
	2112182237744 [label=ReluBackward0]
	2112182237792 -> 2112182237744
	2112182237792 [label=AddBackward0]
	2112182237648 -> 2112182237792
	2112182237648 [label=CudnnBatchNormBackward0]
	2112182237264 -> 2112182237648
	2112182237264 [label=ConvolutionBackward0]
	2112182237408 -> 2112182237264
	2112182237408 [label=ReluBackward0]
	2112182236976 -> 2112182237408
	2112182236976 [label=CudnnBatchNormBackward0]
	2112182236880 -> 2112182236976
	2112182236880 [label=ConvolutionBackward0]
	2112182237888 -> 2112182236880
	2112182237888 [label=ReluBackward0]
	2112182236496 -> 2112182237888
	2112182236496 [label=AddBackward0]
	2112182236400 -> 2112182236496
	2112182236400 [label=CudnnBatchNormBackward0]
	2112182236448 -> 2112182236400
	2112182236448 [label=ConvolutionBackward0]
	2112182235824 -> 2112182236448
	2112182235824 [label=ReluBackward0]
	2112182235872 -> 2112182235824
	2112182235872 [label=CudnnBatchNormBackward0]
	2112182235728 -> 2112182235872
	2112182235728 [label=ConvolutionBackward0]
	2112182235440 -> 2112182235728
	2112182235440 [label=ReluBackward0]
	2112182235488 -> 2112182235440
	2112182235488 [label=AddBackward0]
	2112182235200 -> 2112182235488
	2112182235200 [label=CudnnBatchNormBackward0]
	2112182234960 -> 2112182235200
	2112182234960 [label=ConvolutionBackward0]
	2112182234768 -> 2112182234960
	2112182234768 [label=ReluBackward0]
	2112182234384 -> 2112182234768
	2112182234384 [label=CudnnBatchNormBackward0]
	2112182234624 -> 2112182234384
	2112182234624 [label=ConvolutionBackward0]
	2112182235248 -> 2112182234624
	2112182235248 [label=ReluBackward0]
	2112182233904 -> 2112182235248
	2112182233904 [label=AddBackward0]
	2112182234144 -> 2112182233904
	2112182234144 [label=CudnnBatchNormBackward0]
	2112182233808 -> 2112182234144
	2112182233808 [label=ConvolutionBackward0]
	2112182233520 -> 2112182233808
	2112182233520 [label=ReluBackward0]
	2112182233568 -> 2112182233520
	2112182233568 [label=CudnnBatchNormBackward0]
	2112182233280 -> 2112182233568
	2112182233280 [label=ConvolutionBackward0]
	2112182233184 -> 2112182233280
	2112182233184 [label=ReluBackward0]
	2112182232848 -> 2112182233184
	2112182232848 [label=AddBackward0]
	2112182232656 -> 2112182232848
	2112182232656 [label=CudnnBatchNormBackward0]
	2112182232704 -> 2112182232656
	2112182232704 [label=ConvolutionBackward0]
	2112182232320 -> 2112182232704
	2112182232320 [label=ReluBackward0]
	2112182232080 -> 2112182232320
	2112182232080 [label=CudnnBatchNormBackward0]
	2112182232032 -> 2112182232080
	2112182232032 [label=ConvolutionBackward0]
	2112182232800 -> 2112182232032
	2112182232800 [label=ReluBackward0]
	2112182231600 -> 2112182232800
	2112182231600 [label=AddBackward0]
	2112182231552 -> 2112182231600
	2112182231552 [label=CudnnBatchNormBackward0]
	2112182231360 -> 2112182231552
	2112182231360 [label=ConvolutionBackward0]
	2112182231264 -> 2112182231360
	2112182231264 [label=ReluBackward0]
	2112182230928 -> 2112182231264
	2112182230928 [label=CudnnBatchNormBackward0]
	2112182230736 -> 2112182230928
	2112182230736 [label=ConvolutionBackward0]
	2112182231744 -> 2112182230736
	2112182231744 [label=MaxPool2DWithIndicesBackward0]
	2112182230448 -> 2112182231744
	2112182230448 [label=ReluBackward0]
	2112182230256 -> 2112182230448
	2112182230256 [label=CudnnBatchNormBackward0]
	2112182230160 -> 2112182230256
	2112182230160 [label=ConvolutionBackward0]
	2112182226944 -> 2112182230160
	2112156528336 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2112156528336 -> 2112182226944
	2112182226944 [label=AccumulateGrad]
	2112182225072 -> 2112182230256
	2112158338128 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2112158338128 -> 2112182225072
	2112182225072 [label=AccumulateGrad]
	2112182230640 -> 2112182230256
	2112158337840 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2112158337840 -> 2112182230640
	2112182230640 [label=AccumulateGrad]
	2112182230592 -> 2112182230736
	2112158338320 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2112158338320 -> 2112182230592
	2112182230592 [label=AccumulateGrad]
	2112182230880 -> 2112182230928
	2112158338416 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2112158338416 -> 2112182230880
	2112182230880 [label=AccumulateGrad]
	2112182231072 -> 2112182230928
	2112158338512 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2112158338512 -> 2112182231072
	2112182231072 [label=AccumulateGrad]
	2112182231120 -> 2112182231360
	2112158337744 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2112158337744 -> 2112182231120
	2112182231120 [label=AccumulateGrad]
	2112182231408 -> 2112182231552
	2112158337648 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2112158337648 -> 2112182231408
	2112182231408 [label=AccumulateGrad]
	2112182231648 -> 2112182231552
	2112158336880 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2112158336880 -> 2112182231648
	2112182231648 [label=AccumulateGrad]
	2112182231744 -> 2112182231600
	2112182231696 -> 2112182232032
	2112158336976 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2112158336976 -> 2112182231696
	2112182231696 [label=AccumulateGrad]
	2112182232224 -> 2112182232080
	2112158337072 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2112158337072 -> 2112182232224
	2112182232224 [label=AccumulateGrad]
	2112182232176 -> 2112182232080
	2112158337168 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2112158337168 -> 2112182232176
	2112182232176 [label=AccumulateGrad]
	2112182232368 -> 2112182232704
	2112158337456 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2112158337456 -> 2112182232368
	2112182232368 [label=AccumulateGrad]
	2112182232560 -> 2112182232656
	2112158336496 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2112158336496 -> 2112182232560
	2112182232560 [label=AccumulateGrad]
	2112182232464 -> 2112182232656
	2112158336400 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2112158336400 -> 2112182232464
	2112182232464 [label=AccumulateGrad]
	2112182232800 -> 2112182232848
	2112182233040 -> 2112182233280
	2112158336304 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2112158336304 -> 2112182233040
	2112182233040 [label=AccumulateGrad]
	2112182233328 -> 2112182233568
	2112158336112 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2112158336112 -> 2112182233328
	2112182233328 [label=AccumulateGrad]
	2112182233664 -> 2112182233568
	2112156520272 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2112156520272 -> 2112182233664
	2112182233664 [label=AccumulateGrad]
	2112182233424 -> 2112182233808
	2112156520656 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2112156520656 -> 2112182233424
	2112182233424 [label=AccumulateGrad]
	2112182234048 -> 2112182234144
	2112156520752 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2112156520752 -> 2112182234048
	2112182234048 [label=AccumulateGrad]
	2112182233952 -> 2112182234144
	2112156520848 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2112156520848 -> 2112182233952
	2112182233952 [label=AccumulateGrad]
	2112182234000 -> 2112182233904
	2112182234000 [label=CudnnBatchNormBackward0]
	2112182232944 -> 2112182234000
	2112182232944 [label=ConvolutionBackward0]
	2112182233184 -> 2112182232944
	2112182232992 -> 2112182232944
	2112158336208 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2112158336208 -> 2112182232992
	2112182232992 [label=AccumulateGrad]
	2112182233616 -> 2112182234000
	2112158335728 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2112158335728 -> 2112182233616
	2112182233616 [label=AccumulateGrad]
	2112182233760 -> 2112182234000
	2112158335056 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2112158335056 -> 2112182233760
	2112182233760 [label=AccumulateGrad]
	2112182234240 -> 2112182234624
	2112156521232 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2112156521232 -> 2112182234240
	2112182234240 [label=AccumulateGrad]
	2112182234480 -> 2112182234384
	2112156521328 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2112156521328 -> 2112182234480
	2112182234480 [label=AccumulateGrad]
	2112182234720 -> 2112182234384
	2112156521424 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2112156521424 -> 2112182234720
	2112182234720 [label=AccumulateGrad]
	2112182235008 -> 2112182234960
	2112156521808 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2112156521808 -> 2112182235008
	2112182235008 [label=AccumulateGrad]
	2112182234864 -> 2112182235200
	2112156521904 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2112156521904 -> 2112182234864
	2112182234864 [label=AccumulateGrad]
	2112182235056 -> 2112182235200
	2112156522000 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2112156522000 -> 2112182235056
	2112182235056 [label=AccumulateGrad]
	2112182235248 -> 2112182235488
	2112182235344 -> 2112182235728
	2112156522960 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2112156522960 -> 2112182235344
	2112182235344 [label=AccumulateGrad]
	2112182235968 -> 2112182235872
	2112156523056 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2112156523056 -> 2112182235968
	2112182235968 [label=AccumulateGrad]
	2112182235920 -> 2112182235872
	2112156523152 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2112156523152 -> 2112182235920
	2112182235920 [label=AccumulateGrad]
	2112182236016 -> 2112182236448
	2112156523536 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2112156523536 -> 2112182236016
	2112182236016 [label=AccumulateGrad]
	2112182236352 -> 2112182236400
	2112156523632 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2112156523632 -> 2112182236352
	2112182236352 [label=AccumulateGrad]
	2112182236544 -> 2112182236400
	2112156523728 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2112156523728 -> 2112182236544
	2112182236544 [label=AccumulateGrad]
	2112182236304 -> 2112182236496
	2112182236304 [label=CudnnBatchNormBackward0]
	2112182235536 -> 2112182236304
	2112182235536 [label=ConvolutionBackward0]
	2112182235440 -> 2112182235536
	2112182235584 -> 2112182235536
	2112156522384 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2112156522384 -> 2112182235584
	2112182235584 [label=AccumulateGrad]
	2112182236160 -> 2112182236304
	2112156522480 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2112156522480 -> 2112182236160
	2112182236160 [label=AccumulateGrad]
	2112182236208 -> 2112182236304
	2112156522576 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2112156522576 -> 2112182236208
	2112182236208 [label=AccumulateGrad]
	2112182236688 -> 2112182236880
	2112156524112 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2112156524112 -> 2112182236688
	2112182236688 [label=AccumulateGrad]
	2112182236784 -> 2112182236976
	2112156524208 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2112156524208 -> 2112182236784
	2112182236784 [label=AccumulateGrad]
	2112182237168 -> 2112182236976
	2112156524304 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2112156524304 -> 2112182237168
	2112182237168 [label=AccumulateGrad]
	2112182237312 -> 2112182237264
	2112156524688 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2112156524688 -> 2112182237312
	2112182237312 [label=AccumulateGrad]
	2112182237456 -> 2112182237648
	2112156524784 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2112156524784 -> 2112182237456
	2112182237456 [label=AccumulateGrad]
	2112182237600 -> 2112182237648
	2112156524880 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2112156524880 -> 2112182237600
	2112182237600 [label=AccumulateGrad]
	2112182237888 -> 2112182237792
	2112182237936 -> 2112182238368
	2112156525840 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2112156525840 -> 2112182237936
	2112182237936 [label=AccumulateGrad]
	2112182238272 -> 2112182238464
	2112156525936 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2112156525936 -> 2112182238272
	2112182238272 [label=AccumulateGrad]
	2112182238224 -> 2112182238464
	2112156526032 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2112156526032 -> 2112182238224
	2112182238224 [label=AccumulateGrad]
	2112182238560 -> 2112182238752
	2112156526416 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2112156526416 -> 2112182238560
	2112182238560 [label=AccumulateGrad]
	2112182238944 -> 2112182225456
	2112156526512 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2112156526512 -> 2112182238944
	2112182238944 [label=AccumulateGrad]
	2112182239184 -> 2112182225456
	2112156526608 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2112156526608 -> 2112182239184
	2112182239184 [label=AccumulateGrad]
	2112182227952 -> 2112182228336
	2112182227952 [label=CudnnBatchNormBackward0]
	2112182238080 -> 2112182227952
	2112182238080 [label=ConvolutionBackward0]
	2112182237744 -> 2112182238080
	2112182237840 -> 2112182238080
	2112156525264 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2112156525264 -> 2112182237840
	2112182237840 [label=AccumulateGrad]
	2112182238608 -> 2112182227952
	2112156525360 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2112156525360 -> 2112182238608
	2112182238608 [label=AccumulateGrad]
	2112182238848 -> 2112182227952
	2112156525456 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2112156525456 -> 2112182238848
	2112182238848 [label=AccumulateGrad]
	2112182224784 -> 2112182230976
	2112156526992 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2112156526992 -> 2112182224784
	2112182224784 [label=AccumulateGrad]
	2112182230832 -> 2112182231456
	2112156527088 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2112156527088 -> 2112182230832
	2112182230832 [label=AccumulateGrad]
	2112182231936 -> 2112182231456
	2112156527184 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2112156527184 -> 2112182231936
	2112182231936 [label=AccumulateGrad]
	2112182232416 -> 2112182232752
	2112156527568 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2112156527568 -> 2112182232416
	2112182232416 [label=AccumulateGrad]
	2112182233376 -> 2112182233856
	2112156527664 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2112156527664 -> 2112182233376
	2112182233376 [label=AccumulateGrad]
	2112182233232 -> 2112182233856
	2112156527760 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2112156527760 -> 2112182233232
	2112182233232 [label=AccumulateGrad]
	2112182233712 -> 2112182234336
	2112182235776 -> 2112182236592
	2112182235776 [label=TBackward0]
	2112182234192 -> 2112182235776
	2112156528432 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2112156528432 -> 2112182234192
	2112182234192 [label=AccumulateGrad]
	2112182236592 -> 2112155910896
}
