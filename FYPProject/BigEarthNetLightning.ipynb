{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Of Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Library modules\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platformâ€™s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch and Deep Learning Libaries\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "from torch.nn import functional as F  # Functional module for defining functions and loss functions.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Data handling and batching\n",
    "import torchvision  # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Image datasets and transformations.\n",
    "import torchvision.datasets as datasets  # Specific datasets for vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "from torchvision.datasets import MNIST, EuroSAT  # Standard datasets.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations.\n",
    "from torchvision.models import ResNet18_Weights  # ResNet-18 model with pretrained weights.\n",
    "from torchsummary import summary  # Model summary.\n",
    "import torchsummary  # Model summaries.\n",
    "import torchmetrics  # Model evaluation metrics.\n",
    "from torchmetrics import MeanMetric, Accuracy  # Accuracy metrics.\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassF1Score, MulticlassRecall, MulticlassPrecision, MulticlassAccuracy\n",
    ")  # Classification metrics.\n",
    "from torchviz import make_dot  # Model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for class imbalance.\n",
    "from torchcam.methods import GradCAM  # Grad-CAM for model interpretability.\n",
    "from torchcam.utils import overlay_mask  # Overlay mask for visualizations.\n",
    "import pytorch_lightning as pl  # Training management.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, Callback  # Callbacks.\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Logger for TensorBoard.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation.\n",
    "from rasterio.enums import Resampling  # Resampling for raster resizing.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation, Analysis and Visualization Libraries\n",
    "import pandas as pd  # Data analysis and manipulation.\n",
    "import numpy as np  # Array operations and computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics.\n",
    "import matplotlib.pyplot as plt  # Static and interactive plotting.\n",
    "import seaborn as sns  # High-level interface for statistical graphics.\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm  # Progress bar for loops.\n",
    "from PIL import Image  # Image handling and manipulation.\n",
    "import ast  # Parsing Python code.\n",
    "import requests  # HTTP requests.\n",
    "import zstandard as zstd  # Compression and decompression.\n",
    "from collections import Counter  # Counting hashable objects.\n",
    "import certifi  # Certificates for HTTPS.\n",
    "import ssl  # Secure connections.\n",
    "import urllib.request  # URL handling.\n",
    "import kaggle  # Kaggle API for datasets.\n",
    "from IPython.display import Image  # Display images in notebooks.\n",
    "from pathlib import Path # File system path handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Seed and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda (GPU: NVIDIA GeForce RTX 3050)\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42  \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device} {'(GPU: ' + torch.cuda.get_device_name(0) + ')' if device.type == 'cuda' else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    dataset_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%'\n",
    "    combined_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%\\CombinedImages'\n",
    "    metadata_path: str =r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\metadata_50_percent.csv'\n",
    "    metadata_csv = pd.read_csv(metadata_path)\n",
    "    img_size: int = 120\n",
    "    img_mean, img_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    num_classes: int = 19\n",
    "    band_channels: int = 13\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 10\n",
    "    model_name: str = 'resnet18'\n",
    "    num_workers: int = os.cpu_count() // 2\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = list(Path(root_dir).rglob(\"*.tif\"))\n",
    "        self.metadata = pd.read_csv(DatasetConfig.metadata_path)\n",
    "\n",
    "        # Create a mapping from patch_id to labels\n",
    "        self.patch_to_labels = dict(zip(self.metadata['patch_id'], self.metadata['labels']))\n",
    "        self.image_paths = list(Path(root_dir).rglob(\"*.tif\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = src.read() \n",
    "             \n",
    "        label = self.get_label(image_path)  \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_label(self, image_path):\n",
    "        patch_id = image_path.stem\n",
    "        labels = self.patch_to_labels.get(patch_id, None)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 2352\n"
     ]
    }
   ],
   "source": [
    "dataset = BigEarthNetSubset(DatasetConfig.combined_path)\n",
    "print(f\"Dataset length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: 172800, Label: ['Broad-leaved forest', 'Coniferous forest', 'Mixed forest', 'Transitional woodland, shrub']\n"
     ]
    }
   ],
   "source": [
    "image, label = dataset[5]\n",
    "print(f\"Image shape: {image.size}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 231,  231,  231, ...,  189,  189,  189],\n",
       "        [ 231,  231,  231, ...,  189,  189,  189],\n",
       "        [ 231,  231,  231, ...,  189,  189,  189],\n",
       "        ...,\n",
       "        [ 150,  150,  150, ...,  165,  165,  165],\n",
       "        [ 150,  150,  150, ...,  165,  165,  165],\n",
       "        [ 150,  150,  150, ...,  165,  165,  165]],\n",
       "\n",
       "       [[ 210,  250,  310, ...,  183,  177,  209],\n",
       "        [ 194,  130,  171, ...,  185,  203,  228],\n",
       "        [ 189,  150,  183, ...,  184,  225,  256],\n",
       "        ...,\n",
       "        [ 133,  128,  144, ...,  105,  198,  285],\n",
       "        [ 116,  131,  149, ...,  169,  290,  281],\n",
       "        [ 109,  141,  159, ...,  233,  257,  274]],\n",
       "\n",
       "       [[ 477,  522,  557, ...,  469,  482,  532],\n",
       "        [ 473,  404,  428, ...,  446,  520,  618],\n",
       "        [ 453,  432,  469, ...,  463,  590,  644],\n",
       "        ...,\n",
       "        [ 319,  317,  373, ...,  318,  479,  636],\n",
       "        [ 343,  330,  382, ...,  485,  630,  672],\n",
       "        [ 314,  341,  384, ...,  584,  634,  646]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3047, 3047, 3047, ..., 3259, 3259, 3259],\n",
       "        [3047, 3047, 3047, ..., 3259, 3259, 3259],\n",
       "        [3047, 3047, 3047, ..., 3259, 3259, 3259],\n",
       "        ...,\n",
       "        [3457, 3457, 3457, ..., 3124, 3124, 3124],\n",
       "        [3457, 3457, 3457, ..., 3124, 3124, 3124],\n",
       "        [3457, 3457, 3457, ..., 3124, 3124, 3124]],\n",
       "\n",
       "       [[1823, 1823, 1890, ...,  976, 1200, 1200],\n",
       "        [1823, 1823, 1890, ...,  976, 1200, 1200],\n",
       "        [1735, 1735, 1595, ..., 1074, 1335, 1335],\n",
       "        ...,\n",
       "        [1264, 1264, 1536, ..., 1217, 1870, 1870],\n",
       "        [1205, 1205, 1491, ..., 1939, 2250, 2250],\n",
       "        [1205, 1205, 1491, ..., 1939, 2250, 2250]],\n",
       "\n",
       "       [[ 998,  998, 1033, ...,  441,  552,  552],\n",
       "        [ 998,  998, 1033, ...,  441,  552,  552],\n",
       "        [ 905,  905,  810, ...,  478,  620,  620],\n",
       "        ...,\n",
       "        [ 560,  560,  663, ...,  574,  903,  903],\n",
       "        [ 516,  516,  617, ...,  963, 1171, 1171],\n",
       "        [ 516,  516,  617, ...,  963, 1171, 1171]]], dtype=uint16)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 231,  231,  231, ...,  189,  189,  189],\n",
       "         [ 231,  231,  231, ...,  189,  189,  189],\n",
       "         [ 231,  231,  231, ...,  189,  189,  189],\n",
       "         ...,\n",
       "         [ 150,  150,  150, ...,  165,  165,  165],\n",
       "         [ 150,  150,  150, ...,  165,  165,  165],\n",
       "         [ 150,  150,  150, ...,  165,  165,  165]],\n",
       " \n",
       "        [[ 210,  250,  310, ...,  183,  177,  209],\n",
       "         [ 194,  130,  171, ...,  185,  203,  228],\n",
       "         [ 189,  150,  183, ...,  184,  225,  256],\n",
       "         ...,\n",
       "         [ 133,  128,  144, ...,  105,  198,  285],\n",
       "         [ 116,  131,  149, ...,  169,  290,  281],\n",
       "         [ 109,  141,  159, ...,  233,  257,  274]],\n",
       " \n",
       "        [[ 477,  522,  557, ...,  469,  482,  532],\n",
       "         [ 473,  404,  428, ...,  446,  520,  618],\n",
       "         [ 453,  432,  469, ...,  463,  590,  644],\n",
       "         ...,\n",
       "         [ 319,  317,  373, ...,  318,  479,  636],\n",
       "         [ 343,  330,  382, ...,  485,  630,  672],\n",
       "         [ 314,  341,  384, ...,  584,  634,  646]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[3047, 3047, 3047, ..., 3259, 3259, 3259],\n",
       "         [3047, 3047, 3047, ..., 3259, 3259, 3259],\n",
       "         [3047, 3047, 3047, ..., 3259, 3259, 3259],\n",
       "         ...,\n",
       "         [3457, 3457, 3457, ..., 3124, 3124, 3124],\n",
       "         [3457, 3457, 3457, ..., 3124, 3124, 3124],\n",
       "         [3457, 3457, 3457, ..., 3124, 3124, 3124]],\n",
       " \n",
       "        [[1823, 1823, 1890, ...,  976, 1200, 1200],\n",
       "         [1823, 1823, 1890, ...,  976, 1200, 1200],\n",
       "         [1735, 1735, 1595, ..., 1074, 1335, 1335],\n",
       "         ...,\n",
       "         [1264, 1264, 1536, ..., 1217, 1870, 1870],\n",
       "         [1205, 1205, 1491, ..., 1939, 2250, 2250],\n",
       "         [1205, 1205, 1491, ..., 1939, 2250, 2250]],\n",
       " \n",
       "        [[ 998,  998, 1033, ...,  441,  552,  552],\n",
       "         [ 998,  998, 1033, ...,  441,  552,  552],\n",
       "         [ 905,  905,  810, ...,  478,  620,  620],\n",
       "         ...,\n",
       "         [ 560,  560,  663, ...,  574,  903,  903],\n",
       "         [ 516,  516,  617, ...,  963, 1171, 1171],\n",
       "         [ 516,  516,  617, ...,  963, 1171, 1171]]], dtype=uint16),\n",
       " \"['Broad-leaved forest', 'Coniferous forest', 'Mixed forest', 'Transitional woodland, shrub']\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubsetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubsetModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        pass\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        pass\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubsetCallback(Callback):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        pass\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
