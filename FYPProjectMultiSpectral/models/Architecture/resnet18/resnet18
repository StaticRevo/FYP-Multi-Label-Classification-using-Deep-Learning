digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1612629008624 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1612630569072 [label=AddmmBackward0]
	1612630557072 -> 1612630569072
	1612623527856 [label="fc.bias
 (19)" fillcolor=lightblue]
	1612623527856 -> 1612630557072
	1612630557072 [label=AccumulateGrad]
	1612630558416 -> 1612630569072
	1612630558416 [label=ViewBackward0]
	1612630561728 -> 1612630558416
	1612630561728 [label=MeanBackward1]
	1612630564464 -> 1612630561728
	1612630564464 [label=ReluBackward0]
	1612630568400 -> 1612630564464
	1612630568400 [label=AddBackward0]
	1612630557216 -> 1612630568400
	1612630557216 [label=CudnnBatchNormBackward0]
	1612630565568 -> 1612630557216
	1612630565568 [label=ConvolutionBackward0]
	1612630558848 -> 1612630565568
	1612630558848 [label=ReluBackward0]
	1612630563888 -> 1612630558848
	1612630563888 [label=CudnnBatchNormBackward0]
	1612630566528 -> 1612630563888
	1612630566528 [label=ConvolutionBackward0]
	1612630568592 -> 1612630566528
	1612630568592 [label=ReluBackward0]
	1612630557552 -> 1612630568592
	1612630557552 [label=AddBackward0]
	1612630570560 -> 1612630557552
	1612630570560 [label=CudnnBatchNormBackward0]
	1612630567728 -> 1612630570560
	1612630567728 [label=ConvolutionBackward0]
	1612630562256 -> 1612630567728
	1612630562256 [label=ReluBackward0]
	1612630557600 -> 1612630562256
	1612630557600 [label=CudnnBatchNormBackward0]
	1612630564848 -> 1612630557600
	1612630564848 [label=ConvolutionBackward0]
	1612630569600 -> 1612630564848
	1612630569600 [label=ReluBackward0]
	1612630559856 -> 1612630569600
	1612630559856 [label=AddBackward0]
	1612630562736 -> 1612630559856
	1612630562736 [label=CudnnBatchNormBackward0]
	1612630566816 -> 1612630562736
	1612630566816 [label=ConvolutionBackward0]
	1612630560912 -> 1612630566816
	1612630560912 [label=ReluBackward0]
	1612630559088 -> 1612630560912
	1612630559088 [label=CudnnBatchNormBackward0]
	1612630562880 -> 1612630559088
	1612630562880 [label=ConvolutionBackward0]
	1612630563600 -> 1612630562880
	1612630563600 [label=ReluBackward0]
	1612630572336 -> 1612630563600
	1612630572336 [label=AddBackward0]
	1612630559568 -> 1612630572336
	1612630559568 [label=CudnnBatchNormBackward0]
	1612630560384 -> 1612630559568
	1612630560384 [label=ConvolutionBackward0]
	1612630563360 -> 1612630560384
	1612630563360 [label=ReluBackward0]
	1612630570608 -> 1612630563360
	1612630570608 [label=CudnnBatchNormBackward0]
	1612630566096 -> 1612630570608
	1612630566096 [label=ConvolutionBackward0]
	1612630568352 -> 1612630566096
	1612630568352 [label=ReluBackward0]
	1612630557840 -> 1612630568352
	1612630557840 [label=AddBackward0]
	1612630559136 -> 1612630557840
	1612630559136 [label=CudnnBatchNormBackward0]
	1612630572048 -> 1612630559136
	1612630572048 [label=ConvolutionBackward0]
	1612630561248 -> 1612630572048
	1612630561248 [label=ReluBackward0]
	1612630556736 -> 1612630561248
	1612630556736 [label=CudnnBatchNormBackward0]
	1612630568832 -> 1612630556736
	1612630568832 [label=ConvolutionBackward0]
	1612630572912 -> 1612630568832
	1612630572912 [label=ReluBackward0]
	1612630557696 -> 1612630572912
	1612630557696 [label=AddBackward0]
	1612630571424 -> 1612630557696
	1612630571424 [label=CudnnBatchNormBackward0]
	1612630566000 -> 1612630571424
	1612630566000 [label=ConvolutionBackward0]
	1612630568448 -> 1612630566000
	1612630568448 [label=ReluBackward0]
	1612630568304 -> 1612630568448
	1612630568304 [label=CudnnBatchNormBackward0]
	1612630561440 -> 1612630568304
	1612630561440 [label=ConvolutionBackward0]
	1612630572672 -> 1612630561440
	1612630572672 [label=ReluBackward0]
	1612627513920 -> 1612630572672
	1612627513920 [label=AddBackward0]
	1612627524720 -> 1612627513920
	1612627524720 [label=CudnnBatchNormBackward0]
	1612627518672 -> 1612627524720
	1612627518672 [label=ConvolutionBackward0]
	1612627522128 -> 1612627518672
	1612627522128 [label=ReluBackward0]
	1612627523760 -> 1612627522128
	1612627523760 [label=CudnnBatchNormBackward0]
	1612627524432 -> 1612627523760
	1612627524432 [label=ConvolutionBackward0]
	1612627516320 -> 1612627524432
	1612627516320 [label=ReluBackward0]
	1612627522944 -> 1612627516320
	1612627522944 [label=AddBackward0]
	1612627516704 -> 1612627522944
	1612627516704 [label=CudnnBatchNormBackward0]
	1612627520832 -> 1612627516704
	1612627520832 [label=ConvolutionBackward0]
	1612627519776 -> 1612627520832
	1612627519776 [label=ReluBackward0]
	1612627512864 -> 1612627519776
	1612627512864 [label=CudnnBatchNormBackward0]
	1612627524000 -> 1612627512864
	1612627524000 [label=ConvolutionBackward0]
	1612627524768 -> 1612627524000
	1612627524768 [label=MaxPool2DWithIndicesBackward0]
	1612627524480 -> 1612627524768
	1612627524480 [label=ReluBackward0]
	1612627511520 -> 1612627524480
	1612627511520 [label=CudnnBatchNormBackward0]
	1612627524576 -> 1612627511520
	1612627524576 [label=ConvolutionBackward0]
	1612627524528 -> 1612627524576
	1612633222384 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1612633222384 -> 1612627524528
	1612627524528 [label=AccumulateGrad]
	1612627524336 -> 1612627511520
	1612472041008 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1612472041008 -> 1612627524336
	1612627524336 [label=AccumulateGrad]
	1612627511424 -> 1612627511520
	1612472041104 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1612472041104 -> 1612627511424
	1612627511424 [label=AccumulateGrad]
	1612627518384 -> 1612627524000
	1612633210960 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1612633210960 -> 1612627518384
	1612627518384 [label=AccumulateGrad]
	1612627512960 -> 1612627512864
	1612633211056 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1612633211056 -> 1612627512960
	1612627512960 [label=AccumulateGrad]
	1612627520448 -> 1612627512864
	1612633211152 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1612633211152 -> 1612627520448
	1612627520448 [label=AccumulateGrad]
	1612627519824 -> 1612627520832
	1612633211536 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1612633211536 -> 1612627519824
	1612627519824 [label=AccumulateGrad]
	1612627520976 -> 1612627516704
	1612633211632 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1612633211632 -> 1612627520976
	1612627520976 [label=AccumulateGrad]
	1612627513248 -> 1612627516704
	1612633211728 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1612633211728 -> 1612627513248
	1612627513248 [label=AccumulateGrad]
	1612627524768 -> 1612627522944
	1612627524672 -> 1612627524432
	1612633212112 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1612633212112 -> 1612627524672
	1612627524672 [label=AccumulateGrad]
	1612627522848 -> 1612627523760
	1612633212208 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1612633212208 -> 1612627522848
	1612627522848 [label=AccumulateGrad]
	1612627510848 -> 1612627523760
	1612633212304 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1612633212304 -> 1612627510848
	1612627510848 [label=AccumulateGrad]
	1612627515168 -> 1612627518672
	1612633212688 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1612633212688 -> 1612627515168
	1612627515168 [label=AccumulateGrad]
	1612627523136 -> 1612627524720
	1612633212784 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1612633212784 -> 1612627523136
	1612627523136 [label=AccumulateGrad]
	1612627523088 -> 1612627524720
	1612633212880 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1612633212880 -> 1612627523088
	1612627523088 [label=AccumulateGrad]
	1612627516320 -> 1612627513920
	1612627518624 -> 1612630561440
	1612633213840 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1612633213840 -> 1612627518624
	1612627518624 [label=AccumulateGrad]
	1612630558704 -> 1612630568304
	1612633213936 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1612633213936 -> 1612630558704
	1612630558704 [label=AccumulateGrad]
	1612630570272 -> 1612630568304
	1612633214032 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1612633214032 -> 1612630570272
	1612630570272 [label=AccumulateGrad]
	1612630567392 -> 1612630566000
	1612633214416 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1612633214416 -> 1612630567392
	1612630567392 [label=AccumulateGrad]
	1612630561104 -> 1612630571424
	1612633214512 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1612633214512 -> 1612630561104
	1612630561104 [label=AccumulateGrad]
	1612630559712 -> 1612630571424
	1612633214608 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1612633214608 -> 1612630559712
	1612630559712 [label=AccumulateGrad]
	1612630561968 -> 1612630557696
	1612630561968 [label=CudnnBatchNormBackward0]
	1612630562544 -> 1612630561968
	1612630562544 [label=ConvolutionBackward0]
	1612630572672 -> 1612630562544
	1612627519056 -> 1612630562544
	1612633213264 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1612633213264 -> 1612627519056
	1612627519056 [label=AccumulateGrad]
	1612630565856 -> 1612630561968
	1612633213360 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1612633213360 -> 1612630565856
	1612630565856 [label=AccumulateGrad]
	1612630571136 -> 1612630561968
	1612633213456 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1612633213456 -> 1612630571136
	1612630571136 [label=AccumulateGrad]
	1612630569168 -> 1612630568832
	1612633214992 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1612633214992 -> 1612630569168
	1612630569168 [label=AccumulateGrad]
	1612630557792 -> 1612630556736
	1612633215088 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1612633215088 -> 1612630557792
	1612630557792 [label=AccumulateGrad]
	1612630570464 -> 1612630556736
	1612633215184 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1612633215184 -> 1612630570464
	1612630570464 [label=AccumulateGrad]
	1612630568640 -> 1612630572048
	1612633215568 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1612633215568 -> 1612630568640
	1612630568640 [label=AccumulateGrad]
	1612630556928 -> 1612630559136
	1612633215664 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1612633215664 -> 1612630556928
	1612630556928 [label=AccumulateGrad]
	1612630572192 -> 1612630559136
	1612633215760 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1612633215760 -> 1612630572192
	1612630572192 [label=AccumulateGrad]
	1612630572912 -> 1612630557840
	1612630565424 -> 1612630566096
	1612633216720 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1612633216720 -> 1612630565424
	1612630565424 [label=AccumulateGrad]
	1612630570848 -> 1612630570608
	1612633216816 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1612633216816 -> 1612630570848
	1612630570848 [label=AccumulateGrad]
	1612630556976 -> 1612630570608
	1612633216912 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1612633216912 -> 1612630556976
	1612630556976 [label=AccumulateGrad]
	1612630566576 -> 1612630560384
	1612633217296 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1612633217296 -> 1612630566576
	1612630566576 [label=AccumulateGrad]
	1612630558320 -> 1612630559568
	1612633217392 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1612633217392 -> 1612630558320
	1612630558320 [label=AccumulateGrad]
	1612630569936 -> 1612630559568
	1612633217488 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1612633217488 -> 1612630569936
	1612630569936 [label=AccumulateGrad]
	1612630566768 -> 1612630572336
	1612630566768 [label=CudnnBatchNormBackward0]
	1612630569984 -> 1612630566768
	1612630569984 [label=ConvolutionBackward0]
	1612630568352 -> 1612630569984
	1612630570704 -> 1612630569984
	1612633216144 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1612633216144 -> 1612630570704
	1612630570704 [label=AccumulateGrad]
	1612630558128 -> 1612630566768
	1612633216240 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1612633216240 -> 1612630558128
	1612630558128 [label=AccumulateGrad]
	1612630565040 -> 1612630566768
	1612633216336 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1612633216336 -> 1612630565040
	1612630565040 [label=AccumulateGrad]
	1612630560000 -> 1612630562880
	1612633217872 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1612633217872 -> 1612630560000
	1612630560000 [label=AccumulateGrad]
	1612630566336 -> 1612630559088
	1612633217968 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1612633217968 -> 1612630566336
	1612630566336 [label=AccumulateGrad]
	1612630570176 -> 1612630559088
	1612633218064 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1612633218064 -> 1612630570176
	1612630570176 [label=AccumulateGrad]
	1612630562496 -> 1612630566816
	1612633218448 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1612633218448 -> 1612630562496
	1612630562496 [label=AccumulateGrad]
	1612630559760 -> 1612630562736
	1612633218544 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1612633218544 -> 1612630559760
	1612630559760 [label=AccumulateGrad]
	1612630557648 -> 1612630562736
	1612633218640 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1612633218640 -> 1612630557648
	1612630557648 [label=AccumulateGrad]
	1612630563600 -> 1612630559856
	1612630568688 -> 1612630564848
	1612633219600 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1612633219600 -> 1612630568688
	1612630568688 [label=AccumulateGrad]
	1612630569120 -> 1612630557600
	1612633219696 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1612633219696 -> 1612630569120
	1612630569120 [label=AccumulateGrad]
	1612630561680 -> 1612630557600
	1612633219792 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1612633219792 -> 1612630561680
	1612630561680 [label=AccumulateGrad]
	1612630564704 -> 1612630567728
	1612633220176 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1612633220176 -> 1612630564704
	1612630564704 [label=AccumulateGrad]
	1612630563936 -> 1612630570560
	1612633220272 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1612633220272 -> 1612630563936
	1612630563936 [label=AccumulateGrad]
	1612630563696 -> 1612630570560
	1612633220368 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1612633220368 -> 1612630563696
	1612630563696 [label=AccumulateGrad]
	1612630568256 -> 1612630557552
	1612630568256 [label=CudnnBatchNormBackward0]
	1612630570752 -> 1612630568256
	1612630570752 [label=ConvolutionBackward0]
	1612630569600 -> 1612630570752
	1612630564608 -> 1612630570752
	1612633219024 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1612633219024 -> 1612630564608
	1612630564608 [label=AccumulateGrad]
	1612630566720 -> 1612630568256
	1612633219120 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1612633219120 -> 1612630566720
	1612630566720 [label=AccumulateGrad]
	1612630565088 -> 1612630568256
	1612633219216 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1612633219216 -> 1612630565088
	1612630565088 [label=AccumulateGrad]
	1612630558368 -> 1612630566528
	1612633220752 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1612633220752 -> 1612630558368
	1612630558368 [label=AccumulateGrad]
	1612630558560 -> 1612630563888
	1612633220848 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1612633220848 -> 1612630558560
	1612630558560 [label=AccumulateGrad]
	1612630569648 -> 1612630563888
	1612633220944 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1612633220944 -> 1612630569648
	1612630569648 [label=AccumulateGrad]
	1612630562448 -> 1612630565568
	1612633221328 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1612633221328 -> 1612630562448
	1612630562448 [label=AccumulateGrad]
	1612630563024 -> 1612630557216
	1612633221424 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1612633221424 -> 1612630563024
	1612630563024 [label=AccumulateGrad]
	1612630567536 -> 1612630557216
	1612633221520 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1612633221520 -> 1612630567536
	1612630567536 [label=AccumulateGrad]
	1612630568592 -> 1612630568400
	1612630572480 -> 1612630569072
	1612630572480 [label=TBackward0]
	1612630561056 -> 1612630572480
	1612633221808 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1612633221808 -> 1612630561056
	1612630561056 [label=AccumulateGrad]
	1612630569072 -> 1612629008624
}
