digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2705224563888 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2705224152592 [label=AddmmBackward0]
	2705224151392 -> 2705224152592
	2705223482928 [label="fc.bias
 (19)" fillcolor=lightblue]
	2705223482928 -> 2705224151392
	2705224151392 [label=AccumulateGrad]
	2705224151536 -> 2705224152592
	2705224151536 [label=ViewBackward0]
	2705224151008 -> 2705224151536
	2705224151008 [label=MeanBackward1]
	2705224149808 -> 2705224151008
	2705224149808 [label=ReluBackward0]
	2705224149280 -> 2705224149808
	2705224149280 [label=AddBackward0]
	2705224148752 -> 2705224149280
	2705224148752 [label=CudnnBatchNormBackward0]
	2705224148368 -> 2705224148752
	2705224148368 [label=ConvolutionBackward0]
	2705224147312 -> 2705224148368
	2705224147312 [label=ReluBackward0]
	2705224146112 -> 2705224147312
	2705224146112 [label=CudnnBatchNormBackward0]
	2705224145584 -> 2705224146112
	2705224145584 [label=ConvolutionBackward0]
	2705224144528 -> 2705224145584
	2705224144528 [label=ReluBackward0]
	2705224144144 -> 2705224144528
	2705224144144 [label=CudnnBatchNormBackward0]
	2705224143616 -> 2705224144144
	2705224143616 [label=ConvolutionBackward0]
	2705224149424 -> 2705224143616
	2705224149424 [label=ReluBackward0]
	2705224142032 -> 2705224149424
	2705224142032 [label=AddBackward0]
	2705224141504 -> 2705224142032
	2705224141504 [label=CudnnBatchNormBackward0]
	2705224140304 -> 2705224141504
	2705224140304 [label=ConvolutionBackward0]
	2705224139248 -> 2705224140304
	2705224139248 [label=ReluBackward0]
	2705224154992 -> 2705224139248
	2705224154992 [label=CudnnBatchNormBackward0]
	2705224154896 -> 2705224154992
	2705224154896 [label=ConvolutionBackward0]
	2705224154656 -> 2705224154896
	2705224154656 [label=ReluBackward0]
	2705224154272 -> 2705224154656
	2705224154272 [label=CudnnBatchNormBackward0]
	2705224154512 -> 2705224154272
	2705224154512 [label=ConvolutionBackward0]
	2705224141360 -> 2705224154512
	2705224141360 [label=ReluBackward0]
	2705224153744 -> 2705224141360
	2705224153744 [label=AddBackward0]
	2705224153984 -> 2705224153744
	2705224153984 [label=CudnnBatchNormBackward0]
	2705224153600 -> 2705224153984
	2705224153600 [label=ConvolutionBackward0]
	2705224153312 -> 2705224153600
	2705224153312 [label=ReluBackward0]
	2705224153360 -> 2705224153312
	2705224153360 [label=CudnnBatchNormBackward0]
	2705224153024 -> 2705224153360
	2705224153024 [label=ConvolutionBackward0]
	2705224152928 -> 2705224153024
	2705224152928 [label=ReluBackward0]
	2705224152544 -> 2705224152928
	2705224152544 [label=CudnnBatchNormBackward0]
	2705224152352 -> 2705224152544
	2705224152352 [label=ConvolutionBackward0]
	2705224152208 -> 2705224152352
	2705224152208 [label=ReluBackward0]
	2705224151968 -> 2705224152208
	2705224151968 [label=AddBackward0]
	2705224151632 -> 2705224151968
	2705224151632 [label=CudnnBatchNormBackward0]
	2705224151680 -> 2705224151632
	2705224151680 [label=ConvolutionBackward0]
	2705224151296 -> 2705224151680
	2705224151296 [label=ReluBackward0]
	2705224151344 -> 2705224151296
	2705224151344 [label=CudnnBatchNormBackward0]
	2705224151248 -> 2705224151344
	2705224151248 [label=ConvolutionBackward0]
	2705224150576 -> 2705224151248
	2705224150576 [label=ReluBackward0]
	2705224150624 -> 2705224150576
	2705224150624 [label=CudnnBatchNormBackward0]
	2705224150432 -> 2705224150624
	2705224150432 [label=ConvolutionBackward0]
	2705224151824 -> 2705224150432
	2705224151824 [label=ReluBackward0]
	2705224150096 -> 2705224151824
	2705224150096 [label=AddBackward0]
	2705224149904 -> 2705224150096
	2705224149904 [label=CudnnBatchNormBackward0]
	2705224149520 -> 2705224149904
	2705224149520 [label=ConvolutionBackward0]
	2705224149664 -> 2705224149520
	2705224149664 [label=ReluBackward0]
	2705224149184 -> 2705224149664
	2705224149184 [label=CudnnBatchNormBackward0]
	2705224149088 -> 2705224149184
	2705224149088 [label=ConvolutionBackward0]
	2705224148848 -> 2705224149088
	2705224148848 [label=ReluBackward0]
	2705224148464 -> 2705224148848
	2705224148464 [label=CudnnBatchNormBackward0]
	2705224148704 -> 2705224148464
	2705224148704 [label=ConvolutionBackward0]
	2705224150192 -> 2705224148704
	2705224150192 [label=ReluBackward0]
	2705224147936 -> 2705224150192
	2705224147936 [label=AddBackward0]
	2705224148176 -> 2705224147936
	2705224148176 [label=CudnnBatchNormBackward0]
	2705224147792 -> 2705224148176
	2705224147792 [label=ConvolutionBackward0]
	2705224147504 -> 2705224147792
	2705224147504 [label=ReluBackward0]
	2705224147552 -> 2705224147504
	2705224147552 [label=CudnnBatchNormBackward0]
	2705224147216 -> 2705224147552
	2705224147216 [label=ConvolutionBackward0]
	2705224147120 -> 2705224147216
	2705224147120 [label=ReluBackward0]
	2705224146736 -> 2705224147120
	2705224146736 [label=CudnnBatchNormBackward0]
	2705224146544 -> 2705224146736
	2705224146544 [label=ConvolutionBackward0]
	2705224148032 -> 2705224146544
	2705224148032 [label=ReluBackward0]
	2705224146208 -> 2705224148032
	2705224146208 [label=AddBackward0]
	2705224146016 -> 2705224146208
	2705224146016 [label=CudnnBatchNormBackward0]
	2705224146064 -> 2705224146016
	2705224146064 [label=ConvolutionBackward0]
	2705224145632 -> 2705224146064
	2705224145632 [label=ReluBackward0]
	2705224145392 -> 2705224145632
	2705224145392 [label=CudnnBatchNormBackward0]
	2705224145344 -> 2705224145392
	2705224145344 [label=ConvolutionBackward0]
	2705224144960 -> 2705224145344
	2705224144960 [label=ReluBackward0]
	2705224145008 -> 2705224144960
	2705224145008 [label=CudnnBatchNormBackward0]
	2705224144912 -> 2705224145008
	2705224144912 [label=ConvolutionBackward0]
	2705224146160 -> 2705224144912
	2705224146160 [label=ReluBackward0]
	2705224144480 -> 2705224146160
	2705224144480 [label=AddBackward0]
	2705224144384 -> 2705224144480
	2705224144384 [label=CudnnBatchNormBackward0]
	2705224143904 -> 2705224144384
	2705224143904 [label=ConvolutionBackward0]
	2705224143760 -> 2705224143904
	2705224143760 [label=ReluBackward0]
	2705224143520 -> 2705224143760
	2705224143520 [label=CudnnBatchNormBackward0]
	2705224143184 -> 2705224143520
	2705224143184 [label=ConvolutionBackward0]
	2705224143328 -> 2705224143184
	2705224143328 [label=ReluBackward0]
	2705224142848 -> 2705224143328
	2705224142848 [label=CudnnBatchNormBackward0]
	2705224142752 -> 2705224142848
	2705224142752 [label=ConvolutionBackward0]
	2705224144288 -> 2705224142752
	2705224144288 [label=ReluBackward0]
	2705224142320 -> 2705224144288
	2705224142320 [label=AddBackward0]
	2705224142224 -> 2705224142320
	2705224142224 [label=CudnnBatchNormBackward0]
	2705224142272 -> 2705224142224
	2705224142272 [label=ConvolutionBackward0]
	2705224141600 -> 2705224142272
	2705224141600 [label=ReluBackward0]
	2705224141648 -> 2705224141600
	2705224141648 [label=CudnnBatchNormBackward0]
	2705224141456 -> 2705224141648
	2705224141456 [label=ConvolutionBackward0]
	2705224141168 -> 2705224141456
	2705224141168 [label=ReluBackward0]
	2705224141216 -> 2705224141168
	2705224141216 [label=CudnnBatchNormBackward0]
	2705224140880 -> 2705224141216
	2705224140880 [label=ConvolutionBackward0]
	2705224140784 -> 2705224140880
	2705224140784 [label=ReluBackward0]
	2705224140400 -> 2705224140784
	2705224140400 [label=AddBackward0]
	2705224140208 -> 2705224140400
	2705224140208 [label=CudnnBatchNormBackward0]
	2705224140256 -> 2705224140208
	2705224140256 [label=ConvolutionBackward0]
	2705224139824 -> 2705224140256
	2705224139824 [label=ReluBackward0]
	2705224139584 -> 2705224139824
	2705224139584 [label=CudnnBatchNormBackward0]
	2705224139536 -> 2705224139584
	2705224139536 [label=ConvolutionBackward0]
	2705224139152 -> 2705224139536
	2705224139152 [label=ReluBackward0]
	2705224139200 -> 2705224139152
	2705224139200 [label=CudnnBatchNormBackward0]
	2705224139104 -> 2705224139200
	2705224139104 [label=ConvolutionBackward0]
	2705224140352 -> 2705224139104
	2705224140352 [label=ReluBackward0]
	2705224055824 -> 2705224140352
	2705224055824 [label=AddBackward0]
	2705224055296 -> 2705224055824
	2705224055296 [label=CudnnBatchNormBackward0]
	2705224054096 -> 2705224055296
	2705224054096 [label=ConvolutionBackward0]
	2705224053040 -> 2705224054096
	2705224053040 [label=ReluBackward0]
	2705224052656 -> 2705224053040
	2705224052656 [label=CudnnBatchNormBackward0]
	2705224052128 -> 2705224052656
	2705224052128 [label=ConvolutionBackward0]
	2705224051072 -> 2705224052128
	2705224051072 [label=ReluBackward0]
	2705224049872 -> 2705224051072
	2705224049872 [label=CudnnBatchNormBackward0]
	2705224049344 -> 2705224049872
	2705224049344 [label=ConvolutionBackward0]
	2705224055152 -> 2705224049344
	2705224055152 [label=ReluBackward0]
	2705224047760 -> 2705224055152
	2705224047760 [label=AddBackward0]
	2705224047232 -> 2705224047760
	2705224047232 [label=CudnnBatchNormBackward0]
	2705224046848 -> 2705224047232
	2705224046848 [label=ConvolutionBackward0]
	2705224045792 -> 2705224046848
	2705224045792 [label=ReluBackward0]
	2705224044592 -> 2705224045792
	2705224044592 [label=CudnnBatchNormBackward0]
	2705224044064 -> 2705224044592
	2705224044064 [label=ConvolutionBackward0]
	2705224043008 -> 2705224044064
	2705224043008 [label=ReluBackward0]
	2705224042624 -> 2705224043008
	2705224042624 [label=CudnnBatchNormBackward0]
	2705224042096 -> 2705224042624
	2705224042096 [label=ConvolutionBackward0]
	2705224047904 -> 2705224042096
	2705224047904 [label=ReluBackward0]
	2705224056784 -> 2705224047904
	2705224056784 [label=AddBackward0]
	2705224056448 -> 2705224056784
	2705224056448 [label=CudnnBatchNormBackward0]
	2705224056496 -> 2705224056448
	2705224056496 [label=ConvolutionBackward0]
	2705224056112 -> 2705224056496
	2705224056112 [label=ReluBackward0]
	2705224056160 -> 2705224056112
	2705224056160 [label=CudnnBatchNormBackward0]
	2705224056064 -> 2705224056160
	2705224056064 [label=ConvolutionBackward0]
	2705224055392 -> 2705224056064
	2705224055392 [label=ReluBackward0]
	2705224055440 -> 2705224055392
	2705224055440 [label=CudnnBatchNormBackward0]
	2705224055248 -> 2705224055440
	2705224055248 [label=ConvolutionBackward0]
	2705224054960 -> 2705224055248
	2705224054960 [label=ReluBackward0]
	2705224055008 -> 2705224054960
	2705224055008 [label=AddBackward0]
	2705224054672 -> 2705224055008
	2705224054672 [label=CudnnBatchNormBackward0]
	2705224054432 -> 2705224054672
	2705224054432 [label=ConvolutionBackward0]
	2705224054192 -> 2705224054432
	2705224054192 [label=ReluBackward0]
	2705224053808 -> 2705224054192
	2705224053808 [label=CudnnBatchNormBackward0]
	2705224054048 -> 2705224053808
	2705224054048 [label=ConvolutionBackward0]
	2705224053616 -> 2705224054048
	2705224053616 [label=ReluBackward0]
	2705224053376 -> 2705224053616
	2705224053376 [label=CudnnBatchNormBackward0]
	2705224053328 -> 2705224053376
	2705224053328 [label=ConvolutionBackward0]
	2705224054720 -> 2705224053328
	2705224054720 [label=ReluBackward0]
	2705224052848 -> 2705224054720
	2705224052848 [label=AddBackward0]
	2705224052800 -> 2705224052848
	2705224052800 [label=CudnnBatchNormBackward0]
	2705224052560 -> 2705224052800
	2705224052560 [label=ConvolutionBackward0]
	2705224052464 -> 2705224052560
	2705224052464 [label=ReluBackward0]
	2705224052080 -> 2705224052464
	2705224052080 [label=CudnnBatchNormBackward0]
	2705224051888 -> 2705224052080
	2705224051888 [label=ConvolutionBackward0]
	2705224051744 -> 2705224051888
	2705224051744 [label=ReluBackward0]
	2705224051504 -> 2705224051744
	2705224051504 [label=CudnnBatchNormBackward0]
	2705224051168 -> 2705224051504
	2705224051168 [label=ConvolutionBackward0]
	2705224052992 -> 2705224051168
	2705224052992 [label=ReluBackward0]
	2705224050976 -> 2705224052992
	2705224050976 [label=AddBackward0]
	2705224050640 -> 2705224050976
	2705224050640 [label=CudnnBatchNormBackward0]
	2705224050688 -> 2705224050640
	2705224050688 [label=ConvolutionBackward0]
	2705224050304 -> 2705224050688
	2705224050304 [label=ReluBackward0]
	2705224050352 -> 2705224050304
	2705224050352 [label=CudnnBatchNormBackward0]
	2705224050256 -> 2705224050352
	2705224050256 [label=ConvolutionBackward0]
	2705224049584 -> 2705224050256
	2705224049584 [label=ReluBackward0]
	2705224049632 -> 2705224049584
	2705224049632 [label=CudnnBatchNormBackward0]
	2705224049440 -> 2705224049632
	2705224049440 [label=ConvolutionBackward0]
	2705224049152 -> 2705224049440
	2705224049152 [label=MaxPool2DWithIndicesBackward0]
	2705224049200 -> 2705224049152
	2705224049200 [label=ReluBackward0]
	2705224048864 -> 2705224049200
	2705224048864 [label=CudnnBatchNormBackward0]
	2705224048528 -> 2705224048864
	2705224048528 [label=ConvolutionBackward0]
	2705224048672 -> 2705224048528
	2705223482736 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2705223482736 -> 2705224048672
	2705224048672 [label=AccumulateGrad]
	2705224048720 -> 2705224048864
	2705207364208 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2705207364208 -> 2705224048720
	2705224048720 [label=AccumulateGrad]
	2705224049296 -> 2705224048864
	2705207363440 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2705207363440 -> 2705224049296
	2705224049296 [label=AccumulateGrad]
	2705224049056 -> 2705224049440
	2705207364016 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2705207364016 -> 2705224049056
	2705224049056 [label=AccumulateGrad]
	2705224049728 -> 2705224049632
	2705207363056 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2705207363056 -> 2705224049728
	2705224049728 [label=AccumulateGrad]
	2705224049680 -> 2705224049632
	2705207362960 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2705207362960 -> 2705224049680
	2705224049680 [label=AccumulateGrad]
	2705224049776 -> 2705224050256
	2705207362000 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2705207362000 -> 2705224049776
	2705224049776 [label=AccumulateGrad]
	2705224050160 -> 2705224050352
	2705207362096 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2705207362096 -> 2705224050160
	2705224050160 [label=AccumulateGrad]
	2705224050112 -> 2705224050352
	2705207362288 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2705207362288 -> 2705224050112
	2705224050112 [label=AccumulateGrad]
	2705224050448 -> 2705224050688
	2705207362672 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2705207362672 -> 2705224050448
	2705224050448 [label=AccumulateGrad]
	2705224050880 -> 2705224050640
	2705207362864 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2705207362864 -> 2705224050880
	2705224050880 [label=AccumulateGrad]
	2705224050736 -> 2705224050640
	2705207362768 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2705207362768 -> 2705224050736
	2705224050736 [label=AccumulateGrad]
	2705224050832 -> 2705224050976
	2705224050832 [label=CudnnBatchNormBackward0]
	2705224049920 -> 2705224050832
	2705224049920 [label=ConvolutionBackward0]
	2705224049152 -> 2705224049920
	2705224049392 -> 2705224049920
	2705207363536 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2705207363536 -> 2705224049392
	2705224049392 [label=AccumulateGrad]
	2705224050496 -> 2705224050832
	2705207363632 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2705207363632 -> 2705224050496
	2705224050496 [label=AccumulateGrad]
	2705224050784 -> 2705224050832
	2705207363728 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2705207363728 -> 2705224050784
	2705224050784 [label=AccumulateGrad]
	2705224051312 -> 2705224051168
	2705489378672 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2705489378672 -> 2705224051312
	2705224051312 [label=AccumulateGrad]
	2705224051360 -> 2705224051504
	2705489378288 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2705489378288 -> 2705224051360
	2705224051360 [label=AccumulateGrad]
	2705224051840 -> 2705224051504
	2705489379152 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2705489379152 -> 2705224051840
	2705224051840 [label=AccumulateGrad]
	2705224051936 -> 2705224051888
	2705489378768 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2705489378768 -> 2705224051936
	2705224051936 [label=AccumulateGrad]
	2705224052032 -> 2705224052080
	2705489378960 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2705489378960 -> 2705224052032
	2705224052032 [label=AccumulateGrad]
	2705224052272 -> 2705224052080
	2705489379056 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2705489379056 -> 2705224052272
	2705224052272 [label=AccumulateGrad]
	2705224052320 -> 2705224052560
	2705223226512 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2705223226512 -> 2705224052320
	2705224052320 [label=AccumulateGrad]
	2705224052608 -> 2705224052800
	2705223226608 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2705223226608 -> 2705224052608
	2705224052608 [label=AccumulateGrad]
	2705224052896 -> 2705224052800
	2705223226704 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2705223226704 -> 2705224052896
	2705224052896 [label=AccumulateGrad]
	2705224052992 -> 2705224052848
	2705224052944 -> 2705224053328
	2705223227088 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2705223227088 -> 2705224052944
	2705224052944 [label=AccumulateGrad]
	2705224053520 -> 2705224053376
	2705223227184 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2705223227184 -> 2705224053520
	2705224053520 [label=AccumulateGrad]
	2705224053472 -> 2705224053376
	2705223227280 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2705223227280 -> 2705224053472
	2705224053472 [label=AccumulateGrad]
	2705224053664 -> 2705224054048
	2705223227664 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2705223227664 -> 2705224053664
	2705224053664 [label=AccumulateGrad]
	2705224053904 -> 2705224053808
	2705223227760 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2705223227760 -> 2705224053904
	2705224053904 [label=AccumulateGrad]
	2705224054144 -> 2705224053808
	2705223227856 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2705223227856 -> 2705224054144
	2705224054144 [label=AccumulateGrad]
	2705224054480 -> 2705224054432
	2705223228240 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2705223228240 -> 2705224054480
	2705224054480 [label=AccumulateGrad]
	2705224054336 -> 2705224054672
	2705223228336 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2705223228336 -> 2705224054336
	2705224054336 [label=AccumulateGrad]
	2705224054528 -> 2705224054672
	2705223228432 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2705223228432 -> 2705224054528
	2705224054528 [label=AccumulateGrad]
	2705224054720 -> 2705224055008
	2705224054864 -> 2705224055248
	2705223229392 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2705223229392 -> 2705224054864
	2705224054864 [label=AccumulateGrad]
	2705224055536 -> 2705224055440
	2705223229488 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2705223229488 -> 2705224055536
	2705224055536 [label=AccumulateGrad]
	2705224055488 -> 2705224055440
	2705223229584 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2705223229584 -> 2705224055488
	2705224055488 [label=AccumulateGrad]
	2705224055584 -> 2705224056064
	2705223229968 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2705223229968 -> 2705224055584
	2705224055584 [label=AccumulateGrad]
	2705224055968 -> 2705224056160
	2705223230064 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2705223230064 -> 2705224055968
	2705224055968 [label=AccumulateGrad]
	2705224055920 -> 2705224056160
	2705223230160 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2705223230160 -> 2705224055920
	2705224055920 [label=AccumulateGrad]
	2705224056256 -> 2705224056496
	2705223230544 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2705223230544 -> 2705224056256
	2705224056256 [label=AccumulateGrad]
	2705224056688 -> 2705224056448
	2705223230640 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2705223230640 -> 2705224056688
	2705224056688 [label=AccumulateGrad]
	2705224056544 -> 2705224056448
	2705223230736 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2705223230736 -> 2705224056544
	2705224056544 [label=AccumulateGrad]
	2705224056640 -> 2705224056784
	2705224056640 [label=CudnnBatchNormBackward0]
	2705224055728 -> 2705224056640
	2705224055728 [label=ConvolutionBackward0]
	2705224054960 -> 2705224055728
	2705224055200 -> 2705224055728
	2705223228816 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2705223228816 -> 2705224055200
	2705224055200 [label=AccumulateGrad]
	2705224056304 -> 2705224056640
	2705223228912 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2705223228912 -> 2705224056304
	2705224056304 [label=AccumulateGrad]
	2705224056592 -> 2705224056640
	2705223229008 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2705223229008 -> 2705224056592
	2705224056592 [label=AccumulateGrad]
	2705224041040 -> 2705224042096
	2705223231120 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2705223231120 -> 2705224041040
	2705224041040 [label=AccumulateGrad]
	2705224041952 -> 2705224042624
	2705223231216 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2705223231216 -> 2705224041952
	2705224041952 [label=AccumulateGrad]
	2705224043152 -> 2705224042624
	2705223231312 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2705223231312 -> 2705224043152
	2705224043152 [label=AccumulateGrad]
	2705224043680 -> 2705224044064
	2705223231696 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2705223231696 -> 2705224043680
	2705224043680 [label=AccumulateGrad]
	2705224044736 -> 2705224044592
	2705223231792 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2705223231792 -> 2705224044736
	2705224044736 [label=AccumulateGrad]
	2705224045120 -> 2705224044592
	2705223231888 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2705223231888 -> 2705224045120
	2705224045120 [label=AccumulateGrad]
	2705224045648 -> 2705224046848
	2705223232272 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2705223232272 -> 2705224045648
	2705224045648 [label=AccumulateGrad]
	2705224046704 -> 2705224047232
	2705223232368 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2705223232368 -> 2705224046704
	2705224046704 [label=AccumulateGrad]
	2705224047376 -> 2705224047232
	2705223232464 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2705223232464 -> 2705224047376
	2705224047376 [label=AccumulateGrad]
	2705224047904 -> 2705224047760
	2705224048288 -> 2705224049344
	2705223232848 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2705223232848 -> 2705224048288
	2705224048288 [label=AccumulateGrad]
	2705224050016 -> 2705224049872
	2705223232944 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2705223232944 -> 2705224050016
	2705224050016 [label=AccumulateGrad]
	2705224050400 -> 2705224049872
	2705223233040 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2705223233040 -> 2705224050400
	2705224050400 [label=AccumulateGrad]
	2705224050928 -> 2705224052128
	2705223233424 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2705223233424 -> 2705224050928
	2705224050928 [label=AccumulateGrad]
	2705224051984 -> 2705224052656
	2705223233520 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2705223233520 -> 2705224051984
	2705224051984 [label=AccumulateGrad]
	2705224053184 -> 2705224052656
	2705223233616 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2705223233616 -> 2705224053184
	2705224053184 [label=AccumulateGrad]
	2705224053712 -> 2705224054096
	2705223234000 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2705223234000 -> 2705224053712
	2705224053712 [label=AccumulateGrad]
	2705224054768 -> 2705224055296
	2705223234096 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2705223234096 -> 2705224054768
	2705224054768 [label=AccumulateGrad]
	2705224054624 -> 2705224055296
	2705223234192 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2705223234192 -> 2705224054624
	2705224054624 [label=AccumulateGrad]
	2705224055152 -> 2705224055824
	2705224056352 -> 2705224139104
	2705223234576 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2705223234576 -> 2705224056352
	2705224056352 [label=AccumulateGrad]
	2705224139008 -> 2705224139200
	2705223234672 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2705223234672 -> 2705224139008
	2705224139008 [label=AccumulateGrad]
	2705224138960 -> 2705224139200
	2705223234768 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2705223234768 -> 2705224138960
	2705224138960 [label=AccumulateGrad]
	2705224139296 -> 2705224139536
	2705223235152 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2705223235152 -> 2705224139296
	2705224139296 [label=AccumulateGrad]
	2705224139728 -> 2705224139584
	2705223235248 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2705223235248 -> 2705224139728
	2705224139728 [label=AccumulateGrad]
	2705224139680 -> 2705224139584
	2705223235344 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2705223235344 -> 2705224139680
	2705224139680 [label=AccumulateGrad]
	2705224139872 -> 2705224140256
	2705223235728 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2705223235728 -> 2705224139872
	2705224139872 [label=AccumulateGrad]
	2705224140112 -> 2705224140208
	2705223235824 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2705223235824 -> 2705224140112
	2705224140112 [label=AccumulateGrad]
	2705224140016 -> 2705224140208
	2705223235920 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2705223235920 -> 2705224140016
	2705224140016 [label=AccumulateGrad]
	2705224140352 -> 2705224140400
	2705224140640 -> 2705224140880
	2705223236880 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2705223236880 -> 2705224140640
	2705224140640 [label=AccumulateGrad]
	2705224140928 -> 2705224141216
	2705223236976 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2705223236976 -> 2705224140928
	2705224140928 [label=AccumulateGrad]
	2705224141312 -> 2705224141216
	2705223237072 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2705223237072 -> 2705224141312
	2705224141312 [label=AccumulateGrad]
	2705224141072 -> 2705224141456
	2705223237456 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2705223237456 -> 2705224141072
	2705224141072 [label=AccumulateGrad]
	2705224141744 -> 2705224141648
	2705223237552 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2705223237552 -> 2705224141744
	2705224141744 [label=AccumulateGrad]
	2705224141696 -> 2705224141648
	2705223467088 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2705223467088 -> 2705224141696
	2705224141696 [label=AccumulateGrad]
	2705224141792 -> 2705224142272
	2705223467472 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2705223467472 -> 2705224141792
	2705224141792 [label=AccumulateGrad]
	2705224142176 -> 2705224142224
	2705223467568 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2705223467568 -> 2705224142176
	2705224142176 [label=AccumulateGrad]
	2705224142368 -> 2705224142224
	2705223467664 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2705223467664 -> 2705224142368
	2705224142368 [label=AccumulateGrad]
	2705224142128 -> 2705224142320
	2705224142128 [label=CudnnBatchNormBackward0]
	2705224141264 -> 2705224142128
	2705224141264 [label=ConvolutionBackward0]
	2705224140784 -> 2705224141264
	2705224140736 -> 2705224141264
	2705223236304 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2705223236304 -> 2705224140736
	2705224140736 [label=AccumulateGrad]
	2705224141936 -> 2705224142128
	2705223236400 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2705223236400 -> 2705224141936
	2705224141936 [label=AccumulateGrad]
	2705224141984 -> 2705224142128
	2705223236496 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2705223236496 -> 2705224141984
	2705224141984 [label=AccumulateGrad]
	2705224142512 -> 2705224142752
	2705223468048 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2705223468048 -> 2705224142512
	2705224142512 [label=AccumulateGrad]
	2705224142656 -> 2705224142848
	2705223468144 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2705223468144 -> 2705224142656
	2705224142656 [label=AccumulateGrad]
	2705224143040 -> 2705224142848
	2705223468240 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2705223468240 -> 2705224143040
	2705224143040 [label=AccumulateGrad]
	2705224143232 -> 2705224143184
	2705223468624 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2705223468624 -> 2705224143232
	2705224143232 [label=AccumulateGrad]
	2705224143376 -> 2705224143520
	2705223468720 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2705223468720 -> 2705224143376
	2705224143376 [label=AccumulateGrad]
	2705224143856 -> 2705224143520
	2705223468816 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2705223468816 -> 2705224143856
	2705224143856 [label=AccumulateGrad]
	2705224143952 -> 2705224143904
	2705223469200 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2705223469200 -> 2705224143952
	2705224143952 [label=AccumulateGrad]
	2705224144048 -> 2705224144384
	2705223469296 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2705223469296 -> 2705224144048
	2705224144048 [label=AccumulateGrad]
	2705224144096 -> 2705224144384
	2705223469392 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2705223469392 -> 2705224144096
	2705224144096 [label=AccumulateGrad]
	2705224144288 -> 2705224144480
	2705224144240 -> 2705224144912
	2705223469776 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2705223469776 -> 2705224144240
	2705224144240 [label=AccumulateGrad]
	2705224144816 -> 2705224145008
	2705223469872 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2705223469872 -> 2705224144816
	2705224144816 [label=AccumulateGrad]
	2705224144768 -> 2705224145008
	2705223469968 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2705223469968 -> 2705224144768
	2705224144768 [label=AccumulateGrad]
	2705224145104 -> 2705224145344
	2705223470352 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2705223470352 -> 2705224145104
	2705224145104 [label=AccumulateGrad]
	2705224145536 -> 2705224145392
	2705223470448 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2705223470448 -> 2705224145536
	2705224145536 [label=AccumulateGrad]
	2705224145488 -> 2705224145392
	2705223470544 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2705223470544 -> 2705224145488
	2705224145488 [label=AccumulateGrad]
	2705224145680 -> 2705224146064
	2705223470928 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2705223470928 -> 2705224145680
	2705224145680 [label=AccumulateGrad]
	2705224145920 -> 2705224146016
	2705223471024 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2705223471024 -> 2705224145920
	2705224145920 [label=AccumulateGrad]
	2705224145824 -> 2705224146016
	2705223471120 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2705223471120 -> 2705224145824
	2705224145824 [label=AccumulateGrad]
	2705224146160 -> 2705224146208
	2705224146400 -> 2705224146544
	2705223471504 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2705223471504 -> 2705224146400
	2705224146400 [label=AccumulateGrad]
	2705224146688 -> 2705224146736
	2705223471600 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2705223471600 -> 2705224146688
	2705224146688 [label=AccumulateGrad]
	2705224146928 -> 2705224146736
	2705223471696 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2705223471696 -> 2705224146928
	2705224146928 [label=AccumulateGrad]
	2705224146976 -> 2705224147216
	2705223472080 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2705223472080 -> 2705224146976
	2705224146976 [label=AccumulateGrad]
	2705224147264 -> 2705224147552
	2705223472176 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2705223472176 -> 2705224147264
	2705224147264 [label=AccumulateGrad]
	2705224147648 -> 2705224147552
	2705223472272 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2705223472272 -> 2705224147648
	2705224147648 [label=AccumulateGrad]
	2705224147408 -> 2705224147792
	2705223472656 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2705223472656 -> 2705224147408
	2705224147408 [label=AccumulateGrad]
	2705224148080 -> 2705224148176
	2705223472752 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2705223472752 -> 2705224148080
	2705224148080 [label=AccumulateGrad]
	2705224147984 -> 2705224148176
	2705223472848 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2705223472848 -> 2705224147984
	2705224147984 [label=AccumulateGrad]
	2705224148032 -> 2705224147936
	2705224148272 -> 2705224148704
	2705223473232 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2705223473232 -> 2705224148272
	2705224148272 [label=AccumulateGrad]
	2705224148560 -> 2705224148464
	2705223473328 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2705223473328 -> 2705224148560
	2705224148560 [label=AccumulateGrad]
	2705224148800 -> 2705224148464
	2705223473424 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2705223473424 -> 2705224148800
	2705224148800 [label=AccumulateGrad]
	2705224149136 -> 2705224149088
	2705223473808 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2705223473808 -> 2705224149136
	2705224149136 [label=AccumulateGrad]
	2705224148992 -> 2705224149184
	2705223473904 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2705223473904 -> 2705224148992
	2705224148992 [label=AccumulateGrad]
	2705224149376 -> 2705224149184
	2705223474000 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2705223474000 -> 2705224149376
	2705224149376 [label=AccumulateGrad]
	2705224149568 -> 2705224149520
	2705223474384 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2705223474384 -> 2705224149568
	2705224149568 [label=AccumulateGrad]
	2705224149712 -> 2705224149904
	2705223474480 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2705223474480 -> 2705224149712
	2705224149712 [label=AccumulateGrad]
	2705224149856 -> 2705224149904
	2705223474576 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2705223474576 -> 2705224149856
	2705224149856 [label=AccumulateGrad]
	2705224150192 -> 2705224150096
	2705224150144 -> 2705224150432
	2705223474960 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2705223474960 -> 2705224150144
	2705224150144 [label=AccumulateGrad]
	2705224150720 -> 2705224150624
	2705223475056 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2705223475056 -> 2705224150720
	2705224150720 [label=AccumulateGrad]
	2705224150672 -> 2705224150624
	2705223475152 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2705223475152 -> 2705224150672
	2705224150672 [label=AccumulateGrad]
	2705224150768 -> 2705224151248
	2705223475536 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2705223475536 -> 2705224150768
	2705224150768 [label=AccumulateGrad]
	2705224151152 -> 2705224151344
	2705223475632 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2705223475632 -> 2705224151152
	2705224151152 [label=AccumulateGrad]
	2705224151104 -> 2705224151344
	2705223475728 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2705223475728 -> 2705224151104
	2705224151104 [label=AccumulateGrad]
	2705224151440 -> 2705224151680
	2705223476112 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2705223476112 -> 2705224151440
	2705224151440 [label=AccumulateGrad]
	2705224151872 -> 2705224151632
	2705223476208 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2705223476208 -> 2705224151872
	2705224151872 [label=AccumulateGrad]
	2705224151728 -> 2705224151632
	2705223476304 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2705223476304 -> 2705224151728
	2705224151728 [label=AccumulateGrad]
	2705224151824 -> 2705224151968
	2705224152400 -> 2705224152352
	2705223477360 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2705223477360 -> 2705224152400
	2705224152400 [label=AccumulateGrad]
	2705224152496 -> 2705224152544
	2705223477456 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2705223477456 -> 2705224152496
	2705224152496 [label=AccumulateGrad]
	2705224152736 -> 2705224152544
	2705223477552 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2705223477552 -> 2705224152736
	2705224152736 [label=AccumulateGrad]
	2705224152784 -> 2705224153024
	2705223477936 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2705223477936 -> 2705224152784
	2705224152784 [label=AccumulateGrad]
	2705224153072 -> 2705224153360
	2705223478032 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2705223478032 -> 2705224153072
	2705224153072 [label=AccumulateGrad]
	2705224153456 -> 2705224153360
	2705223478128 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2705223478128 -> 2705224153456
	2705224153456 [label=AccumulateGrad]
	2705224153216 -> 2705224153600
	2705223478512 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2705223478512 -> 2705224153216
	2705224153216 [label=AccumulateGrad]
	2705224153888 -> 2705224153984
	2705223478608 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2705223478608 -> 2705224153888
	2705224153888 [label=AccumulateGrad]
	2705224153792 -> 2705224153984
	2705223478704 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2705223478704 -> 2705224153792
	2705224153792 [label=AccumulateGrad]
	2705224153840 -> 2705224153744
	2705224153840 [label=CudnnBatchNormBackward0]
	2705224152688 -> 2705224153840
	2705224152688 [label=ConvolutionBackward0]
	2705224152208 -> 2705224152688
	2705224152160 -> 2705224152688
	2705223476784 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2705223476784 -> 2705224152160
	2705224152160 [label=AccumulateGrad]
	2705224153408 -> 2705224153840
	2705223476880 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2705223476880 -> 2705224153408
	2705224153408 [label=AccumulateGrad]
	2705224153552 -> 2705224153840
	2705223476976 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2705223476976 -> 2705224153552
	2705224153552 [label=AccumulateGrad]
	2705224154080 -> 2705224154512
	2705223479088 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2705223479088 -> 2705224154080
	2705224154080 [label=AccumulateGrad]
	2705224154368 -> 2705224154272
	2705223479184 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2705223479184 -> 2705224154368
	2705224154368 [label=AccumulateGrad]
	2705224154608 -> 2705224154272
	2705223479280 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2705223479280 -> 2705224154608
	2705224154608 [label=AccumulateGrad]
	2705224154944 -> 2705224154896
	2705223479664 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2705223479664 -> 2705224154944
	2705224154944 [label=AccumulateGrad]
	2705224154800 -> 2705224154992
	2705223479760 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2705223479760 -> 2705224154800
	2705224154800 [label=AccumulateGrad]
	2705224139392 -> 2705224154992
	2705223479856 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2705223479856 -> 2705224139392
	2705224139392 [label=AccumulateGrad]
	2705224139920 -> 2705224140304
	2705223480240 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2705223480240 -> 2705224139920
	2705224139920 [label=AccumulateGrad]
	2705224140976 -> 2705224141504
	2705223480336 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2705223480336 -> 2705224140976
	2705224140976 [label=AccumulateGrad]
	2705224140832 -> 2705224141504
	2705223480432 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2705223480432 -> 2705224140832
	2705224140832 [label=AccumulateGrad]
	2705224141360 -> 2705224142032
	2705224142560 -> 2705224143616
	2705223480816 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2705223480816 -> 2705224142560
	2705224142560 [label=AccumulateGrad]
	2705224143472 -> 2705224144144
	2705223480912 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2705223480912 -> 2705224143472
	2705224143472 [label=AccumulateGrad]
	2705224144672 -> 2705224144144
	2705223481008 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2705223481008 -> 2705224144672
	2705224144672 [label=AccumulateGrad]
	2705224145200 -> 2705224145584
	2705223481392 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2705223481392 -> 2705224145200
	2705224145200 [label=AccumulateGrad]
	2705224146256 -> 2705224146112
	2705223481488 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2705223481488 -> 2705224146256
	2705224146256 [label=AccumulateGrad]
	2705224146640 -> 2705224146112
	2705223481584 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2705223481584 -> 2705224146640
	2705224146640 [label=AccumulateGrad]
	2705224147168 -> 2705224148368
	2705223481968 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2705223481968 -> 2705224147168
	2705224147168 [label=AccumulateGrad]
	2705224148224 -> 2705224148752
	2705223482064 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2705223482064 -> 2705224148224
	2705224148224 [label=AccumulateGrad]
	2705224148896 -> 2705224148752
	2705223482160 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2705223482160 -> 2705224148896
	2705224148896 [label=AccumulateGrad]
	2705224149424 -> 2705224149280
	2705224150864 -> 2705224152592
	2705224150864 [label=TBackward0]
	2705224149952 -> 2705224150864
	2705223482832 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2705223482832 -> 2705224149952
	2705224149952 [label=AccumulateGrad]
	2705224152592 -> 2705224563888
}
