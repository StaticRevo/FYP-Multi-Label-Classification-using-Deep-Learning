digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2059614077872 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2059609973824 [label=SigmoidBackward0]
	2059609971136 -> 2059609973824
	2059609971136 [label=AddmmBackward0]
	2059609972864 -> 2059609971136
	2059613888080 [label="model.fc.bias
 (19)" fillcolor=lightblue]
	2059613888080 -> 2059609972864
	2059609972864 [label=AccumulateGrad]
	2059609971280 -> 2059609971136
	2059609971280 [label=ViewBackward0]
	2059609972240 -> 2059609971280
	2059609972240 [label=MeanBackward1]
	2059609969840 -> 2059609972240
	2059609969840 [label=ReluBackward0]
	2059609970272 -> 2059609969840
	2059609970272 [label=AddBackward0]
	2059609967104 -> 2059609970272
	2059609967104 [label=CudnnBatchNormBackward0]
	2059609978480 -> 2059609967104
	2059609978480 [label=ConvolutionBackward0]
	2059609969312 -> 2059609978480
	2059609969312 [label=ReluBackward0]
	2059609979152 -> 2059609969312
	2059609979152 [label=CudnnBatchNormBackward0]
	2059609980352 -> 2059609979152
	2059609980352 [label=ConvolutionBackward0]
	2059609967824 -> 2059609980352
	2059609967824 [label=ReluBackward0]
	2059609972480 -> 2059609967824
	2059609972480 [label=AddBackward0]
	2059609980592 -> 2059609972480
	2059609980592 [label=CudnnBatchNormBackward0]
	2059609978864 -> 2059609980592
	2059609978864 [label=ConvolutionBackward0]
	2059609967728 -> 2059609978864
	2059609967728 [label=ReluBackward0]
	2059609971520 -> 2059609967728
	2059609971520 [label=CudnnBatchNormBackward0]
	2059609980688 -> 2059609971520
	2059609980688 [label=ConvolutionBackward0]
	2059609972048 -> 2059609980688
	2059609972048 [label=ReluBackward0]
	2059609979872 -> 2059609972048
	2059609979872 [label=AddBackward0]
	2059609976848 -> 2059609979872
	2059609976848 [label=CudnnBatchNormBackward0]
	2059609974880 -> 2059609976848
	2059609974880 [label=ConvolutionBackward0]
	2059609974400 -> 2059609974880
	2059609974400 [label=ReluBackward0]
	2059609976080 -> 2059609974400
	2059609976080 [label=CudnnBatchNormBackward0]
	2059609968064 -> 2059609976080
	2059609968064 [label=ConvolutionBackward0]
	2059609976368 -> 2059609968064
	2059609976368 [label=ReluBackward0]
	2059609973056 -> 2059609976368
	2059609973056 [label=AddBackward0]
	2059609971712 -> 2059609973056
	2059609971712 [label=CudnnBatchNormBackward0]
	2059609976944 -> 2059609971712
	2059609976944 [label=ConvolutionBackward0]
	2059609977376 -> 2059609976944
	2059609977376 [label=ReluBackward0]
	2059609980112 -> 2059609977376
	2059609980112 [label=CudnnBatchNormBackward0]
	2059609974640 -> 2059609980112
	2059609974640 [label=ConvolutionBackward0]
	2059609973968 -> 2059609974640
	2059609973968 [label=ReluBackward0]
	2059609979536 -> 2059609973968
	2059609979536 [label=AddBackward0]
	2059609977328 -> 2059609979536
	2059609977328 [label=CudnnBatchNormBackward0]
	2059609979344 -> 2059609977328
	2059609979344 [label=ConvolutionBackward0]
	2059609975840 -> 2059609979344
	2059609975840 [label=ReluBackward0]
	2059609979728 -> 2059609975840
	2059609979728 [label=CudnnBatchNormBackward0]
	2059609967920 -> 2059609979728
	2059609967920 [label=ConvolutionBackward0]
	2059609975216 -> 2059609967920
	2059609975216 [label=ReluBackward0]
	2059609975408 -> 2059609975216
	2059609975408 [label=AddBackward0]
	2059609974736 -> 2059609975408
	2059609974736 [label=CudnnBatchNormBackward0]
	2059609979200 -> 2059609974736
	2059609979200 [label=ConvolutionBackward0]
	2059610934672 -> 2059609979200
	2059610934672 [label=ReluBackward0]
	2059610938944 -> 2059610934672
	2059610938944 [label=CudnnBatchNormBackward0]
	2059610940816 -> 2059610938944
	2059610940816 [label=ConvolutionBackward0]
	2059610944128 -> 2059610940816
	2059610944128 [label=ReluBackward0]
	2059610943984 -> 2059610944128
	2059610943984 [label=AddBackward0]
	2059610943888 -> 2059610943984
	2059610943888 [label=CudnnBatchNormBackward0]
	2059610943744 -> 2059610943888
	2059610943744 [label=ConvolutionBackward0]
	2059610943552 -> 2059610943744
	2059610943552 [label=ReluBackward0]
	2059610944224 -> 2059610943552
	2059610944224 [label=CudnnBatchNormBackward0]
	2059610944320 -> 2059610944224
	2059610944320 [label=ConvolutionBackward0]
	2059610943936 -> 2059610944320
	2059610943936 [label=ReluBackward0]
	2059610944608 -> 2059610943936
	2059610944608 [label=AddBackward0]
	2059610944704 -> 2059610944608
	2059610944704 [label=CudnnBatchNormBackward0]
	2059610944848 -> 2059610944704
	2059610944848 [label=ConvolutionBackward0]
	2059610945040 -> 2059610944848
	2059610945040 [label=ReluBackward0]
	2059610945184 -> 2059610945040
	2059610945184 [label=CudnnBatchNormBackward0]
	2059610945280 -> 2059610945184
	2059610945280 [label=ConvolutionBackward0]
	2059610944656 -> 2059610945280
	2059610944656 [label=MaxPool2DWithIndicesBackward0]
	2059610945808 -> 2059610944656
	2059610945808 [label=ReluBackward0]
	2059610939808 -> 2059610945808
	2059610939808 [label=CudnnBatchNormBackward0]
	2059610932896 -> 2059610939808
	2059610932896 [label=ConvolutionBackward0]
	2059610931648 -> 2059610932896
	2059613194384 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2059613194384 -> 2059610931648
	2059610931648 [label=AccumulateGrad]
	2059610938032 -> 2059610939808
	2059613728080 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	2059613728080 -> 2059610938032
	2059610938032 [label=AccumulateGrad]
	2059610945376 -> 2059610939808
	2059613728848 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	2059613728848 -> 2059610945376
	2059610945376 [label=AccumulateGrad]
	2059610945472 -> 2059610945280
	2059613728752 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2059613728752 -> 2059610945472
	2059610945472 [label=AccumulateGrad]
	2059610945232 -> 2059610945184
	2059613729232 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2059613729232 -> 2059610945232
	2059610945232 [label=AccumulateGrad]
	2059610945088 -> 2059610945184
	2059613729520 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2059613729520 -> 2059610945088
	2059610945088 [label=AccumulateGrad]
	2059610944992 -> 2059610944848
	2059613729904 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2059613729904 -> 2059610944992
	2059610944992 [label=AccumulateGrad]
	2059610944800 -> 2059610944704
	2059613730000 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2059613730000 -> 2059610944800
	2059610944800 [label=AccumulateGrad]
	2059610944752 -> 2059610944704
	2059613730096 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2059613730096 -> 2059610944752
	2059610944752 [label=AccumulateGrad]
	2059610944656 -> 2059610944608
	2059610944512 -> 2059610944320
	2059613730480 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2059613730480 -> 2059610944512
	2059610944512 [label=AccumulateGrad]
	2059610944272 -> 2059610944224
	2059613730576 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2059613730576 -> 2059610944272
	2059610944272 [label=AccumulateGrad]
	2059610943504 -> 2059610944224
	2059613730672 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2059613730672 -> 2059610943504
	2059610943504 [label=AccumulateGrad]
	2059610943600 -> 2059610943744
	2059613731056 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2059613731056 -> 2059610943600
	2059610943600 [label=AccumulateGrad]
	2059610943792 -> 2059610943888
	2059613731152 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2059613731152 -> 2059610943792
	2059610943792 [label=AccumulateGrad]
	2059610943840 -> 2059610943888
	2059613731248 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2059613731248 -> 2059610943840
	2059610943840 [label=AccumulateGrad]
	2059610943936 -> 2059610943984
	2059610944176 -> 2059610940816
	2059613732208 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2059613732208 -> 2059610944176
	2059610944176 [label=AccumulateGrad]
	2059610941056 -> 2059610938944
	2059613732304 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2059613732304 -> 2059610941056
	2059610941056 [label=AccumulateGrad]
	2059610941632 -> 2059610938944
	2059613732400 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2059613732400 -> 2059610941632
	2059610941632 [label=AccumulateGrad]
	2059610939472 -> 2059609979200
	2059613732784 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2059613732784 -> 2059610939472
	2059610939472 [label=AccumulateGrad]
	2059610945712 -> 2059609974736
	2059613880400 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2059613880400 -> 2059610945712
	2059610945712 [label=AccumulateGrad]
	2059610945760 -> 2059609974736
	2059613880496 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2059613880496 -> 2059610945760
	2059610945760 [label=AccumulateGrad]
	2059609972960 -> 2059609975408
	2059609972960 [label=CudnnBatchNormBackward0]
	2059610936736 -> 2059609972960
	2059610936736 [label=ConvolutionBackward0]
	2059610944128 -> 2059610936736
	2059610944080 -> 2059610936736
	2059613731632 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2059613731632 -> 2059610944080
	2059610944080 [label=AccumulateGrad]
	2059610939376 -> 2059609972960
	2059613731728 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2059613731728 -> 2059610939376
	2059610939376 [label=AccumulateGrad]
	2059610938656 -> 2059609972960
	2059613731824 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2059613731824 -> 2059610938656
	2059610938656 [label=AccumulateGrad]
	2059609976320 -> 2059609967920
	2059613880880 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2059613880880 -> 2059609976320
	2059609976320 [label=AccumulateGrad]
	2059609979296 -> 2059609979728
	2059613880976 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2059613880976 -> 2059609979296
	2059609979296 [label=AccumulateGrad]
	2059609972672 -> 2059609979728
	2059613881072 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2059613881072 -> 2059609972672
	2059609972672 [label=AccumulateGrad]
	2059609971952 -> 2059609979344
	2059613881456 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2059613881456 -> 2059609971952
	2059609971952 [label=AccumulateGrad]
	2059609968496 -> 2059609977328
	2059613881552 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2059613881552 -> 2059609968496
	2059609968496 [label=AccumulateGrad]
	2059609976608 -> 2059609977328
	2059613881648 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2059613881648 -> 2059609976608
	2059609976608 [label=AccumulateGrad]
	2059609975216 -> 2059609979536
	2059609971808 -> 2059609974640
	2059613882608 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2059613882608 -> 2059609971808
	2059609971808 [label=AccumulateGrad]
	2059609980832 -> 2059609980112
	2059613882704 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2059613882704 -> 2059609980832
	2059609980832 [label=AccumulateGrad]
	2059609975792 -> 2059609980112
	2059613882800 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2059613882800 -> 2059609975792
	2059609975792 [label=AccumulateGrad]
	2059609973872 -> 2059609976944
	2059613883184 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2059613883184 -> 2059609973872
	2059609973872 [label=AccumulateGrad]
	2059609976176 -> 2059609971712
	2059613883280 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2059613883280 -> 2059609976176
	2059609976176 [label=AccumulateGrad]
	2059609978000 -> 2059609971712
	2059613883376 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2059613883376 -> 2059609978000
	2059609978000 [label=AccumulateGrad]
	2059609976800 -> 2059609973056
	2059609976800 [label=CudnnBatchNormBackward0]
	2059609972432 -> 2059609976800
	2059609972432 [label=ConvolutionBackward0]
	2059609973968 -> 2059609972432
	2059609977088 -> 2059609972432
	2059613882032 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2059613882032 -> 2059609977088
	2059609977088 [label=AccumulateGrad]
	2059609974112 -> 2059609976800
	2059613882128 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2059613882128 -> 2059609974112
	2059609974112 [label=AccumulateGrad]
	2059609974304 -> 2059609976800
	2059613882224 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2059613882224 -> 2059609974304
	2059609974304 [label=AccumulateGrad]
	2059609969264 -> 2059609968064
	2059613883760 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2059613883760 -> 2059609969264
	2059609969264 [label=AccumulateGrad]
	2059609976128 -> 2059609976080
	2059613883856 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2059613883856 -> 2059609976128
	2059609976128 [label=AccumulateGrad]
	2059609976896 -> 2059609976080
	2059613883952 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2059613883952 -> 2059609976896
	2059609976896 [label=AccumulateGrad]
	2059609977904 -> 2059609974880
	2059613884336 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2059613884336 -> 2059609977904
	2059609977904 [label=AccumulateGrad]
	2059609979392 -> 2059609976848
	2059613884432 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2059613884432 -> 2059609979392
	2059609979392 [label=AccumulateGrad]
	2059609971088 -> 2059609976848
	2059613884528 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2059613884528 -> 2059609971088
	2059609971088 [label=AccumulateGrad]
	2059609976368 -> 2059609979872
	2059609978528 -> 2059609980688
	2059613885488 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2059613885488 -> 2059609978528
	2059609978528 [label=AccumulateGrad]
	2059609971472 -> 2059609971520
	2059613885584 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2059613885584 -> 2059609971472
	2059609971472 [label=AccumulateGrad]
	2059609975264 -> 2059609971520
	2059613885680 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2059613885680 -> 2059609975264
	2059609975264 [label=AccumulateGrad]
	2059609976560 -> 2059609978864
	2059613886064 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2059613886064 -> 2059609976560
	2059609976560 [label=AccumulateGrad]
	2059609970128 -> 2059609980592
	2059613886160 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2059613886160 -> 2059609970128
	2059609970128 [label=AccumulateGrad]
	2059609975936 -> 2059609980592
	2059613886256 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2059613886256 -> 2059609975936
	2059609975936 [label=AccumulateGrad]
	2059609980448 -> 2059609972480
	2059609980448 [label=CudnnBatchNormBackward0]
	2059609970800 -> 2059609980448
	2059609970800 [label=ConvolutionBackward0]
	2059609972048 -> 2059609970800
	2059609980400 -> 2059609970800
	2059613884912 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2059613884912 -> 2059609980400
	2059609980400 [label=AccumulateGrad]
	2059609977472 -> 2059609980448
	2059613885008 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2059613885008 -> 2059609977472
	2059609977472 [label=AccumulateGrad]
	2059609975744 -> 2059609980448
	2059613885104 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2059613885104 -> 2059609975744
	2059609975744 [label=AccumulateGrad]
	2059609968016 -> 2059609980352
	2059613886640 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2059613886640 -> 2059609968016
	2059609968016 [label=AccumulateGrad]
	2059609978960 -> 2059609979152
	2059613886736 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2059613886736 -> 2059609978960
	2059609978960 [label=AccumulateGrad]
	2059609966768 -> 2059609979152
	2059613886832 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2059613886832 -> 2059609966768
	2059609966768 [label=AccumulateGrad]
	2059609967056 -> 2059609978480
	2059613887216 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2059613887216 -> 2059609967056
	2059609967056 [label=AccumulateGrad]
	2059609966864 -> 2059609967104
	2059613887312 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2059613887312 -> 2059609966864
	2059609966864 [label=AccumulateGrad]
	2059609968688 -> 2059609967104
	2059613887408 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2059613887408 -> 2059609968688
	2059609968688 [label=AccumulateGrad]
	2059609967824 -> 2059609970272
	2059609967872 -> 2059609971136
	2059609967872 [label=TBackward0]
	2059609968352 -> 2059609967872
	2059613887984 [label="model.fc.weight
 (19, 512)" fillcolor=lightblue]
	2059613887984 -> 2059609968352
	2059609968352 [label=AccumulateGrad]
	2059609973824 -> 2059614077872
}
