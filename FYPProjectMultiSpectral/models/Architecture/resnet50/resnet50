digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2184594994960 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2184588653808 [label=AddmmBackward0]
	2184588653472 -> 2184588653808
	2184585640464 [label="fc.bias
 (19)" fillcolor=lightblue]
	2184585640464 -> 2184588653472
	2184588653472 [label=AccumulateGrad]
	2184588652848 -> 2184588653808
	2184588652848 [label=ViewBackward0]
	2184588652368 -> 2184588652848
	2184588652368 [label=MeanBackward1]
	2184588652032 -> 2184588652368
	2184588652032 [label=ReluBackward0]
	2184588651552 -> 2184588652032
	2184588651552 [label=AddBackward0]
	2184588651072 -> 2184588651552
	2184588651072 [label=CudnnBatchNormBackward0]
	2184588649968 -> 2184588651072
	2184588649968 [label=ConvolutionBackward0]
	2184588649008 -> 2184588649968
	2184588649008 [label=ReluBackward0]
	2184588648672 -> 2184588649008
	2184588648672 [label=CudnnBatchNormBackward0]
	2184588648192 -> 2184588648672
	2184588648192 [label=ConvolutionBackward0]
	2184588641376 -> 2184588648192
	2184588641376 [label=ReluBackward0]
	2184588656592 -> 2184588641376
	2184588656592 [label=CudnnBatchNormBackward0]
	2184588656496 -> 2184588656592
	2184588656496 [label=ConvolutionBackward0]
	2184588650928 -> 2184588656496
	2184588650928 [label=ReluBackward0]
	2184588655920 -> 2184588650928
	2184588655920 [label=AddBackward0]
	2184588656160 -> 2184588655920
	2184588656160 [label=CudnnBatchNormBackward0]
	2184588655824 -> 2184588656160
	2184588655824 [label=ConvolutionBackward0]
	2184588655536 -> 2184588655824
	2184588655536 [label=ReluBackward0]
	2184588655584 -> 2184588655536
	2184588655584 [label=CudnnBatchNormBackward0]
	2184588655296 -> 2184588655584
	2184588655296 [label=ConvolutionBackward0]
	2184588655200 -> 2184588655296
	2184588655200 [label=ReluBackward0]
	2184588654864 -> 2184588655200
	2184588654864 [label=CudnnBatchNormBackward0]
	2184588654672 -> 2184588654864
	2184588654672 [label=ConvolutionBackward0]
	2184588656016 -> 2184588654672
	2184588656016 [label=ReluBackward0]
	2184588654384 -> 2184588656016
	2184588654384 [label=AddBackward0]
	2184588654192 -> 2184588654384
	2184588654192 [label=CudnnBatchNormBackward0]
	2184588654240 -> 2184588654192
	2184588654240 [label=ConvolutionBackward0]
	2184588653856 -> 2184588654240
	2184588653856 [label=ReluBackward0]
	2184588653616 -> 2184588653856
	2184588653616 [label=CudnnBatchNormBackward0]
	2184588653568 -> 2184588653616
	2184588653568 [label=ConvolutionBackward0]
	2184588653232 -> 2184588653568
	2184588653232 [label=ReluBackward0]
	2184588653280 -> 2184588653232
	2184588653280 [label=CudnnBatchNormBackward0]
	2184588653184 -> 2184588653280
	2184588653184 [label=ConvolutionBackward0]
	2184588652560 -> 2184588653184
	2184588652560 [label=ReluBackward0]
	2184588652608 -> 2184588652560
	2184588652608 [label=AddBackward0]
	2184588652464 -> 2184588652608
	2184588652464 [label=CudnnBatchNormBackward0]
	2184588652080 -> 2184588652464
	2184588652080 [label=ConvolutionBackward0]
	2184588652224 -> 2184588652080
	2184588652224 [label=ReluBackward0]
	2184588651792 -> 2184588652224
	2184588651792 [label=CudnnBatchNormBackward0]
	2184588651696 -> 2184588651792
	2184588651696 [label=ConvolutionBackward0]
	2184588651504 -> 2184588651696
	2184588651504 [label=ReluBackward0]
	2184588651120 -> 2184588651504
	2184588651120 [label=CudnnBatchNormBackward0]
	2184588651360 -> 2184588651120
	2184588651360 [label=ConvolutionBackward0]
	2184588652704 -> 2184588651360
	2184588652704 [label=ReluBackward0]
	2184588650640 -> 2184588652704
	2184588650640 [label=AddBackward0]
	2184588650880 -> 2184588650640
	2184588650880 [label=CudnnBatchNormBackward0]
	2184588650544 -> 2184588650880
	2184588650544 [label=ConvolutionBackward0]
	2184588650256 -> 2184588650544
	2184588650256 [label=ReluBackward0]
	2184588650304 -> 2184588650256
	2184588650304 [label=CudnnBatchNormBackward0]
	2184588650016 -> 2184588650304
	2184588650016 [label=ConvolutionBackward0]
	2184588649920 -> 2184588650016
	2184588649920 [label=ReluBackward0]
	2184588649584 -> 2184588649920
	2184588649584 [label=CudnnBatchNormBackward0]
	2184588649392 -> 2184588649584
	2184588649392 [label=ConvolutionBackward0]
	2184588650736 -> 2184588649392
	2184588650736 [label=ReluBackward0]
	2184588649104 -> 2184588650736
	2184588649104 [label=AddBackward0]
	2184588648912 -> 2184588649104
	2184588648912 [label=CudnnBatchNormBackward0]
	2184588648960 -> 2184588648912
	2184588648960 [label=ConvolutionBackward0]
	2184588648576 -> 2184588648960
	2184588648576 [label=ReluBackward0]
	2184588648336 -> 2184588648576
	2184588648336 [label=CudnnBatchNormBackward0]
	2184588648288 -> 2184588648336
	2184588648288 [label=ConvolutionBackward0]
	2184588647952 -> 2184588648288
	2184588647952 [label=ReluBackward0]
	2184588648000 -> 2184588647952
	2184588648000 [label=CudnnBatchNormBackward0]
	2184588647904 -> 2184588648000
	2184588647904 [label=ConvolutionBackward0]
	2184588649056 -> 2184588647904
	2184588649056 [label=ReluBackward0]
	2184588646080 -> 2184588649056
	2184588646080 [label=AddBackward0]
	2184588647712 -> 2184588646080
	2184588647712 [label=CudnnBatchNormBackward0]
	2184588646848 -> 2184588647712
	2184588646848 [label=ConvolutionBackward0]
	2184588642480 -> 2184588646848
	2184588642480 [label=ReluBackward0]
	2184588645360 -> 2184588642480
	2184588645360 [label=CudnnBatchNormBackward0]
	2184588647568 -> 2184588645360
	2184588647568 [label=ConvolutionBackward0]
	2184588644544 -> 2184588647568
	2184588644544 [label=ReluBackward0]
	2184588641616 -> 2184588644544
	2184588641616 [label=CudnnBatchNormBackward0]
	2184588643344 -> 2184588641616
	2184588643344 [label=ConvolutionBackward0]
	2184588645264 -> 2184588643344
	2184588645264 [label=ReluBackward0]
	2184588644736 -> 2184588645264
	2184588644736 [label=AddBackward0]
	2184588642192 -> 2184588644736
	2184588642192 [label=CudnnBatchNormBackward0]
	2184588647136 -> 2184588642192
	2184588647136 [label=ConvolutionBackward0]
	2184588647184 -> 2184588647136
	2184588647184 [label=ReluBackward0]
	2184588645456 -> 2184588647184
	2184588645456 [label=CudnnBatchNormBackward0]
	2184588647472 -> 2184588645456
	2184588647472 [label=ConvolutionBackward0]
	2184588643584 -> 2184588647472
	2184588643584 [label=ReluBackward0]
	2184588647328 -> 2184588643584
	2184588647328 [label=CudnnBatchNormBackward0]
	2184588647280 -> 2184588647328
	2184588647280 [label=ConvolutionBackward0]
	2184588647232 -> 2184588647280
	2184588647232 [label=ReluBackward0]
	2184588646944 -> 2184588647232
	2184588646944 [label=AddBackward0]
	2184588641760 -> 2184588646944
	2184588641760 [label=CudnnBatchNormBackward0]
	2184588643872 -> 2184588641760
	2184588643872 [label=ConvolutionBackward0]
	2184588640944 -> 2184588643872
	2184588640944 [label=ReluBackward0]
	2184588642240 -> 2184588640944
	2184588642240 [label=CudnnBatchNormBackward0]
	2184588643632 -> 2184588642240
	2184588643632 [label=ConvolutionBackward0]
	2184588644640 -> 2184588643632
	2184588644640 [label=ReluBackward0]
	2184588641040 -> 2184588644640
	2184588641040 [label=CudnnBatchNormBackward0]
	2184588644064 -> 2184588641040
	2184588644064 [label=ConvolutionBackward0]
	2184588644928 -> 2184588644064
	2184588644928 [label=ReluBackward0]
	2184588646368 -> 2184588644928
	2184588646368 [label=AddBackward0]
	2184588689952 -> 2184588646368
	2184588689952 [label=CudnnBatchNormBackward0]
	2184588690816 -> 2184588689952
	2184588690816 [label=ConvolutionBackward0]
	2184588701472 -> 2184588690816
	2184588701472 [label=ReluBackward0]
	2184588703104 -> 2184588701472
	2184588703104 [label=CudnnBatchNormBackward0]
	2184588700464 -> 2184588703104
	2184588700464 [label=ConvolutionBackward0]
	2184588704112 -> 2184588700464
	2184588704112 [label=ReluBackward0]
	2184588703680 -> 2184588704112
	2184588703680 [label=CudnnBatchNormBackward0]
	2184588698784 -> 2184588703680
	2184588698784 [label=ConvolutionBackward0]
	2184588705408 -> 2184588698784
	2184588705408 [label=ReluBackward0]
	2184588700944 -> 2184588705408
	2184588700944 [label=AddBackward0]
	2184595217216 -> 2184588700944
	2184595217216 [label=CudnnBatchNormBackward0]
	2184595216112 -> 2184595217216
	2184595216112 [label=ConvolutionBackward0]
	2184595215152 -> 2184595216112
	2184595215152 [label=ReluBackward0]
	2184595214816 -> 2184595215152
	2184595214816 [label=CudnnBatchNormBackward0]
	2184595214336 -> 2184595214816
	2184595214336 [label=ConvolutionBackward0]
	2184595213376 -> 2184595214336
	2184595213376 [label=ReluBackward0]
	2184595212272 -> 2184595213376
	2184595212272 [label=CudnnBatchNormBackward0]
	2184595211792 -> 2184595212272
	2184595211792 [label=ConvolutionBackward0]
	2184595217072 -> 2184595211792
	2184595217072 [label=ReluBackward0]
	2184595210352 -> 2184595217072
	2184595210352 [label=AddBackward0]
	2184595217648 -> 2184595210352
	2184595217648 [label=CudnnBatchNormBackward0]
	2184595217264 -> 2184595217648
	2184595217264 [label=ConvolutionBackward0]
	2184595217408 -> 2184595217264
	2184595217408 [label=ReluBackward0]
	2184595216976 -> 2184595217408
	2184595216976 [label=CudnnBatchNormBackward0]
	2184595216880 -> 2184595216976
	2184595216880 [label=ConvolutionBackward0]
	2184595216688 -> 2184595216880
	2184595216688 [label=ReluBackward0]
	2184595216304 -> 2184595216688
	2184595216304 [label=CudnnBatchNormBackward0]
	2184595216544 -> 2184595216304
	2184595216544 [label=ConvolutionBackward0]
	2184595210496 -> 2184595216544
	2184595210496 [label=ReluBackward0]
	2184595215824 -> 2184595210496
	2184595215824 [label=AddBackward0]
	2184595216064 -> 2184595215824
	2184595216064 [label=CudnnBatchNormBackward0]
	2184595215728 -> 2184595216064
	2184595215728 [label=ConvolutionBackward0]
	2184595215440 -> 2184595215728
	2184595215440 [label=ReluBackward0]
	2184595215488 -> 2184595215440
	2184595215488 [label=CudnnBatchNormBackward0]
	2184595215200 -> 2184595215488
	2184595215200 [label=ConvolutionBackward0]
	2184595215104 -> 2184595215200
	2184595215104 [label=ReluBackward0]
	2184595214768 -> 2184595215104
	2184595214768 [label=CudnnBatchNormBackward0]
	2184595214576 -> 2184595214768
	2184595214576 [label=ConvolutionBackward0]
	2184595214432 -> 2184595214576
	2184595214432 [label=ReluBackward0]
	2184595214240 -> 2184595214432
	2184595214240 [label=AddBackward0]
	2184595213904 -> 2184595214240
	2184595213904 [label=CudnnBatchNormBackward0]
	2184595213952 -> 2184595213904
	2184595213952 [label=ConvolutionBackward0]
	2184595213616 -> 2184595213952
	2184595213616 [label=ReluBackward0]
	2184595213664 -> 2184595213616
	2184595213664 [label=CudnnBatchNormBackward0]
	2184595213568 -> 2184595213664
	2184595213568 [label=ConvolutionBackward0]
	2184595212944 -> 2184595213568
	2184595212944 [label=ReluBackward0]
	2184595212992 -> 2184595212944
	2184595212992 [label=CudnnBatchNormBackward0]
	2184595212848 -> 2184595212992
	2184595212848 [label=ConvolutionBackward0]
	2184595214096 -> 2184595212848
	2184595214096 [label=ReluBackward0]
	2184595212512 -> 2184595214096
	2184595212512 [label=AddBackward0]
	2184595212368 -> 2184595212512
	2184595212368 [label=CudnnBatchNormBackward0]
	2184595211984 -> 2184595212368
	2184595211984 [label=ConvolutionBackward0]
	2184595212128 -> 2184595211984
	2184595212128 [label=ReluBackward0]
	2184595211696 -> 2184595212128
	2184595211696 [label=CudnnBatchNormBackward0]
	2184595211600 -> 2184595211696
	2184595211600 [label=ConvolutionBackward0]
	2184595211408 -> 2184595211600
	2184595211408 [label=ReluBackward0]
	2184595211024 -> 2184595211408
	2184595211024 [label=CudnnBatchNormBackward0]
	2184595211264 -> 2184595211024
	2184595211264 [label=ConvolutionBackward0]
	2184595212608 -> 2184595211264
	2184595212608 [label=ReluBackward0]
	2184595210544 -> 2184595212608
	2184595210544 [label=AddBackward0]
	2184595210784 -> 2184595210544
	2184595210784 [label=CudnnBatchNormBackward0]
	2184595210448 -> 2184595210784
	2184595210448 [label=ConvolutionBackward0]
	2184595218128 -> 2184595210448
	2184595218128 [label=ReluBackward0]
	2184595218464 -> 2184595218128
	2184595218464 [label=CudnnBatchNormBackward0]
	2184595218560 -> 2184595218464
	2184595218560 [label=ConvolutionBackward0]
	2184595218752 -> 2184595218560
	2184595218752 [label=ReluBackward0]
	2184595218896 -> 2184595218752
	2184595218896 [label=CudnnBatchNormBackward0]
	2184595218992 -> 2184595218896
	2184595218992 [label=ConvolutionBackward0]
	2184595219184 -> 2184595218992
	2184595219184 [label=MaxPool2DWithIndicesBackward0]
	2184595219328 -> 2184595219184
	2184595219328 [label=ReluBackward0]
	2184595219424 -> 2184595219328
	2184595219424 [label=CudnnBatchNormBackward0]
	2184595219520 -> 2184595219424
	2184595219520 [label=ConvolutionBackward0]
	2184595219712 -> 2184595219520
	2184585640272 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2184585640272 -> 2184595219712
	2184595219712 [label=AccumulateGrad]
	2184595219472 -> 2184595219424
	2184610794576 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2184610794576 -> 2184595219472
	2184595219472 [label=AccumulateGrad]
	2184595219232 -> 2184595219424
	2184610794288 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2184610794288 -> 2184595219232
	2184595219232 [label=AccumulateGrad]
	2184595219136 -> 2184595218992
	2184610794192 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2184610794192 -> 2184595219136
	2184595219136 [label=AccumulateGrad]
	2184595218944 -> 2184595218896
	2184610794096 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2184610794096 -> 2184595218944
	2184595218944 [label=AccumulateGrad]
	2184595218800 -> 2184595218896
	2184610793328 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2184610793328 -> 2184595218800
	2184595218800 [label=AccumulateGrad]
	2184595218704 -> 2184595218560
	2184610793424 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2184610793424 -> 2184595218704
	2184595218704 [label=AccumulateGrad]
	2184595218512 -> 2184595218464
	2184610793520 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2184610793520 -> 2184595218512
	2184595218512 [label=AccumulateGrad]
	2184595218368 -> 2184595218464
	2184610793616 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2184610793616 -> 2184595218368
	2184595218368 [label=AccumulateGrad]
	2184595218272 -> 2184595210448
	2184610793904 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2184610793904 -> 2184595218272
	2184595218272 [label=AccumulateGrad]
	2184595210688 -> 2184595210784
	2184610792944 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2184610792944 -> 2184595210688
	2184595210688 [label=AccumulateGrad]
	2184595210592 -> 2184595210784
	2184610792848 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2184610792848 -> 2184595210592
	2184595210592 [label=AccumulateGrad]
	2184595210640 -> 2184595210544
	2184595210640 [label=CudnnBatchNormBackward0]
	2184595218656 -> 2184595210640
	2184595218656 [label=ConvolutionBackward0]
	2184595219184 -> 2184595218656
	2184595219040 -> 2184595218656
	2184610794768 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2184610794768 -> 2184595219040
	2184595219040 [label=AccumulateGrad]
	2184595210304 -> 2184595210640
	2184610794864 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2184610794864 -> 2184595210304
	2184595210304 [label=AccumulateGrad]
	2184595210400 -> 2184595210640
	2184610794960 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2184610794960 -> 2184595210400
	2184595210400 [label=AccumulateGrad]
	2184595210880 -> 2184595211264
	2184610792656 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2184610792656 -> 2184595210880
	2184595210880 [label=AccumulateGrad]
	2184595211120 -> 2184595211024
	2184610792176 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2184610792176 -> 2184595211120
	2184595211120 [label=AccumulateGrad]
	2184595211360 -> 2184595211024
	2184610791504 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2184610791504 -> 2184595211360
	2184595211360 [label=AccumulateGrad]
	2184595211648 -> 2184595211600
	2184610792752 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2184610792752 -> 2184595211648
	2184595211648 [label=AccumulateGrad]
	2184595211504 -> 2184595211696
	2184610792560 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2184610792560 -> 2184595211504
	2184595211504 [label=AccumulateGrad]
	2184595211888 -> 2184595211696
	2184585989968 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2184585989968 -> 2184595211888
	2184595211888 [label=AccumulateGrad]
	2184595212032 -> 2184595211984
	2184585990352 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2184585990352 -> 2184595212032
	2184595212032 [label=AccumulateGrad]
	2184595212176 -> 2184595212368
	2184585990448 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2184585990448 -> 2184595212176
	2184595212176 [label=AccumulateGrad]
	2184595212320 -> 2184595212368
	2184585990544 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2184585990544 -> 2184595212320
	2184595212320 [label=AccumulateGrad]
	2184595212608 -> 2184595212512
	2184595212560 -> 2184595212848
	2184585990928 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2184585990928 -> 2184595212560
	2184595212560 [label=AccumulateGrad]
	2184595213088 -> 2184595212992
	2184585991024 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2184585991024 -> 2184595213088
	2184595213088 [label=AccumulateGrad]
	2184595213040 -> 2184595212992
	2184585991120 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2184585991120 -> 2184595213040
	2184595213040 [label=AccumulateGrad]
	2184595213136 -> 2184595213568
	2184585991504 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2184585991504 -> 2184595213136
	2184595213136 [label=AccumulateGrad]
	2184595213472 -> 2184595213664
	2184585991600 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2184585991600 -> 2184595213472
	2184595213472 [label=AccumulateGrad]
	2184595213424 -> 2184595213664
	2184585991696 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2184585991696 -> 2184595213424
	2184595213424 [label=AccumulateGrad]
	2184595213760 -> 2184595213952
	2184585992080 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2184585992080 -> 2184595213760
	2184595213760 [label=AccumulateGrad]
	2184595214144 -> 2184595213904
	2184585992176 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2184585992176 -> 2184595214144
	2184595214144 [label=AccumulateGrad]
	2184595214000 -> 2184595213904
	2184585992272 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2184585992272 -> 2184595214000
	2184595214000 [label=AccumulateGrad]
	2184595214096 -> 2184595214240
	2184595214624 -> 2184595214576
	2184585993232 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2184585993232 -> 2184595214624
	2184595214624 [label=AccumulateGrad]
	2184595214720 -> 2184595214768
	2184585993328 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2184585993328 -> 2184595214720
	2184595214720 [label=AccumulateGrad]
	2184595214912 -> 2184595214768
	2184585993424 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2184585993424 -> 2184595214912
	2184595214912 [label=AccumulateGrad]
	2184595214960 -> 2184595215200
	2184585993808 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2184585993808 -> 2184595214960
	2184595214960 [label=AccumulateGrad]
	2184595215248 -> 2184595215488
	2184585993904 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2184585993904 -> 2184595215248
	2184595215248 [label=AccumulateGrad]
	2184595215584 -> 2184595215488
	2184585994000 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2184585994000 -> 2184595215584
	2184595215584 [label=AccumulateGrad]
	2184595215344 -> 2184595215728
	2184585994384 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2184585994384 -> 2184595215344
	2184595215344 [label=AccumulateGrad]
	2184595215968 -> 2184595216064
	2184585994480 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2184585994480 -> 2184595215968
	2184595215968 [label=AccumulateGrad]
	2184595215872 -> 2184595216064
	2184585994576 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2184585994576 -> 2184595215872
	2184595215872 [label=AccumulateGrad]
	2184595215920 -> 2184595215824
	2184595215920 [label=CudnnBatchNormBackward0]
	2184595214864 -> 2184595215920
	2184595214864 [label=ConvolutionBackward0]
	2184595214432 -> 2184595214864
	2184595214384 -> 2184595214864
	2184585992656 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2184585992656 -> 2184595214384
	2184595214384 [label=AccumulateGrad]
	2184595215536 -> 2184595215920
	2184585992752 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2184585992752 -> 2184595215536
	2184595215536 [label=AccumulateGrad]
	2184595215680 -> 2184595215920
	2184585992848 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2184585992848 -> 2184595215680
	2184595215680 [label=AccumulateGrad]
	2184595216160 -> 2184595216544
	2184585994960 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2184585994960 -> 2184595216160
	2184595216160 [label=AccumulateGrad]
	2184595216400 -> 2184595216304
	2184585995056 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2184585995056 -> 2184595216400
	2184595216400 [label=AccumulateGrad]
	2184595216640 -> 2184595216304
	2184585995152 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2184585995152 -> 2184595216640
	2184595216640 [label=AccumulateGrad]
	2184595216928 -> 2184595216880
	2184585995536 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2184585995536 -> 2184595216928
	2184595216928 [label=AccumulateGrad]
	2184595216784 -> 2184595216976
	2184585995632 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2184585995632 -> 2184595216784
	2184595216784 [label=AccumulateGrad]
	2184595217168 -> 2184595216976
	2184585995728 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2184585995728 -> 2184595217168
	2184595217168 [label=AccumulateGrad]
	2184595217312 -> 2184595217264
	2184585996112 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2184585996112 -> 2184595217312
	2184595217312 [label=AccumulateGrad]
	2184595217456 -> 2184595217648
	2184585996208 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2184585996208 -> 2184595217456
	2184595217456 [label=AccumulateGrad]
	2184595217600 -> 2184595217648
	2184585996304 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2184585996304 -> 2184595217600
	2184595217600 [label=AccumulateGrad]
	2184595210496 -> 2184595210352
	2184595210832 -> 2184595211792
	2184585996688 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2184585996688 -> 2184595210832
	2184595210832 [label=AccumulateGrad]
	2184595212416 -> 2184595212272
	2184585996784 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2184585996784 -> 2184595212416
	2184595212416 [label=AccumulateGrad]
	2184595212752 -> 2184595212272
	2184585996880 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2184585996880 -> 2184595212752
	2184595212752 [label=AccumulateGrad]
	2184595213232 -> 2184595214336
	2184585997264 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2184585997264 -> 2184595213232
	2184595213232 [label=AccumulateGrad]
	2184595214192 -> 2184595214816
	2184585997360 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2184585997360 -> 2184595214192
	2184595214192 [label=AccumulateGrad]
	2184595215296 -> 2184595214816
	2184585997456 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2184585997456 -> 2184595215296
	2184595215296 [label=AccumulateGrad]
	2184595215776 -> 2184595216112
	2184585997840 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2184585997840 -> 2184595215776
	2184595215776 [label=AccumulateGrad]
	2184595216736 -> 2184595217216
	2184585997936 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2184585997936 -> 2184595216736
	2184595216736 [label=AccumulateGrad]
	2184595216592 -> 2184595217216
	2184585998032 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2184585998032 -> 2184595216592
	2184595216592 [label=AccumulateGrad]
	2184595217072 -> 2184588700944
	2184279572192 -> 2184588698784
	2184585998416 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2184585998416 -> 2184279572192
	2184279572192 [label=AccumulateGrad]
	2184588690912 -> 2184588703680
	2184585998512 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2184585998512 -> 2184588690912
	2184588690912 [label=AccumulateGrad]
	2184588702192 -> 2184588703680
	2184585998608 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2184585998608 -> 2184588702192
	2184588702192 [label=AccumulateGrad]
	2184588701376 -> 2184588700464
	2184585998992 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2184585998992 -> 2184588701376
	2184588701376 [label=AccumulateGrad]
	2184588691056 -> 2184588703104
	2184585999088 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2184585999088 -> 2184588691056
	2184588691056 [label=AccumulateGrad]
	2184588703968 -> 2184588703104
	2184585999184 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2184585999184 -> 2184588703968
	2184588703968 [label=AccumulateGrad]
	2184588694032 -> 2184588690816
	2184585999568 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2184585999568 -> 2184588694032
	2184588694032 [label=AccumulateGrad]
	2184588700992 -> 2184588689952
	2184585999664 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2184585999664 -> 2184588700992
	2184588700992 [label=AccumulateGrad]
	2184588691968 -> 2184588689952
	2184585999760 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2184585999760 -> 2184588691968
	2184588691968 [label=AccumulateGrad]
	2184588705408 -> 2184588646368
	2184588645120 -> 2184588644064
	2184586000720 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2184586000720 -> 2184588645120
	2184588645120 [label=AccumulateGrad]
	2184588640752 -> 2184588641040
	2184586000816 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2184586000816 -> 2184588640752
	2184588640752 [label=AccumulateGrad]
	2184588645504 -> 2184588641040
	2184586000912 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2184586000912 -> 2184588645504
	2184588645504 [label=AccumulateGrad]
	2184588645168 -> 2184588643632
	2184586001296 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184586001296 -> 2184588645168
	2184588645168 [label=AccumulateGrad]
	2184588645744 -> 2184588642240
	2184586001392 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2184586001392 -> 2184588645744
	2184588645744 [label=AccumulateGrad]
	2184588642384 -> 2184588642240
	2184586001488 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2184586001488 -> 2184588642384
	2184588642384 [label=AccumulateGrad]
	2184588643920 -> 2184588643872
	2184586001872 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2184586001872 -> 2184588643920
	2184588643920 [label=AccumulateGrad]
	2184588640320 -> 2184588641760
	2184586001968 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2184586001968 -> 2184588640320
	2184588640320 [label=AccumulateGrad]
	2184588644832 -> 2184588641760
	2184586002064 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2184586002064 -> 2184588644832
	2184588644832 [label=AccumulateGrad]
	2184588641424 -> 2184588646944
	2184588641424 [label=CudnnBatchNormBackward0]
	2184588647040 -> 2184588641424
	2184588647040 [label=ConvolutionBackward0]
	2184588644928 -> 2184588647040
	2184588646224 -> 2184588647040
	2184586000144 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2184586000144 -> 2184588646224
	2184588646224 [label=AccumulateGrad]
	2184588641280 -> 2184588641424
	2184586000240 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2184586000240 -> 2184588641280
	2184588641280 [label=AccumulateGrad]
	2184588642960 -> 2184588641424
	2184586000336 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2184586000336 -> 2184588642960
	2184588642960 [label=AccumulateGrad]
	2184588646608 -> 2184588647280
	2184585625680 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2184585625680 -> 2184588646608
	2184588646608 [label=AccumulateGrad]
	2184588645552 -> 2184588647328
	2184585625776 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2184585625776 -> 2184588645552
	2184588645552 [label=AccumulateGrad]
	2184588646992 -> 2184588647328
	2184585625872 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2184585625872 -> 2184588646992
	2184588646992 [label=AccumulateGrad]
	2184588642768 -> 2184588647472
	2184585626256 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184585626256 -> 2184588642768
	2184588642768 [label=AccumulateGrad]
	2184588643104 -> 2184588645456
	2184585626352 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2184585626352 -> 2184588643104
	2184588643104 [label=AccumulateGrad]
	2184588640656 -> 2184588645456
	2184585626448 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2184585626448 -> 2184588640656
	2184588640656 [label=AccumulateGrad]
	2184588644208 -> 2184588647136
	2184585626832 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2184585626832 -> 2184588644208
	2184588644208 [label=AccumulateGrad]
	2184588644256 -> 2184588642192
	2184585626928 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2184585626928 -> 2184588644256
	2184588644256 [label=AccumulateGrad]
	2184588640464 -> 2184588642192
	2184585627024 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2184585627024 -> 2184588640464
	2184588640464 [label=AccumulateGrad]
	2184588647232 -> 2184588644736
	2184588644112 -> 2184588643344
	2184585627408 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2184585627408 -> 2184588644112
	2184588644112 [label=AccumulateGrad]
	2184588646752 -> 2184588641616
	2184585627504 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2184585627504 -> 2184588646752
	2184588646752 [label=AccumulateGrad]
	2184588641808 -> 2184588641616
	2184585627600 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2184585627600 -> 2184588641808
	2184588641808 [label=AccumulateGrad]
	2184588642912 -> 2184588647568
	2184585627984 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184585627984 -> 2184588642912
	2184588642912 [label=AccumulateGrad]
	2184588645792 -> 2184588645360
	2184585628080 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2184585628080 -> 2184588645792
	2184588645792 [label=AccumulateGrad]
	2184588643008 -> 2184588645360
	2184585628176 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2184585628176 -> 2184588643008
	2184588643008 [label=AccumulateGrad]
	2184588642624 -> 2184588646848
	2184585628560 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2184585628560 -> 2184588642624
	2184588642624 [label=AccumulateGrad]
	2184588646656 -> 2184588647712
	2184585628656 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2184585628656 -> 2184588646656
	2184588646656 [label=AccumulateGrad]
	2184588644448 -> 2184588647712
	2184585628752 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2184585628752 -> 2184588644448
	2184588644448 [label=AccumulateGrad]
	2184588645264 -> 2184588646080
	2184588644688 -> 2184588647904
	2184585629136 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2184585629136 -> 2184588644688
	2184588644688 [label=AccumulateGrad]
	2184588647808 -> 2184588648000
	2184585629232 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2184585629232 -> 2184588647808
	2184588647808 [label=AccumulateGrad]
	2184588645648 -> 2184588648000
	2184585629328 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2184585629328 -> 2184588645648
	2184588645648 [label=AccumulateGrad]
	2184588648096 -> 2184588648288
	2184585629712 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184585629712 -> 2184588648096
	2184588648096 [label=AccumulateGrad]
	2184588648480 -> 2184588648336
	2184585629808 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2184585629808 -> 2184588648480
	2184588648480 [label=AccumulateGrad]
	2184588648432 -> 2184588648336
	2184585629904 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2184585629904 -> 2184588648432
	2184588648432 [label=AccumulateGrad]
	2184588648624 -> 2184588648960
	2184585630288 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2184585630288 -> 2184588648624
	2184588648624 [label=AccumulateGrad]
	2184588648816 -> 2184588648912
	2184585630384 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2184585630384 -> 2184588648816
	2184588648816 [label=AccumulateGrad]
	2184588648720 -> 2184588648912
	2184585630480 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2184585630480 -> 2184588648720
	2184588648720 [label=AccumulateGrad]
	2184588649056 -> 2184588649104
	2184588649248 -> 2184588649392
	2184585630864 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2184585630864 -> 2184588649248
	2184588649248 [label=AccumulateGrad]
	2184588649536 -> 2184588649584
	2184585630960 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2184585630960 -> 2184588649536
	2184588649536 [label=AccumulateGrad]
	2184588649728 -> 2184588649584
	2184585631056 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2184585631056 -> 2184588649728
	2184588649728 [label=AccumulateGrad]
	2184588649776 -> 2184588650016
	2184585631440 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184585631440 -> 2184588649776
	2184588649776 [label=AccumulateGrad]
	2184588650064 -> 2184588650304
	2184585631536 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2184585631536 -> 2184588650064
	2184588650064 [label=AccumulateGrad]
	2184588650400 -> 2184588650304
	2184585631632 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2184585631632 -> 2184588650400
	2184588650400 [label=AccumulateGrad]
	2184588650160 -> 2184588650544
	2184585632016 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2184585632016 -> 2184588650160
	2184588650160 [label=AccumulateGrad]
	2184588650784 -> 2184588650880
	2184585632112 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2184585632112 -> 2184588650784
	2184588650784 [label=AccumulateGrad]
	2184588650688 -> 2184588650880
	2184585632208 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2184585632208 -> 2184588650688
	2184588650688 [label=AccumulateGrad]
	2184588650736 -> 2184588650640
	2184588650976 -> 2184588651360
	2184585632592 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2184585632592 -> 2184588650976
	2184588650976 [label=AccumulateGrad]
	2184588651216 -> 2184588651120
	2184585632688 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2184585632688 -> 2184588651216
	2184588651216 [label=AccumulateGrad]
	2184588651456 -> 2184588651120
	2184585632784 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2184585632784 -> 2184588651456
	2184588651456 [label=AccumulateGrad]
	2184588651744 -> 2184588651696
	2184585633168 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184585633168 -> 2184588651744
	2184588651744 [label=AccumulateGrad]
	2184588651600 -> 2184588651792
	2184585633264 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2184585633264 -> 2184588651600
	2184588651600 [label=AccumulateGrad]
	2184588651984 -> 2184588651792
	2184585633360 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2184585633360 -> 2184588651984
	2184588651984 [label=AccumulateGrad]
	2184588652128 -> 2184588652080
	2184585633744 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2184585633744 -> 2184588652128
	2184588652128 [label=AccumulateGrad]
	2184588652272 -> 2184588652464
	2184585633840 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2184585633840 -> 2184588652272
	2184588652272 [label=AccumulateGrad]
	2184588652416 -> 2184588652464
	2184585633936 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2184585633936 -> 2184588652416
	2184588652416 [label=AccumulateGrad]
	2184588652704 -> 2184588652608
	2184588652752 -> 2184588653184
	2184585634896 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2184585634896 -> 2184588652752
	2184588652752 [label=AccumulateGrad]
	2184588653088 -> 2184588653280
	2184585634992 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2184585634992 -> 2184588653088
	2184588653088 [label=AccumulateGrad]
	2184588653040 -> 2184588653280
	2184585635088 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2184585635088 -> 2184588653040
	2184588653040 [label=AccumulateGrad]
	2184588653376 -> 2184588653568
	2184585635472 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2184585635472 -> 2184588653376
	2184588653376 [label=AccumulateGrad]
	2184588653760 -> 2184588653616
	2184585635568 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2184585635568 -> 2184588653760
	2184588653760 [label=AccumulateGrad]
	2184588653712 -> 2184588653616
	2184585635664 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2184585635664 -> 2184588653712
	2184588653712 [label=AccumulateGrad]
	2184588653904 -> 2184588654240
	2184585636048 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2184585636048 -> 2184588653904
	2184588653904 [label=AccumulateGrad]
	2184588654096 -> 2184588654192
	2184585636144 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2184585636144 -> 2184588654096
	2184588654096 [label=AccumulateGrad]
	2184588654000 -> 2184588654192
	2184585636240 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2184585636240 -> 2184588654000
	2184588654000 [label=AccumulateGrad]
	2184588654336 -> 2184588654384
	2184588654336 [label=CudnnBatchNormBackward0]
	2184588653424 -> 2184588654336
	2184588653424 [label=ConvolutionBackward0]
	2184588652560 -> 2184588653424
	2184588652944 -> 2184588653424
	2184585634320 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2184585634320 -> 2184588652944
	2184588652944 [label=AccumulateGrad]
	2184588654144 -> 2184588654336
	2184585634416 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2184585634416 -> 2184588654144
	2184588654144 [label=AccumulateGrad]
	2184588654048 -> 2184588654336
	2184585634512 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2184585634512 -> 2184588654048
	2184588654048 [label=AccumulateGrad]
	2184588654528 -> 2184588654672
	2184585636624 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2184585636624 -> 2184588654528
	2184588654528 [label=AccumulateGrad]
	2184588654816 -> 2184588654864
	2184585636720 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2184585636720 -> 2184588654816
	2184588654816 [label=AccumulateGrad]
	2184588655008 -> 2184588654864
	2184585636816 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2184585636816 -> 2184588655008
	2184588655008 [label=AccumulateGrad]
	2184588655056 -> 2184588655296
	2184585637200 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2184585637200 -> 2184588655056
	2184588655056 [label=AccumulateGrad]
	2184588655344 -> 2184588655584
	2184585637296 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2184585637296 -> 2184588655344
	2184588655344 [label=AccumulateGrad]
	2184588655680 -> 2184588655584
	2184585637392 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2184585637392 -> 2184588655680
	2184588655680 [label=AccumulateGrad]
	2184588655440 -> 2184588655824
	2184585637776 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2184585637776 -> 2184588655440
	2184588655440 [label=AccumulateGrad]
	2184588656064 -> 2184588656160
	2184585637872 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2184585637872 -> 2184588656064
	2184588656064 [label=AccumulateGrad]
	2184588655968 -> 2184588656160
	2184585637968 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2184585637968 -> 2184588655968
	2184588655968 [label=AccumulateGrad]
	2184588656016 -> 2184588655920
	2184588656256 -> 2184588656496
	2184585638352 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2184585638352 -> 2184588656256
	2184588656256 [label=AccumulateGrad]
	2184588656400 -> 2184588656592
	2184585638448 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2184585638448 -> 2184588656400
	2184588656400 [label=AccumulateGrad]
	2184588641952 -> 2184588656592
	2184585638544 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2184585638544 -> 2184588641952
	2184588641952 [label=AccumulateGrad]
	2184588645216 -> 2184588648192
	2184585638928 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2184585638928 -> 2184588645216
	2184588645216 [label=AccumulateGrad]
	2184588648048 -> 2184588648672
	2184585639024 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2184585639024 -> 2184588648048
	2184588648048 [label=AccumulateGrad]
	2184588649152 -> 2184588648672
	2184585639120 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2184585639120 -> 2184588649152
	2184588649152 [label=AccumulateGrad]
	2184588649632 -> 2184588649968
	2184585639504 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2184585639504 -> 2184588649632
	2184588649632 [label=AccumulateGrad]
	2184588650592 -> 2184588651072
	2184585639600 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2184585639600 -> 2184588650592
	2184588650592 [label=AccumulateGrad]
	2184588650448 -> 2184588651072
	2184585639696 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2184585639696 -> 2184588650448
	2184588650448 [label=AccumulateGrad]
	2184588650928 -> 2184588651552
	2184588652992 -> 2184588653808
	2184588652992 [label=TBackward0]
	2184588651408 -> 2184588652992
	2184585640368 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2184585640368 -> 2184588651408
	2184588651408 [label=AccumulateGrad]
	2184588653808 -> 2184594994960
}
