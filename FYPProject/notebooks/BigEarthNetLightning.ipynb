{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Of Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Library modules\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platformâ€™s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch and Deep Learning Libaries\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "from torch.nn import functional as F  # Functional module for defining functions and loss functions.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Data handling and batching\n",
    "import torchvision  # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Image datasets and transformations.\n",
    "import torchvision.datasets as datasets  # Specific datasets for vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "from torchvision.datasets import MNIST, EuroSAT  # Standard datasets.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations.\n",
    "from torchvision.models import ResNet18_Weights  # ResNet-18 model with pretrained weights.\n",
    "from torchsummary import summary  # Model summary.\n",
    "import torchmetrics  # Model evaluation metrics.\n",
    "from torchmetrics import MeanMetric, Accuracy  # Accuracy metrics.\n",
    "from torchmetrics.classification import (\n",
    "    MultilabelF1Score, MultilabelRecall, MultilabelPrecision, MultilabelAccuracy\n",
    ")  # Classification metrics.\n",
    "from torchviz import make_dot  # Model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for class imbalance.\n",
    "from torchcam.methods import GradCAM  # Grad-CAM for model interpretability.\n",
    "from torchcam.utils import overlay_mask  # Overlay mask for visualizations.\n",
    "import pytorch_lightning as pl  # Training management.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, Callback  # Callbacks.\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Logger for TensorBoard.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation.\n",
    "from rasterio.enums import Resampling  # Resampling for raster resizing.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation, Analysis and Visualization Libraries\n",
    "import pandas as pd  # Data analysis and manipulation.\n",
    "import numpy as np  # Array operations and computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics.\n",
    "import matplotlib.pyplot as plt  # Static and interactive plotting.\n",
    "import seaborn as sns  # High-level interface for statistical graphics.\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm  # Progress bar for loops.\n",
    "from PIL import Image  # Image handling and manipulation.\n",
    "import ast  # Parsing Python code.\n",
    "import requests  # HTTP requests.\n",
    "import zstandard as zstd  # Compression and decompression.\n",
    "from collections import Counter  # Counting hashable objects.\n",
    "import certifi  # Certificates for HTTPS.\n",
    "import ssl  # Secure connections.\n",
    "import urllib.request  # URL handling.\n",
    "import kaggle  # Kaggle API for datasets.\n",
    "from IPython.display import Image  # Display images in notebooks.\n",
    "from pathlib import Path # File system path handling.\n",
    "from typing import Dict, List, Tuple  # Type hints.\n",
    "import sys  # System-specific parameters and functions.\n",
    "import time # Time access and conversions.\n",
    "import logging # Logging facility for Python.\n",
    "\n",
    "# Custom Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Seed and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda (GPU: NVIDIA GeForce RTX 3050)\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42  \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Render plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device} {'(GPU: ' + torch.cuda.get_device_name(0) + ')' if device.type == 'cuda' else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Config Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    dataset_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%'\n",
    "    combined_path: str = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\50%\\CombinedRGBImagesTIF'\n",
    "    metadata_path: str =r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\Subsets\\metadata_50_percent.csv'\n",
    "    metadata_csv = pd.read_csv(metadata_path)\n",
    "    img_size: int = 120\n",
    "    img_mean, img_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    num_classes: int = 19\n",
    "    band_channels: int = 3 #13\n",
    "    valid_pct: float = 0.1\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 10\n",
    "    model_name: str = 'resnet18'\n",
    "    num_workers: int = 2 #os.cpu_count() // 2\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arable land': 0, 'Broad-leaved forest': 1, 'Land principally occupied by agriculture, with significant areas of natural vegetation': 2, 'Pastures': 3, 'Urban fabric': 4, 'Complex cultivation patterns': 5, 'Mixed forest': 6, 'Industrial or commercial units': 7, 'Coniferous forest': 8, 'Transitional woodland, shrub': 9, 'Natural grassland and sparsely vegetated areas': 10, 'Inland waters': 11, 'Marine waters': 12, 'Inland wetlands': 13, 'Moors, heathland and sclerophyllous vegetation': 14, 'Permanent crops': 15, 'Agro-forestry areas': 16, 'Beaches, dunes, sands': 17, 'Coastal wetlands': 18}\n",
      "{0: 'Arable land', 1: 'Broad-leaved forest', 2: 'Land principally occupied by agriculture, with significant areas of natural vegetation', 3: 'Pastures', 4: 'Urban fabric', 5: 'Complex cultivation patterns', 6: 'Mixed forest', 7: 'Industrial or commercial units', 8: 'Coniferous forest', 9: 'Transitional woodland, shrub', 10: 'Natural grassland and sparsely vegetated areas', 11: 'Inland waters', 12: 'Marine waters', 13: 'Inland wetlands', 14: 'Moors, heathland and sclerophyllous vegetation', 15: 'Permanent crops', 16: 'Agro-forestry areas', 17: 'Beaches, dunes, sands', 18: 'Coastal wetlands'}\n"
     ]
    }
   ],
   "source": [
    "# Check if the labels are strings and need to be converted\n",
    "if isinstance(DatasetConfig.metadata_csv['labels'].iloc[0], str):\n",
    "    DatasetConfig.metadata_csv['labels'] = DatasetConfig.metadata_csv['labels'].apply(ast.literal_eval)\n",
    "\n",
    "# Get unique class labels\n",
    "class_labels = DatasetConfig.metadata_csv['labels'].explode().unique()\n",
    "\n",
    "# Create a dictionary mapping class labels to indices\n",
    "class_labels_dict = {label: idx for idx, label in enumerate(class_labels)}\n",
    "\n",
    "# Create a reversed dictionary mapping indices to class labels\n",
    "reversed_class_labels_dict = {idx: label for label, idx in class_labels_dict.items()}\n",
    "\n",
    "print(class_labels_dict)\n",
    "print(reversed_class_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label: list, num_classes=DatasetConfig.num_classes):\n",
    "    target = torch.zeros(num_classes)\n",
    "    for l in label:\n",
    "        if l in class_labels_dict:\n",
    "            target[class_labels_dict[l]] = 1.0\n",
    "    return target\n",
    "\n",
    "def decode_target(\n",
    "    target: list,\n",
    "    text_labels: bool = False,\n",
    "    threshold: float = 0.4,\n",
    "    cls_labels: dict = None,\n",
    "):\n",
    "    result = []\n",
    "    for i, x in enumerate(target):\n",
    "        if x >= threshold:\n",
    "            if text_labels:\n",
    "                result.append(cls_labels[i] + \"(\" + str(i) + \")\")\n",
    "            else:\n",
    "                result.append(str(i))\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = list(Path(root_dir).rglob(\"*.tif\"))\n",
    "        self.metadata = pd.read_csv(DatasetConfig.metadata_path)\n",
    "        self.patch_to_labels = dict(zip(self.metadata['patch_id'], self.metadata['labels']))\n",
    "        self.image_paths = list(Path(root_dir).rglob(\"*.tif\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = src.read([1, 2, 3])\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.get_label(image_path)  \n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def get_label(self, image_path):\n",
    "        patch_id = image_path.stem\n",
    "        labels = self.patch_to_labels.get(patch_id, None)\n",
    "        if labels is None:\n",
    "            return torch.zeros(DatasetConfig.num_classes)  # Return an all-zero tensor as a fallback\n",
    "    \n",
    "        # Convert the labels string to an actual list if needed\n",
    "        if isinstance(labels, str):\n",
    "            labels = ast.literal_eval(labels) \n",
    "    \n",
    "        encoded = encode_label(labels)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1195\n"
     ]
    }
   ],
   "source": [
    "train_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'train']\n",
    "\n",
    "train_dataset = BigEarthNetSubset(df=train_df, root_dir=DatasetConfig.combined_path, transform=ModelConfig.train_transforms)\n",
    "print(f\"Dataset length: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: <built-in method size of Tensor object at 0x0000018DCD7ECE90>, Label: tensor([0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image, label = train_dataset[5]\n",
    "print(f\"Image shape: {image.size}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BigEarthNetSubset object at 0x0000018DCDDF13D0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8782, -1.8782, -1.8782,  ..., -1.7754, -1.7583, -1.7583],\n",
       "         [-1.8782, -1.8782, -1.8782,  ..., -1.7754, -1.7583, -1.7583],\n",
       "         [-1.8782, -1.8782, -1.8782,  ..., -1.7925, -1.7754, -1.7754],\n",
       "         ...,\n",
       "         [-1.8953, -1.8953, -1.8953,  ..., -1.8610, -1.8610, -1.8610],\n",
       "         [-1.8953, -1.8953, -1.8953,  ..., -1.8610, -1.8439, -1.8439],\n",
       "         [-1.8953, -1.8953, -1.8953,  ..., -1.8610, -1.8439, -1.8439]],\n",
       "\n",
       "        [[-1.5805, -1.5805, -1.5805,  ..., -1.2829, -1.2654, -1.2654],\n",
       "         [-1.5805, -1.5805, -1.5805,  ..., -1.2829, -1.2654, -1.2654],\n",
       "         [-1.5805, -1.5805, -1.5805,  ..., -1.3004, -1.2829, -1.2829],\n",
       "         ...,\n",
       "         [-1.5455, -1.5455, -1.5455,  ..., -1.5280, -1.5280, -1.5280],\n",
       "         [-1.5455, -1.5455, -1.5455,  ..., -1.5280, -1.5105, -1.5105],\n",
       "         [-1.5455, -1.5455, -1.5455,  ..., -1.5280, -1.5105, -1.5105]],\n",
       "\n",
       "        [[-1.6302, -1.6302, -1.6476,  ..., -1.5256, -1.5081, -1.5081],\n",
       "         [-1.6302, -1.6302, -1.6476,  ..., -1.5256, -1.5081, -1.5081],\n",
       "         [-1.6302, -1.6302, -1.6476,  ..., -1.5430, -1.5256, -1.5256],\n",
       "         ...,\n",
       "         [-1.6302, -1.6302, -1.6476,  ..., -1.6302, -1.6127, -1.6127],\n",
       "         [-1.6302, -1.6302, -1.6476,  ..., -1.6302, -1.6127, -1.6127],\n",
       "         [-1.6302, -1.6302, -1.6476,  ..., -1.6302, -1.6127, -1.6127]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Data Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubsetDataModule(pl.LightningDataModule):    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_transform = ModelConfig.train_transforms\n",
    "        self.val_transform = ModelConfig.val_transforms\n",
    "        self.test_transform = ModelConfig.test_transforms\n",
    "\n",
    "        train_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'train']\n",
    "        val_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'validation']\n",
    "        test_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'test']\n",
    "\n",
    "        self.train_dataset = BigEarthNetSubset(df=train_df, root_dir=DatasetConfig.combined_path, transform=self.train_transform)\n",
    "        self.val_dataset = BigEarthNetSubset(df=val_df, root_dir=DatasetConfig.combined_path, transform=self.val_transform)\n",
    "        self.test_dataset = BigEarthNetSubset(df=test_df, root_dir=DatasetConfig.combined_path, transform=self.test_transform)\n",
    "        print(f\"Number of samples in train set: {len(self.train_dataset)}, val set: {len(self.val_dataset)}, test set: {len(self.test_dataset)}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=ModelConfig.batch_size, num_workers=2, pin_memory=True, shuffle=True, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=ModelConfig.batch_size, num_workers=2, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=ModelConfig.batch_size, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 1195, val set: 606, test set: 551\n"
     ]
    }
   ],
   "source": [
    "data_module = BigEarthNetSubsetDataModule()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 606\n"
     ]
    }
   ],
   "source": [
    "valid_df = DatasetConfig.metadata_csv[DatasetConfig.metadata_csv['split'] == 'validation']\n",
    "\n",
    "valid_dataset = BigEarthNetSubset(df=valid_df, root_dir=DatasetConfig.combined_path, transform=ModelConfig.val_transforms)\n",
    "print(f\"Dataset length: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensors, *, mean, std):\n",
    "    for c in range(DatasetConfig.band_channels):\n",
    "        tensors[:, c, :, :].mul_(std[c]).add_(mean[c])\n",
    "\n",
    "    return torch.clamp(tensors, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first batch\n",
    "validation_loader = data_module.val_dataloader()\n",
    "batch = next(iter(validation_loader))\n",
    "\n",
    "images = denormalize(batch[0], mean=DatasetConfig.img_mean, std=DatasetConfig.img_std)\n",
    "\n",
    "plt.figure(figsize=(32, 32))\n",
    "\n",
    "grid_img = make_grid(images, nrow=8, padding=5, pad_value=1.0)\n",
    "\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "#plt.savefig(\"bigEarthNet.png\", bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigEarthNetSubsetModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BigEarthNetSubsetModel, self).__init__()\n",
    "        # Load the ResNet-18 model\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        # Modify the final layer to output 19 classes\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, DatasetConfig.num_classes)\n",
    "        # Addition of a sigmoid activation function for muylti-label classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Passing the model to the GPU\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Accuracy metrics\n",
    "        self.train_acc = MultilabelAccuracy(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_acc = MultilabelAccuracy(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_acc = MultilabelAccuracy(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "        # Recall metrics\n",
    "        self.train_recall = MultilabelRecall(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_recall = MultilabelRecall(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_recall = MultilabelRecall(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "        # Precision metrics\n",
    "        self.train_precision = MultilabelPrecision(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_precision = MultilabelPrecision(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_precision = MultilabelPrecision(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "        # F1 Score metrics\n",
    "        self.train_f1 = MultilabelF1Score(num_labels=DatasetConfig.num_classes)\n",
    "        self.val_f1 = MultilabelF1Score(num_labels=DatasetConfig.num_classes)\n",
    "        self.test_f1 = MultilabelF1Score(num_labels=DatasetConfig.num_classes)\n",
    "\n",
    "        #torch.summary(self.model, (DatasetConfig.band_channels, ModelConfig.img_size, ModelConfig.img_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.train_acc(logits, y)\n",
    "        recall = self.train_recall(logits, y)\n",
    "        f1 = self.train_f1(logits, y)\n",
    "        precision = self.train_precision(logits, y)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_recall', recall, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_f1', f1, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.val_acc(logits, y)\n",
    "        recall = self.val_recall(logits, y)\n",
    "        f1 = self.val_f1(logits, y)\n",
    "        precision = self.val_precision(logits, y)\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_recall', recall, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.test_acc(logits, y)\n",
    "        recall = self.test_recall(logits, y)\n",
    "        f1 = self.test_f1(logits, y)\n",
    "        precision = self.test_precision(logits, y)\n",
    "\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_recall', recall, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_precision', precision, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_acc_epoch', self.train_acc.compute())\n",
    "        self.log('train_recall_epoch', self.train_recall.compute())\n",
    "        self.log('train_f1_epoch', self.train_f1.compute())\n",
    "        self.log('train_precision_epoch', self.train_precision.compute())\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log('val_acc_epoch', self.val_acc.compute())\n",
    "        self.log('val_recall_epoch', self.val_recall.compute())\n",
    "        self.log('val_f1_epoch', self.val_f1.compute())\n",
    "        self.log('val_precision_epoch', self.val_precision.compute())\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        self.log('test_acc_epoch', self.test_acc.compute())\n",
    "        self.log('test_recall_epoch', self.test_recall.compute())\n",
    "        self.log('test_f1_epoch', self.test_f1.compute())\n",
    "        self.log('test_precision_epoch', self.test_precision.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Callback Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback for additional logging\n",
    "class BigEarthNetSubsetCallback(Callback):\n",
    "    def __init__(self, checkpoint_path, model_name):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "        checkpoint_path = os.path.join(self.checkpoint_path, f\"epoch={epoch}_{self.model_name}.ckpt\")\n",
    "        torch.save(pl_module.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        base_path = os.path.join(self.checkpoint_path, f\"final_{self.model_name}\")\n",
    "        final_model_path = f\"{base_path}.ckpt\"\n",
    "        counter = 1\n",
    "\n",
    "        # Check if the path exists and increment the counter until a unique path is found\n",
    "        while os.path.exists(final_model_path):\n",
    "            final_model_path = f\"{base_path}_{counter}.ckpt\"\n",
    "            counter += 1\n",
    "\n",
    "        torch.save(pl_module.state_dict(), final_model_path)\n",
    "        print(f\"Final model saved at: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 1195, val set: 606, test set: 551\n"
     ]
    }
   ],
   "source": [
    "dm = BigEarthNetSubsetDataModule()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 60, 60]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 60, 60]             128\n",
      "              ReLU-3           [-1, 64, 60, 60]               0\n",
      "         MaxPool2d-4           [-1, 64, 30, 30]               0\n",
      "            Conv2d-5           [-1, 64, 30, 30]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 30, 30]             128\n",
      "              ReLU-7           [-1, 64, 30, 30]               0\n",
      "            Conv2d-8           [-1, 64, 30, 30]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 30, 30]             128\n",
      "             ReLU-10           [-1, 64, 30, 30]               0\n",
      "       BasicBlock-11           [-1, 64, 30, 30]               0\n",
      "           Conv2d-12           [-1, 64, 30, 30]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 30, 30]             128\n",
      "             ReLU-14           [-1, 64, 30, 30]               0\n",
      "           Conv2d-15           [-1, 64, 30, 30]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 30, 30]             128\n",
      "             ReLU-17           [-1, 64, 30, 30]               0\n",
      "       BasicBlock-18           [-1, 64, 30, 30]               0\n",
      "           Conv2d-19          [-1, 128, 15, 15]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 15, 15]             256\n",
      "             ReLU-21          [-1, 128, 15, 15]               0\n",
      "           Conv2d-22          [-1, 128, 15, 15]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 15, 15]             256\n",
      "           Conv2d-24          [-1, 128, 15, 15]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 15, 15]             256\n",
      "             ReLU-26          [-1, 128, 15, 15]               0\n",
      "       BasicBlock-27          [-1, 128, 15, 15]               0\n",
      "           Conv2d-28          [-1, 128, 15, 15]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 15, 15]             256\n",
      "             ReLU-30          [-1, 128, 15, 15]               0\n",
      "           Conv2d-31          [-1, 128, 15, 15]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 15, 15]             256\n",
      "             ReLU-33          [-1, 128, 15, 15]               0\n",
      "       BasicBlock-34          [-1, 128, 15, 15]               0\n",
      "           Conv2d-35            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "             ReLU-37            [-1, 256, 8, 8]               0\n",
      "           Conv2d-38            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
      "           Conv2d-40            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
      "             ReLU-42            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-43            [-1, 256, 8, 8]               0\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "             ReLU-46            [-1, 256, 8, 8]               0\n",
      "           Conv2d-47            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 8, 8]             512\n",
      "             ReLU-49            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-50            [-1, 256, 8, 8]               0\n",
      "           Conv2d-51            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-53            [-1, 512, 4, 4]               0\n",
      "           Conv2d-54            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-56            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-58            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-59            [-1, 512, 4, 4]               0\n",
      "           Conv2d-60            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-62            [-1, 512, 4, 4]               0\n",
      "           Conv2d-63            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-65            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-66            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 19]           9,747\n",
      "================================================================\n",
      "Total params: 11,186,259\n",
      "Trainable params: 11,186,259\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.16\n",
      "Forward/backward pass size (MB): 18.38\n",
      "Params size (MB): 42.67\n",
      "Estimated Total Size (MB): 61.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BigEarthNetSubsetModel()\n",
    "summary(model.model, input_size=(DatasetConfig.band_channels, DatasetConfig.img_size, DatasetConfig.img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name            | Type                | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0  | model           | ResNet              | 11.2 M | train\n",
      "1  | sigmoid         | Sigmoid             | 0      | train\n",
      "2  | train_acc       | MultilabelAccuracy  | 0      | train\n",
      "3  | val_acc         | MultilabelAccuracy  | 0      | train\n",
      "4  | test_acc        | MultilabelAccuracy  | 0      | train\n",
      "5  | train_recall    | MultilabelRecall    | 0      | train\n",
      "6  | val_recall      | MultilabelRecall    | 0      | train\n",
      "7  | test_recall     | MultilabelRecall    | 0      | train\n",
      "8  | train_precision | MultilabelPrecision | 0      | train\n",
      "9  | val_precision   | MultilabelPrecision | 0      | train\n",
      "10 | test_precision  | MultilabelPrecision | 0      | train\n",
      "11 | train_f1        | MultilabelF1Score   | 0      | train\n",
      "12 | val_f1          | MultilabelF1Score   | 0      | train\n",
      "13 | test_f1         | MultilabelF1Score   | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.745    Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 1195, val set: 606, test set: 551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f71843dbb47497e8132fda4beff7a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "persistent_workers option needs num_workers > 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 18\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Model Training\u001b[39;00m\n\u001b[0;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m      9\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39mcheckpoint_dir,\n\u001b[0;32m     10\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[model_callback]\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, data_module)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    540\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:197\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_data()\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:225\u001b[0m, in \u001b[0;36m_FitLoop.setup_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: resetting train dataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    224\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_source\n\u001b[1;32m--> 225\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m _request_dataloader(source)\n\u001b[0;32m    226\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataloader()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataloader, CombinedLoader):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:325\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[1;34m(data_source)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    The requested dataloader\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_source\u001b[38;5;241m.\u001b[39mdataloader()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:292\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39m_call_lightning_datamodule_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:189\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 189\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[83], line 17\u001b[0m, in \u001b[0;36mBigEarthNetSubsetDataModule.train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset, batch_size\u001b[38;5;241m=\u001b[39mModelConfig\u001b[38;5;241m.\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\lightning_fabric\\utilities\\data.py:324\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m store_explicit_arg \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_explicit_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[1;32m--> 324\u001b[0m init(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pl_inside_init\u001b[39m\u001b[38;5;124m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:255\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefetch_factor option should be non-negative\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m persistent_workers \u001b[38;5;129;01mand\u001b[39;00m num_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersistent_workers option needs num_workers > 0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m=\u001b[39m num_workers\n",
      "\u001b[1;31mValueError\u001b[0m: persistent_workers option needs num_workers > 0"
     ]
    }
   ],
   "source": [
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model_resnet18_eurosat_notpretrained\")\n",
    "\n",
    "checkpoint_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests'\n",
    "model_callback = BigEarthNetSubsetCallback(checkpoint_dir, ModelConfig.model_name)\n",
    "\n",
    "# Model Training\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    max_epochs=10,\n",
    "    logger=logger,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[model_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
