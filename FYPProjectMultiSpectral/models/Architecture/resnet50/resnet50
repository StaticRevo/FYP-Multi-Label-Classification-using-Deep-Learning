digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2096615895120 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2096679499232 [label=AddmmBackward0]
	2096679498896 -> 2096679499232
	2096622252784 [label="fc.bias
 (19)" fillcolor=lightblue]
	2096622252784 -> 2096679498896
	2096679498896 [label=AccumulateGrad]
	2096679498272 -> 2096679499232
	2096679498272 [label=ViewBackward0]
	2096679497792 -> 2096679498272
	2096679497792 [label=MeanBackward1]
	2096679497456 -> 2096679497792
	2096679497456 [label=ReluBackward0]
	2096679496976 -> 2096679497456
	2096679496976 [label=AddBackward0]
	2096679496496 -> 2096679496976
	2096679496496 [label=CudnnBatchNormBackward0]
	2096679495392 -> 2096679496496
	2096679495392 [label=ConvolutionBackward0]
	2096679494432 -> 2096679495392
	2096679494432 [label=ReluBackward0]
	2096679490352 -> 2096679494432
	2096679490352 [label=CudnnBatchNormBackward0]
	2096679492416 -> 2096679490352
	2096679492416 [label=ConvolutionBackward0]
	2096679501680 -> 2096679492416
	2096679501680 [label=ReluBackward0]
	2096679501440 -> 2096679501680
	2096679501440 [label=CudnnBatchNormBackward0]
	2096679501392 -> 2096679501440
	2096679501392 [label=ConvolutionBackward0]
	2096679496352 -> 2096679501392
	2096679496352 [label=ReluBackward0]
	2096679500960 -> 2096679496352
	2096679500960 [label=AddBackward0]
	2096679500912 -> 2096679500960
	2096679500912 [label=CudnnBatchNormBackward0]
	2096679500720 -> 2096679500912
	2096679500720 [label=ConvolutionBackward0]
	2096679500624 -> 2096679500720
	2096679500624 [label=ReluBackward0]
	2096679500288 -> 2096679500624
	2096679500288 [label=CudnnBatchNormBackward0]
	2096679500096 -> 2096679500288
	2096679500096 [label=ConvolutionBackward0]
	2096679499952 -> 2096679500096
	2096679499952 [label=ReluBackward0]
	2096679499760 -> 2096679499952
	2096679499760 [label=CudnnBatchNormBackward0]
	2096679499424 -> 2096679499760
	2096679499424 [label=ConvolutionBackward0]
	2096679501104 -> 2096679499424
	2096679501104 [label=ReluBackward0]
	2096679499280 -> 2096679501104
	2096679499280 [label=AddBackward0]
	2096679498944 -> 2096679499280
	2096679498944 [label=CudnnBatchNormBackward0]
	2096679498992 -> 2096679498944
	2096679498992 [label=ConvolutionBackward0]
	2096679498656 -> 2096679498992
	2096679498656 [label=ReluBackward0]
	2096679498704 -> 2096679498656
	2096679498704 [label=CudnnBatchNormBackward0]
	2096679498608 -> 2096679498704
	2096679498608 [label=ConvolutionBackward0]
	2096679497984 -> 2096679498608
	2096679497984 [label=ReluBackward0]
	2096679498032 -> 2096679497984
	2096679498032 [label=CudnnBatchNormBackward0]
	2096679497888 -> 2096679498032
	2096679497888 [label=ConvolutionBackward0]
	2096679497600 -> 2096679497888
	2096679497600 [label=ReluBackward0]
	2096679497648 -> 2096679497600
	2096679497648 [label=AddBackward0]
	2096679497360 -> 2096679497648
	2096679497360 [label=CudnnBatchNormBackward0]
	2096679497120 -> 2096679497360
	2096679497120 [label=ConvolutionBackward0]
	2096679496928 -> 2096679497120
	2096679496928 [label=ReluBackward0]
	2096679496544 -> 2096679496928
	2096679496544 [label=CudnnBatchNormBackward0]
	2096679496784 -> 2096679496544
	2096679496784 [label=ConvolutionBackward0]
	2096679496400 -> 2096679496784
	2096679496400 [label=ReluBackward0]
	2096679496160 -> 2096679496400
	2096679496160 [label=CudnnBatchNormBackward0]
	2096679496112 -> 2096679496160
	2096679496112 [label=ConvolutionBackward0]
	2096679497408 -> 2096679496112
	2096679497408 [label=ReluBackward0]
	2096679495680 -> 2096679497408
	2096679495680 [label=AddBackward0]
	2096679495632 -> 2096679495680
	2096679495632 [label=CudnnBatchNormBackward0]
	2096679495440 -> 2096679495632
	2096679495440 [label=ConvolutionBackward0]
	2096679495344 -> 2096679495440
	2096679495344 [label=ReluBackward0]
	2096679495008 -> 2096679495344
	2096679495008 [label=CudnnBatchNormBackward0]
	2096679494816 -> 2096679495008
	2096679494816 [label=ConvolutionBackward0]
	2096679494672 -> 2096679494816
	2096679494672 [label=ReluBackward0]
	2096679494480 -> 2096679494672
	2096679494480 [label=CudnnBatchNormBackward0]
	2096679494144 -> 2096679494480
	2096679494144 [label=ConvolutionBackward0]
	2096679495824 -> 2096679494144
	2096679495824 [label=ReluBackward0]
	2096679490544 -> 2096679495824
	2096679490544 [label=AddBackward0]
	2096679489344 -> 2096679490544
	2096679489344 [label=CudnnBatchNormBackward0]
	2096679490592 -> 2096679489344
	2096679490592 [label=ConvolutionBackward0]
	2096679491072 -> 2096679490592
	2096679491072 [label=ReluBackward0]
	2096679493376 -> 2096679491072
	2096679493376 [label=CudnnBatchNormBackward0]
	2096679492992 -> 2096679493376
	2096679492992 [label=ConvolutionBackward0]
	2096679487184 -> 2096679492992
	2096679487184 [label=ReluBackward0]
	2096679486128 -> 2096679487184
	2096679486128 [label=CudnnBatchNormBackward0]
	2096679490880 -> 2096679486128
	2096679490880 [label=ConvolutionBackward0]
	2096679490448 -> 2096679490880
	2096679490448 [label=ReluBackward0]
	2096679488816 -> 2096679490448
	2096679488816 [label=AddBackward0]
	2096679491024 -> 2096679488816
	2096679491024 [label=CudnnBatchNormBackward0]
	2096679494048 -> 2096679491024
	2096679494048 [label=ConvolutionBackward0]
	2096679490688 -> 2096679494048
	2096679490688 [label=ReluBackward0]
	2096679493472 -> 2096679490688
	2096679493472 [label=CudnnBatchNormBackward0]
	2096679492704 -> 2096679493472
	2096679492704 [label=ConvolutionBackward0]
	2096679491936 -> 2096679492704
	2096679491936 [label=ReluBackward0]
	2096679493616 -> 2096679491936
	2096679493616 [label=CudnnBatchNormBackward0]
	2096679488912 -> 2096679493616
	2096679488912 [label=ConvolutionBackward0]
	2096679486464 -> 2096679488912
	2096679486464 [label=ReluBackward0]
	2096679488624 -> 2096679486464
	2096679488624 [label=AddBackward0]
	2096679491456 -> 2096679488624
	2096679491456 [label=CudnnBatchNormBackward0]
	2096679488576 -> 2096679491456
	2096679488576 [label=ConvolutionBackward0]
	2096679493280 -> 2096679488576
	2096679493280 [label=ReluBackward0]
	2096679489632 -> 2096679493280
	2096679489632 [label=CudnnBatchNormBackward0]
	2096679486944 -> 2096679489632
	2096679486944 [label=ConvolutionBackward0]
	2096679486704 -> 2096679486944
	2096679486704 [label=ReluBackward0]
	2096679491552 -> 2096679486704
	2096679491552 [label=CudnnBatchNormBackward0]
	2096679491504 -> 2096679491552
	2096679491504 [label=ConvolutionBackward0]
	2096679490160 -> 2096679491504
	2096679490160 [label=ReluBackward0]
	2096679487520 -> 2096679490160
	2096679487520 [label=AddBackward0]
	2096679490112 -> 2096679487520
	2096679490112 [label=CudnnBatchNormBackward0]
	2096679490784 -> 2096679490112
	2096679490784 [label=ConvolutionBackward0]
	2096679485840 -> 2096679490784
	2096679485840 [label=ReluBackward0]
	2096682825472 -> 2096679485840
	2096682825472 [label=CudnnBatchNormBackward0]
	2096682827680 -> 2096682825472
	2096682827680 [label=ConvolutionBackward0]
	2096682826672 -> 2096682827680
	2096682826672 [label=ReluBackward0]
	2096682827392 -> 2096682826672
	2096682827392 [label=CudnnBatchNormBackward0]
	2096682826960 -> 2096682827392
	2096682826960 [label=ConvolutionBackward0]
	2096682826384 -> 2096682826960
	2096682826384 [label=ReluBackward0]
	2096682826912 -> 2096682826384
	2096682826912 [label=AddBackward0]
	2096679569584 -> 2096682826912
	2096679569584 [label=CudnnBatchNormBackward0]
	2096616251232 -> 2096679569584
	2096616251232 [label=ConvolutionBackward0]
	2096616250272 -> 2096616251232
	2096616250272 [label=ReluBackward0]
	2096616249936 -> 2096616250272
	2096616249936 [label=CudnnBatchNormBackward0]
	2096616249456 -> 2096616249936
	2096616249456 [label=ConvolutionBackward0]
	2096616248496 -> 2096616249456
	2096616248496 [label=ReluBackward0]
	2096616247392 -> 2096616248496
	2096616247392 [label=CudnnBatchNormBackward0]
	2096616246912 -> 2096616247392
	2096616246912 [label=ConvolutionBackward0]
	2096682826720 -> 2096616246912
	2096682826720 [label=ReluBackward0]
	2096616245472 -> 2096682826720
	2096616245472 [label=AddBackward0]
	2096616244992 -> 2096616245472
	2096616244992 [label=CudnnBatchNormBackward0]
	2096616244656 -> 2096616244992
	2096616244656 [label=ConvolutionBackward0]
	2096616243696 -> 2096616244656
	2096616243696 [label=ReluBackward0]
	2096616251616 -> 2096616243696
	2096616251616 [label=CudnnBatchNormBackward0]
	2096616251520 -> 2096616251616
	2096616251520 [label=ConvolutionBackward0]
	2096616251328 -> 2096616251520
	2096616251328 [label=ReluBackward0]
	2096616250944 -> 2096616251328
	2096616250944 [label=CudnnBatchNormBackward0]
	2096616251184 -> 2096616250944
	2096616251184 [label=ConvolutionBackward0]
	2096616245616 -> 2096616251184
	2096616245616 [label=ReluBackward0]
	2096616250464 -> 2096616245616
	2096616250464 [label=AddBackward0]
	2096616250704 -> 2096616250464
	2096616250704 [label=CudnnBatchNormBackward0]
	2096616250368 -> 2096616250704
	2096616250368 [label=ConvolutionBackward0]
	2096616250080 -> 2096616250368
	2096616250080 [label=ReluBackward0]
	2096616250128 -> 2096616250080
	2096616250128 [label=CudnnBatchNormBackward0]
	2096616249840 -> 2096616250128
	2096616249840 [label=ConvolutionBackward0]
	2096616249744 -> 2096616249840
	2096616249744 [label=ReluBackward0]
	2096616249408 -> 2096616249744
	2096616249408 [label=CudnnBatchNormBackward0]
	2096616249216 -> 2096616249408
	2096616249216 [label=ConvolutionBackward0]
	2096616250560 -> 2096616249216
	2096616250560 [label=ReluBackward0]
	2096616248928 -> 2096616250560
	2096616248928 [label=AddBackward0]
	2096616248736 -> 2096616248928
	2096616248736 [label=CudnnBatchNormBackward0]
	2096616248784 -> 2096616248736
	2096616248784 [label=ConvolutionBackward0]
	2096616248400 -> 2096616248784
	2096616248400 [label=ReluBackward0]
	2096616248160 -> 2096616248400
	2096616248160 [label=CudnnBatchNormBackward0]
	2096616248112 -> 2096616248160
	2096616248112 [label=ConvolutionBackward0]
	2096616247776 -> 2096616248112
	2096616247776 [label=ReluBackward0]
	2096616247824 -> 2096616247776
	2096616247824 [label=CudnnBatchNormBackward0]
	2096616247728 -> 2096616247824
	2096616247728 [label=ConvolutionBackward0]
	2096616247104 -> 2096616247728
	2096616247104 [label=ReluBackward0]
	2096616247152 -> 2096616247104
	2096616247152 [label=AddBackward0]
	2096616247008 -> 2096616247152
	2096616247008 [label=CudnnBatchNormBackward0]
	2096616246624 -> 2096616247008
	2096616246624 [label=ConvolutionBackward0]
	2096616246768 -> 2096616246624
	2096616246768 [label=ReluBackward0]
	2096616246336 -> 2096616246768
	2096616246336 [label=CudnnBatchNormBackward0]
	2096616246240 -> 2096616246336
	2096616246240 [label=ConvolutionBackward0]
	2096616246048 -> 2096616246240
	2096616246048 [label=ReluBackward0]
	2096616245664 -> 2096616246048
	2096616245664 [label=CudnnBatchNormBackward0]
	2096616245904 -> 2096616245664
	2096616245904 [label=ConvolutionBackward0]
	2096616247248 -> 2096616245904
	2096616247248 [label=ReluBackward0]
	2096616245184 -> 2096616247248
	2096616245184 [label=AddBackward0]
	2096616245424 -> 2096616245184
	2096616245424 [label=CudnnBatchNormBackward0]
	2096616245088 -> 2096616245424
	2096616245088 [label=ConvolutionBackward0]
	2096616244800 -> 2096616245088
	2096616244800 [label=ReluBackward0]
	2096616244848 -> 2096616244800
	2096616244848 [label=CudnnBatchNormBackward0]
	2096616244560 -> 2096616244848
	2096616244560 [label=ConvolutionBackward0]
	2096616244464 -> 2096616244560
	2096616244464 [label=ReluBackward0]
	2096616244128 -> 2096616244464
	2096616244128 [label=CudnnBatchNormBackward0]
	2096616243936 -> 2096616244128
	2096616243936 [label=ConvolutionBackward0]
	2096616245280 -> 2096616243936
	2096616245280 [label=ReluBackward0]
	2096616243648 -> 2096616245280
	2096616243648 [label=AddBackward0]
	2096616243456 -> 2096616243648
	2096616243456 [label=CudnnBatchNormBackward0]
	2096616243504 -> 2096616243456
	2096616243504 [label=ConvolutionBackward0]
	2096616252288 -> 2096616243504
	2096616252288 [label=ReluBackward0]
	2096616252624 -> 2096616252288
	2096616252624 [label=CudnnBatchNormBackward0]
	2096616252720 -> 2096616252624
	2096616252720 [label=ConvolutionBackward0]
	2096616252912 -> 2096616252720
	2096616252912 [label=ReluBackward0]
	2096616253056 -> 2096616252912
	2096616253056 [label=CudnnBatchNormBackward0]
	2096616253152 -> 2096616253056
	2096616253152 [label=ConvolutionBackward0]
	2096616253344 -> 2096616253152
	2096616253344 [label=MaxPool2DWithIndicesBackward0]
	2096616253488 -> 2096616253344
	2096616253488 [label=ReluBackward0]
	2096616253584 -> 2096616253488
	2096616253584 [label=CudnnBatchNormBackward0]
	2096616253680 -> 2096616253584
	2096616253680 [label=ConvolutionBackward0]
	2096616253872 -> 2096616253680
	2096622252592 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2096622252592 -> 2096616253872
	2096616253872 [label=AccumulateGrad]
	2096616253632 -> 2096616253584
	2096688169040 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2096688169040 -> 2096616253632
	2096616253632 [label=AccumulateGrad]
	2096616253392 -> 2096616253584
	2096688169136 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2096688169136 -> 2096616253392
	2096616253392 [label=AccumulateGrad]
	2096616253296 -> 2096616253152
	2096689134864 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2096689134864 -> 2096616253296
	2096616253296 [label=AccumulateGrad]
	2096616253104 -> 2096616253056
	2096689134768 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2096689134768 -> 2096616253104
	2096616253104 [label=AccumulateGrad]
	2096616252960 -> 2096616253056
	2096689134000 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2096689134000 -> 2096616252960
	2096616252960 [label=AccumulateGrad]
	2096616252864 -> 2096616252720
	2096689134096 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2096689134096 -> 2096616252864
	2096616252864 [label=AccumulateGrad]
	2096616252672 -> 2096616252624
	2096689134192 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2096689134192 -> 2096616252672
	2096616252672 [label=AccumulateGrad]
	2096616252528 -> 2096616252624
	2096689134288 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2096689134288 -> 2096616252528
	2096616252528 [label=AccumulateGrad]
	2096616252432 -> 2096616243504
	2096689134576 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2096689134576 -> 2096616252432
	2096616252432 [label=AccumulateGrad]
	2096616243360 -> 2096616243456
	2096689133616 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2096689133616 -> 2096616243360
	2096616243360 [label=AccumulateGrad]
	2096616243264 -> 2096616243456
	2096689133520 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2096689133520 -> 2096616243264
	2096616243264 [label=AccumulateGrad]
	2096616243600 -> 2096616243648
	2096616243600 [label=CudnnBatchNormBackward0]
	2096616252816 -> 2096616243600
	2096616252816 [label=ConvolutionBackward0]
	2096616253344 -> 2096616252816
	2096616253200 -> 2096616252816
	2096689134960 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2096689134960 -> 2096616253200
	2096616253200 [label=AccumulateGrad]
	2096616243408 -> 2096616243600
	2096689135056 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2096689135056 -> 2096616243408
	2096616243408 [label=AccumulateGrad]
	2096616243312 -> 2096616243600
	2096689135152 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2096689135152 -> 2096616243312
	2096616243312 [label=AccumulateGrad]
	2096616243792 -> 2096616243936
	2096689133328 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2096689133328 -> 2096616243792
	2096616243792 [label=AccumulateGrad]
	2096616244080 -> 2096616244128
	2096689132848 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2096689132848 -> 2096616244080
	2096616244080 [label=AccumulateGrad]
	2096616244272 -> 2096616244128
	2096689131120 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2096689131120 -> 2096616244272
	2096616244272 [label=AccumulateGrad]
	2096616244320 -> 2096616244560
	2096689133424 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2096689133424 -> 2096616244320
	2096616244320 [label=AccumulateGrad]
	2096616244608 -> 2096616244848
	2096689133232 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2096689133232 -> 2096616244608
	2096616244608 [label=AccumulateGrad]
	2096616244944 -> 2096616244848
	2096622635056 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2096622635056 -> 2096616244944
	2096616244944 [label=AccumulateGrad]
	2096616244704 -> 2096616245088
	2096622635440 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2096622635440 -> 2096616244704
	2096616244704 [label=AccumulateGrad]
	2096616245328 -> 2096616245424
	2096622635536 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2096622635536 -> 2096616245328
	2096616245328 [label=AccumulateGrad]
	2096616245232 -> 2096616245424
	2096622635632 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2096622635632 -> 2096616245232
	2096616245232 [label=AccumulateGrad]
	2096616245280 -> 2096616245184
	2096616245520 -> 2096616245904
	2096622636016 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2096622636016 -> 2096616245520
	2096616245520 [label=AccumulateGrad]
	2096616245760 -> 2096616245664
	2096622636112 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2096622636112 -> 2096616245760
	2096616245760 [label=AccumulateGrad]
	2096616246000 -> 2096616245664
	2096622636208 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2096622636208 -> 2096616246000
	2096616246000 [label=AccumulateGrad]
	2096616246288 -> 2096616246240
	2096622636592 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2096622636592 -> 2096616246288
	2096616246288 [label=AccumulateGrad]
	2096616246144 -> 2096616246336
	2096622636688 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2096622636688 -> 2096616246144
	2096616246144 [label=AccumulateGrad]
	2096616246528 -> 2096616246336
	2096622636784 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2096622636784 -> 2096616246528
	2096616246528 [label=AccumulateGrad]
	2096616246672 -> 2096616246624
	2096622637168 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2096622637168 -> 2096616246672
	2096616246672 [label=AccumulateGrad]
	2096616246816 -> 2096616247008
	2096622637264 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2096622637264 -> 2096616246816
	2096616246816 [label=AccumulateGrad]
	2096616246960 -> 2096616247008
	2096622637360 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2096622637360 -> 2096616246960
	2096616246960 [label=AccumulateGrad]
	2096616247248 -> 2096616247152
	2096616247296 -> 2096616247728
	2096622638320 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2096622638320 -> 2096616247296
	2096616247296 [label=AccumulateGrad]
	2096616247632 -> 2096616247824
	2096622638416 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2096622638416 -> 2096616247632
	2096616247632 [label=AccumulateGrad]
	2096616247584 -> 2096616247824
	2096622638512 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2096622638512 -> 2096616247584
	2096616247584 [label=AccumulateGrad]
	2096616247920 -> 2096616248112
	2096622638896 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2096622638896 -> 2096616247920
	2096616247920 [label=AccumulateGrad]
	2096616248304 -> 2096616248160
	2096622638992 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2096622638992 -> 2096616248304
	2096616248304 [label=AccumulateGrad]
	2096616248256 -> 2096616248160
	2096622639088 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2096622639088 -> 2096616248256
	2096616248256 [label=AccumulateGrad]
	2096616248448 -> 2096616248784
	2096622639472 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2096622639472 -> 2096616248448
	2096616248448 [label=AccumulateGrad]
	2096616248640 -> 2096616248736
	2096622639568 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2096622639568 -> 2096616248640
	2096616248640 [label=AccumulateGrad]
	2096616248544 -> 2096616248736
	2096622639664 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2096622639664 -> 2096616248544
	2096616248544 [label=AccumulateGrad]
	2096616248880 -> 2096616248928
	2096616248880 [label=CudnnBatchNormBackward0]
	2096616247968 -> 2096616248880
	2096616247968 [label=ConvolutionBackward0]
	2096616247104 -> 2096616247968
	2096616247488 -> 2096616247968
	2096622637744 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2096622637744 -> 2096616247488
	2096616247488 [label=AccumulateGrad]
	2096616248688 -> 2096616248880
	2096622637840 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2096622637840 -> 2096616248688
	2096616248688 [label=AccumulateGrad]
	2096616248592 -> 2096616248880
	2096622637936 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2096622637936 -> 2096616248592
	2096616248592 [label=AccumulateGrad]
	2096616249072 -> 2096616249216
	2096622640048 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2096622640048 -> 2096616249072
	2096616249072 [label=AccumulateGrad]
	2096616249360 -> 2096616249408
	2096622640144 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2096622640144 -> 2096616249360
	2096616249360 [label=AccumulateGrad]
	2096616249552 -> 2096616249408
	2096622640240 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2096622640240 -> 2096616249552
	2096616249552 [label=AccumulateGrad]
	2096616249600 -> 2096616249840
	2096622640624 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2096622640624 -> 2096616249600
	2096616249600 [label=AccumulateGrad]
	2096616249888 -> 2096616250128
	2096622640720 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2096622640720 -> 2096616249888
	2096616249888 [label=AccumulateGrad]
	2096616250224 -> 2096616250128
	2096622640816 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2096622640816 -> 2096616250224
	2096616250224 [label=AccumulateGrad]
	2096616249984 -> 2096616250368
	2096622641200 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2096622641200 -> 2096616249984
	2096616249984 [label=AccumulateGrad]
	2096616250608 -> 2096616250704
	2096622641296 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2096622641296 -> 2096616250608
	2096616250608 [label=AccumulateGrad]
	2096616250512 -> 2096616250704
	2096622641392 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2096622641392 -> 2096616250512
	2096616250512 [label=AccumulateGrad]
	2096616250560 -> 2096616250464
	2096616250800 -> 2096616251184
	2096622641776 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2096622641776 -> 2096616250800
	2096616250800 [label=AccumulateGrad]
	2096616251040 -> 2096616250944
	2096622641872 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2096622641872 -> 2096616251040
	2096616251040 [label=AccumulateGrad]
	2096616251280 -> 2096616250944
	2096622641968 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2096622641968 -> 2096616251280
	2096616251280 [label=AccumulateGrad]
	2096616251568 -> 2096616251520
	2096622642352 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2096622642352 -> 2096616251568
	2096616251568 [label=AccumulateGrad]
	2096616251424 -> 2096616251616
	2096622642448 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2096622642448 -> 2096616251424
	2096616251424 [label=AccumulateGrad]
	2096616251808 -> 2096616251616
	2096622642544 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2096622642544 -> 2096616251808
	2096616251808 [label=AccumulateGrad]
	2096616243552 -> 2096616244656
	2096622642928 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2096622642928 -> 2096616243552
	2096616243552 [label=AccumulateGrad]
	2096616244512 -> 2096616244992
	2096622643024 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2096622643024 -> 2096616244512
	2096616244512 [label=AccumulateGrad]
	2096616245136 -> 2096616244992
	2096622643120 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2096622643120 -> 2096616245136
	2096616245136 [label=AccumulateGrad]
	2096616245616 -> 2096616245472
	2096616245952 -> 2096616246912
	2096622643504 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2096622643504 -> 2096616245952
	2096616245952 [label=AccumulateGrad]
	2096616247536 -> 2096616247392
	2096622643600 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2096622643600 -> 2096616247536
	2096616247536 [label=AccumulateGrad]
	2096616247872 -> 2096616247392
	2096622643696 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2096622643696 -> 2096616247872
	2096616247872 [label=AccumulateGrad]
	2096616248352 -> 2096616249456
	2096622644080 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2096622644080 -> 2096616248352
	2096616248352 [label=AccumulateGrad]
	2096616249312 -> 2096616249936
	2096622644176 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2096622644176 -> 2096616249312
	2096616249312 [label=AccumulateGrad]
	2096616250416 -> 2096616249936
	2096622644272 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2096622644272 -> 2096616250416
	2096616250416 [label=AccumulateGrad]
	2096616250896 -> 2096616251232
	2096622644656 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2096622644656 -> 2096616250896
	2096616250896 [label=AccumulateGrad]
	2096616251712 -> 2096679569584
	2096622644752 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2096622644752 -> 2096616251712
	2096616251712 [label=AccumulateGrad]
	2096616251856 -> 2096679569584
	2096622644848 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2096622644848 -> 2096616251856
	2096616251856 [label=AccumulateGrad]
	2096682826720 -> 2096682826912
	2096682825856 -> 2096682826960
	2096622645808 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2096622645808 -> 2096682825856
	2096682825856 [label=AccumulateGrad]
	2096682827104 -> 2096682827392
	2096622645904 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2096622645904 -> 2096682827104
	2096682827104 [label=AccumulateGrad]
	2096682826288 -> 2096682827392
	2096622646000 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2096622646000 -> 2096682826288
	2096682826288 [label=AccumulateGrad]
	2096682826768 -> 2096682827680
	2096622646384 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2096622646384 -> 2096682826768
	2096682826768 [label=AccumulateGrad]
	2096682825568 -> 2096682825472
	2096622646480 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2096622646480 -> 2096682825568
	2096682825568 [label=AccumulateGrad]
	2096682827296 -> 2096682825472
	2096622646576 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2096622646576 -> 2096682827296
	2096682827296 [label=AccumulateGrad]
	2096682826816 -> 2096679490784
	2096622646960 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2096622646960 -> 2096682826816
	2096682826816 [label=AccumulateGrad]
	2096679485552 -> 2096679490112
	2096622647056 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2096622647056 -> 2096679485552
	2096679485552 [label=AccumulateGrad]
	2096679493664 -> 2096679490112
	2096622647152 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2096622647152 -> 2096679493664
	2096679493664 [label=AccumulateGrad]
	2096679491600 -> 2096679487520
	2096679491600 [label=CudnnBatchNormBackward0]
	2096679491312 -> 2096679491600
	2096679491312 [label=ConvolutionBackward0]
	2096682826384 -> 2096679491312
	2096682827200 -> 2096679491312
	2096622645232 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2096622645232 -> 2096682827200
	2096682827200 [label=AccumulateGrad]
	2096682825904 -> 2096679491600
	2096622645328 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2096622645328 -> 2096682825904
	2096682825904 [label=AccumulateGrad]
	2096682827344 -> 2096679491600
	2096622645424 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2096622645424 -> 2096682827344
	2096682827344 [label=AccumulateGrad]
	2096679493520 -> 2096679491504
	2096622647536 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2096622647536 -> 2096679493520
	2096679493520 [label=AccumulateGrad]
	2096679488192 -> 2096679491552
	2096622647632 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2096622647632 -> 2096679488192
	2096679488192 [label=AccumulateGrad]
	2096679490016 -> 2096679491552
	2096622647728 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2096622647728 -> 2096679490016
	2096679490016 [label=AccumulateGrad]
	2096679487280 -> 2096679486944
	2096622648112 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2096622648112 -> 2096679487280
	2096679487280 [label=AccumulateGrad]
	2096679487712 -> 2096679489632
	2096622648208 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2096622648208 -> 2096679487712
	2096679487712 [label=AccumulateGrad]
	2096679492032 -> 2096679489632
	2096622648304 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2096622648304 -> 2096679492032
	2096679492032 [label=AccumulateGrad]
	2096679489200 -> 2096679488576
	2096622648688 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2096622648688 -> 2096679489200
	2096679489200 [label=AccumulateGrad]
	2096679491216 -> 2096679491456
	2096622648784 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2096622648784 -> 2096679491216
	2096679491216 [label=AccumulateGrad]
	2096679489536 -> 2096679491456
	2096622648880 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2096622648880 -> 2096679489536
	2096679489536 [label=AccumulateGrad]
	2096679490160 -> 2096679488624
	2096679489248 -> 2096679488912
	2096622649264 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2096622649264 -> 2096679489248
	2096679489248 [label=AccumulateGrad]
	2096679487664 -> 2096679493616
	2096622239824 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2096622239824 -> 2096679487664
	2096679487664 [label=AccumulateGrad]
	2096679492464 -> 2096679493616
	2096622239920 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2096622239920 -> 2096679492464
	2096679492464 [label=AccumulateGrad]
	2096679489968 -> 2096679492704
	2096622240304 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2096622240304 -> 2096679489968
	2096679489968 [label=AccumulateGrad]
	2096679492512 -> 2096679493472
	2096622240400 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2096622240400 -> 2096679492512
	2096679492512 [label=AccumulateGrad]
	2096679486896 -> 2096679493472
	2096622240496 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2096622240496 -> 2096679486896
	2096679486896 [label=AccumulateGrad]
	2096679491888 -> 2096679494048
	2096622240880 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2096622240880 -> 2096679491888
	2096679491888 [label=AccumulateGrad]
	2096679489728 -> 2096679491024
	2096622240976 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2096622240976 -> 2096679489728
	2096679489728 [label=AccumulateGrad]
	2096679487136 -> 2096679491024
	2096622241072 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2096622241072 -> 2096679487136
	2096679487136 [label=AccumulateGrad]
	2096679486464 -> 2096679488816
	2096679492320 -> 2096679490880
	2096622241456 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2096622241456 -> 2096679492320
	2096679492320 [label=AccumulateGrad]
	2096679493136 -> 2096679486128
	2096622241552 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2096622241552 -> 2096679493136
	2096679493136 [label=AccumulateGrad]
	2096679491744 -> 2096679486128
	2096622241648 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2096622241648 -> 2096679491744
	2096679491744 [label=AccumulateGrad]
	2096679493904 -> 2096679492992
	2096622242032 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2096622242032 -> 2096679493904
	2096679493904 [label=AccumulateGrad]
	2096679488768 -> 2096679493376
	2096622242128 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2096622242128 -> 2096679488768
	2096679488768 [label=AccumulateGrad]
	2096679488336 -> 2096679493376
	2096622242224 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2096622242224 -> 2096679488336
	2096679488336 [label=AccumulateGrad]
	2096679489488 -> 2096679490592
	2096622242608 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2096622242608 -> 2096679489488
	2096679489488 [label=AccumulateGrad]
	2096679491840 -> 2096679489344
	2096622242704 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2096622242704 -> 2096679491840
	2096679491840 [label=AccumulateGrad]
	2096679490496 -> 2096679489344
	2096622242800 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2096622242800 -> 2096679490496
	2096679490496 [label=AccumulateGrad]
	2096679490448 -> 2096679490544
	2096679494288 -> 2096679494144
	2096622243184 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2096622243184 -> 2096679494288
	2096679494288 [label=AccumulateGrad]
	2096679494336 -> 2096679494480
	2096622243280 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2096622243280 -> 2096679494336
	2096679494336 [label=AccumulateGrad]
	2096679494768 -> 2096679494480
	2096622243376 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2096622243376 -> 2096679494768
	2096679494768 [label=AccumulateGrad]
	2096679494864 -> 2096679494816
	2096622243760 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2096622243760 -> 2096679494864
	2096679494864 [label=AccumulateGrad]
	2096679494960 -> 2096679495008
	2096622243856 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2096622243856 -> 2096679494960
	2096679494960 [label=AccumulateGrad]
	2096679495152 -> 2096679495008
	2096622243952 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2096622243952 -> 2096679495152
	2096679495152 [label=AccumulateGrad]
	2096679495200 -> 2096679495440
	2096622244336 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2096622244336 -> 2096679495200
	2096679495200 [label=AccumulateGrad]
	2096679495488 -> 2096679495632
	2096622244432 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2096622244432 -> 2096679495488
	2096679495488 [label=AccumulateGrad]
	2096679495728 -> 2096679495632
	2096622244528 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2096622244528 -> 2096679495728
	2096679495728 [label=AccumulateGrad]
	2096679495824 -> 2096679495680
	2096679495776 -> 2096679496112
	2096622244912 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2096622244912 -> 2096679495776
	2096679495776 [label=AccumulateGrad]
	2096679496304 -> 2096679496160
	2096622245008 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2096622245008 -> 2096679496304
	2096679496304 [label=AccumulateGrad]
	2096679496256 -> 2096679496160
	2096622245104 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2096622245104 -> 2096679496256
	2096679496256 [label=AccumulateGrad]
	2096679496448 -> 2096679496784
	2096622245488 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2096622245488 -> 2096679496448
	2096679496448 [label=AccumulateGrad]
	2096679496640 -> 2096679496544
	2096622245584 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2096622245584 -> 2096679496640
	2096679496640 [label=AccumulateGrad]
	2096679496880 -> 2096679496544
	2096622245680 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2096622245680 -> 2096679496880
	2096679496880 [label=AccumulateGrad]
	2096679497168 -> 2096679497120
	2096622246064 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2096622246064 -> 2096679497168
	2096679497168 [label=AccumulateGrad]
	2096679497024 -> 2096679497360
	2096622246160 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2096622246160 -> 2096679497024
	2096679497024 [label=AccumulateGrad]
	2096679497216 -> 2096679497360
	2096622246256 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2096622246256 -> 2096679497216
	2096679497216 [label=AccumulateGrad]
	2096679497408 -> 2096679497648
	2096679497504 -> 2096679497888
	2096622247216 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2096622247216 -> 2096679497504
	2096679497504 [label=AccumulateGrad]
	2096679498128 -> 2096679498032
	2096622247312 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2096622247312 -> 2096679498128
	2096679498128 [label=AccumulateGrad]
	2096679498080 -> 2096679498032
	2096622247408 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2096622247408 -> 2096679498080
	2096679498080 [label=AccumulateGrad]
	2096679498176 -> 2096679498608
	2096622247792 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2096622247792 -> 2096679498176
	2096679498176 [label=AccumulateGrad]
	2096679498512 -> 2096679498704
	2096622247888 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2096622247888 -> 2096679498512
	2096679498512 [label=AccumulateGrad]
	2096679498464 -> 2096679498704
	2096622247984 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2096622247984 -> 2096679498464
	2096679498464 [label=AccumulateGrad]
	2096679498800 -> 2096679498992
	2096622248368 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2096622248368 -> 2096679498800
	2096679498800 [label=AccumulateGrad]
	2096679499184 -> 2096679498944
	2096622248464 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2096622248464 -> 2096679499184
	2096679499184 [label=AccumulateGrad]
	2096679499040 -> 2096679498944
	2096622248560 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2096622248560 -> 2096679499040
	2096679499040 [label=AccumulateGrad]
	2096679499136 -> 2096679499280
	2096679499136 [label=CudnnBatchNormBackward0]
	2096679498320 -> 2096679499136
	2096679498320 [label=ConvolutionBackward0]
	2096679497600 -> 2096679498320
	2096679497840 -> 2096679498320
	2096622246640 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2096622246640 -> 2096679497840
	2096679497840 [label=AccumulateGrad]
	2096679498848 -> 2096679499136
	2096622246736 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2096622246736 -> 2096679498848
	2096679498848 [label=AccumulateGrad]
	2096679499088 -> 2096679499136
	2096622246832 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2096622246832 -> 2096679499088
	2096679499088 [label=AccumulateGrad]
	2096679499568 -> 2096679499424
	2096622248944 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2096622248944 -> 2096679499568
	2096679499568 [label=AccumulateGrad]
	2096679499616 -> 2096679499760
	2096622249040 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2096622249040 -> 2096679499616
	2096679499616 [label=AccumulateGrad]
	2096679500048 -> 2096679499760
	2096622249136 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2096622249136 -> 2096679500048
	2096679500048 [label=AccumulateGrad]
	2096679500144 -> 2096679500096
	2096622249520 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2096622249520 -> 2096679500144
	2096679500144 [label=AccumulateGrad]
	2096679500240 -> 2096679500288
	2096622249616 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2096622249616 -> 2096679500240
	2096679500240 [label=AccumulateGrad]
	2096679500432 -> 2096679500288
	2096622249712 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2096622249712 -> 2096679500432
	2096679500432 [label=AccumulateGrad]
	2096679500480 -> 2096679500720
	2096622250096 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2096622250096 -> 2096679500480
	2096679500480 [label=AccumulateGrad]
	2096679500768 -> 2096679500912
	2096622250192 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2096622250192 -> 2096679500768
	2096679500768 [label=AccumulateGrad]
	2096679501008 -> 2096679500912
	2096622250288 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2096622250288 -> 2096679501008
	2096679501008 [label=AccumulateGrad]
	2096679501104 -> 2096679500960
	2096679501056 -> 2096679501392
	2096622250672 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2096622250672 -> 2096679501056
	2096679501056 [label=AccumulateGrad]
	2096679501584 -> 2096679501440
	2096622250768 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2096622250768 -> 2096679501584
	2096679501584 [label=AccumulateGrad]
	2096679501536 -> 2096679501440
	2096622250864 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2096622250864 -> 2096679501536
	2096679501536 [label=AccumulateGrad]
	2096679501728 -> 2096679492416
	2096622251248 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2096622251248 -> 2096679501728
	2096679501728 [label=AccumulateGrad]
	2096679487424 -> 2096679490352
	2096622251344 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2096622251344 -> 2096679487424
	2096679487424 [label=AccumulateGrad]
	2096679494576 -> 2096679490352
	2096622251440 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2096622251440 -> 2096679494576
	2096679494576 [label=AccumulateGrad]
	2096679495056 -> 2096679495392
	2096622251824 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2096622251824 -> 2096679495056
	2096679495056 [label=AccumulateGrad]
	2096679496016 -> 2096679496496
	2096622251920 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2096622251920 -> 2096679496016
	2096679496016 [label=AccumulateGrad]
	2096679495872 -> 2096679496496
	2096622252016 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2096622252016 -> 2096679495872
	2096679495872 [label=AccumulateGrad]
	2096679496352 -> 2096679496976
	2096679498416 -> 2096679499232
	2096679498416 [label=TBackward0]
	2096679496832 -> 2096679498416
	2096622252688 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2096622252688 -> 2096679496832
	2096679496832 [label=AccumulateGrad]
	2096679499232 -> 2096615895120
}
