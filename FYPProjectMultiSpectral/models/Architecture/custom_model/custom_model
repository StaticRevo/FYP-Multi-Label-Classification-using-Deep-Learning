digraph {
	graph [size="86.7,86.7"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1768676537360 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1768648067872 [label=AddmmBackward0]
	1768648072432 -> 1768648067872
	1768676079280 [label="29.bias
 (19)" fillcolor=lightblue]
	1768676079280 -> 1768648072432
	1768648072432 [label=AccumulateGrad]
	1768648075744 -> 1768648067872
	1768648075744 [label=NativeDropoutBackward0]
	1768648073488 -> 1768648075744
	1768648073488 [label=ViewBackward0]
	1768648069120 -> 1768648073488
	1768648069120 [label=MeanBackward1]
	1768648073680 -> 1768648069120
	1768648073680 [label=MulBackward0]
	1768648066192 -> 1768648073680
	1768648066192 [label=MulBackward0]
	1768648072336 -> 1768648066192
	1768648072336 [label=ReluBackward0]
	1768648075888 -> 1768648072336
	1768648075888 [label=AddBackward0]
	1768648069408 -> 1768648075888
	1768648069408 [label=CudnnBatchNormBackward0]
	1768648078864 -> 1768648069408
	1768648078864 [label=ConvolutionBackward0]
	1768648066672 -> 1768648078864
	1768648066672 [label=ReluBackward0]
	1768648074592 -> 1768648066672
	1768648074592 [label=CudnnBatchNormBackward0]
	1768648078528 -> 1768648074592
	1768648078528 [label=ConvolutionBackward0]
	1768648073152 -> 1768648078528
	1768648073152 [label=ReluBackward0]
	1768648079056 -> 1768648073152
	1768648079056 [label=CudnnBatchNormBackward0]
	1768648067104 -> 1768648079056
	1768648067104 [label=ConvolutionBackward0]
	1768648078240 -> 1768648067104
	1768648078240 [label=ConvolutionBackward0]
	1768648078576 -> 1768648078240
	1768648078576 [label=MulBackward0]
	1768648066720 -> 1768648078576
	1768648066720 [label=ReluBackward0]
	1768648067440 -> 1768648066720
	1768648067440 [label=AddBackward0]
	1768648079344 -> 1768648067440
	1768648079344 [label=CudnnBatchNormBackward0]
	1768648071568 -> 1768648079344
	1768648071568 [label=ConvolutionBackward0]
	1768648076032 -> 1768648071568
	1768648076032 [label=ReluBackward0]
	1768648075024 -> 1768648076032
	1768648075024 [label=CudnnBatchNormBackward0]
	1768648066768 -> 1768648075024
	1768648066768 [label=ConvolutionBackward0]
	1768648078192 -> 1768648066768
	1768648078192 [label=ConvolutionBackward0]
	1768648069456 -> 1768648078192
	1768648069456 [label=CatBackward0]
	1768648077616 -> 1768648069456
	1768648077616 [label=ConvolutionBackward0]
	1768648073200 -> 1768648077616
	1768648073200 [label=ReluBackward0]
	1768648080304 -> 1768648073200
	1768648080304 [label=CudnnBatchNormBackward0]
	1768648081168 -> 1768648080304
	1768648081168 [label=ConvolutionBackward0]
	1768648076896 -> 1768648081168
	1768648076896 [label=ConvolutionBackward0]
	1768648072480 -> 1768648076896
	1768648072480 [label=MulBackward0]
	1768648076464 -> 1768648072480
	1768648076464 [label=ReluBackward0]
	1768648067536 -> 1768648076464
	1768648067536 [label=AddBackward0]
	1768648073824 -> 1768648067536
	1768648073824 [label=CudnnBatchNormBackward0]
	1768648073920 -> 1768648073824
	1768648073920 [label=ConvolutionBackward0]
	1768648068352 -> 1768648073920
	1768648068352 [label=ReluBackward0]
	1768648078096 -> 1768648068352
	1768648078096 [label=CudnnBatchNormBackward0]
	1768648066624 -> 1768648078096
	1768648066624 [label=ConvolutionBackward0]
	1768648077904 -> 1768648066624
	1768648077904 [label=ConvolutionBackward0]
	1768648076416 -> 1768648077904
	1768648076416 [label=CatBackward0]
	1768648066432 -> 1768648076416
	1768648066432 [label=ConvolutionBackward0]
	1768648078768 -> 1768648066432
	1768648078768 [label=ReluBackward0]
	1768648078480 -> 1768648078768
	1768648078480 [label=CudnnBatchNormBackward0]
	1768648074256 -> 1768648078480
	1768648074256 [label=ConvolutionBackward0]
	1768648079872 -> 1768648074256
	1768648079872 [label=ConvolutionBackward0]
	1768648070464 -> 1768648079872
	1768648070464 [label=MulBackward0]
	1768648069600 -> 1768648070464
	1768648069600 [label=MulBackward0]
	1768648081312 -> 1768648069600
	1768648081312 [label=MulBackward0]
	1768811346864 -> 1768648081312
	1768811346864 [label=ReluBackward0]
	1768768680528 -> 1768811346864
	1768768680528 [label=AddBackward0]
	1768768681920 -> 1768768680528
	1768768681920 [label=CudnnBatchNormBackward0]
	1768768683840 -> 1768768681920
	1768768683840 [label=ConvolutionBackward0]
	1768768682304 -> 1768768683840
	1768768682304 [label=ReluBackward0]
	1768768679424 -> 1768768682304
	1768768679424 [label=CudnnBatchNormBackward0]
	1768768683264 -> 1768768679424
	1768768683264 [label=ConvolutionBackward0]
	1768768681536 -> 1768768683264
	1768768681536 [label=ReluBackward0]
	1768768683024 -> 1768768681536
	1768768683024 [label=CudnnBatchNormBackward0]
	1768768679232 -> 1768768683024
	1768768679232 [label=ConvolutionBackward0]
	1768768682784 -> 1768768679232
	1768768682784 [label=ConvolutionBackward0]
	1768768678896 -> 1768768682784
	1768768678896 [label=ReluBackward0]
	1768768678752 -> 1768768678896
	1768768678752 [label=CudnnBatchNormBackward0]
	1768768680864 -> 1768768678752
	1768768680864 [label=ConvolutionBackward0]
	1768768681728 -> 1768768680864
	1768687615728 [label="0.weight
 (16, 12, 1, 1)" fillcolor=lightblue]
	1768687615728 -> 1768768681728
	1768768681728 [label=AccumulateGrad]
	1768768678656 -> 1768768678752
	1768687615440 [label="1.weight
 (16)" fillcolor=lightblue]
	1768687615440 -> 1768768678656
	1768768678656 [label=AccumulateGrad]
	1768768682880 -> 1768768678752
	1768687615824 [label="1.bias
 (16)" fillcolor=lightblue]
	1768687615824 -> 1768768682880
	1768768682880 [label=AccumulateGrad]
	1768768679136 -> 1768768682784
	1768721681488 [label="3.depthwise.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	1768721681488 -> 1768768679136
	1768768679136 [label=AccumulateGrad]
	1768768682448 -> 1768768679232
	1768721680720 [label="3.pointwise.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	1768721680720 -> 1768768682448
	1768768682448 [label=AccumulateGrad]
	1768768681152 -> 1768768683024
	1768721680432 [label="4.weight
 (32)" fillcolor=lightblue]
	1768721680432 -> 1768768681152
	1768768681152 [label=AccumulateGrad]
	1768768681392 -> 1768768683024
	1768721680528 [label="4.bias
 (32)" fillcolor=lightblue]
	1768721680528 -> 1768768681392
	1768768681392 [label=AccumulateGrad]
	1768768683984 -> 1768768683264
	1768721681008 [label="6.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1768721681008 -> 1768768683984
	1768768683984 [label=AccumulateGrad]
	1768768682016 -> 1768768679424
	1768721681104 [label="6.bn1.weight
 (32)" fillcolor=lightblue]
	1768721681104 -> 1768768682016
	1768768682016 [label=AccumulateGrad]
	1768768679952 -> 1768768679424
	1768721681200 [label="6.bn1.bias
 (32)" fillcolor=lightblue]
	1768721681200 -> 1768768679952
	1768768679952 [label=AccumulateGrad]
	1768768683888 -> 1768768683840
	1768721680240 [label="6.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1768721680240 -> 1768768683888
	1768768683888 [label=AccumulateGrad]
	1768768683600 -> 1768768681920
	1768721679472 [label="6.bn2.weight
 (32)" fillcolor=lightblue]
	1768721679472 -> 1768768683600
	1768768683600 [label=AccumulateGrad]
	1768768683216 -> 1768768681920
	1768721679184 [label="6.bn2.bias
 (32)" fillcolor=lightblue]
	1768721679184 -> 1768768683216
	1768768683216 [label=AccumulateGrad]
	1768768681536 -> 1768768680528
	1768648073056 -> 1768648081312
	1768648073056 [label=ViewBackward0]
	1768768683360 -> 1768648073056
	1768768683360 [label=SigmoidBackward0]
	1768768679472 -> 1768768683360
	1768768679472 [label=AddmmBackward0]
	1768768682352 -> 1768768679472
	1768721679952 [label="7.fc2.bias
 (32)" fillcolor=lightblue]
	1768721679952 -> 1768768682352
	1768768682352 [label=AccumulateGrad]
	1768768681296 -> 1768768679472
	1768768681296 [label=ReluBackward0]
	1768768682112 -> 1768768681296
	1768768682112 [label=AddmmBackward0]
	1768768681680 -> 1768768682112
	1768721679760 [label="7.fc1.bias
 (2)" fillcolor=lightblue]
	1768721679760 -> 1768768681680
	1768768681680 [label=AccumulateGrad]
	1768768682256 -> 1768768682112
	1768768682256 [label=MeanBackward1]
	1768768683168 -> 1768768682256
	1768768683168 [label=ViewBackward0]
	1768811346864 -> 1768768683168
	1768768682544 -> 1768768682112
	1768768682544 [label=TBackward0]
	1768768681776 -> 1768768682544
	1768721679664 [label="7.fc1.weight
 (2, 32)" fillcolor=lightblue]
	1768721679664 -> 1768768681776
	1768768681776 [label=AccumulateGrad]
	1768768681584 -> 1768768679472
	1768768681584 [label=TBackward0]
	1768768678944 -> 1768768681584
	1768721679856 [label="7.fc2.weight
 (32, 2)" fillcolor=lightblue]
	1768721679856 -> 1768768678944
	1768768678944 [label=AccumulateGrad]
	1768648067776 -> 1768648069600
	1768648067776 [label=SigmoidBackward0]
	1768768682400 -> 1768648067776
	1768768682400 [label=ConvolutionBackward0]
	1768768679088 -> 1768768682400
	1768768679088 [label=SplitWithSizesBackward0]
	1768768682160 -> 1768768679088
	1768768682160 [label=ReluBackward0]
	1768768679280 -> 1768768682160
	1768768679280 [label=CudnnBatchNormBackward0]
	1768768679664 -> 1768768679280
	1768768679664 [label=ConvolutionBackward0]
	1768768678992 -> 1768768679664
	1768768678992 [label=CatBackward0]
	1768768679760 -> 1768768678992
	1768768679760 [label=AdaptiveAvgPool2DBackward0]
	1768648081312 -> 1768768679760
	1768768683936 -> 1768768678992
	1768768683936 [label=PermuteBackward0]
	1768768680096 -> 1768768683936
	1768768680096 [label=AdaptiveAvgPool2DBackward0]
	1768648081312 -> 1768768680096
	1768768680384 -> 1768768679664
	1768721680144 [label="8.conv1.weight
 (8, 32, 1, 1)" fillcolor=lightblue]
	1768721680144 -> 1768768680384
	1768768680384 [label=AccumulateGrad]
	1768768681200 -> 1768768679280
	1768721680048 [label="8.bn1.weight
 (8)" fillcolor=lightblue]
	1768721680048 -> 1768768681200
	1768768681200 [label=AccumulateGrad]
	1768768681872 -> 1768768679280
	1768721679088 [label="8.bn1.bias
 (8)" fillcolor=lightblue]
	1768721679088 -> 1768768681872
	1768768681872 [label=AccumulateGrad]
	1768768683792 -> 1768768682400
	1768721678512 [label="8.conv_h.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1768721678512 -> 1768768683792
	1768768683792 [label=AccumulateGrad]
	1768648071184 -> 1768648070464
	1768648071184 [label=PermuteBackward0]
	1768648075072 -> 1768648071184
	1768648075072 [label=SigmoidBackward0]
	1768768683072 -> 1768648075072
	1768768683072 [label=ConvolutionBackward0]
	1768768679088 -> 1768768683072
	1768768679568 -> 1768768683072
	1768721678608 [label="8.conv_w.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1768721678608 -> 1768768679568
	1768768679568 [label=AccumulateGrad]
	1768648072816 -> 1768648079872
	1768721678896 [label="9.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	1768721678896 -> 1768648072816
	1768648072816 [label=AccumulateGrad]
	1768648070704 -> 1768648074256
	1768721678800 [label="9.pointwise.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	1768721678800 -> 1768648070704
	1768648070704 [label=AccumulateGrad]
	1768648069312 -> 1768648078480
	1768676071408 [label="10.weight
 (64)" fillcolor=lightblue]
	1768676071408 -> 1768648069312
	1768648069312 [label=AccumulateGrad]
	1768648068784 -> 1768648078480
	1768676071504 [label="10.bias
 (64)" fillcolor=lightblue]
	1768676071504 -> 1768648068784
	1768648068784 [label=AccumulateGrad]
	1768648076800 -> 1768648066432
	1768676071888 [label="12.conv_dil1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1768676071888 -> 1768648076800
	1768648076800 [label=AccumulateGrad]
	1768648074016 -> 1768648066432
	1768676071984 [label="12.conv_dil1.bias
 (64)" fillcolor=lightblue]
	1768676071984 -> 1768648074016
	1768648074016 [label=AccumulateGrad]
	1768648071904 -> 1768648076416
	1768648071904 [label=ConvolutionBackward0]
	1768648078768 -> 1768648071904
	1768648079584 -> 1768648071904
	1768676072080 [label="12.conv_dil2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1768676072080 -> 1768648079584
	1768648079584 [label=AccumulateGrad]
	1768648076272 -> 1768648071904
	1768676072176 [label="12.conv_dil2.bias
 (64)" fillcolor=lightblue]
	1768676072176 -> 1768648076272
	1768648076272 [label=AccumulateGrad]
	1768648077136 -> 1768648076416
	1768648077136 [label=ConvolutionBackward0]
	1768648078768 -> 1768648077136
	1768648072288 -> 1768648077136
	1768676072272 [label="12.conv_dil3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1768676072272 -> 1768648072288
	1768648072288 [label=AccumulateGrad]
	1768648069984 -> 1768648077136
	1768676072368 [label="12.conv_dil3.bias
 (64)" fillcolor=lightblue]
	1768676072368 -> 1768648069984
	1768648069984 [label=AccumulateGrad]
	1768648076176 -> 1768648077904
	1768676072464 [label="12.fuse.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	1768676072464 -> 1768648076176
	1768648076176 [label=AccumulateGrad]
	1768648066912 -> 1768648077904
	1768676072560 [label="12.fuse.bias
 (64)" fillcolor=lightblue]
	1768676072560 -> 1768648066912
	1768648066912 [label=AccumulateGrad]
	1768648079536 -> 1768648066624
	1768676072656 [label="13.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1768676072656 -> 1768648079536
	1768648079536 [label=AccumulateGrad]
	1768648078720 -> 1768648078096
	1768676072752 [label="13.bn1.weight
 (64)" fillcolor=lightblue]
	1768676072752 -> 1768648078720
	1768648078720 [label=AccumulateGrad]
	1768648079968 -> 1768648078096
	1768676072848 [label="13.bn1.bias
 (64)" fillcolor=lightblue]
	1768676072848 -> 1768648079968
	1768648079968 [label=AccumulateGrad]
	1768648065712 -> 1768648073920
	1768676073232 [label="13.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1768676073232 -> 1768648065712
	1768648065712 [label=AccumulateGrad]
	1768648077088 -> 1768648073824
	1768676073328 [label="13.bn2.weight
 (64)" fillcolor=lightblue]
	1768676073328 -> 1768648077088
	1768648077088 [label=AccumulateGrad]
	1768648081120 -> 1768648073824
	1768676073424 [label="13.bn2.bias
 (64)" fillcolor=lightblue]
	1768676073424 -> 1768648081120
	1768648081120 [label=AccumulateGrad]
	1768648077904 -> 1768648067536
	1768648066144 -> 1768648072480
	1768648066144 [label=UnsqueezeBackward0]
	1768648079392 -> 1768648066144
	1768648079392 [label=UnsqueezeBackward0]
	1768648067488 -> 1768648079392
	1768648067488 [label=SigmoidBackward0]
	1768648075264 -> 1768648067488
	1768648075264 [label=SqueezeBackward1]
	1768648074784 -> 1768648075264
	1768648074784 [label=ConvolutionBackward0]
	1768648070656 -> 1768648074784
	1768648070656 [label=UnsqueezeBackward0]
	1768768682208 -> 1768648070656
	1768768682208 [label=SqueezeBackward1]
	1768768682736 -> 1768768682208
	1768768682736 [label=SqueezeBackward1]
	1768768682496 -> 1768768682736
	1768768682496 [label=MeanBackward1]
	1768648076464 -> 1768768682496
	1768648067920 -> 1768648074784
	1768676073808 [label="14.conv.weight
 (1, 1, 3)" fillcolor=lightblue]
	1768676073808 -> 1768648067920
	1768648067920 [label=AccumulateGrad]
	1768648078816 -> 1768648076896
	1768676073904 [label="15.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	1768676073904 -> 1768648078816
	1768648078816 [label=AccumulateGrad]
	1768648068832 -> 1768648081168
	1768676074000 [label="15.pointwise.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1768676074000 -> 1768648068832
	1768648068832 [label=AccumulateGrad]
	1768648078000 -> 1768648080304
	1768676074096 [label="16.weight
 (128)" fillcolor=lightblue]
	1768676074096 -> 1768648078000
	1768648078000 [label=AccumulateGrad]
	1768648079440 -> 1768648080304
	1768676074192 [label="16.bias
 (128)" fillcolor=lightblue]
	1768676074192 -> 1768648079440
	1768648079440 [label=AccumulateGrad]
	1768648073584 -> 1768648077616
	1768676074576 [label="18.conv_dil1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1768676074576 -> 1768648073584
	1768648073584 [label=AccumulateGrad]
	1768648080256 -> 1768648077616
	1768676074672 [label="18.conv_dil1.bias
 (128)" fillcolor=lightblue]
	1768676074672 -> 1768648080256
	1768648080256 [label=AccumulateGrad]
	1768648065568 -> 1768648069456
	1768648065568 [label=ConvolutionBackward0]
	1768648073200 -> 1768648065568
	1768648078288 -> 1768648065568
	1768676074768 [label="18.conv_dil2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1768676074768 -> 1768648078288
	1768648078288 [label=AccumulateGrad]
	1768648080064 -> 1768648065568
	1768676074864 [label="18.conv_dil2.bias
 (128)" fillcolor=lightblue]
	1768676074864 -> 1768648080064
	1768648080064 [label=AccumulateGrad]
	1768648078672 -> 1768648069456
	1768648078672 [label=ConvolutionBackward0]
	1768648073200 -> 1768648078672
	1768648077376 -> 1768648078672
	1768676074960 [label="18.conv_dil3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1768676074960 -> 1768648077376
	1768648077376 [label=AccumulateGrad]
	1768648065328 -> 1768648078672
	1768676075056 [label="18.conv_dil3.bias
 (128)" fillcolor=lightblue]
	1768676075056 -> 1768648065328
	1768648065328 [label=AccumulateGrad]
	1768648080496 -> 1768648078192
	1768676075152 [label="18.fuse.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	1768676075152 -> 1768648080496
	1768648080496 [label=AccumulateGrad]
	1768648077280 -> 1768648078192
	1768676075248 [label="18.fuse.bias
 (128)" fillcolor=lightblue]
	1768676075248 -> 1768648077280
	1768648077280 [label=AccumulateGrad]
	1768648069696 -> 1768648066768
	1768676075344 [label="19.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1768676075344 -> 1768648069696
	1768648069696 [label=AccumulateGrad]
	1768648080016 -> 1768648075024
	1768676075440 [label="19.bn1.weight
 (128)" fillcolor=lightblue]
	1768676075440 -> 1768648080016
	1768648080016 [label=AccumulateGrad]
	1768648079008 -> 1768648075024
	1768676075536 [label="19.bn1.bias
 (128)" fillcolor=lightblue]
	1768676075536 -> 1768648079008
	1768648079008 [label=AccumulateGrad]
	1768648074400 -> 1768648071568
	1768676075920 [label="19.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1768676075920 -> 1768648074400
	1768648074400 [label=AccumulateGrad]
	1768648069552 -> 1768648079344
	1768676076016 [label="19.bn2.weight
 (128)" fillcolor=lightblue]
	1768676076016 -> 1768648069552
	1768648069552 [label=AccumulateGrad]
	1768648074304 -> 1768648079344
	1768676076112 [label="19.bn2.bias
 (128)" fillcolor=lightblue]
	1768676076112 -> 1768648074304
	1768648074304 [label=AccumulateGrad]
	1768648078192 -> 1768648067440
	1768648069792 -> 1768648078576
	1768648069792 [label=SigmoidBackward0]
	1768648065664 -> 1768648069792
	1768648065664 [label=ConvolutionBackward0]
	1768648065424 -> 1768648065664
	1768648065424 [label=NativeDropoutBackward0]
	1768648071712 -> 1768648065424
	1768648071712 [label=ReluBackward0]
	1768648069648 -> 1768648071712
	1768648069648 [label=ConvolutionBackward0]
	1768648073872 -> 1768648069648
	1768648073872 [label=MeanBackward1]
	1768648066720 -> 1768648073872
	1768648073104 -> 1768648069648
	1768676076496 [label="20.fc1.weight
 (8, 128, 1, 1)" fillcolor=lightblue]
	1768676076496 -> 1768648073104
	1768648073104 [label=AccumulateGrad]
	1768648076320 -> 1768648069648
	1768676076592 [label="20.fc1.bias
 (8)" fillcolor=lightblue]
	1768676076592 -> 1768648076320
	1768648076320 [label=AccumulateGrad]
	1768648067344 -> 1768648065664
	1768676076688 [label="20.fc2.weight
 (128, 8, 1, 1)" fillcolor=lightblue]
	1768676076688 -> 1768648067344
	1768648067344 [label=AccumulateGrad]
	1768648080832 -> 1768648065664
	1768676076784 [label="20.fc2.bias
 (128)" fillcolor=lightblue]
	1768676076784 -> 1768648080832
	1768648080832 [label=AccumulateGrad]
	1768648077760 -> 1768648078240
	1768676076880 [label="21.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	1768676076880 -> 1768648077760
	1768648077760 [label=AccumulateGrad]
	1768648071616 -> 1768648067104
	1768676076976 [label="21.pointwise.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1768676076976 -> 1768648071616
	1768648071616 [label=AccumulateGrad]
	1768648077664 -> 1768648079056
	1768676077072 [label="22.weight
 (256)" fillcolor=lightblue]
	1768676077072 -> 1768648077664
	1768648077664 [label=AccumulateGrad]
	1768648075504 -> 1768648079056
	1768676077168 [label="22.bias
 (256)" fillcolor=lightblue]
	1768676077168 -> 1768648075504
	1768648075504 [label=AccumulateGrad]
	1768648070224 -> 1768648078528
	1768676077552 [label="24.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1768676077552 -> 1768648070224
	1768648070224 [label=AccumulateGrad]
	1768648068016 -> 1768648074592
	1768676077648 [label="24.bn1.weight
 (256)" fillcolor=lightblue]
	1768676077648 -> 1768648068016
	1768648068016 [label=AccumulateGrad]
	1768648070320 -> 1768648074592
	1768676077744 [label="24.bn1.bias
 (256)" fillcolor=lightblue]
	1768676077744 -> 1768648070320
	1768648070320 [label=AccumulateGrad]
	1768648077952 -> 1768648078864
	1768676078128 [label="24.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1768676078128 -> 1768648077952
	1768648077952 [label=AccumulateGrad]
	1768648076752 -> 1768648069408
	1768676078224 [label="24.bn2.weight
 (256)" fillcolor=lightblue]
	1768676078224 -> 1768648076752
	1768648076752 [label=AccumulateGrad]
	1768648070512 -> 1768648069408
	1768676078320 [label="24.bn2.bias
 (256)" fillcolor=lightblue]
	1768676078320 -> 1768648070512
	1768648070512 [label=AccumulateGrad]
	1768648073152 -> 1768648075888
	1768648070752 -> 1768648066192
	1768648070752 [label=ViewBackward0]
	1768648075936 -> 1768648070752
	1768648075936 [label=SigmoidBackward0]
	1768648066384 -> 1768648075936
	1768648066384 [label=AddmmBackward0]
	1768648075408 -> 1768648066384
	1768676078992 [label="25.channel_att.fc2.bias
 (256)" fillcolor=lightblue]
	1768676078992 -> 1768648075408
	1768648075408 [label=AccumulateGrad]
	1768648068256 -> 1768648066384
	1768648068256 [label=ReluBackward0]
	1768648070176 -> 1768648068256
	1768648070176 [label=AddmmBackward0]
	1768648065088 -> 1768648070176
	1768676078800 [label="25.channel_att.fc1.bias
 (16)" fillcolor=lightblue]
	1768676078800 -> 1768648065088
	1768648065088 [label=AccumulateGrad]
	1768648074832 -> 1768648070176
	1768648074832 [label=MeanBackward1]
	1768648072144 -> 1768648074832
	1768648072144 [label=ViewBackward0]
	1768648072336 -> 1768648072144
	1768648071376 -> 1768648070176
	1768648071376 [label=TBackward0]
	1768648071088 -> 1768648071376
	1768676078704 [label="25.channel_att.fc1.weight
 (16, 256)" fillcolor=lightblue]
	1768676078704 -> 1768648071088
	1768648071088 [label=AccumulateGrad]
	1768648066240 -> 1768648066384
	1768648066240 [label=TBackward0]
	1768648065136 -> 1768648066240
	1768676078896 [label="25.channel_att.fc2.weight
 (256, 16)" fillcolor=lightblue]
	1768676078896 -> 1768648065136
	1768648065136 [label=AccumulateGrad]
	1768648068304 -> 1768648073680
	1768648068304 [label=SigmoidBackward0]
	1768648074976 -> 1768648068304
	1768648074976 [label=ConvolutionBackward0]
	1768648070560 -> 1768648074976
	1768648070560 [label=CatBackward0]
	1768648066816 -> 1768648070560
	1768648066816 [label=MeanBackward1]
	1768648066192 -> 1768648066816
	1768648076656 -> 1768648070560
	1768648076656 [label=MaxBackward0]
	1768648066192 -> 1768648076656
	1768648072912 -> 1768648074976
	1768676079088 [label="25.spatial_att.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	1768676079088 -> 1768648072912
	1768648072912 [label=AccumulateGrad]
	1768648078912 -> 1768648067872
	1768648078912 [label=TBackward0]
	1768648081264 -> 1768648078912
	1768676079184 [label="29.weight
 (19, 256)" fillcolor=lightblue]
	1768676079184 -> 1768648081264
	1768648081264 [label=AccumulateGrad]
	1768648067872 -> 1768676537360
}
