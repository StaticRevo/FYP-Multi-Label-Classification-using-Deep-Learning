digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2926047123728 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2926208291440 [label=AddmmBackward0]
	2926208291200 -> 2926208291440
	2926047654160 [label="fc.bias
 (19)" fillcolor=lightblue]
	2926047654160 -> 2926208291200
	2926208291200 [label=AccumulateGrad]
	2926208301328 -> 2926208291440
	2926208301328 [label=ViewBackward0]
	2926208294848 -> 2926208301328
	2926208294848 [label=MeanBackward1]
	2926208294752 -> 2926208294848
	2926208294752 [label=ReluBackward0]
	2926165673440 -> 2926208294752
	2926165673440 [label=AddBackward0]
	2926165679824 -> 2926165673440
	2926165679824 [label=CudnnBatchNormBackward0]
	2926165678960 -> 2926165679824
	2926165678960 [label=ConvolutionBackward0]
	2926165681744 -> 2926165678960
	2926165681744 [label=ReluBackward0]
	2926165679968 -> 2926165681744
	2926165679968 [label=CudnnBatchNormBackward0]
	2926165684192 -> 2926165679968
	2926165684192 [label=ConvolutionBackward0]
	2926165676848 -> 2926165684192
	2926165676848 [label=ReluBackward0]
	2926165682464 -> 2926165676848
	2926165682464 [label=AddBackward0]
	2926165672048 -> 2926165682464
	2926165672048 [label=CudnnBatchNormBackward0]
	2926165683856 -> 2926165672048
	2926165683856 [label=ConvolutionBackward0]
	2926165681552 -> 2926165683856
	2926165681552 [label=ReluBackward0]
	2926165685200 -> 2926165681552
	2926165685200 [label=CudnnBatchNormBackward0]
	2926165681264 -> 2926165685200
	2926165681264 [label=ConvolutionBackward0]
	2926165675216 -> 2926165681264
	2926165675216 [label=ReluBackward0]
	2926165676800 -> 2926165675216
	2926165676800 [label=AddBackward0]
	2926165683040 -> 2926165676800
	2926165683040 [label=CudnnBatchNormBackward0]
	2926165683376 -> 2926165683040
	2926165683376 [label=ConvolutionBackward0]
	2926165672768 -> 2926165683376
	2926165672768 [label=ReluBackward0]
	2926165684960 -> 2926165672768
	2926165684960 [label=CudnnBatchNormBackward0]
	2926165681648 -> 2926165684960
	2926165681648 [label=ConvolutionBackward0]
	2926165678912 -> 2926165681648
	2926165678912 [label=ReluBackward0]
	2926165685776 -> 2926165678912
	2926165685776 [label=AddBackward0]
	2926165674544 -> 2926165685776
	2926165674544 [label=CudnnBatchNormBackward0]
	2926165672816 -> 2926165674544
	2926165672816 [label=ConvolutionBackward0]
	2926165682368 -> 2926165672816
	2926165682368 [label=ReluBackward0]
	2926165675840 -> 2926165682368
	2926165675840 [label=CudnnBatchNormBackward0]
	2926165678720 -> 2926165675840
	2926165678720 [label=ConvolutionBackward0]
	2926165684048 -> 2926165678720
	2926165684048 [label=ReluBackward0]
	2926165677904 -> 2926165684048
	2926165677904 [label=AddBackward0]
	2926165685056 -> 2926165677904
	2926165685056 [label=CudnnBatchNormBackward0]
	2926165685728 -> 2926165685056
	2926165685728 [label=ConvolutionBackward0]
	2926165673776 -> 2926165685728
	2926165673776 [label=ReluBackward0]
	2926165683136 -> 2926165673776
	2926165683136 [label=CudnnBatchNormBackward0]
	2926165681360 -> 2926165683136
	2926165681360 [label=ConvolutionBackward0]
	2926165684672 -> 2926165681360
	2926165684672 [label=ReluBackward0]
	2926165672480 -> 2926165684672
	2926165672480 [label=AddBackward0]
	2926165684528 -> 2926165672480
	2926165684528 [label=CudnnBatchNormBackward0]
	2926165685872 -> 2926165684528
	2926165685872 [label=ConvolutionBackward0]
	2926165680736 -> 2926165685872
	2926165680736 [label=ReluBackward0]
	2926165685008 -> 2926165680736
	2926165685008 [label=CudnnBatchNormBackward0]
	2926165684912 -> 2926165685008
	2926165684912 [label=ConvolutionBackward0]
	2926165680160 -> 2926165684912
	2926165680160 [label=ReluBackward0]
	2926165672624 -> 2926165680160
	2926165672624 [label=AddBackward0]
	2926165672096 -> 2926165672624
	2926165672096 [label=CudnnBatchNormBackward0]
	2926165672960 -> 2926165672096
	2926165672960 [label=ConvolutionBackward0]
	2926165676464 -> 2926165672960
	2926165676464 [label=ReluBackward0]
	2926165675360 -> 2926165676464
	2926165675360 [label=CudnnBatchNormBackward0]
	2926165674352 -> 2926165675360
	2926165674352 [label=ConvolutionBackward0]
	2926165675168 -> 2926165674352
	2926165675168 [label=ReluBackward0]
	2926165675984 -> 2926165675168
	2926165675984 [label=AddBackward0]
	2926165674256 -> 2926165675984
	2926165674256 [label=CudnnBatchNormBackward0]
	2926165680256 -> 2926165674256
	2926165680256 [label=ConvolutionBackward0]
	2926165683088 -> 2926165680256
	2926165683088 [label=ReluBackward0]
	2926165674160 -> 2926165683088
	2926165674160 [label=CudnnBatchNormBackward0]
	2926165681120 -> 2926165674160
	2926165681120 [label=ConvolutionBackward0]
	2926165682656 -> 2926165681120
	2926165682656 [label=MaxPool2DWithIndicesBackward0]
	2926165672000 -> 2926165682656
	2926165672000 [label=ReluBackward0]
	2926165677568 -> 2926165672000
	2926165677568 [label=CudnnBatchNormBackward0]
	2926165679728 -> 2926165677568
	2926165679728 [label=ConvolutionBackward0]
	2926165679152 -> 2926165679728
	2926047653968 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2926047653968 -> 2926165679152
	2926165679152 [label=AccumulateGrad]
	2926165680208 -> 2926165677568
	2926170018096 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2926170018096 -> 2926165680208
	2926165680208 [label=AccumulateGrad]
	2926165676128 -> 2926165677568
	2926170017328 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2926170017328 -> 2926165676128
	2926165676128 [label=AccumulateGrad]
	2926165674304 -> 2926165681120
	2926170017424 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2926170017424 -> 2926165674304
	2926165674304 [label=AccumulateGrad]
	2926165677232 -> 2926165674160
	2926170017520 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2926170017520 -> 2926165677232
	2926165677232 [label=AccumulateGrad]
	2926165683328 -> 2926165674160
	2926170017616 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2926170017616 -> 2926165683328
	2926165683328 [label=AccumulateGrad]
	2926165677808 -> 2926165680256
	2926170017904 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2926170017904 -> 2926165677808
	2926165677808 [label=AccumulateGrad]
	2926165683952 -> 2926165674256
	2926170016944 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2926170016944 -> 2926165683952
	2926165683952 [label=AccumulateGrad]
	2926165683808 -> 2926165674256
	2926170016848 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2926170016848 -> 2926165683808
	2926165683808 [label=AccumulateGrad]
	2926165682656 -> 2926165675984
	2926165672672 -> 2926165674352
	2926170015984 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2926170015984 -> 2926165672672
	2926165672672 [label=AccumulateGrad]
	2926165675552 -> 2926165675360
	2926170016176 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2926170016176 -> 2926165675552
	2926165675552 [label=AccumulateGrad]
	2926165673920 -> 2926165675360
	2926170016272 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2926170016272 -> 2926165673920
	2926165673920 [label=AccumulateGrad]
	2926165673488 -> 2926165672960
	2926170016752 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2926170016752 -> 2926165673488
	2926165673488 [label=AccumulateGrad]
	2926165673152 -> 2926165672096
	2926170016656 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2926170016656 -> 2926165673152
	2926165673152 [label=AccumulateGrad]
	2926165680400 -> 2926165672096
	2926170015696 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2926170015696 -> 2926165680400
	2926165680400 [label=AccumulateGrad]
	2926165675168 -> 2926165672624
	2926165674688 -> 2926165684912
	2926170015216 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2926170015216 -> 2926165674688
	2926165674688 [label=AccumulateGrad]
	2926165681600 -> 2926165685008
	2926170015504 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2926170015504 -> 2926165681600
	2926165681600 [label=AccumulateGrad]
	2926165679248 -> 2926165685008
	2926170015408 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2926170015408 -> 2926165679248
	2926165679248 [label=AccumulateGrad]
	2926165672432 -> 2926165685872
	2926047646288 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2926047646288 -> 2926165672432
	2926165672432 [label=AccumulateGrad]
	2926165681456 -> 2926165684528
	2926047646384 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2926047646384 -> 2926165681456
	2926165681456 [label=AccumulateGrad]
	2926165673632 -> 2926165684528
	2926047646480 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2926047646480 -> 2926165673632
	2926165673632 [label=AccumulateGrad]
	2926165683568 -> 2926165672480
	2926165683568 [label=CudnnBatchNormBackward0]
	2926165673296 -> 2926165683568
	2926165673296 [label=ConvolutionBackward0]
	2926165680160 -> 2926165673296
	2926165685152 -> 2926165673296
	2926170015312 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2926170015312 -> 2926165685152
	2926165685152 [label=AccumulateGrad]
	2926165683424 -> 2926165683568
	2926170014064 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2926170014064 -> 2926165683424
	2926165683424 [label=AccumulateGrad]
	2926165678096 -> 2926165683568
	2926170014160 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2926170014160 -> 2926165678096
	2926165678096 [label=AccumulateGrad]
	2926165681696 -> 2926165681360
	2926047646864 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2926047646864 -> 2926165681696
	2926165681696 [label=AccumulateGrad]
	2926165684096 -> 2926165683136
	2926047646960 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2926047646960 -> 2926165684096
	2926165684096 [label=AccumulateGrad]
	2926165684000 -> 2926165683136
	2926047647056 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2926047647056 -> 2926165684000
	2926165684000 [label=AccumulateGrad]
	2926165676656 -> 2926165685728
	2926047647440 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2926047647440 -> 2926165676656
	2926165676656 [label=AccumulateGrad]
	2926165681408 -> 2926165685056
	2926047647536 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2926047647536 -> 2926165681408
	2926165681408 [label=AccumulateGrad]
	2926165677424 -> 2926165685056
	2926047647632 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2926047647632 -> 2926165677424
	2926165677424 [label=AccumulateGrad]
	2926165684672 -> 2926165677904
	2926165680832 -> 2926165678720
	2926047648592 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2926047648592 -> 2926165680832
	2926165680832 [label=AccumulateGrad]
	2926165676944 -> 2926165675840
	2926047648688 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2926047648688 -> 2926165676944
	2926165676944 [label=AccumulateGrad]
	2926165680304 -> 2926165675840
	2926047648784 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2926047648784 -> 2926165680304
	2926165680304 [label=AccumulateGrad]
	2926165682800 -> 2926165672816
	2926047649168 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2926047649168 -> 2926165682800
	2926165682800 [label=AccumulateGrad]
	2926165680640 -> 2926165674544
	2926047649264 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2926047649264 -> 2926165680640
	2926165680640 [label=AccumulateGrad]
	2926165684624 -> 2926165674544
	2926047649360 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2926047649360 -> 2926165684624
	2926165684624 [label=AccumulateGrad]
	2926165675024 -> 2926165685776
	2926165675024 [label=CudnnBatchNormBackward0]
	2926165682608 -> 2926165675024
	2926165682608 [label=ConvolutionBackward0]
	2926165684048 -> 2926165682608
	2926165682512 -> 2926165682608
	2926047648016 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2926047648016 -> 2926165682512
	2926165682512 [label=AccumulateGrad]
	2926165685248 -> 2926165675024
	2926047648112 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2926047648112 -> 2926165685248
	2926165685248 [label=AccumulateGrad]
	2926165682416 -> 2926165675024
	2926047648208 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2926047648208 -> 2926165682416
	2926165682416 [label=AccumulateGrad]
	2926165679440 -> 2926165681648
	2926047649744 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2926047649744 -> 2926165679440
	2926165679440 [label=AccumulateGrad]
	2926165672720 -> 2926165684960
	2926047649840 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2926047649840 -> 2926165672720
	2926165672720 [label=AccumulateGrad]
	2926165683184 -> 2926165684960
	2926047649936 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2926047649936 -> 2926165683184
	2926165683184 [label=AccumulateGrad]
	2926165676896 -> 2926165683376
	2926047650320 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2926047650320 -> 2926165676896
	2926165676896 [label=AccumulateGrad]
	2926165677952 -> 2926165683040
	2926047650416 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2926047650416 -> 2926165677952
	2926165677952 [label=AccumulateGrad]
	2926165674784 -> 2926165683040
	2926047650512 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2926047650512 -> 2926165674784
	2926165674784 [label=AccumulateGrad]
	2926165678912 -> 2926165676800
	2926165675744 -> 2926165681264
	2926047651472 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2926047651472 -> 2926165675744
	2926165675744 [label=AccumulateGrad]
	2926165682992 -> 2926165685200
	2926047651568 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2926047651568 -> 2926165682992
	2926165682992 [label=AccumulateGrad]
	2926165675072 -> 2926165685200
	2926047651664 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2926047651664 -> 2926165675072
	2926165675072 [label=AccumulateGrad]
	2926165684384 -> 2926165683856
	2926047652048 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2926047652048 -> 2926165684384
	2926165684384 [label=AccumulateGrad]
	2926165675936 -> 2926165672048
	2926047652144 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2926047652144 -> 2926165675936
	2926165675936 [label=AccumulateGrad]
	2926165683664 -> 2926165672048
	2926047652240 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2926047652240 -> 2926165683664
	2926165683664 [label=AccumulateGrad]
	2926165677280 -> 2926165682464
	2926165677280 [label=CudnnBatchNormBackward0]
	2926165685632 -> 2926165677280
	2926165685632 [label=ConvolutionBackward0]
	2926165675216 -> 2926165685632
	2926165682176 -> 2926165685632
	2926047650896 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2926047650896 -> 2926165682176
	2926165682176 [label=AccumulateGrad]
	2926165675696 -> 2926165677280
	2926047650992 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2926047650992 -> 2926165675696
	2926165675696 [label=AccumulateGrad]
	2926165680976 -> 2926165677280
	2926047651088 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2926047651088 -> 2926165680976
	2926165680976 [label=AccumulateGrad]
	2926165684864 -> 2926165684192
	2926047652624 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2926047652624 -> 2926165684864
	2926165684864 [label=AccumulateGrad]
	2926165679104 -> 2926165679968
	2926047652720 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2926047652720 -> 2926165679104
	2926165679104 [label=AccumulateGrad]
	2926165676512 -> 2926165679968
	2926047652816 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2926047652816 -> 2926165676512
	2926165676512 [label=AccumulateGrad]
	2926165674064 -> 2926165678960
	2926047653200 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2926047653200 -> 2926165674064
	2926165674064 [label=AccumulateGrad]
	2926165674016 -> 2926165679824
	2926047653296 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2926047653296 -> 2926165674016
	2926165674016 [label=AccumulateGrad]
	2926165683232 -> 2926165679824
	2926047653392 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2926047653392 -> 2926165683232
	2926165683232 [label=AccumulateGrad]
	2926165676848 -> 2926165673440
	2926208301568 -> 2926208291440
	2926208301568 [label=TBackward0]
	2926208300752 -> 2926208301568
	2926047654064 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2926047654064 -> 2926208300752
	2926208300752 [label=AccumulateGrad]
	2926208291440 -> 2926047123728
}
