digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2462557581200 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2462600099104 [label=AddmmBackward0]
	2462600098768 -> 2462600099104
	2462559745040 [label="fc.bias
 (19)" fillcolor=lightblue]
	2462559745040 -> 2462600098768
	2462600098768 [label=AccumulateGrad]
	2462600098144 -> 2462600099104
	2462600098144 [label=ViewBackward0]
	2462600097664 -> 2462600098144
	2462600097664 [label=MeanBackward1]
	2462600097328 -> 2462600097664
	2462600097328 [label=ReluBackward0]
	2462600096848 -> 2462600097328
	2462600096848 [label=AddBackward0]
	2462600096368 -> 2462600096848
	2462600096368 [label=CudnnBatchNormBackward0]
	2462600095264 -> 2462600096368
	2462600095264 [label=ConvolutionBackward0]
	2462600094304 -> 2462600095264
	2462600094304 [label=ReluBackward0]
	2462600089216 -> 2462600094304
	2462600089216 [label=CudnnBatchNormBackward0]
	2462600092720 -> 2462600089216
	2462600092720 [label=ConvolutionBackward0]
	2462600101792 -> 2462600092720
	2462600101792 [label=ReluBackward0]
	2462600101600 -> 2462600101792
	2462600101600 [label=CudnnBatchNormBackward0]
	2462600101408 -> 2462600101600
	2462600101408 [label=ConvolutionBackward0]
	2462600096224 -> 2462600101408
	2462600096224 [label=ReluBackward0]
	2462600101120 -> 2462600096224
	2462600101120 [label=AddBackward0]
	2462600100928 -> 2462600101120
	2462600100928 [label=CudnnBatchNormBackward0]
	2462600100976 -> 2462600100928
	2462600100976 [label=ConvolutionBackward0]
	2462600100592 -> 2462600100976
	2462600100592 [label=ReluBackward0]
	2462600100352 -> 2462600100592
	2462600100352 [label=CudnnBatchNormBackward0]
	2462600100304 -> 2462600100352
	2462600100304 [label=ConvolutionBackward0]
	2462600099968 -> 2462600100304
	2462600099968 [label=ReluBackward0]
	2462600100016 -> 2462600099968
	2462600100016 [label=CudnnBatchNormBackward0]
	2462600099920 -> 2462600100016
	2462600099920 [label=ConvolutionBackward0]
	2462600101072 -> 2462600099920
	2462600101072 [label=ReluBackward0]
	2462600099536 -> 2462600101072
	2462600099536 [label=AddBackward0]
	2462600099440 -> 2462600099536
	2462600099440 [label=CudnnBatchNormBackward0]
	2462600099008 -> 2462600099440
	2462600099008 [label=ConvolutionBackward0]
	2462600098864 -> 2462600099008
	2462600098864 [label=ReluBackward0]
	2462600098672 -> 2462600098864
	2462600098672 [label=CudnnBatchNormBackward0]
	2462600098336 -> 2462600098672
	2462600098336 [label=ConvolutionBackward0]
	2462600098480 -> 2462600098336
	2462600098480 [label=ReluBackward0]
	2462600098048 -> 2462600098480
	2462600098048 [label=CudnnBatchNormBackward0]
	2462600097952 -> 2462600098048
	2462600097952 [label=ConvolutionBackward0]
	2462600097760 -> 2462600097952
	2462600097760 [label=ReluBackward0]
	2462600097376 -> 2462600097760
	2462600097376 [label=AddBackward0]
	2462600097616 -> 2462600097376
	2462600097616 [label=CudnnBatchNormBackward0]
	2462600097280 -> 2462600097616
	2462600097280 [label=ConvolutionBackward0]
	2462600096992 -> 2462600097280
	2462600096992 [label=ReluBackward0]
	2462600097040 -> 2462600096992
	2462600097040 [label=CudnnBatchNormBackward0]
	2462600096752 -> 2462600097040
	2462600096752 [label=ConvolutionBackward0]
	2462600096656 -> 2462600096752
	2462600096656 [label=ReluBackward0]
	2462600096320 -> 2462600096656
	2462600096320 [label=CudnnBatchNormBackward0]
	2462600096128 -> 2462600096320
	2462600096128 [label=ConvolutionBackward0]
	2462600097472 -> 2462600096128
	2462600097472 [label=ReluBackward0]
	2462600095840 -> 2462600097472
	2462600095840 [label=AddBackward0]
	2462600095648 -> 2462600095840
	2462600095648 [label=CudnnBatchNormBackward0]
	2462600095696 -> 2462600095648
	2462600095696 [label=ConvolutionBackward0]
	2462600095312 -> 2462600095696
	2462600095312 [label=ReluBackward0]
	2462600095072 -> 2462600095312
	2462600095072 [label=CudnnBatchNormBackward0]
	2462600095024 -> 2462600095072
	2462600095024 [label=ConvolutionBackward0]
	2462600094688 -> 2462600095024
	2462600094688 [label=ReluBackward0]
	2462600094736 -> 2462600094688
	2462600094736 [label=CudnnBatchNormBackward0]
	2462600094640 -> 2462600094736
	2462600094640 [label=ConvolutionBackward0]
	2462600095792 -> 2462600094640
	2462600095792 [label=ReluBackward0]
	2462600094256 -> 2462600095792
	2462600094256 [label=AddBackward0]
	2462600094160 -> 2462600094256
	2462600094160 [label=CudnnBatchNormBackward0]
	2462600091856 -> 2462600094160
	2462600091856 [label=ConvolutionBackward0]
	2462600089984 -> 2462600091856
	2462600089984 [label=ReluBackward0]
	2462600092624 -> 2462600089984
	2462600092624 [label=CudnnBatchNormBackward0]
	2462600089408 -> 2462600092624
	2462600089408 [label=ConvolutionBackward0]
	2462600090032 -> 2462600089408
	2462600090032 [label=ReluBackward0]
	2462600093680 -> 2462600090032
	2462600093680 [label=CudnnBatchNormBackward0]
	2462600089360 -> 2462600093680
	2462600089360 [label=ConvolutionBackward0]
	2462600094064 -> 2462600089360
	2462600094064 [label=ReluBackward0]
	2462600091328 -> 2462600094064
	2462600091328 [label=AddBackward0]
	2462600091088 -> 2462600091328
	2462600091088 [label=CudnnBatchNormBackward0]
	2462600087200 -> 2462600091088
	2462600087200 [label=ConvolutionBackward0]
	2462600092912 -> 2462600087200
	2462600092912 [label=ReluBackward0]
	2462600088208 -> 2462600092912
	2462600088208 [label=CudnnBatchNormBackward0]
	2462600093008 -> 2462600088208
	2462600093008 [label=ConvolutionBackward0]
	2462600090944 -> 2462600093008
	2462600090944 [label=ReluBackward0]
	2462600088400 -> 2462600090944
	2462600088400 [label=CudnnBatchNormBackward0]
	2462600093968 -> 2462600088400
	2462600093968 [label=ConvolutionBackward0]
	2462600085856 -> 2462600093968
	2462600085856 [label=ReluBackward0]
	2462600091712 -> 2462600085856
	2462600091712 [label=AddBackward0]
	2462600089120 -> 2462600091712
	2462600089120 [label=CudnnBatchNormBackward0]
	2462600090608 -> 2462600089120
	2462600090608 [label=ConvolutionBackward0]
	2462600085712 -> 2462600090608
	2462600085712 [label=ReluBackward0]
	2462600087008 -> 2462600085712
	2462600087008 [label=CudnnBatchNormBackward0]
	2462600093344 -> 2462600087008
	2462600093344 [label=ConvolutionBackward0]
	2462600086624 -> 2462600093344
	2462600086624 [label=ReluBackward0]
	2462600086576 -> 2462600086624
	2462600086576 [label=CudnnBatchNormBackward0]
	2462600090464 -> 2462600086576
	2462600090464 [label=ConvolutionBackward0]
	2462600088160 -> 2462600090464
	2462600088160 [label=ReluBackward0]
	2462600092336 -> 2462600088160
	2462600092336 [label=AddBackward0]
	2462600088592 -> 2462600092336
	2462600088592 [label=CudnnBatchNormBackward0]
	2462600092240 -> 2462600088592
	2462600092240 [label=ConvolutionBackward0]
	2462600090800 -> 2462600092240
	2462600090800 [label=ReluBackward0]
	2462625446256 -> 2462600090800
	2462625446256 [label=CudnnBatchNormBackward0]
	2462625445440 -> 2462625446256
	2462625445440 [label=ConvolutionBackward0]
	2462625445824 -> 2462625445440
	2462625445824 [label=ReluBackward0]
	2462625447648 -> 2462625445824
	2462625447648 [label=CudnnBatchNormBackward0]
	2462625446928 -> 2462625447648
	2462625446928 [label=ConvolutionBackward0]
	2462625447072 -> 2462625446928
	2462625447072 [label=ReluBackward0]
	2462625446688 -> 2462625447072
	2462625446688 [label=AddBackward0]
	2462625446112 -> 2462625446688
	2462625446112 [label=CudnnBatchNormBackward0]
	2462625446832 -> 2462625446112
	2462625446832 [label=ConvolutionBackward0]
	2462558052000 -> 2462625446832
	2462558052000 [label=ReluBackward0]
	2462558051664 -> 2462558052000
	2462558051664 [label=CudnnBatchNormBackward0]
	2462558051184 -> 2462558051664
	2462558051184 [label=ConvolutionBackward0]
	2462558050224 -> 2462558051184
	2462558050224 [label=ReluBackward0]
	2462558049120 -> 2462558050224
	2462558049120 [label=CudnnBatchNormBackward0]
	2462558048640 -> 2462558049120
	2462558048640 [label=ConvolutionBackward0]
	2462625445872 -> 2462558048640
	2462625445872 [label=ReluBackward0]
	2462558047200 -> 2462625445872
	2462558047200 [label=AddBackward0]
	2462558046720 -> 2462558047200
	2462558046720 [label=CudnnBatchNormBackward0]
	2462558046384 -> 2462558046720
	2462558046384 [label=ConvolutionBackward0]
	2462558045424 -> 2462558046384
	2462558045424 [label=ReluBackward0]
	2462558044320 -> 2462558045424
	2462558044320 [label=CudnnBatchNormBackward0]
	2462558052576 -> 2462558044320
	2462558052576 [label=ConvolutionBackward0]
	2462558052288 -> 2462558052576
	2462558052288 [label=ReluBackward0]
	2462558052336 -> 2462558052288
	2462558052336 [label=CudnnBatchNormBackward0]
	2462558052048 -> 2462558052336
	2462558052048 [label=ConvolutionBackward0]
	2462558047344 -> 2462558052048
	2462558047344 [label=ReluBackward0]
	2462558051856 -> 2462558047344
	2462558051856 [label=AddBackward0]
	2462558051568 -> 2462558051856
	2462558051568 [label=CudnnBatchNormBackward0]
	2462558051328 -> 2462558051568
	2462558051328 [label=ConvolutionBackward0]
	2462558051136 -> 2462558051328
	2462558051136 [label=ReluBackward0]
	2462558050752 -> 2462558051136
	2462558050752 [label=CudnnBatchNormBackward0]
	2462558050992 -> 2462558050752
	2462558050992 [label=ConvolutionBackward0]
	2462558050608 -> 2462558050992
	2462558050608 [label=ReluBackward0]
	2462558050368 -> 2462558050608
	2462558050368 [label=CudnnBatchNormBackward0]
	2462558050320 -> 2462558050368
	2462558050320 [label=ConvolutionBackward0]
	2462558051616 -> 2462558050320
	2462558051616 [label=ReluBackward0]
	2462558049888 -> 2462558051616
	2462558049888 [label=AddBackward0]
	2462558049840 -> 2462558049888
	2462558049840 [label=CudnnBatchNormBackward0]
	2462558049648 -> 2462558049840
	2462558049648 [label=ConvolutionBackward0]
	2462558049552 -> 2462558049648
	2462558049552 [label=ReluBackward0]
	2462558049216 -> 2462558049552
	2462558049216 [label=CudnnBatchNormBackward0]
	2462558049024 -> 2462558049216
	2462558049024 [label=ConvolutionBackward0]
	2462558048880 -> 2462558049024
	2462558048880 [label=ReluBackward0]
	2462558048688 -> 2462558048880
	2462558048688 [label=CudnnBatchNormBackward0]
	2462558048352 -> 2462558048688
	2462558048352 [label=ConvolutionBackward0]
	2462558048496 -> 2462558048352
	2462558048496 [label=ReluBackward0]
	2462558048064 -> 2462558048496
	2462558048064 [label=AddBackward0]
	2462558047968 -> 2462558048064
	2462558047968 [label=CudnnBatchNormBackward0]
	2462558048016 -> 2462558047968
	2462558048016 [label=ConvolutionBackward0]
	2462558047392 -> 2462558048016
	2462558047392 [label=ReluBackward0]
	2462558047440 -> 2462558047392
	2462558047440 [label=CudnnBatchNormBackward0]
	2462558047296 -> 2462558047440
	2462558047296 [label=ConvolutionBackward0]
	2462558047008 -> 2462558047296
	2462558047008 [label=ReluBackward0]
	2462558047056 -> 2462558047008
	2462558047056 [label=CudnnBatchNormBackward0]
	2462558046768 -> 2462558047056
	2462558046768 [label=ConvolutionBackward0]
	2462558047872 -> 2462558046768
	2462558047872 [label=ReluBackward0]
	2462558046576 -> 2462558047872
	2462558046576 [label=AddBackward0]
	2462558046288 -> 2462558046576
	2462558046288 [label=CudnnBatchNormBackward0]
	2462558046048 -> 2462558046288
	2462558046048 [label=ConvolutionBackward0]
	2462558045856 -> 2462558046048
	2462558045856 [label=ReluBackward0]
	2462558045472 -> 2462558045856
	2462558045472 [label=CudnnBatchNormBackward0]
	2462558045712 -> 2462558045472
	2462558045712 [label=ConvolutionBackward0]
	2462558045328 -> 2462558045712
	2462558045328 [label=ReluBackward0]
	2462558045088 -> 2462558045328
	2462558045088 [label=CudnnBatchNormBackward0]
	2462558045040 -> 2462558045088
	2462558045040 [label=ConvolutionBackward0]
	2462558046336 -> 2462558045040
	2462558046336 [label=ReluBackward0]
	2462558044608 -> 2462558046336
	2462558044608 [label=AddBackward0]
	2462558044560 -> 2462558044608
	2462558044560 [label=CudnnBatchNormBackward0]
	2462558044368 -> 2462558044560
	2462558044368 [label=ConvolutionBackward0]
	2462558053056 -> 2462558044368
	2462558053056 [label=ReluBackward0]
	2462558053392 -> 2462558053056
	2462558053392 [label=CudnnBatchNormBackward0]
	2462558053488 -> 2462558053392
	2462558053488 [label=ConvolutionBackward0]
	2462558053680 -> 2462558053488
	2462558053680 [label=ReluBackward0]
	2462558053824 -> 2462558053680
	2462558053824 [label=CudnnBatchNormBackward0]
	2462558053920 -> 2462558053824
	2462558053920 [label=ConvolutionBackward0]
	2462558054112 -> 2462558053920
	2462558054112 [label=MaxPool2DWithIndicesBackward0]
	2462558054256 -> 2462558054112
	2462558054256 [label=ReluBackward0]
	2462558054352 -> 2462558054256
	2462558054352 [label=CudnnBatchNormBackward0]
	2462558054448 -> 2462558054352
	2462558054448 [label=ConvolutionBackward0]
	2462558054640 -> 2462558054448
	2462559744848 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2462559744848 -> 2462558054640
	2462558054640 [label=AccumulateGrad]
	2462558054400 -> 2462558054352
	2462574215248 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2462574215248 -> 2462558054400
	2462558054400 [label=AccumulateGrad]
	2462558054160 -> 2462558054352
	2462574215344 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2462574215344 -> 2462558054160
	2462558054160 [label=AccumulateGrad]
	2462558054064 -> 2462558053920
	2462575033808 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2462575033808 -> 2462558054064
	2462558054064 [label=AccumulateGrad]
	2462558053872 -> 2462558053824
	2462575033712 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2462575033712 -> 2462558053872
	2462558053872 [label=AccumulateGrad]
	2462558053728 -> 2462558053824
	2462575032944 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2462575032944 -> 2462558053728
	2462558053728 [label=AccumulateGrad]
	2462558053632 -> 2462558053488
	2462575033040 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462575033040 -> 2462558053632
	2462558053632 [label=AccumulateGrad]
	2462558053440 -> 2462558053392
	2462575033136 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2462575033136 -> 2462558053440
	2462558053440 [label=AccumulateGrad]
	2462558053296 -> 2462558053392
	2462575033232 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2462575033232 -> 2462558053296
	2462558053296 [label=AccumulateGrad]
	2462558053200 -> 2462558044368
	2462575033520 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2462575033520 -> 2462558053200
	2462558053200 [label=AccumulateGrad]
	2462558044416 -> 2462558044560
	2462575032560 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2462575032560 -> 2462558044416
	2462558044416 [label=AccumulateGrad]
	2462558044656 -> 2462558044560
	2462575032464 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2462575032464 -> 2462558044656
	2462558044656 [label=AccumulateGrad]
	2462558044752 -> 2462558044608
	2462558044752 [label=CudnnBatchNormBackward0]
	2462558053584 -> 2462558044752
	2462558053584 [label=ConvolutionBackward0]
	2462558054112 -> 2462558053584
	2462558053968 -> 2462558053584
	2462574215632 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2462574215632 -> 2462558053968
	2462558053968 [label=AccumulateGrad]
	2462558044272 -> 2462558044752
	2462575034192 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2462575034192 -> 2462558044272
	2462558044272 [label=AccumulateGrad]
	2462558044224 -> 2462558044752
	2462575033904 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2462575033904 -> 2462558044224
	2462558044224 [label=AccumulateGrad]
	2462558044704 -> 2462558045040
	2462575032272 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2462575032272 -> 2462558044704
	2462558044704 [label=AccumulateGrad]
	2462558045232 -> 2462558045088
	2462575031792 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2462575031792 -> 2462558045232
	2462558045232 [label=AccumulateGrad]
	2462558045184 -> 2462558045088
	2462575030064 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2462575030064 -> 2462558045184
	2462558045184 [label=AccumulateGrad]
	2462558045376 -> 2462558045712
	2462575032368 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462575032368 -> 2462558045376
	2462558045376 [label=AccumulateGrad]
	2462558045568 -> 2462558045472
	2462575032176 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2462575032176 -> 2462558045568
	2462558045568 [label=AccumulateGrad]
	2462558045808 -> 2462558045472
	2462560061776 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2462560061776 -> 2462558045808
	2462558045808 [label=AccumulateGrad]
	2462558046096 -> 2462558046048
	2462560062160 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2462560062160 -> 2462558046096
	2462558046096 [label=AccumulateGrad]
	2462558045952 -> 2462558046288
	2462560062256 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2462560062256 -> 2462558045952
	2462558045952 [label=AccumulateGrad]
	2462558046144 -> 2462558046288
	2462560062352 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2462560062352 -> 2462558046144
	2462558046144 [label=AccumulateGrad]
	2462558046336 -> 2462558046576
	2462558046672 -> 2462558046768
	2462560062736 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2462560062736 -> 2462558046672
	2462558046672 [label=AccumulateGrad]
	2462558046816 -> 2462558047056
	2462560062832 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2462560062832 -> 2462558046816
	2462558046816 [label=AccumulateGrad]
	2462558047152 -> 2462558047056
	2462560062928 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2462560062928 -> 2462558047152
	2462558047152 [label=AccumulateGrad]
	2462558046912 -> 2462558047296
	2462560063312 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462560063312 -> 2462558046912
	2462558046912 [label=AccumulateGrad]
	2462558047536 -> 2462558047440
	2462560063408 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2462560063408 -> 2462558047536
	2462558047536 [label=AccumulateGrad]
	2462558047488 -> 2462558047440
	2462560063504 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2462560063504 -> 2462558047488
	2462558047488 [label=AccumulateGrad]
	2462558047584 -> 2462558048016
	2462560063888 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2462560063888 -> 2462558047584
	2462558047584 [label=AccumulateGrad]
	2462558047920 -> 2462558047968
	2462560063984 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2462560063984 -> 2462558047920
	2462558047920 [label=AccumulateGrad]
	2462558048112 -> 2462558047968
	2462560064080 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2462560064080 -> 2462558048112
	2462558048112 [label=AccumulateGrad]
	2462558047872 -> 2462558048064
	2462558048400 -> 2462558048352
	2462560065040 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2462560065040 -> 2462558048400
	2462558048400 [label=AccumulateGrad]
	2462558048544 -> 2462558048688
	2462560065136 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2462560065136 -> 2462558048544
	2462558048544 [label=AccumulateGrad]
	2462558048976 -> 2462558048688
	2462560065232 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2462560065232 -> 2462558048976
	2462558048976 [label=AccumulateGrad]
	2462558049072 -> 2462558049024
	2462560065616 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2462560065616 -> 2462558049072
	2462558049072 [label=AccumulateGrad]
	2462558049168 -> 2462558049216
	2462560065712 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2462560065712 -> 2462558049168
	2462558049168 [label=AccumulateGrad]
	2462558049360 -> 2462558049216
	2462560065808 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2462560065808 -> 2462558049360
	2462558049360 [label=AccumulateGrad]
	2462558049408 -> 2462558049648
	2462560066192 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2462560066192 -> 2462558049408
	2462558049408 [label=AccumulateGrad]
	2462558049696 -> 2462558049840
	2462560066288 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2462560066288 -> 2462558049696
	2462558049696 [label=AccumulateGrad]
	2462558049936 -> 2462558049840
	2462560066384 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2462560066384 -> 2462558049936
	2462558049936 [label=AccumulateGrad]
	2462558050032 -> 2462558049888
	2462558050032 [label=CudnnBatchNormBackward0]
	2462558048928 -> 2462558050032
	2462558048928 [label=ConvolutionBackward0]
	2462558048496 -> 2462558048928
	2462558048448 -> 2462558048928
	2462560064464 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2462560064464 -> 2462558048448
	2462558048448 [label=AccumulateGrad]
	2462558049312 -> 2462558050032
	2462560064560 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2462560064560 -> 2462558049312
	2462558049312 [label=AccumulateGrad]
	2462558049504 -> 2462558050032
	2462560064656 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2462560064656 -> 2462558049504
	2462558049504 [label=AccumulateGrad]
	2462558049984 -> 2462558050320
	2462560066768 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2462560066768 -> 2462558049984
	2462558049984 [label=AccumulateGrad]
	2462558050512 -> 2462558050368
	2462560066864 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2462560066864 -> 2462558050512
	2462558050512 [label=AccumulateGrad]
	2462558050464 -> 2462558050368
	2462560066960 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2462560066960 -> 2462558050464
	2462558050464 [label=AccumulateGrad]
	2462558050656 -> 2462558050992
	2462560067344 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2462560067344 -> 2462558050656
	2462558050656 [label=AccumulateGrad]
	2462558050848 -> 2462558050752
	2462560067440 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2462560067440 -> 2462558050848
	2462558050848 [label=AccumulateGrad]
	2462558051088 -> 2462558050752
	2462560067536 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2462560067536 -> 2462558051088
	2462558051088 [label=AccumulateGrad]
	2462558051376 -> 2462558051328
	2462560067920 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2462560067920 -> 2462558051376
	2462558051376 [label=AccumulateGrad]
	2462558051232 -> 2462558051568
	2462560068016 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2462560068016 -> 2462558051232
	2462558051232 [label=AccumulateGrad]
	2462558051424 -> 2462558051568
	2462560068112 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2462560068112 -> 2462558051424
	2462558051424 [label=AccumulateGrad]
	2462558051616 -> 2462558051856
	2462558051952 -> 2462558052048
	2462560068496 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2462560068496 -> 2462558051952
	2462558051952 [label=AccumulateGrad]
	2462558052096 -> 2462558052336
	2462560068592 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2462560068592 -> 2462558052096
	2462558052096 [label=AccumulateGrad]
	2462558052432 -> 2462558052336
	2462560068688 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2462560068688 -> 2462558052432
	2462558052432 [label=AccumulateGrad]
	2462558052192 -> 2462558052576
	2462560069072 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2462560069072 -> 2462558052192
	2462558052192 [label=AccumulateGrad]
	2462558044464 -> 2462558044320
	2462560069168 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2462560069168 -> 2462558044464
	2462558044464 [label=AccumulateGrad]
	2462558044800 -> 2462558044320
	2462560069264 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2462560069264 -> 2462558044800
	2462558044800 [label=AccumulateGrad]
	2462558045280 -> 2462558046384
	2462560069648 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2462560069648 -> 2462558045280
	2462558045280 [label=AccumulateGrad]
	2462558046240 -> 2462558046720
	2462560069744 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2462560069744 -> 2462558046240
	2462558046240 [label=AccumulateGrad]
	2462558046864 -> 2462558046720
	2462560069840 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2462560069840 -> 2462558046864
	2462558046864 [label=AccumulateGrad]
	2462558047344 -> 2462558047200
	2462558047680 -> 2462558048640
	2462560070224 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2462560070224 -> 2462558047680
	2462558047680 [label=AccumulateGrad]
	2462558049264 -> 2462558049120
	2462560070320 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2462560070320 -> 2462558049264
	2462558049264 [label=AccumulateGrad]
	2462558049600 -> 2462558049120
	2462560070416 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2462560070416 -> 2462558049600
	2462558049600 [label=AccumulateGrad]
	2462558050080 -> 2462558051184
	2462560070800 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2462560070800 -> 2462558050080
	2462558050080 [label=AccumulateGrad]
	2462558051040 -> 2462558051664
	2462560070896 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2462560070896 -> 2462558051040
	2462558051040 [label=AccumulateGrad]
	2462558052144 -> 2462558051664
	2462560070992 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2462560070992 -> 2462558052144
	2462558052144 [label=AccumulateGrad]
	2462558052480 -> 2462625446832
	2462560071376 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2462560071376 -> 2462558052480
	2462558052480 [label=AccumulateGrad]
	2462625447024 -> 2462625446112
	2462560071472 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2462560071472 -> 2462625447024
	2462625447024 [label=AccumulateGrad]
	2462625445920 -> 2462625446112
	2462560071568 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2462560071568 -> 2462625445920
	2462625445920 [label=AccumulateGrad]
	2462625445872 -> 2462625446688
	2462625447168 -> 2462625446928
	2462560072528 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2462560072528 -> 2462625447168
	2462625447168 [label=AccumulateGrad]
	2462625447120 -> 2462625447648
	2462560072624 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2462560072624 -> 2462625447120
	2462625447120 [label=AccumulateGrad]
	2462625447744 -> 2462625447648
	2462560072720 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2462560072720 -> 2462625447744
	2462625447744 [label=AccumulateGrad]
	2462625447264 -> 2462625445440
	2462560073104 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462560073104 -> 2462625447264
	2462625447264 [label=AccumulateGrad]
	2462625445632 -> 2462625446256
	2462560073200 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2462560073200 -> 2462625445632
	2462625445632 [label=AccumulateGrad]
	2462625446592 -> 2462625446256
	2462560073296 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2462560073296 -> 2462625446592
	2462625446592 [label=AccumulateGrad]
	2462600085664 -> 2462600092240
	2462560073680 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2462560073680 -> 2462600085664
	2462600085664 [label=AccumulateGrad]
	2462600090416 -> 2462600088592
	2462560073776 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2462560073776 -> 2462600090416
	2462600090416 [label=AccumulateGrad]
	2462600090128 -> 2462600088592
	2462560073872 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2462560073872 -> 2462600090128
	2462600090128 [label=AccumulateGrad]
	2462600093920 -> 2462600092336
	2462600093920 [label=CudnnBatchNormBackward0]
	2462600199472 -> 2462600093920
	2462600199472 [label=ConvolutionBackward0]
	2462625447072 -> 2462600199472
	2462625447360 -> 2462600199472
	2462560071952 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2462560071952 -> 2462625447360
	2462625447360 [label=AccumulateGrad]
	2462600088256 -> 2462600093920
	2462560072048 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2462560072048 -> 2462600088256
	2462600088256 [label=AccumulateGrad]
	2462625447552 -> 2462600093920
	2462560072144 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2462560072144 -> 2462625447552
	2462625447552 [label=AccumulateGrad]
	2462600088448 -> 2462600090464
	2462560074256 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2462560074256 -> 2462600088448
	2462600088448 [label=AccumulateGrad]
	2462600087824 -> 2462600086576
	2462560074352 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2462560074352 -> 2462600087824
	2462600087824 [label=AccumulateGrad]
	2462600092288 -> 2462600086576
	2462560074448 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2462560074448 -> 2462600092288
	2462600092288 [label=AccumulateGrad]
	2462600090896 -> 2462600093344
	2462560074832 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462560074832 -> 2462600090896
	2462600090896 [label=AccumulateGrad]
	2462600085568 -> 2462600087008
	2462560074928 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2462560074928 -> 2462600085568
	2462600085568 [label=AccumulateGrad]
	2462600087344 -> 2462600087008
	2462560075024 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2462560075024 -> 2462600087344
	2462600087344 [label=AccumulateGrad]
	2462600086720 -> 2462600090608
	2462560075408 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2462560075408 -> 2462600086720
	2462600086720 [label=AccumulateGrad]
	2462600093152 -> 2462600089120
	2462560075504 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2462560075504 -> 2462600093152
	2462600093152 [label=AccumulateGrad]
	2462600087632 -> 2462600089120
	2462560075600 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2462560075600 -> 2462600087632
	2462600087632 [label=AccumulateGrad]
	2462600088160 -> 2462600091712
	2462600087248 -> 2462600093968
	2462559731984 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2462559731984 -> 2462600087248
	2462600087248 [label=AccumulateGrad]
	2462600090992 -> 2462600088400
	2462559732080 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2462559732080 -> 2462600090992
	2462600090992 [label=AccumulateGrad]
	2462600087392 -> 2462600088400
	2462559732176 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2462559732176 -> 2462600087392
	2462600087392 [label=AccumulateGrad]
	2462600088736 -> 2462600093008
	2462559732560 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462559732560 -> 2462600088736
	2462600088736 [label=AccumulateGrad]
	2462600093776 -> 2462600088208
	2462559732656 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2462559732656 -> 2462600093776
	2462600093776 [label=AccumulateGrad]
	2462600088016 -> 2462600088208
	2462559732752 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2462559732752 -> 2462600088016
	2462600088016 [label=AccumulateGrad]
	2462600089840 -> 2462600087200
	2462559733136 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2462559733136 -> 2462600089840
	2462600089840 [label=AccumulateGrad]
	2462600087872 -> 2462600091088
	2462559733232 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2462559733232 -> 2462600087872
	2462600087872 [label=AccumulateGrad]
	2462600086768 -> 2462600091088
	2462559733328 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2462559733328 -> 2462600086768
	2462600086768 [label=AccumulateGrad]
	2462600085856 -> 2462600091328
	2462600088304 -> 2462600089360
	2462559733712 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2462559733712 -> 2462600088304
	2462600088304 [label=AccumulateGrad]
	2462600092432 -> 2462600093680
	2462559733808 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2462559733808 -> 2462600092432
	2462600092432 [label=AccumulateGrad]
	2462600086480 -> 2462600093680
	2462559733904 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2462559733904 -> 2462600086480
	2462600086480 [label=AccumulateGrad]
	2462600087920 -> 2462600089408
	2462559734288 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462559734288 -> 2462600087920
	2462600087920 [label=AccumulateGrad]
	2462600090560 -> 2462600092624
	2462559734384 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2462559734384 -> 2462600090560
	2462600090560 [label=AccumulateGrad]
	2462600093440 -> 2462600092624
	2462559734480 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2462559734480 -> 2462600093440
	2462600093440 [label=AccumulateGrad]
	2462600093200 -> 2462600091856
	2462559734864 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2462559734864 -> 2462600093200
	2462600093200 [label=AccumulateGrad]
	2462600087536 -> 2462600094160
	2462559734960 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2462559734960 -> 2462600087536
	2462600087536 [label=AccumulateGrad]
	2462600091280 -> 2462600094160
	2462559735056 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2462559735056 -> 2462600091280
	2462600091280 [label=AccumulateGrad]
	2462600094064 -> 2462600094256
	2462600094016 -> 2462600094640
	2462559735440 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2462559735440 -> 2462600094016
	2462600094016 [label=AccumulateGrad]
	2462600094544 -> 2462600094736
	2462559735536 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2462559735536 -> 2462600094544
	2462600094544 [label=AccumulateGrad]
	2462600094496 -> 2462600094736
	2462559735632 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2462559735632 -> 2462600094496
	2462600094496 [label=AccumulateGrad]
	2462600094832 -> 2462600095024
	2462559736016 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462559736016 -> 2462600094832
	2462600094832 [label=AccumulateGrad]
	2462600095216 -> 2462600095072
	2462559736112 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2462559736112 -> 2462600095216
	2462600095216 [label=AccumulateGrad]
	2462600095168 -> 2462600095072
	2462559736208 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2462559736208 -> 2462600095168
	2462600095168 [label=AccumulateGrad]
	2462600095360 -> 2462600095696
	2462559736592 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2462559736592 -> 2462600095360
	2462600095360 [label=AccumulateGrad]
	2462600095552 -> 2462600095648
	2462559736688 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2462559736688 -> 2462600095552
	2462600095552 [label=AccumulateGrad]
	2462600095456 -> 2462600095648
	2462559736784 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2462559736784 -> 2462600095456
	2462600095456 [label=AccumulateGrad]
	2462600095792 -> 2462600095840
	2462600095984 -> 2462600096128
	2462559737168 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2462559737168 -> 2462600095984
	2462600095984 [label=AccumulateGrad]
	2462600096272 -> 2462600096320
	2462559737264 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2462559737264 -> 2462600096272
	2462600096272 [label=AccumulateGrad]
	2462600096464 -> 2462600096320
	2462559737360 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2462559737360 -> 2462600096464
	2462600096464 [label=AccumulateGrad]
	2462600096512 -> 2462600096752
	2462559737744 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462559737744 -> 2462600096512
	2462600096512 [label=AccumulateGrad]
	2462600096800 -> 2462600097040
	2462559737840 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2462559737840 -> 2462600096800
	2462600096800 [label=AccumulateGrad]
	2462600097136 -> 2462600097040
	2462559737936 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2462559737936 -> 2462600097136
	2462600097136 [label=AccumulateGrad]
	2462600096896 -> 2462600097280
	2462559738320 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2462559738320 -> 2462600096896
	2462600096896 [label=AccumulateGrad]
	2462600097520 -> 2462600097616
	2462559738416 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2462559738416 -> 2462600097520
	2462600097520 [label=AccumulateGrad]
	2462600097424 -> 2462600097616
	2462559738512 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2462559738512 -> 2462600097424
	2462600097424 [label=AccumulateGrad]
	2462600097472 -> 2462600097376
	2462600098000 -> 2462600097952
	2462559739472 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2462559739472 -> 2462600098000
	2462600098000 [label=AccumulateGrad]
	2462600097856 -> 2462600098048
	2462559739568 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2462559739568 -> 2462600097856
	2462600097856 [label=AccumulateGrad]
	2462600098240 -> 2462600098048
	2462559739664 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2462559739664 -> 2462600098240
	2462600098240 [label=AccumulateGrad]
	2462600098384 -> 2462600098336
	2462559740048 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462559740048 -> 2462600098384
	2462600098384 [label=AccumulateGrad]
	2462600098528 -> 2462600098672
	2462559740144 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2462559740144 -> 2462600098528
	2462600098528 [label=AccumulateGrad]
	2462600098960 -> 2462600098672
	2462559740240 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2462559740240 -> 2462600098960
	2462600098960 [label=AccumulateGrad]
	2462600099056 -> 2462600099008
	2462559740624 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2462559740624 -> 2462600099056
	2462600099056 [label=AccumulateGrad]
	2462600099152 -> 2462600099440
	2462559740720 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2462559740720 -> 2462600099152
	2462600099152 [label=AccumulateGrad]
	2462600099200 -> 2462600099440
	2462559740816 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2462559740816 -> 2462600099200
	2462600099200 [label=AccumulateGrad]
	2462600099344 -> 2462600099536
	2462600099344 [label=CudnnBatchNormBackward0]
	2462600098576 -> 2462600099344
	2462600098576 [label=ConvolutionBackward0]
	2462600097760 -> 2462600098576
	2462600098096 -> 2462600098576
	2462559738896 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2462559738896 -> 2462600098096
	2462600098096 [label=AccumulateGrad]
	2462600098912 -> 2462600099344
	2462559738992 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2462559738992 -> 2462600098912
	2462600098912 [label=AccumulateGrad]
	2462600098816 -> 2462600099344
	2462559739088 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2462559739088 -> 2462600098816
	2462600098816 [label=AccumulateGrad]
	2462600099296 -> 2462600099920
	2462559741200 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2462559741200 -> 2462600099296
	2462600099296 [label=AccumulateGrad]
	2462600099824 -> 2462600100016
	2462559741296 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2462559741296 -> 2462600099824
	2462600099824 [label=AccumulateGrad]
	2462600099776 -> 2462600100016
	2462559741392 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2462559741392 -> 2462600099776
	2462600099776 [label=AccumulateGrad]
	2462600100112 -> 2462600100304
	2462559741776 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462559741776 -> 2462600100112
	2462600100112 [label=AccumulateGrad]
	2462600100496 -> 2462600100352
	2462559741872 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2462559741872 -> 2462600100496
	2462600100496 [label=AccumulateGrad]
	2462600100448 -> 2462600100352
	2462559741968 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2462559741968 -> 2462600100448
	2462600100448 [label=AccumulateGrad]
	2462600100640 -> 2462600100976
	2462559742352 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2462559742352 -> 2462600100640
	2462600100640 [label=AccumulateGrad]
	2462600100832 -> 2462600100928
	2462559742448 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2462559742448 -> 2462600100832
	2462600100832 [label=AccumulateGrad]
	2462600100736 -> 2462600100928
	2462559742544 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2462559742544 -> 2462600100736
	2462600100736 [label=AccumulateGrad]
	2462600101072 -> 2462600101120
	2462600101264 -> 2462600101408
	2462559742928 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2462559742928 -> 2462600101264
	2462600101264 [label=AccumulateGrad]
	2462600101552 -> 2462600101600
	2462559743024 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2462559743024 -> 2462600101552
	2462600101552 [label=AccumulateGrad]
	2462600101744 -> 2462600101600
	2462559743120 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2462559743120 -> 2462600101744
	2462600101744 [label=AccumulateGrad]
	2462600101696 -> 2462600092720
	2462559743504 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462559743504 -> 2462600101696
	2462600101696 [label=AccumulateGrad]
	2462600086336 -> 2462600089216
	2462559743600 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2462559743600 -> 2462600086336
	2462600086336 [label=AccumulateGrad]
	2462600094448 -> 2462600089216
	2462559743696 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2462559743696 -> 2462600094448
	2462600094448 [label=AccumulateGrad]
	2462600094928 -> 2462600095264
	2462559744080 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2462559744080 -> 2462600094928
	2462600094928 [label=AccumulateGrad]
	2462600095888 -> 2462600096368
	2462559744176 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2462559744176 -> 2462600095888
	2462600095888 [label=AccumulateGrad]
	2462600095744 -> 2462600096368
	2462559744272 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2462559744272 -> 2462600095744
	2462600095744 [label=AccumulateGrad]
	2462600096224 -> 2462600096848
	2462600098288 -> 2462600099104
	2462600098288 [label=TBackward0]
	2462600096704 -> 2462600098288
	2462559744944 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2462559744944 -> 2462600096704
	2462600096704 [label=AccumulateGrad]
	2462600099104 -> 2462557581200
}
