digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1673726932176 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1673723211120 [label=AddmmBackward0]
	1673723211264 -> 1673723211120
	1673723442192 [label="fc.bias
 (19)" fillcolor=lightblue]
	1673723442192 -> 1673723211264
	1673723211264 [label=AccumulateGrad]
	1673723211312 -> 1673723211120
	1673723211312 [label=ViewBackward0]
	1673723211408 -> 1673723211312
	1673723211408 [label=MeanBackward1]
	1673723211552 -> 1673723211408
	1673723211552 [label=ReluBackward0]
	1673723211648 -> 1673723211552
	1673723211648 [label=AddBackward0]
	1673723211744 -> 1673723211648
	1673723211744 [label=CudnnBatchNormBackward0]
	1673723211888 -> 1673723211744
	1673723211888 [label=ConvolutionBackward0]
	1673723212080 -> 1673723211888
	1673723212080 [label=ReluBackward0]
	1673723212224 -> 1673723212080
	1673723212224 [label=CudnnBatchNormBackward0]
	1673723212320 -> 1673723212224
	1673723212320 [label=ConvolutionBackward0]
	1673723211696 -> 1673723212320
	1673723211696 [label=ReluBackward0]
	1673723212464 -> 1673723211696
	1673723212464 [label=AddBackward0]
	1673723212560 -> 1673723212464
	1673723212560 [label=CudnnBatchNormBackward0]
	1673723212704 -> 1673723212560
	1673723212704 [label=ConvolutionBackward0]
	1673723212896 -> 1673723212704
	1673723212896 [label=ReluBackward0]
	1673723213040 -> 1673723212896
	1673723213040 [label=CudnnBatchNormBackward0]
	1673723213136 -> 1673723213040
	1673723213136 [label=ConvolutionBackward0]
	1673723213328 -> 1673723213136
	1673723213328 [label=ReluBackward0]
	1673723213472 -> 1673723213328
	1673723213472 [label=AddBackward0]
	1673723213568 -> 1673723213472
	1673723213568 [label=CudnnBatchNormBackward0]
	1673723213712 -> 1673723213568
	1673723213712 [label=ConvolutionBackward0]
	1673723213904 -> 1673723213712
	1673723213904 [label=ReluBackward0]
	1673723214048 -> 1673723213904
	1673723214048 [label=CudnnBatchNormBackward0]
	1673723214144 -> 1673723214048
	1673723214144 [label=ConvolutionBackward0]
	1673723213520 -> 1673723214144
	1673723213520 [label=ReluBackward0]
	1673723214432 -> 1673723213520
	1673723214432 [label=AddBackward0]
	1673723214528 -> 1673723214432
	1673723214528 [label=CudnnBatchNormBackward0]
	1673723214672 -> 1673723214528
	1673723214672 [label=ConvolutionBackward0]
	1673723214864 -> 1673723214672
	1673723214864 [label=ReluBackward0]
	1673723215008 -> 1673723214864
	1673723215008 [label=CudnnBatchNormBackward0]
	1673723215104 -> 1673723215008
	1673723215104 [label=ConvolutionBackward0]
	1673723215248 -> 1673723215104
	1673723215248 [label=ReluBackward0]
	1673612212976 -> 1673723215248
	1673612212976 [label=AddBackward0]
	1673612214224 -> 1673612212976
	1673612214224 [label=CudnnBatchNormBackward0]
	1673612213888 -> 1673612214224
	1673612213888 [label=ConvolutionBackward0]
	1673612213696 -> 1673612213888
	1673612213696 [label=ReluBackward0]
	1673612211776 -> 1673612213696
	1673612211776 [label=CudnnBatchNormBackward0]
	1673612211920 -> 1673612211776
	1673612211920 [label=ConvolutionBackward0]
	1673612212064 -> 1673612211920
	1673612212064 [label=ReluBackward0]
	1673612212736 -> 1673612212064
	1673612212736 [label=AddBackward0]
	1673612212832 -> 1673612212736
	1673612212832 [label=CudnnBatchNormBackward0]
	1673612213072 -> 1673612212832
	1673612213072 [label=ConvolutionBackward0]
	1673612213552 -> 1673612213072
	1673612213552 [label=ReluBackward0]
	1673612212304 -> 1673612213552
	1673612212304 [label=CudnnBatchNormBackward0]
	1673612213456 -> 1673612212304
	1673612213456 [label=ConvolutionBackward0]
	1673727034368 -> 1673612213456
	1673727034368 [label=ReluBackward0]
	1673727034512 -> 1673727034368
	1673727034512 [label=AddBackward0]
	1673727034608 -> 1673727034512
	1673727034608 [label=CudnnBatchNormBackward0]
	1673727034752 -> 1673727034608
	1673727034752 [label=ConvolutionBackward0]
	1673727034944 -> 1673727034752
	1673727034944 [label=ReluBackward0]
	1673727035088 -> 1673727034944
	1673727035088 [label=CudnnBatchNormBackward0]
	1673727035184 -> 1673727035088
	1673727035184 [label=ConvolutionBackward0]
	1673727034560 -> 1673727035184
	1673727034560 [label=ReluBackward0]
	1673727035472 -> 1673727034560
	1673727035472 [label=AddBackward0]
	1673727035568 -> 1673727035472
	1673727035568 [label=CudnnBatchNormBackward0]
	1673727035712 -> 1673727035568
	1673727035712 [label=ConvolutionBackward0]
	1673727035904 -> 1673727035712
	1673727035904 [label=ReluBackward0]
	1673727036048 -> 1673727035904
	1673727036048 [label=CudnnBatchNormBackward0]
	1673727036144 -> 1673727036048
	1673727036144 [label=ConvolutionBackward0]
	1673727035520 -> 1673727036144
	1673727035520 [label=MaxPool2DWithIndicesBackward0]
	1673727036432 -> 1673727035520
	1673727036432 [label=ReluBackward0]
	1673727036528 -> 1673727036432
	1673727036528 [label=CudnnBatchNormBackward0]
	1673727036624 -> 1673727036528
	1673727036624 [label=ConvolutionBackward0]
	1673727036816 -> 1673727036624
	1673612308176 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1673612308176 -> 1673727036816
	1673727036816 [label=AccumulateGrad]
	1673727036576 -> 1673727036528
	1673612296368 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1673612296368 -> 1673727036576
	1673727036576 [label=AccumulateGrad]
	1673727036240 -> 1673727036528
	1673612296464 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1673612296464 -> 1673727036240
	1673727036240 [label=AccumulateGrad]
	1673727036336 -> 1673727036144
	1673612296848 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673612296848 -> 1673727036336
	1673727036336 [label=AccumulateGrad]
	1673727036096 -> 1673727036048
	1673612296944 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1673612296944 -> 1673727036096
	1673727036096 [label=AccumulateGrad]
	1673727035952 -> 1673727036048
	1673612297040 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1673612297040 -> 1673727035952
	1673727035952 [label=AccumulateGrad]
	1673727035856 -> 1673727035712
	1673612297424 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673612297424 -> 1673727035856
	1673727035856 [label=AccumulateGrad]
	1673727035664 -> 1673727035568
	1673612297520 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1673612297520 -> 1673727035664
	1673727035664 [label=AccumulateGrad]
	1673727035616 -> 1673727035568
	1673612297616 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1673612297616 -> 1673727035616
	1673727035616 [label=AccumulateGrad]
	1673727035520 -> 1673727035472
	1673727035376 -> 1673727035184
	1673612298000 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673612298000 -> 1673727035376
	1673727035376 [label=AccumulateGrad]
	1673727035136 -> 1673727035088
	1673612298096 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1673612298096 -> 1673727035136
	1673727035136 [label=AccumulateGrad]
	1673727034992 -> 1673727035088
	1673612298192 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1673612298192 -> 1673727034992
	1673727034992 [label=AccumulateGrad]
	1673727034896 -> 1673727034752
	1673612298576 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1673612298576 -> 1673727034896
	1673727034896 [label=AccumulateGrad]
	1673727034704 -> 1673727034608
	1673612298672 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1673612298672 -> 1673727034704
	1673727034704 [label=AccumulateGrad]
	1673727034656 -> 1673727034608
	1673612298768 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1673612298768 -> 1673727034656
	1673727034656 [label=AccumulateGrad]
	1673727034560 -> 1673727034512
	1673727034320 -> 1673612213456
	1673612299728 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1673612299728 -> 1673727034320
	1673727034320 [label=AccumulateGrad]
	1673727034176 -> 1673612212304
	1673612299824 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1673612299824 -> 1673727034176
	1673727034176 [label=AccumulateGrad]
	1673727034128 -> 1673612212304
	1673612299920 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1673612299920 -> 1673727034128
	1673727034128 [label=AccumulateGrad]
	1673612213648 -> 1673612213072
	1673612300304 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1673612300304 -> 1673612213648
	1673612213648 [label=AccumulateGrad]
	1673612213024 -> 1673612212832
	1673612300400 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1673612300400 -> 1673612213024
	1673612213024 [label=AccumulateGrad]
	1673612212880 -> 1673612212832
	1673612300496 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1673612300496 -> 1673612212880
	1673612212880 [label=AccumulateGrad]
	1673612212784 -> 1673612212736
	1673612212784 [label=CudnnBatchNormBackward0]
	1673612213360 -> 1673612212784
	1673612213360 [label=ConvolutionBackward0]
	1673727034368 -> 1673612213360
	1673727034416 -> 1673612213360
	1673612299152 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1673612299152 -> 1673727034416
	1673727034416 [label=AccumulateGrad]
	1673612213168 -> 1673612212784
	1673612299248 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1673612299248 -> 1673612213168
	1673612213168 [label=AccumulateGrad]
	1673612213120 -> 1673612212784
	1673612299344 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1673612299344 -> 1673612213120
	1673612213120 [label=AccumulateGrad]
	1673612212496 -> 1673612211920
	1673612300880 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1673612300880 -> 1673612212496
	1673612212496 [label=AccumulateGrad]
	1673612211824 -> 1673612211776
	1673612300976 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1673612300976 -> 1673612211824
	1673612211824 [label=AccumulateGrad]
	1673612211632 -> 1673612211776
	1673612301072 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1673612301072 -> 1673612211632
	1673612211632 [label=AccumulateGrad]
	1673612213744 -> 1673612213888
	1673612301456 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1673612301456 -> 1673612213744
	1673612213744 [label=AccumulateGrad]
	1673612213936 -> 1673612214224
	1673612301552 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1673612301552 -> 1673612213936
	1673612213936 [label=AccumulateGrad]
	1673612214128 -> 1673612214224
	1673612301648 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1673612301648 -> 1673612214128
	1673612214128 [label=AccumulateGrad]
	1673612212064 -> 1673612212976
	1673723215200 -> 1673723215104
	1673612302608 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1673612302608 -> 1673723215200
	1673723215200 [label=AccumulateGrad]
	1673723215056 -> 1673723215008
	1673612302704 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1673612302704 -> 1673723215056
	1673723215056 [label=AccumulateGrad]
	1673723214912 -> 1673723215008
	1673612302800 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1673612302800 -> 1673723214912
	1673723214912 [label=AccumulateGrad]
	1673723214816 -> 1673723214672
	1673612303184 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1673612303184 -> 1673723214816
	1673723214816 [label=AccumulateGrad]
	1673723214624 -> 1673723214528
	1673612303280 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1673612303280 -> 1673723214624
	1673723214624 [label=AccumulateGrad]
	1673723214576 -> 1673723214528
	1673612303376 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1673612303376 -> 1673723214576
	1673723214576 [label=AccumulateGrad]
	1673723214480 -> 1673723214432
	1673723214480 [label=CudnnBatchNormBackward0]
	1673723215152 -> 1673723214480
	1673723215152 [label=ConvolutionBackward0]
	1673723215248 -> 1673723215152
	1673612213504 -> 1673723215152
	1673612302032 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1673612302032 -> 1673612213504
	1673612213504 [label=AccumulateGrad]
	1673723214768 -> 1673723214480
	1673612302128 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1673612302128 -> 1673723214768
	1673723214768 [label=AccumulateGrad]
	1673723214720 -> 1673723214480
	1673612302224 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1673612302224 -> 1673723214720
	1673723214720 [label=AccumulateGrad]
	1673723214336 -> 1673723214144
	1673612303760 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1673612303760 -> 1673723214336
	1673723214336 [label=AccumulateGrad]
	1673723214096 -> 1673723214048
	1673612303856 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1673612303856 -> 1673723214096
	1673723214096 [label=AccumulateGrad]
	1673723213952 -> 1673723214048
	1673612303952 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1673612303952 -> 1673723213952
	1673723213952 [label=AccumulateGrad]
	1673723213856 -> 1673723213712
	1673612304336 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1673612304336 -> 1673723213856
	1673723213856 [label=AccumulateGrad]
	1673723213664 -> 1673723213568
	1673612304432 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1673612304432 -> 1673723213664
	1673723213664 [label=AccumulateGrad]
	1673723213616 -> 1673723213568
	1673612304528 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1673612304528 -> 1673723213616
	1673723213616 [label=AccumulateGrad]
	1673723213520 -> 1673723213472
	1673723213280 -> 1673723213136
	1673612305488 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1673612305488 -> 1673723213280
	1673723213280 [label=AccumulateGrad]
	1673723213088 -> 1673723213040
	1673612305584 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1673612305584 -> 1673723213088
	1673723213088 [label=AccumulateGrad]
	1673723212944 -> 1673723213040
	1673612305680 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1673612305680 -> 1673723212944
	1673723212944 [label=AccumulateGrad]
	1673723212848 -> 1673723212704
	1673612306064 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1673612306064 -> 1673723212848
	1673723212848 [label=AccumulateGrad]
	1673723212656 -> 1673723212560
	1673612306160 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1673612306160 -> 1673723212656
	1673723212656 [label=AccumulateGrad]
	1673723212608 -> 1673723212560
	1673612306256 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1673612306256 -> 1673723212608
	1673723212608 [label=AccumulateGrad]
	1673723212512 -> 1673723212464
	1673723212512 [label=CudnnBatchNormBackward0]
	1673723213232 -> 1673723212512
	1673723213232 [label=ConvolutionBackward0]
	1673723213328 -> 1673723213232
	1673723213376 -> 1673723213232
	1673612304912 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1673612304912 -> 1673723213376
	1673723213376 [label=AccumulateGrad]
	1673723212800 -> 1673723212512
	1673612305008 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1673612305008 -> 1673723212800
	1673723212800 [label=AccumulateGrad]
	1673723212752 -> 1673723212512
	1673612305104 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1673612305104 -> 1673723212752
	1673723212752 [label=AccumulateGrad]
	1673723210544 -> 1673723212320
	1673612306640 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1673612306640 -> 1673723210544
	1673723210544 [label=AccumulateGrad]
	1673723212272 -> 1673723212224
	1673612306736 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1673612306736 -> 1673723212272
	1673723212272 [label=AccumulateGrad]
	1673723212128 -> 1673723212224
	1673612306832 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1673612306832 -> 1673723212128
	1673723212128 [label=AccumulateGrad]
	1673723212032 -> 1673723211888
	1673612307216 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1673612307216 -> 1673723212032
	1673723212032 [label=AccumulateGrad]
	1673723211840 -> 1673723211744
	1673612307312 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1673612307312 -> 1673723211840
	1673723211840 [label=AccumulateGrad]
	1673723211792 -> 1673723211744
	1673612307408 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1673612307408 -> 1673723211792
	1673723211792 [label=AccumulateGrad]
	1673723211696 -> 1673723211648
	1673723211360 -> 1673723211120
	1673723211360 [label=TBackward0]
	1673723211600 -> 1673723211360
	1673612308080 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1673612308080 -> 1673723211600
	1673723211600 [label=AccumulateGrad]
	1673723211120 -> 1673726932176
}
