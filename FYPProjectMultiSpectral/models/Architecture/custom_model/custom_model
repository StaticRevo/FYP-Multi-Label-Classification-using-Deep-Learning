digraph {
	graph [size="89.1,89.1"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1665120816976 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1665120875616 [label=AddmmBackward0]
	1665120875232 -> 1665120875616
	1665093464528 [label="29.bias
 (19)" fillcolor=lightblue]
	1665093464528 -> 1665120875232
	1665120875232 [label=AccumulateGrad]
	1665120874560 -> 1665120875616
	1665120874560 [label=NativeDropoutBackward0]
	1665120874032 -> 1665120874560
	1665120874032 [label=ViewBackward0]
	1665120873648 -> 1665120874032
	1665120873648 [label=MeanBackward1]
	1665120873120 -> 1665120873648
	1665120873120 [label=MulBackward0]
	1665120872592 -> 1665120873120
	1665120872592 [label=MulBackward0]
	1665120871392 -> 1665120872592
	1665120871392 [label=ReluBackward0]
	1665120871008 -> 1665120871392
	1665120871008 [label=AddBackward0]
	1665120870480 -> 1665120871008
	1665120870480 [label=MulBackward0]
	1665120869280 -> 1665120870480
	1665120869280 [label=DivBackward0]
	1665120868752 -> 1665120869280
	1665120868752 [label=CudnnBatchNormBackward0]
	1665120868224 -> 1665120868752
	1665120868224 [label=ConvolutionBackward0]
	1665120867168 -> 1665120868224
	1665120867168 [label=NativeDropoutBackward0]
	1665120866784 -> 1665120867168
	1665120866784 [label=ReluBackward0]
	1665120866256 -> 1665120866784
	1665120866256 [label=CudnnBatchNormBackward0]
	1665120865728 -> 1665120866256
	1665120865728 [label=ConvolutionBackward0]
	1665120870336 -> 1665120865728
	1665120870336 [label=ReluBackward0]
	1665120864144 -> 1665120870336
	1665120864144 [label=CudnnBatchNormBackward0]
	1665120863616 -> 1665120864144
	1665120863616 [label=ConvolutionBackward0]
	1665120862560 -> 1665120863616
	1665120862560 [label=ConvolutionBackward0]
	1665120878160 -> 1665120862560
	1665120878160 [label=MulBackward0]
	1665120878208 -> 1665120878160
	1665120878208 [label=ReluBackward0]
	1665120877824 -> 1665120878208
	1665120877824 [label=AddBackward0]
	1665120877632 -> 1665120877824
	1665120877632 [label=MulBackward0]
	1665120877680 -> 1665120877632
	1665120877680 [label=DivBackward0]
	1665120877584 -> 1665120877680
	1665120877584 [label=CudnnBatchNormBackward0]
	1665120877248 -> 1665120877584
	1665120877248 [label=ConvolutionBackward0]
	1665120877152 -> 1665120877248
	1665120877152 [label=NativeDropoutBackward0]
	1665120876768 -> 1665120877152
	1665120876768 [label=ReluBackward0]
	1665120876576 -> 1665120876768
	1665120876576 [label=CudnnBatchNormBackward0]
	1665120876480 -> 1665120876576
	1665120876480 [label=ConvolutionBackward0]
	1665120877776 -> 1665120876480
	1665120877776 [label=ConvolutionBackward0]
	1665120876048 -> 1665120877776
	1665120876048 [label=CatBackward0]
	1665120875904 -> 1665120876048
	1665120875904 [label=ConvolutionBackward0]
	1665120875520 -> 1665120875904
	1665120875520 [label=ReluBackward0]
	1665120875376 -> 1665120875520
	1665120875376 [label=CudnnBatchNormBackward0]
	1665120875184 -> 1665120875376
	1665120875184 [label=ConvolutionBackward0]
	1665120874896 -> 1665120875184
	1665120874896 [label=ConvolutionBackward0]
	1665120874944 -> 1665120874896
	1665120874944 [label=MulBackward0]
	1665120874464 -> 1665120874944
	1665120874464 [label=ReluBackward0]
	1665120874512 -> 1665120874464
	1665120874512 [label=AddBackward0]
	1665120874416 -> 1665120874512
	1665120874416 [label=MulBackward0]
	1665120873936 -> 1665120874416
	1665120873936 [label=DivBackward0]
	1665120873840 -> 1665120873936
	1665120873840 [label=CudnnBatchNormBackward0]
	1665120873792 -> 1665120873840
	1665120873792 [label=ConvolutionBackward0]
	1665120873408 -> 1665120873792
	1665120873408 [label=NativeDropoutBackward0]
	1665120873456 -> 1665120873408
	1665120873456 [label=ReluBackward0]
	1665120873360 -> 1665120873456
	1665120873360 [label=CudnnBatchNormBackward0]
	1665120873024 -> 1665120873360
	1665120873024 [label=ConvolutionBackward0]
	1665120874320 -> 1665120873024
	1665120874320 [label=ConvolutionBackward0]
	1665120872832 -> 1665120874320
	1665120872832 [label=CatBackward0]
	1665120872160 -> 1665120872832
	1665120872160 [label=ConvolutionBackward0]
	1665120872304 -> 1665120872160
	1665120872304 [label=ReluBackward0]
	1665120871632 -> 1665120872304
	1665120871632 [label=CudnnBatchNormBackward0]
	1665120871872 -> 1665120871632
	1665120871872 [label=ConvolutionBackward0]
	1665120871440 -> 1665120871872
	1665120871440 [label=ConvolutionBackward0]
	1665120871200 -> 1665120871440
	1665120871200 [label=MulBackward0]
	1665120871248 -> 1665120871200
	1665120871248 [label=MulBackward0]
	1665120870768 -> 1665120871248
	1665120870768 [label=MulBackward0]
	1665120870816 -> 1665120870768
	1665120870816 [label=ReluBackward0]
	1665120870432 -> 1665120870816
	1665120870432 [label=AddBackward0]
	1665120870240 -> 1665120870432
	1665120870240 [label=MulBackward0]
	1665120870288 -> 1665120870240
	1665120870288 [label=DivBackward0]
	1665120870192 -> 1665120870288
	1665120870192 [label=CudnnBatchNormBackward0]
	1665120869856 -> 1665120870192
	1665120869856 [label=ConvolutionBackward0]
	1665120869760 -> 1665120869856
	1665120869760 [label=NativeDropoutBackward0]
	1665120869376 -> 1665120869760
	1665120869376 [label=ReluBackward0]
	1665120869184 -> 1665120869376
	1665120869184 [label=CudnnBatchNormBackward0]
	1665120869088 -> 1665120869184
	1665120869088 [label=ConvolutionBackward0]
	1665120870384 -> 1665120869088
	1665120870384 [label=ReluBackward0]
	1665120868656 -> 1665120870384
	1665120868656 [label=CudnnBatchNormBackward0]
	1665120868560 -> 1665120868656
	1665120868560 [label=ConvolutionBackward0]
	1665120868320 -> 1665120868560
	1665120868320 [label=ConvolutionBackward0]
	1665120867936 -> 1665120868320
	1665120867936 [label=ReluBackward0]
	1665120867984 -> 1665120867936
	1665120867984 [label=CudnnBatchNormBackward0]
	1665120867792 -> 1665120867984
	1665120867792 [label=ConvolutionBackward0]
	1665120867504 -> 1665120867792
	1665061127856 [label="0.weight
 (16, 12, 1, 1)" fillcolor=lightblue]
	1665061127856 -> 1665120867504
	1665120867504 [label=AccumulateGrad]
	1665120868080 -> 1665120867984
	1665061128624 [label="1.weight
 (16)" fillcolor=lightblue]
	1665061128624 -> 1665120868080
	1665120868080 [label=AccumulateGrad]
	1665120868032 -> 1665120867984
	1665061127568 [label="1.bias
 (16)" fillcolor=lightblue]
	1665061127568 -> 1665120868032
	1665120868032 [label=AccumulateGrad]
	1665120868128 -> 1665120868320
	1665061128144 [label="3.depthwise.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	1665061128144 -> 1665120868128
	1665120868128 [label=AccumulateGrad]
	1665120868608 -> 1665120868560
	1665061128240 [label="3.pointwise.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	1665061128240 -> 1665120868608
	1665120868608 [label=AccumulateGrad]
	1665120868464 -> 1665120868656
	1665061128048 [label="4.weight
 (32)" fillcolor=lightblue]
	1665061128048 -> 1665120868464
	1665120868464 [label=AccumulateGrad]
	1665120869040 -> 1665120868656
	1665061128336 [label="4.bias
 (32)" fillcolor=lightblue]
	1665061128336 -> 1665120869040
	1665120869040 [label=AccumulateGrad]
	1665120868848 -> 1665120869088
	1665061126608 [label="6.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1665061126608 -> 1665120868848
	1665120868848 [label=AccumulateGrad]
	1665120868992 -> 1665120869184
	1665061127376 [label="6.bn1.weight
 (32)" fillcolor=lightblue]
	1665061127376 -> 1665120868992
	1665120868992 [label=AccumulateGrad]
	1665120869568 -> 1665120869184
	1665061126320 [label="6.bn1.bias
 (32)" fillcolor=lightblue]
	1665061126320 -> 1665120869568
	1665120869568 [label=AccumulateGrad]
	1665120869616 -> 1665120869856
	1665061126896 [label="6.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1665061126896 -> 1665120869616
	1665120869616 [label=AccumulateGrad]
	1665120869904 -> 1665120870192
	1665061126800 [label="6.bn2.weight
 (32)" fillcolor=lightblue]
	1665061126800 -> 1665120869904
	1665120869904 [label=AccumulateGrad]
	1665120870048 -> 1665120870192
	1665061126992 [label="6.bn2.bias
 (32)" fillcolor=lightblue]
	1665061126992 -> 1665120870048
	1665120870048 [label=AccumulateGrad]
	1665120870384 -> 1665120870432
	1665120870672 -> 1665120870768
	1665120870672 [label=ViewBackward0]
	1665120870144 -> 1665120870672
	1665120870144 [label=SigmoidBackward0]
	1665120869712 -> 1665120870144
	1665120869712 [label=AddmmBackward0]
	1665120869232 -> 1665120869712
	1665343437616 [label="7.fc2.bias
 (32)" fillcolor=lightblue]
	1665343437616 -> 1665120869232
	1665120869232 [label=AccumulateGrad]
	1665120869520 -> 1665120869712
	1665120869520 [label=ReluBackward0]
	1665120869664 -> 1665120869520
	1665120869664 [label=AddmmBackward0]
	1665120868800 -> 1665120869664
	1665343438768 [label="7.fc1.bias
 (2)" fillcolor=lightblue]
	1665343438768 -> 1665120868800
	1665120868800 [label=AccumulateGrad]
	1665120868704 -> 1665120869664
	1665120868704 [label=MeanBackward1]
	1665120867408 -> 1665120868704
	1665120867408 [label=ViewBackward0]
	1665120870816 -> 1665120867408
	1665120868512 -> 1665120869664
	1665120868512 [label=TBackward0]
	1665120867456 -> 1665120868512
	1665061126224 [label="7.fc1.weight
 (2, 32)" fillcolor=lightblue]
	1665061126224 -> 1665120867456
	1665120867456 [label=AccumulateGrad]
	1665120870720 -> 1665120869712
	1665120870720 [label=TBackward0]
	1665120867648 -> 1665120870720
	1665343438000 [label="7.fc2.weight
 (32, 2)" fillcolor=lightblue]
	1665343438000 -> 1665120867648
	1665120867648 [label=AccumulateGrad]
	1665120870912 -> 1665120871248
	1665120870912 [label=SigmoidBackward0]
	1665120870096 -> 1665120870912
	1665120870096 [label=ConvolutionBackward0]
	1665120867744 -> 1665120870096
	1665120867744 [label=SplitWithSizesBackward0]
	1665120869136 -> 1665120867744
	1665120869136 [label=ReluBackward0]
	1665120867552 -> 1665120869136
	1665120867552 [label=CudnnBatchNormBackward0]
	1665120867216 -> 1665120867552
	1665120867216 [label=ConvolutionBackward0]
	1665120867120 -> 1665120867216
	1665120867120 [label=CatBackward0]
	1665120866736 -> 1665120867120
	1665120866736 [label=AdaptiveAvgPool2DBackward0]
	1665120870768 -> 1665120866736
	1665120867024 -> 1665120867120
	1665120867024 [label=PermuteBackward0]
	1665120866688 -> 1665120867024
	1665120866688 [label=AdaptiveAvgPool2DBackward0]
	1665120870768 -> 1665120866688
	1665120866976 -> 1665120867216
	1665343436080 [label="8.conv1.weight
 (8, 32, 1, 1)" fillcolor=lightblue]
	1665343436080 -> 1665120866976
	1665120866976 [label=AccumulateGrad]
	1665120867264 -> 1665120867552
	1665343438480 [label="8.bn1.weight
 (8)" fillcolor=lightblue]
	1665343438480 -> 1665120867264
	1665120867264 [label=AccumulateGrad]
	1665120868176 -> 1665120867552
	1665343436272 [label="8.bn1.bias
 (8)" fillcolor=lightblue]
	1665343436272 -> 1665120868176
	1665120868176 [label=AccumulateGrad]
	1665120869328 -> 1665120870096
	1665343438672 [label="8.conv_h.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1665343438672 -> 1665120869328
	1665120869328 [label=AccumulateGrad]
	1665120871152 -> 1665120871200
	1665120871152 [label=PermuteBackward0]
	1665120870576 -> 1665120871152
	1665120870576 [label=SigmoidBackward0]
	1665120868272 -> 1665120870576
	1665120868272 [label=ConvolutionBackward0]
	1665120867744 -> 1665120868272
	1665120866880 -> 1665120868272
	1665343438384 [label="8.conv_w.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1665343438384 -> 1665120866880
	1665120866880 [label=AccumulateGrad]
	1665120871104 -> 1665120871440
	1665093456656 [label="9.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	1665093456656 -> 1665120871104
	1665120871104 [label=AccumulateGrad]
	1665120871488 -> 1665120871872
	1665093456752 [label="9.pointwise.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	1665093456752 -> 1665120871488
	1665120871488 [label=AccumulateGrad]
	1665120871728 -> 1665120871632
	1665343438576 [label="10.weight
 (64)" fillcolor=lightblue]
	1665343438576 -> 1665120871728
	1665120871728 [label=AccumulateGrad]
	1665120871968 -> 1665120871632
	1665093456848 [label="10.bias
 (64)" fillcolor=lightblue]
	1665093456848 -> 1665120871968
	1665120871968 [label=AccumulateGrad]
	1665120872208 -> 1665120872160
	1665093457424 [label="12.conv_dil1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1665093457424 -> 1665120872208
	1665120872208 [label=AccumulateGrad]
	1665120872400 -> 1665120872160
	1665093457520 [label="12.conv_dil1.bias
 (64)" fillcolor=lightblue]
	1665093457520 -> 1665120872400
	1665120872400 [label=AccumulateGrad]
	1665120872352 -> 1665120872832
	1665120872352 [label=ConvolutionBackward0]
	1665120872304 -> 1665120872352
	1665120871680 -> 1665120872352
	1665093457616 [label="12.conv_dil2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1665093457616 -> 1665120871680
	1665120871680 [label=AccumulateGrad]
	1665120871824 -> 1665120872352
	1665093457712 [label="12.conv_dil2.bias
 (64)" fillcolor=lightblue]
	1665093457712 -> 1665120871824
	1665120871824 [label=AccumulateGrad]
	1665120872496 -> 1665120872832
	1665120872496 [label=ConvolutionBackward0]
	1665120872304 -> 1665120872496
	1665120870624 -> 1665120872496
	1665093457808 [label="12.conv_dil3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1665093457808 -> 1665120870624
	1665120870624 [label=AccumulateGrad]
	1665120871776 -> 1665120872496
	1665093457904 [label="12.conv_dil3.bias
 (64)" fillcolor=lightblue]
	1665093457904 -> 1665120871776
	1665120871776 [label=AccumulateGrad]
	1665120872736 -> 1665120874320
	1665093458000 [label="12.fuse.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	1665093458000 -> 1665120872736
	1665120872736 [label=AccumulateGrad]
	1665120872688 -> 1665120874320
	1665093458096 [label="12.fuse.bias
 (64)" fillcolor=lightblue]
	1665093458096 -> 1665120872688
	1665120872688 [label=AccumulateGrad]
	1665120872928 -> 1665120873024
	1665093458192 [label="13.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1665093458192 -> 1665120872928
	1665120872928 [label=AccumulateGrad]
	1665120873072 -> 1665120873360
	1665093457328 [label="13.bn1.weight
 (64)" fillcolor=lightblue]
	1665093457328 -> 1665120873072
	1665120873072 [label=AccumulateGrad]
	1665120873216 -> 1665120873360
	1665093458288 [label="13.bn1.bias
 (64)" fillcolor=lightblue]
	1665093458288 -> 1665120873216
	1665120873216 [label=AccumulateGrad]
	1665120873552 -> 1665120873792
	1665093458768 [label="13.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1665093458768 -> 1665120873552
	1665120873552 [label=AccumulateGrad]
	1665120873984 -> 1665120873840
	1665093458672 [label="13.bn2.weight
 (64)" fillcolor=lightblue]
	1665093458672 -> 1665120873984
	1665120873984 [label=AccumulateGrad]
	1665120874128 -> 1665120873840
	1665093458864 [label="13.bn2.bias
 (64)" fillcolor=lightblue]
	1665093458864 -> 1665120874128
	1665120874128 [label=AccumulateGrad]
	1665120874320 -> 1665120874512
	1665120874608 -> 1665120874944
	1665120874608 [label=UnsqueezeBackward0]
	1665120874080 -> 1665120874608
	1665120874080 [label=UnsqueezeBackward0]
	1665120873888 -> 1665120874080
	1665120873888 [label=SigmoidBackward0]
	1665120872880 -> 1665120873888
	1665120872880 [label=SqueezeBackward1]
	1665120873312 -> 1665120872880
	1665120873312 [label=ConvolutionBackward0]
	1665120872016 -> 1665120873312
	1665120872016 [label=UnsqueezeBackward0]
	1665120867072 -> 1665120872016
	1665120867072 [label=SqueezeBackward1]
	1665120871344 -> 1665120867072
	1665120871344 [label=SqueezeBackward1]
	1665120866544 -> 1665120871344
	1665120866544 [label=MeanBackward1]
	1665120874464 -> 1665120866544
	1665120872784 -> 1665120873312
	1665093459344 [label="14.conv.weight
 (1, 1, 3)" fillcolor=lightblue]
	1665093459344 -> 1665120872784
	1665120872784 [label=AccumulateGrad]
	1665120874848 -> 1665120874896
	1665093459440 [label="15.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	1665093459440 -> 1665120874848
	1665120874848 [label=AccumulateGrad]
	1665120874800 -> 1665120875184
	1665093459536 [label="15.pointwise.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1665093459536 -> 1665120874800
	1665120874800 [label=AccumulateGrad]
	1665120875472 -> 1665120875376
	1665093459248 [label="16.weight
 (128)" fillcolor=lightblue]
	1665093459248 -> 1665120875472
	1665120875472 [label=AccumulateGrad]
	1665120875424 -> 1665120875376
	1665093459632 [label="16.bias
 (128)" fillcolor=lightblue]
	1665093459632 -> 1665120875424
	1665120875424 [label=AccumulateGrad]
	1665120875664 -> 1665120875904
	1665093460112 [label="18.conv_dil1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1665093460112 -> 1665120875664
	1665120875664 [label=AccumulateGrad]
	1665120875712 -> 1665120875904
	1665093460208 [label="18.conv_dil1.bias
 (128)" fillcolor=lightblue]
	1665093460208 -> 1665120875712
	1665120875712 [label=AccumulateGrad]
	1665120876096 -> 1665120876048
	1665120876096 [label=ConvolutionBackward0]
	1665120875520 -> 1665120876096
	1665120875136 -> 1665120876096
	1665093460304 [label="18.conv_dil2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1665093460304 -> 1665120875136
	1665120875136 [label=AccumulateGrad]
	1665120875568 -> 1665120876096
	1665093460400 [label="18.conv_dil2.bias
 (128)" fillcolor=lightblue]
	1665093460400 -> 1665120875568
	1665120875568 [label=AccumulateGrad]
	1665120875952 -> 1665120876048
	1665120875952 [label=ConvolutionBackward0]
	1665120875520 -> 1665120875952
	1665120874272 -> 1665120875952
	1665093460496 [label="18.conv_dil3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1665093460496 -> 1665120874272
	1665120874272 [label=AccumulateGrad]
	1665120874992 -> 1665120875952
	1665093460592 [label="18.conv_dil3.bias
 (128)" fillcolor=lightblue]
	1665093460592 -> 1665120874992
	1665120874992 [label=AccumulateGrad]
	1665120876192 -> 1665120877776
	1665093460688 [label="18.fuse.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	1665093460688 -> 1665120876192
	1665120876192 [label=AccumulateGrad]
	1665120876432 -> 1665120877776
	1665093460784 [label="18.fuse.bias
 (128)" fillcolor=lightblue]
	1665093460784 -> 1665120876432
	1665120876432 [label=AccumulateGrad]
	1665120876240 -> 1665120876480
	1665093460880 [label="19.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1665093460880 -> 1665120876240
	1665120876240 [label=AccumulateGrad]
	1665120876384 -> 1665120876576
	1665093460016 [label="19.bn1.weight
 (128)" fillcolor=lightblue]
	1665093460016 -> 1665120876384
	1665120876384 [label=AccumulateGrad]
	1665120876960 -> 1665120876576
	1665093460976 [label="19.bn1.bias
 (128)" fillcolor=lightblue]
	1665093460976 -> 1665120876960
	1665120876960 [label=AccumulateGrad]
	1665120877008 -> 1665120877248
	1665093461456 [label="19.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1665093461456 -> 1665120877008
	1665120877008 [label=AccumulateGrad]
	1665120877296 -> 1665120877584
	1665093461360 [label="19.bn2.weight
 (128)" fillcolor=lightblue]
	1665093461360 -> 1665120877296
	1665120877296 [label=AccumulateGrad]
	1665120877440 -> 1665120877584
	1665093461552 [label="19.bn2.bias
 (128)" fillcolor=lightblue]
	1665093461552 -> 1665120877440
	1665120877440 [label=AccumulateGrad]
	1665120877776 -> 1665120877824
	1665120878064 -> 1665120878160
	1665120878064 [label=SigmoidBackward0]
	1665120877536 -> 1665120878064
	1665120877536 [label=ConvolutionBackward0]
	1665120877104 -> 1665120877536
	1665120877104 [label=NativeDropoutBackward0]
	1665120876720 -> 1665120877104
	1665120876720 [label=ReluBackward0]
	1665120876528 -> 1665120876720
	1665120876528 [label=ConvolutionBackward0]
	1665120876000 -> 1665120876528
	1665120876000 [label=MeanBackward1]
	1665120878208 -> 1665120876000
	1665120875328 -> 1665120876528
	1665093462032 [label="20.fc1.weight
 (8, 128, 1, 1)" fillcolor=lightblue]
	1665093462032 -> 1665120875328
	1665120875328 [label=AccumulateGrad]
	1665120877488 -> 1665120877536
	1665093462128 [label="20.fc2.weight
 (128, 8, 1, 1)" fillcolor=lightblue]
	1665093462128 -> 1665120877488
	1665120877488 [label=AccumulateGrad]
	1665120878304 -> 1665120862560
	1665093462224 [label="21.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	1665093462224 -> 1665120878304
	1665120878304 [label=AccumulateGrad]
	1665120862416 -> 1665120863616
	1665093462320 [label="21.pointwise.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1665093462320 -> 1665120862416
	1665120862416 [label=AccumulateGrad]
	1665120863472 -> 1665120864144
	1665093461936 [label="22.weight
 (256)" fillcolor=lightblue]
	1665093461936 -> 1665120863472
	1665120863472 [label=AccumulateGrad]
	1665120865200 -> 1665120864144
	1665093462416 [label="22.bias
 (256)" fillcolor=lightblue]
	1665093462416 -> 1665120865200
	1665120865200 [label=AccumulateGrad]
	1665120864672 -> 1665120865728
	1665093462896 [label="24.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1665093462896 -> 1665120864672
	1665120864672 [label=AccumulateGrad]
	1665120865584 -> 1665120866256
	1665093462800 [label="24.bn1.weight
 (256)" fillcolor=lightblue]
	1665093462800 -> 1665120865584
	1665120865584 [label=AccumulateGrad]
	1665120867312 -> 1665120866256
	1665093462992 [label="24.bn1.bias
 (256)" fillcolor=lightblue]
	1665093462992 -> 1665120867312
	1665120867312 [label=AccumulateGrad]
	1665120867840 -> 1665120868224
	1665093463472 [label="24.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1665093463472 -> 1665120867840
	1665120867840 [label=AccumulateGrad]
	1665120868896 -> 1665120868752
	1665093463376 [label="24.bn2.weight
 (256)" fillcolor=lightblue]
	1665093463376 -> 1665120868896
	1665120868896 [label=AccumulateGrad]
	1665120869808 -> 1665120868752
	1665093463568 [label="24.bn2.bias
 (256)" fillcolor=lightblue]
	1665093463568 -> 1665120869808
	1665120869808 [label=AccumulateGrad]
	1665120870336 -> 1665120871008
	1665120872064 -> 1665120872592
	1665120872064 [label=ViewBackward0]
	1665120869952 -> 1665120872064
	1665120869952 [label=SigmoidBackward0]
	1665120868368 -> 1665120869952
	1665120868368 [label=AddmmBackward0]
	1665120865056 -> 1665120868368
	1665093464240 [label="25.channel_att.fc2.bias
 (256)" fillcolor=lightblue]
	1665093464240 -> 1665120865056
	1665120865056 [label=AccumulateGrad]
	1665120867696 -> 1665120868368
	1665120867696 [label=ReluBackward0]
	1665120866640 -> 1665120867696
	1665120866640 [label=AddmmBackward0]
	1665120864000 -> 1665120866640
	1665093464048 [label="25.channel_att.fc1.bias
 (16)" fillcolor=lightblue]
	1665093464048 -> 1665120864000
	1665120864000 [label=AccumulateGrad]
	1665120862944 -> 1665120866640
	1665120862944 [label=MeanBackward1]
	1665120878112 -> 1665120862944
	1665120878112 [label=ViewBackward0]
	1665120871392 -> 1665120878112
	1665120863088 -> 1665120866640
	1665120863088 [label=TBackward0]
	1665120876912 -> 1665120863088
	1665093463952 [label="25.channel_att.fc1.weight
 (16, 256)" fillcolor=lightblue]
	1665093463952 -> 1665120876912
	1665120876912 [label=AccumulateGrad]
	1665120870864 -> 1665120868368
	1665120870864 [label=TBackward0]
	1665120877056 -> 1665120870864
	1665093464144 [label="25.channel_att.fc2.weight
 (256, 16)" fillcolor=lightblue]
	1665093464144 -> 1665120877056
	1665120877056 [label=AccumulateGrad]
	1665120872448 -> 1665120873120
	1665120872448 [label=SigmoidBackward0]
	1665120869424 -> 1665120872448
	1665120869424 [label=ConvolutionBackward0]
	1665120878016 -> 1665120869424
	1665120878016 [label=CatBackward0]
	1665120864528 -> 1665120878016
	1665120864528 [label=MeanBackward1]
	1665120872592 -> 1665120864528
	1665120876624 -> 1665120878016
	1665120876624 [label=MaxBackward0]
	1665120872592 -> 1665120876624
	1665120866112 -> 1665120869424
	1665093464432 [label="25.spatial_att.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	1665093464432 -> 1665120866112
	1665120866112 [label=AccumulateGrad]
	1665120874704 -> 1665120875616
	1665120874704 [label=TBackward0]
	1665120872976 -> 1665120874704
	1665093464336 [label="29.weight
 (19, 256)" fillcolor=lightblue]
	1665093464336 -> 1665120872976
	1665120872976 [label=AccumulateGrad]
	1665120875616 -> 1665120816976
}
