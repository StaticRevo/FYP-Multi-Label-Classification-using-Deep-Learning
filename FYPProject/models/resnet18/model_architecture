digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2203677898800 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2203643437808 [label=SigmoidBackward0]
	2203643437664 -> 2203643437808
	2203643437664 [label=AddmmBackward0]
	2203643436272 -> 2203643437664
	2203677886608 [label="model.fc.bias
 (19)" fillcolor=lightblue]
	2203677886608 -> 2203643436272
	2203643436272 [label=AccumulateGrad]
	2203643437712 -> 2203643437664
	2203643437712 [label=ViewBackward0]
	2203643436224 -> 2203643437712
	2203643436224 [label=MeanBackward1]
	2203643438192 -> 2203643436224
	2203643438192 [label=ReluBackward0]
	2203643438288 -> 2203643438192
	2203643438288 [label=AddBackward0]
	2203643438384 -> 2203643438288
	2203643438384 [label=CudnnBatchNormBackward0]
	2203643438528 -> 2203643438384
	2203643438528 [label=ConvolutionBackward0]
	2203643438720 -> 2203643438528
	2203643438720 [label=ReluBackward0]
	2203643438864 -> 2203643438720
	2203643438864 [label=CudnnBatchNormBackward0]
	2203643438960 -> 2203643438864
	2203643438960 [label=ConvolutionBackward0]
	2203643438336 -> 2203643438960
	2203643438336 [label=ReluBackward0]
	2203643440208 -> 2203643438336
	2203643440208 [label=AddBackward0]
	2203643440304 -> 2203643440208
	2203643440304 [label=CudnnBatchNormBackward0]
	2203643440448 -> 2203643440304
	2203643440448 [label=ConvolutionBackward0]
	2203643440640 -> 2203643440448
	2203643440640 [label=ReluBackward0]
	2203643440784 -> 2203643440640
	2203643440784 [label=CudnnBatchNormBackward0]
	2203643440880 -> 2203643440784
	2203643440880 [label=ConvolutionBackward0]
	2203643441696 -> 2203643440880
	2203643441696 [label=ReluBackward0]
	2203643441840 -> 2203643441696
	2203643441840 [label=AddBackward0]
	2203643442032 -> 2203643441840
	2203643442032 [label=CudnnBatchNormBackward0]
	2203643442464 -> 2203643442032
	2203643442464 [label=ConvolutionBackward0]
	2203643436800 -> 2203643442464
	2203643436800 [label=ReluBackward0]
	2203643437040 -> 2203643436800
	2203643437040 [label=CudnnBatchNormBackward0]
	2203643437136 -> 2203643437040
	2203643437136 [label=ConvolutionBackward0]
	2203643441888 -> 2203643437136
	2203643441888 [label=ReluBackward0]
	2203643442944 -> 2203643441888
	2203643442944 [label=AddBackward0]
	2203643436944 -> 2203643442944
	2203643436944 [label=CudnnBatchNormBackward0]
	2203643429120 -> 2203643436944
	2203643429120 [label=ConvolutionBackward0]
	2203643442608 -> 2203643429120
	2203643442608 [label=ReluBackward0]
	2203643442752 -> 2203643442608
	2203643442752 [label=CudnnBatchNormBackward0]
	2203643442416 -> 2203643442752
	2203643442416 [label=ConvolutionBackward0]
	2203643443376 -> 2203643442416
	2203643443376 [label=ReluBackward0]
	2203643441504 -> 2203643443376
	2203643441504 [label=AddBackward0]
	2203643436752 -> 2203643441504
	2203643436752 [label=CudnnBatchNormBackward0]
	2203643436080 -> 2203643436752
	2203643436080 [label=ConvolutionBackward0]
	2203643435888 -> 2203643436080
	2203643435888 [label=ReluBackward0]
	2203643435744 -> 2203643435888
	2203643435744 [label=CudnnBatchNormBackward0]
	2203643435648 -> 2203643435744
	2203643435648 [label=ConvolutionBackward0]
	2203643436848 -> 2203643435648
	2203643436848 [label=ReluBackward0]
	2203643435360 -> 2203643436848
	2203643435360 [label=AddBackward0]
	2203643435264 -> 2203643435360
	2203643435264 [label=CudnnBatchNormBackward0]
	2203643435120 -> 2203643435264
	2203643435120 [label=ConvolutionBackward0]
	2203643434928 -> 2203643435120
	2203643434928 [label=ReluBackward0]
	2203643434784 -> 2203643434928
	2203643434784 [label=CudnnBatchNormBackward0]
	2203643434688 -> 2203643434784
	2203643434688 [label=ConvolutionBackward0]
	2203643434496 -> 2203643434688
	2203643434496 [label=ReluBackward0]
	2203643434352 -> 2203643434496
	2203643434352 [label=AddBackward0]
	2203643434256 -> 2203643434352
	2203643434256 [label=CudnnBatchNormBackward0]
	2203643434112 -> 2203643434256
	2203643434112 [label=ConvolutionBackward0]
	2203643433920 -> 2203643434112
	2203643433920 [label=ReluBackward0]
	2203643433776 -> 2203643433920
	2203643433776 [label=CudnnBatchNormBackward0]
	2203643433680 -> 2203643433776
	2203643433680 [label=ConvolutionBackward0]
	2203643434304 -> 2203643433680
	2203643434304 [label=ReluBackward0]
	2203643433392 -> 2203643434304
	2203643433392 [label=AddBackward0]
	2203643429168 -> 2203643433392
	2203643429168 [label=CudnnBatchNormBackward0]
	2203643429360 -> 2203643429168
	2203643429360 [label=ConvolutionBackward0]
	2203643429552 -> 2203643429360
	2203643429552 [label=ReluBackward0]
	2203643429696 -> 2203643429552
	2203643429696 [label=CudnnBatchNormBackward0]
	2203643429984 -> 2203643429696
	2203643429984 [label=ConvolutionBackward0]
	2203643433344 -> 2203643429984
	2203643433344 [label=MaxPool2DWithIndicesBackward0]
	2203643430272 -> 2203643433344
	2203643430272 [label=ReluBackward0]
	2203643430320 -> 2203643430272
	2203643430320 [label=CudnnBatchNormBackward0]
	2203643430464 -> 2203643430320
	2203643430464 [label=ConvolutionBackward0]
	2203643431088 -> 2203643430464
	2203677678032 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2203677678032 -> 2203643431088
	2203643431088 [label=AccumulateGrad]
	2203643430416 -> 2203643430320
	2203677678224 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	2203677678224 -> 2203643430416
	2203643430416 [label=AccumulateGrad]
	2203643430560 -> 2203643430320
	2203677678320 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	2203677678320 -> 2203643430560
	2203643430560 [label=AccumulateGrad]
	2203643430176 -> 2203643429984
	2203677678704 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2203677678704 -> 2203643430176
	2203643430176 [label=AccumulateGrad]
	2203643429936 -> 2203643429696
	2203677678800 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2203677678800 -> 2203643429936
	2203643429936 [label=AccumulateGrad]
	2203643429600 -> 2203643429696
	2203677678896 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2203677678896 -> 2203643429600
	2203643429600 [label=AccumulateGrad]
	2203643429504 -> 2203643429360
	2203677679280 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2203677679280 -> 2203643429504
	2203643429504 [label=AccumulateGrad]
	2203643429264 -> 2203643429168
	2203677679376 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2203677679376 -> 2203643429264
	2203643429264 [label=AccumulateGrad]
	2203643429216 -> 2203643429168
	2203677679472 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2203677679472 -> 2203643429216
	2203643429216 [label=AccumulateGrad]
	2203643433344 -> 2203643433392
	2203643433488 -> 2203643433680
	2203677679856 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2203677679856 -> 2203643433488
	2203643433488 [label=AccumulateGrad]
	2203643433728 -> 2203643433776
	2203677679952 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2203677679952 -> 2203643433728
	2203643433728 [label=AccumulateGrad]
	2203643433872 -> 2203643433776
	2203677680048 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2203677680048 -> 2203643433872
	2203643433872 [label=AccumulateGrad]
	2203643433968 -> 2203643434112
	2203677680432 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2203677680432 -> 2203643433968
	2203643433968 [label=AccumulateGrad]
	2203643434160 -> 2203643434256
	2203677680528 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2203677680528 -> 2203643434160
	2203643434160 [label=AccumulateGrad]
	2203643434208 -> 2203643434256
	2203677680624 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2203677680624 -> 2203643434208
	2203643434208 [label=AccumulateGrad]
	2203643434304 -> 2203643434352
	2203643434544 -> 2203643434688
	2203677681584 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2203677681584 -> 2203643434544
	2203643434544 [label=AccumulateGrad]
	2203643434736 -> 2203643434784
	2203677681680 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2203677681680 -> 2203643434736
	2203643434736 [label=AccumulateGrad]
	2203643434880 -> 2203643434784
	2203677681776 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2203677681776 -> 2203643434880
	2203643434880 [label=AccumulateGrad]
	2203643434976 -> 2203643435120
	2203677682160 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2203677682160 -> 2203643434976
	2203643434976 [label=AccumulateGrad]
	2203643435168 -> 2203643435264
	2203677682256 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2203677682256 -> 2203643435168
	2203643435168 [label=AccumulateGrad]
	2203643435216 -> 2203643435264
	2203677682352 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2203677682352 -> 2203643435216
	2203643435216 [label=AccumulateGrad]
	2203643435312 -> 2203643435360
	2203643435312 [label=CudnnBatchNormBackward0]
	2203643434592 -> 2203643435312
	2203643434592 [label=ConvolutionBackward0]
	2203643434496 -> 2203643434592
	2203643434448 -> 2203643434592
	2203677681008 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2203677681008 -> 2203643434448
	2203643434448 [label=AccumulateGrad]
	2203643435024 -> 2203643435312
	2203677681104 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2203677681104 -> 2203643435024
	2203643435024 [label=AccumulateGrad]
	2203643435072 -> 2203643435312
	2203677681200 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2203677681200 -> 2203643435072
	2203643435072 [label=AccumulateGrad]
	2203643435456 -> 2203643435648
	2203677682736 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2203677682736 -> 2203643435456
	2203643435456 [label=AccumulateGrad]
	2203643435696 -> 2203643435744
	2203677682832 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2203677682832 -> 2203643435696
	2203643435696 [label=AccumulateGrad]
	2203643435840 -> 2203643435744
	2203677682928 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2203677682928 -> 2203643435840
	2203643435840 [label=AccumulateGrad]
	2203643435936 -> 2203643436080
	2203677683312 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2203677683312 -> 2203643435936
	2203643435936 [label=AccumulateGrad]
	2203643436128 -> 2203643436752
	2203677683408 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2203677683408 -> 2203643436128
	2203643436128 [label=AccumulateGrad]
	2203643432144 -> 2203643436752
	2203677683504 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2203677683504 -> 2203643432144
	2203643432144 [label=AccumulateGrad]
	2203643436848 -> 2203643441504
	2203643442992 -> 2203643442416
	2203677684464 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2203677684464 -> 2203643442992
	2203643442992 [label=AccumulateGrad]
	2203643442512 -> 2203643442752
	2203677684560 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2203677684560 -> 2203643442512
	2203643442512 [label=AccumulateGrad]
	2203643442224 -> 2203643442752
	2203677684656 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2203677684656 -> 2203643442224
	2203643442224 [label=AccumulateGrad]
	2203643441936 -> 2203643429120
	2203677685040 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2203677685040 -> 2203643441936
	2203643441936 [label=AccumulateGrad]
	2203643428976 -> 2203643436944
	2203677685136 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2203677685136 -> 2203643428976
	2203643428976 [label=AccumulateGrad]
	2203643436656 -> 2203643436944
	2203677685232 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2203677685232 -> 2203643436656
	2203643436656 [label=AccumulateGrad]
	2203643443136 -> 2203643442944
	2203643443136 [label=CudnnBatchNormBackward0]
	2203643442800 -> 2203643443136
	2203643442800 [label=ConvolutionBackward0]
	2203643443376 -> 2203643442800
	2203643437328 -> 2203643442800
	2203677683888 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2203677683888 -> 2203643437328
	2203643437328 [label=AccumulateGrad]
	2203643441984 -> 2203643443136
	2203677683984 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2203677683984 -> 2203643441984
	2203643441984 [label=AccumulateGrad]
	2203643442896 -> 2203643443136
	2203677684080 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2203677684080 -> 2203643442896
	2203643442896 [label=AccumulateGrad]
	2203643443712 -> 2203643437136
	2203677685616 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2203677685616 -> 2203643443712
	2203643443712 [label=AccumulateGrad]
	2203643437088 -> 2203643437040
	2203677685712 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2203677685712 -> 2203643437088
	2203643437088 [label=AccumulateGrad]
	2203643439680 -> 2203643437040
	2203677685808 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2203677685808 -> 2203643439680
	2203643439680 [label=AccumulateGrad]
	2203643436896 -> 2203643442464
	2203677686192 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2203677686192 -> 2203643436896
	2203643436896 [label=AccumulateGrad]
	2203643442272 -> 2203643442032
	2203677686288 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2203677686288 -> 2203643442272
	2203643442272 [label=AccumulateGrad]
	2203643442080 -> 2203643442032
	2203677686384 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2203677686384 -> 2203643442080
	2203643442080 [label=AccumulateGrad]
	2203643441888 -> 2203643441840
	2203643441024 -> 2203643440880
	2203677687344 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2203677687344 -> 2203643441024
	2203643441024 [label=AccumulateGrad]
	2203643440832 -> 2203643440784
	2203677687440 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2203677687440 -> 2203643440832
	2203643440832 [label=AccumulateGrad]
	2203643440688 -> 2203643440784
	2203677687536 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2203677687536 -> 2203643440688
	2203643440688 [label=AccumulateGrad]
	2203643440592 -> 2203643440448
	2203677884592 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2203677884592 -> 2203643440592
	2203643440592 [label=AccumulateGrad]
	2203643440400 -> 2203643440304
	2203677884688 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2203677884688 -> 2203643440400
	2203643440400 [label=AccumulateGrad]
	2203643440352 -> 2203643440304
	2203677884784 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2203677884784 -> 2203643440352
	2203643440352 [label=AccumulateGrad]
	2203643440256 -> 2203643440208
	2203643440256 [label=CudnnBatchNormBackward0]
	2203643440976 -> 2203643440256
	2203643440976 [label=ConvolutionBackward0]
	2203643441696 -> 2203643440976
	2203643441744 -> 2203643440976
	2203677686768 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2203677686768 -> 2203643441744
	2203643441744 [label=AccumulateGrad]
	2203643440544 -> 2203643440256
	2203677686864 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2203677686864 -> 2203643440544
	2203643440544 [label=AccumulateGrad]
	2203643440496 -> 2203643440256
	2203677686960 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2203677686960 -> 2203643440496
	2203643440496 [label=AccumulateGrad]
	2203643439008 -> 2203643438960
	2203677885168 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2203677885168 -> 2203643439008
	2203643439008 [label=AccumulateGrad]
	2203643438912 -> 2203643438864
	2203677885264 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2203677885264 -> 2203643438912
	2203643438912 [label=AccumulateGrad]
	2203643438768 -> 2203643438864
	2203677885360 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2203677885360 -> 2203643438768
	2203643438768 [label=AccumulateGrad]
	2203643438672 -> 2203643438528
	2203677885744 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2203677885744 -> 2203643438672
	2203643438672 [label=AccumulateGrad]
	2203643438480 -> 2203643438384
	2203677885840 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2203677885840 -> 2203643438480
	2203643438480 [label=AccumulateGrad]
	2203643438432 -> 2203643438384
	2203677885936 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2203677885936 -> 2203643438432
	2203643438432 [label=AccumulateGrad]
	2203643438336 -> 2203643438288
	2203643436320 -> 2203643437664
	2203643436320 [label=TBackward0]
	2203643438240 -> 2203643436320
	2203677886512 [label="model.fc.weight
 (19, 512)" fillcolor=lightblue]
	2203677886512 -> 2203643438240
	2203643438240 [label=AccumulateGrad]
	2203643437808 -> 2203677898800
}
