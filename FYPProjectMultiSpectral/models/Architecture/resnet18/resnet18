digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2635861390832 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2635831525728 [label=AddmmBackward0]
	2635831534128 -> 2635831525728
	2635861527344 [label="fc.bias
 (19)" fillcolor=lightblue]
	2635861527344 -> 2635831534128
	2635831534128 [label=AccumulateGrad]
	2635831536720 -> 2635831525728
	2635831536720 [label=ViewBackward0]
	2635831533888 -> 2635831536720
	2635831533888 [label=MeanBackward1]
	2635831534512 -> 2635831533888
	2635831534512 [label=ReluBackward0]
	2635831530720 -> 2635831534512
	2635831530720 [label=AddBackward0]
	2635831538736 -> 2635831530720
	2635831538736 [label=CudnnBatchNormBackward0]
	2635831531056 -> 2635831538736
	2635831531056 [label=ConvolutionBackward0]
	2635831538640 -> 2635831531056
	2635831538640 [label=ReluBackward0]
	2635831532688 -> 2635831538640
	2635831532688 [label=CudnnBatchNormBackward0]
	2635831541664 -> 2635831532688
	2635831541664 [label=ConvolutionBackward0]
	2635831536192 -> 2635831541664
	2635831536192 [label=ReluBackward0]
	2635831531824 -> 2635831536192
	2635831531824 [label=AddBackward0]
	2635831529472 -> 2635831531824
	2635831529472 [label=CudnnBatchNormBackward0]
	2635831531104 -> 2635831529472
	2635831531104 [label=ConvolutionBackward0]
	2635831532592 -> 2635831531104
	2635831532592 [label=ReluBackward0]
	2635831538208 -> 2635831532592
	2635831538208 [label=CudnnBatchNormBackward0]
	2635831541232 -> 2635831538208
	2635831541232 [label=ConvolutionBackward0]
	2635831537440 -> 2635831541232
	2635831537440 [label=ReluBackward0]
	2635831534656 -> 2635831537440
	2635831534656 [label=AddBackward0]
	2635831533072 -> 2635831534656
	2635831533072 [label=CudnnBatchNormBackward0]
	2635831537680 -> 2635831533072
	2635831537680 [label=ConvolutionBackward0]
	2635831537776 -> 2635831537680
	2635831537776 [label=ReluBackward0]
	2635831534608 -> 2635831537776
	2635831534608 [label=CudnnBatchNormBackward0]
	2635831538160 -> 2635831534608
	2635831538160 [label=ConvolutionBackward0]
	2635831533984 -> 2635831538160
	2635831533984 [label=ReluBackward0]
	2635831535520 -> 2635831533984
	2635831535520 [label=AddBackward0]
	2635831537104 -> 2635831535520
	2635831537104 [label=CudnnBatchNormBackward0]
	2635831529664 -> 2635831537104
	2635831529664 [label=ConvolutionBackward0]
	2635831528752 -> 2635831529664
	2635831528752 [label=ReluBackward0]
	2635831534032 -> 2635831528752
	2635831534032 [label=CudnnBatchNormBackward0]
	2635831531968 -> 2635831534032
	2635831531968 [label=ConvolutionBackward0]
	2635831529760 -> 2635831531968
	2635831529760 [label=ReluBackward0]
	2635831532880 -> 2635831529760
	2635831532880 [label=AddBackward0]
	2635831528848 -> 2635831532880
	2635831528848 [label=CudnnBatchNormBackward0]
	2635831531248 -> 2635831528848
	2635831531248 [label=ConvolutionBackward0]
	2635831536960 -> 2635831531248
	2635831536960 [label=ReluBackward0]
	2635831526304 -> 2635831536960
	2635831526304 [label=CudnnBatchNormBackward0]
	2635831540176 -> 2635831526304
	2635831540176 [label=ConvolutionBackward0]
	2635831537968 -> 2635831540176
	2635831537968 [label=ReluBackward0]
	2635831532640 -> 2635831537968
	2635831532640 [label=AddBackward0]
	2635831535232 -> 2635831532640
	2635831535232 [label=CudnnBatchNormBackward0]
	2635831541376 -> 2635831535232
	2635831541376 [label=ConvolutionBackward0]
	2635831538544 -> 2635831541376
	2635831538544 [label=ReluBackward0]
	2635831537824 -> 2635831538544
	2635831537824 [label=CudnnBatchNormBackward0]
	2635831539408 -> 2635831537824
	2635831539408 [label=ConvolutionBackward0]
	2635831532016 -> 2635831539408
	2635831532016 [label=ReluBackward0]
	2635831535280 -> 2635831532016
	2635831535280 [label=AddBackward0]
	2635831395424 -> 2635831535280
	2635831395424 [label=CudnnBatchNormBackward0]
	2635831398112 -> 2635831395424
	2635831398112 [label=ConvolutionBackward0]
	2635831398544 -> 2635831398112
	2635831398544 [label=ReluBackward0]
	2635831398400 -> 2635831398544
	2635831398400 [label=CudnnBatchNormBackward0]
	2635831398736 -> 2635831398400
	2635831398736 [label=ConvolutionBackward0]
	2635831396624 -> 2635831398736
	2635831396624 [label=ReluBackward0]
	2635831394608 -> 2635831396624
	2635831394608 [label=AddBackward0]
	2635831398352 -> 2635831394608
	2635831398352 [label=CudnnBatchNormBackward0]
	2635831396720 -> 2635831398352
	2635831396720 [label=ConvolutionBackward0]
	2635831397824 -> 2635831396720
	2635831397824 [label=ReluBackward0]
	2635831395328 -> 2635831397824
	2635831395328 [label=CudnnBatchNormBackward0]
	2635831395760 -> 2635831395328
	2635831395760 [label=ConvolutionBackward0]
	2635831394992 -> 2635831395760
	2635831394992 [label=MaxPool2DWithIndicesBackward0]
	2635831395280 -> 2635831394992
	2635831395280 [label=ReluBackward0]
	2635831396096 -> 2635831395280
	2635831396096 [label=CudnnBatchNormBackward0]
	2635831397872 -> 2635831396096
	2635831397872 [label=ConvolutionBackward0]
	2635831397632 -> 2635831397872
	2635861527152 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2635861527152 -> 2635831397632
	2635831397632 [label=AccumulateGrad]
	2635831398208 -> 2635831396096
	2635831076784 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2635831076784 -> 2635831398208
	2635831398208 [label=AccumulateGrad]
	2635831397920 -> 2635831396096
	2635831076496 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2635831076496 -> 2635831397920
	2635831397920 [label=AccumulateGrad]
	2635831395376 -> 2635831395760
	2635831076976 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2635831076976 -> 2635831395376
	2635831395376 [label=AccumulateGrad]
	2635831397392 -> 2635831395328
	2635831077072 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2635831077072 -> 2635831397392
	2635831397392 [label=AccumulateGrad]
	2635831396960 -> 2635831395328
	2635831077168 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2635831077168 -> 2635831396960
	2635831396960 [label=AccumulateGrad]
	2635831398688 -> 2635831396720
	2635831076400 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2635831076400 -> 2635831398688
	2635831398688 [label=AccumulateGrad]
	2635831397968 -> 2635831398352
	2635831076304 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2635831076304 -> 2635831397968
	2635831397968 [label=AccumulateGrad]
	2635831394752 -> 2635831398352
	2635831075536 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2635831075536 -> 2635831394752
	2635831394752 [label=AccumulateGrad]
	2635831394992 -> 2635831394608
	2635831395040 -> 2635831398736
	2635831075632 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2635831075632 -> 2635831395040
	2635831395040 [label=AccumulateGrad]
	2635831395808 -> 2635831398400
	2635831075728 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2635831075728 -> 2635831395808
	2635831395808 [label=AccumulateGrad]
	2635831397248 -> 2635831398400
	2635831075824 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2635831075824 -> 2635831397248
	2635831397248 [label=AccumulateGrad]
	2635831394368 -> 2635831398112
	2635831076112 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2635831076112 -> 2635831394368
	2635831394368 [label=AccumulateGrad]
	2635831395856 -> 2635831395424
	2635831075152 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2635831075152 -> 2635831395856
	2635831395856 [label=AccumulateGrad]
	2635831398976 -> 2635831395424
	2635831075056 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2635831075056 -> 2635831398976
	2635831398976 [label=AccumulateGrad]
	2635831396624 -> 2635831535280
	2635831536000 -> 2635831539408
	2635831074960 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2635831074960 -> 2635831536000
	2635831536000 [label=AccumulateGrad]
	2635831535568 -> 2635831537824
	2635831074864 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2635831074864 -> 2635831535568
	2635831535568 [label=AccumulateGrad]
	2635831538784 -> 2635831537824
	2635861813936 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2635861813936 -> 2635831538784
	2635831538784 [label=AccumulateGrad]
	2635831532304 -> 2635831541376
	2635861814320 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2635861814320 -> 2635831532304
	2635831532304 [label=AccumulateGrad]
	2635831533456 -> 2635831535232
	2635861814416 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2635861814416 -> 2635831533456
	2635831533456 [label=AccumulateGrad]
	2635831527312 -> 2635831535232
	2635861814512 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2635861814512 -> 2635831527312
	2635831527312 [label=AccumulateGrad]
	2635831533360 -> 2635831532640
	2635831533360 [label=CudnnBatchNormBackward0]
	2635831535136 -> 2635831533360
	2635831535136 [label=ConvolutionBackward0]
	2635831532016 -> 2635831535136
	2635831530048 -> 2635831535136
	2635831072368 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2635831072368 -> 2635831530048
	2635831530048 [label=AccumulateGrad]
	2635831531488 -> 2635831533360
	2635831072560 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2635831072560 -> 2635831531488
	2635831531488 [label=AccumulateGrad]
	2635831536912 -> 2635831533360
	2635831074192 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2635831074192 -> 2635831536912
	2635831536912 [label=AccumulateGrad]
	2635831530336 -> 2635831540176
	2635861814896 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2635861814896 -> 2635831530336
	2635831530336 [label=AccumulateGrad]
	2635831535040 -> 2635831526304
	2635861814992 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2635861814992 -> 2635831535040
	2635831535040 [label=AccumulateGrad]
	2635831539264 -> 2635831526304
	2635861815088 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2635861815088 -> 2635831539264
	2635831539264 [label=AccumulateGrad]
	2635831528656 -> 2635831531248
	2635861815472 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2635861815472 -> 2635831528656
	2635831528656 [label=AccumulateGrad]
	2635831539360 -> 2635831528848
	2635861815568 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2635861815568 -> 2635831539360
	2635831539360 [label=AccumulateGrad]
	2635831534944 -> 2635831528848
	2635861815664 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2635861815664 -> 2635831534944
	2635831534944 [label=AccumulateGrad]
	2635831537968 -> 2635831532880
	2635831539216 -> 2635831531968
	2635861816624 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2635861816624 -> 2635831539216
	2635831539216 [label=AccumulateGrad]
	2635831527360 -> 2635831534032
	2635861816720 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2635861816720 -> 2635831527360
	2635831527360 [label=AccumulateGrad]
	2635831527984 -> 2635831534032
	2635861816816 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2635861816816 -> 2635831527984
	2635831527984 [label=AccumulateGrad]
	2635831533744 -> 2635831529664
	2635861817200 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2635861817200 -> 2635831533744
	2635831533744 [label=AccumulateGrad]
	2635831525872 -> 2635831537104
	2635861817296 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2635861817296 -> 2635831525872
	2635831525872 [label=AccumulateGrad]
	2635831527168 -> 2635831537104
	2635861817392 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2635861817392 -> 2635831527168
	2635831527168 [label=AccumulateGrad]
	2635831532064 -> 2635831535520
	2635831532064 [label=CudnnBatchNormBackward0]
	2635831538928 -> 2635831532064
	2635831538928 [label=ConvolutionBackward0]
	2635831529760 -> 2635831538928
	2635831528608 -> 2635831538928
	2635861816048 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2635861816048 -> 2635831528608
	2635831528608 [label=AccumulateGrad]
	2635831528272 -> 2635831532064
	2635861816144 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2635861816144 -> 2635831528272
	2635831528272 [label=AccumulateGrad]
	2635831529568 -> 2635831532064
	2635861816240 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2635861816240 -> 2635831529568
	2635831529568 [label=AccumulateGrad]
	2635831535616 -> 2635831538160
	2635861817776 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2635861817776 -> 2635831535616
	2635831535616 [label=AccumulateGrad]
	2635831533504 -> 2635831534608
	2635861817872 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2635861817872 -> 2635831533504
	2635831533504 [label=AccumulateGrad]
	2635831530000 -> 2635831534608
	2635861817968 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2635861817968 -> 2635831530000
	2635831530000 [label=AccumulateGrad]
	2635831537728 -> 2635831537680
	2635861818352 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2635861818352 -> 2635831537728
	2635831537728 [label=AccumulateGrad]
	2635831537344 -> 2635831533072
	2635861818448 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2635861818448 -> 2635831537344
	2635831537344 [label=AccumulateGrad]
	2635831535424 -> 2635831533072
	2635861818544 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2635861818544 -> 2635831535424
	2635831535424 [label=AccumulateGrad]
	2635831533984 -> 2635831534656
	2635831534704 -> 2635831541232
	2635861524656 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2635861524656 -> 2635831534704
	2635831534704 [label=AccumulateGrad]
	2635831532112 -> 2635831538208
	2635861524752 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2635861524752 -> 2635831532112
	2635831532112 [label=AccumulateGrad]
	2635831536624 -> 2635831538208
	2635861524848 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2635861524848 -> 2635831536624
	2635831536624 [label=AccumulateGrad]
	2635831536768 -> 2635831531104
	2635861525232 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2635861525232 -> 2635831536768
	2635831536768 [label=AccumulateGrad]
	2635831530144 -> 2635831529472
	2635861525328 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2635861525328 -> 2635831530144
	2635831530144 [label=AccumulateGrad]
	2635831532784 -> 2635831529472
	2635861525424 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2635861525424 -> 2635831532784
	2635831532784 [label=AccumulateGrad]
	2635831536816 -> 2635831531824
	2635831536816 [label=CudnnBatchNormBackward0]
	2635831538400 -> 2635831536816
	2635831538400 [label=ConvolutionBackward0]
	2635831537440 -> 2635831538400
	2635831533312 -> 2635831538400
	2635861818928 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2635861818928 -> 2635831533312
	2635831533312 [label=AccumulateGrad]
	2635831541280 -> 2635831536816
	2635861819024 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2635861819024 -> 2635831541280
	2635831541280 [label=AccumulateGrad]
	2635831534320 -> 2635831536816
	2635861819120 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2635861819120 -> 2635831534320
	2635831534320 [label=AccumulateGrad]
	2635831537392 -> 2635831541664
	2635861525808 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2635861525808 -> 2635831537392
	2635831537392 [label=AccumulateGrad]
	2635831533168 -> 2635831532688
	2635861525904 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2635861525904 -> 2635831533168
	2635831533168 [label=AccumulateGrad]
	2635831532352 -> 2635831532688
	2635861526000 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2635861526000 -> 2635831532352
	2635831532352 [label=AccumulateGrad]
	2635831535184 -> 2635831531056
	2635861526384 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2635861526384 -> 2635831535184
	2635831535184 [label=AccumulateGrad]
	2635831538112 -> 2635831538736
	2635861526480 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2635861526480 -> 2635831538112
	2635831538112 [label=AccumulateGrad]
	2635831533936 -> 2635831538736
	2635861526576 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2635861526576 -> 2635831533936
	2635831533936 [label=AccumulateGrad]
	2635831536192 -> 2635831530720
	2635831540944 -> 2635831525728
	2635831540944 [label=TBackward0]
	2635831539312 -> 2635831540944
	2635861527248 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2635861527248 -> 2635831539312
	2635831539312 [label=AccumulateGrad]
	2635831525728 -> 2635861390832
}
