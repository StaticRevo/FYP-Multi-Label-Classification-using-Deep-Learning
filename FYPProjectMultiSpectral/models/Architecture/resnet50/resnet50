digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1816587878576 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1816626033904 [label=AddmmBackward0]
	1816626033568 -> 1816626033904
	1816588255984 [label="fc.bias
 (19)" fillcolor=lightblue]
	1816588255984 -> 1816626033568
	1816626033568 [label=AccumulateGrad]
	1816626032944 -> 1816626033904
	1816626032944 [label=ViewBackward0]
	1816626026224 -> 1816626032944
	1816626026224 [label=MeanBackward1]
	1816626032176 -> 1816626026224
	1816626032176 [label=ReluBackward0]
	1816626028240 -> 1816626032176
	1816626028240 [label=AddBackward0]
	1816626036496 -> 1816626028240
	1816626036496 [label=CudnnBatchNormBackward0]
	1816626036640 -> 1816626036496
	1816626036640 [label=ConvolutionBackward0]
	1816626036016 -> 1816626036640
	1816626036016 [label=ReluBackward0]
	1816626036064 -> 1816626036016
	1816626036064 [label=CudnnBatchNormBackward0]
	1816626035920 -> 1816626036064
	1816626035920 [label=ConvolutionBackward0]
	1816626035632 -> 1816626035920
	1816626035632 [label=ReluBackward0]
	1816626035680 -> 1816626035632
	1816626035680 [label=CudnnBatchNormBackward0]
	1816626035392 -> 1816626035680
	1816626035392 [label=ConvolutionBackward0]
	1816626036688 -> 1816626035392
	1816626036688 [label=ReluBackward0]
	1816626035200 -> 1816626036688
	1816626035200 [label=AddBackward0]
	1816626034912 -> 1816626035200
	1816626034912 [label=CudnnBatchNormBackward0]
	1816626034672 -> 1816626034912
	1816626034672 [label=ConvolutionBackward0]
	1816626034480 -> 1816626034672
	1816626034480 [label=ReluBackward0]
	1816626034096 -> 1816626034480
	1816626034096 [label=CudnnBatchNormBackward0]
	1816626034336 -> 1816626034096
	1816626034336 [label=ConvolutionBackward0]
	1816626033952 -> 1816626034336
	1816626033952 [label=ReluBackward0]
	1816626033712 -> 1816626033952
	1816626033712 [label=CudnnBatchNormBackward0]
	1816626033664 -> 1816626033712
	1816626033664 [label=ConvolutionBackward0]
	1816626034960 -> 1816626033664
	1816626034960 [label=ReluBackward0]
	1816626033232 -> 1816626034960
	1816626033232 [label=AddBackward0]
	1816626033184 -> 1816626033232
	1816626033184 [label=CudnnBatchNormBackward0]
	1816626032992 -> 1816626033184
	1816626032992 [label=ConvolutionBackward0]
	1816626032896 -> 1816626032992
	1816626032896 [label=ReluBackward0]
	1816626032560 -> 1816626032896
	1816626032560 [label=CudnnBatchNormBackward0]
	1816626023872 -> 1816626032560
	1816626023872 [label=ConvolutionBackward0]
	1816626027376 -> 1816626023872
	1816626027376 [label=ReluBackward0]
	1816626031456 -> 1816626027376
	1816626031456 [label=CudnnBatchNormBackward0]
	1816626026896 -> 1816626031456
	1816626026896 [label=ConvolutionBackward0]
	1816626023584 -> 1816626026896
	1816626023584 [label=ReluBackward0]
	1816626022864 -> 1816626023584
	1816626022864 [label=AddBackward0]
	1816626026080 -> 1816626022864
	1816626026080 [label=CudnnBatchNormBackward0]
	1816626022816 -> 1816626026080
	1816626022816 [label=ConvolutionBackward0]
	1816626020944 -> 1816626022816
	1816626020944 [label=ReluBackward0]
	1816626022480 -> 1816626020944
	1816626022480 [label=CudnnBatchNormBackward0]
	1816626023968 -> 1816626022480
	1816626023968 [label=ConvolutionBackward0]
	1816626026272 -> 1816626023968
	1816626026272 [label=ReluBackward0]
	1816626031216 -> 1816626026272
	1816626031216 [label=CudnnBatchNormBackward0]
	1816626025456 -> 1816626031216
	1816626025456 [label=ConvolutionBackward0]
	1816626029872 -> 1816626025456
	1816626029872 [label=ReluBackward0]
	1816626029056 -> 1816626029872
	1816626029056 [label=AddBackward0]
	1816626025552 -> 1816626029056
	1816626025552 [label=CudnnBatchNormBackward0]
	1816626024832 -> 1816626025552
	1816626024832 [label=ConvolutionBackward0]
	1816626023680 -> 1816626024832
	1816626023680 [label=ReluBackward0]
	1816626023920 -> 1816626023680
	1816626023920 [label=CudnnBatchNormBackward0]
	1816626030400 -> 1816626023920
	1816626030400 [label=ConvolutionBackward0]
	1816626030880 -> 1816626030400
	1816626030880 [label=ReluBackward0]
	1816626032128 -> 1816626030880
	1816626032128 [label=CudnnBatchNormBackward0]
	1816626032464 -> 1816626032128
	1816626032464 [label=ConvolutionBackward0]
	1816626025600 -> 1816626032464
	1816626025600 [label=ReluBackward0]
	1816626025216 -> 1816626025600
	1816626025216 [label=AddBackward0]
	1816626027088 -> 1816626025216
	1816626027088 [label=CudnnBatchNormBackward0]
	1816626030112 -> 1816626027088
	1816626030112 [label=ConvolutionBackward0]
	1816626022432 -> 1816626030112
	1816626022432 [label=ReluBackward0]
	1816626028336 -> 1816626022432
	1816626028336 [label=CudnnBatchNormBackward0]
	1816626021280 -> 1816626028336
	1816626021280 [label=ConvolutionBackward0]
	1816626021328 -> 1816626021280
	1816626021328 [label=ReluBackward0]
	1816626020464 -> 1816626021328
	1816626020464 [label=CudnnBatchNormBackward0]
	1816626031504 -> 1816626020464
	1816626031504 [label=ConvolutionBackward0]
	1816626024928 -> 1816626031504
	1816626024928 [label=ReluBackward0]
	1816626023392 -> 1816626024928
	1816626023392 [label=AddBackward0]
	1816626026368 -> 1816626023392
	1816626026368 [label=CudnnBatchNormBackward0]
	1816626027808 -> 1816626026368
	1816626027808 [label=ConvolutionBackward0]
	1816626031120 -> 1816626027808
	1816626031120 [label=ReluBackward0]
	1816626031936 -> 1816626031120
	1816626031936 [label=CudnnBatchNormBackward0]
	1816626032320 -> 1816626031936
	1816626032320 [label=ConvolutionBackward0]
	1816584514928 -> 1816626032320
	1816584514928 [label=ReluBackward0]
	1816584514592 -> 1816584514928
	1816584514592 [label=CudnnBatchNormBackward0]
	1816584514112 -> 1816584514592
	1816584514112 [label=ConvolutionBackward0]
	1816626029728 -> 1816584514112
	1816626029728 [label=ReluBackward0]
	1816584512672 -> 1816626029728
	1816584512672 [label=AddBackward0]
	1816584512192 -> 1816584512672
	1816584512192 [label=CudnnBatchNormBackward0]
	1816584511088 -> 1816584512192
	1816584511088 [label=ConvolutionBackward0]
	1816584510128 -> 1816584511088
	1816584510128 [label=ReluBackward0]
	1816584509792 -> 1816584510128
	1816584509792 [label=CudnnBatchNormBackward0]
	1816584509312 -> 1816584509792
	1816584509312 [label=ConvolutionBackward0]
	1816584508352 -> 1816584509312
	1816584508352 [label=ReluBackward0]
	1816584507248 -> 1816584508352
	1816584507248 [label=CudnnBatchNormBackward0]
	1816584506768 -> 1816584507248
	1816584506768 [label=ConvolutionBackward0]
	1816584512048 -> 1816584506768
	1816584512048 [label=ReluBackward0]
	1816584505328 -> 1816584512048
	1816584505328 [label=AddBackward0]
	1816584504848 -> 1816584505328
	1816584504848 [label=CudnnBatchNormBackward0]
	1816584504512 -> 1816584504848
	1816584504512 [label=ConvolutionBackward0]
	1816584503552 -> 1816584504512
	1816584503552 [label=ReluBackward0]
	1816584515312 -> 1816584503552
	1816584515312 [label=CudnnBatchNormBackward0]
	1816584515216 -> 1816584515312
	1816584515216 [label=ConvolutionBackward0]
	1816584515024 -> 1816584515216
	1816584515024 [label=ReluBackward0]
	1816584514640 -> 1816584515024
	1816584514640 [label=CudnnBatchNormBackward0]
	1816584514880 -> 1816584514640
	1816584514880 [label=ConvolutionBackward0]
	1816584514496 -> 1816584514880
	1816584514496 [label=ReluBackward0]
	1816584514256 -> 1816584514496
	1816584514256 [label=AddBackward0]
	1816584514208 -> 1816584514256
	1816584514208 [label=CudnnBatchNormBackward0]
	1816584514016 -> 1816584514208
	1816584514016 [label=ConvolutionBackward0]
	1816584513920 -> 1816584514016
	1816584513920 [label=ReluBackward0]
	1816584513584 -> 1816584513920
	1816584513584 [label=CudnnBatchNormBackward0]
	1816584513392 -> 1816584513584
	1816584513392 [label=ConvolutionBackward0]
	1816584513248 -> 1816584513392
	1816584513248 [label=ReluBackward0]
	1816584513056 -> 1816584513248
	1816584513056 [label=CudnnBatchNormBackward0]
	1816584512720 -> 1816584513056
	1816584512720 [label=ConvolutionBackward0]
	1816584514400 -> 1816584512720
	1816584514400 [label=ReluBackward0]
	1816584512576 -> 1816584514400
	1816584512576 [label=AddBackward0]
	1816584512240 -> 1816584512576
	1816584512240 [label=CudnnBatchNormBackward0]
	1816584512288 -> 1816584512240
	1816584512288 [label=ConvolutionBackward0]
	1816584511952 -> 1816584512288
	1816584511952 [label=ReluBackward0]
	1816584512000 -> 1816584511952
	1816584512000 [label=CudnnBatchNormBackward0]
	1816584511904 -> 1816584512000
	1816584511904 [label=ConvolutionBackward0]
	1816584511280 -> 1816584511904
	1816584511280 [label=ReluBackward0]
	1816584511328 -> 1816584511280
	1816584511328 [label=CudnnBatchNormBackward0]
	1816584511184 -> 1816584511328
	1816584511184 [label=ConvolutionBackward0]
	1816584512432 -> 1816584511184
	1816584512432 [label=ReluBackward0]
	1816584510848 -> 1816584512432
	1816584510848 [label=AddBackward0]
	1816584510704 -> 1816584510848
	1816584510704 [label=CudnnBatchNormBackward0]
	1816584510320 -> 1816584510704
	1816584510320 [label=ConvolutionBackward0]
	1816584510464 -> 1816584510320
	1816584510464 [label=ReluBackward0]
	1816584510032 -> 1816584510464
	1816584510032 [label=CudnnBatchNormBackward0]
	1816584509936 -> 1816584510032
	1816584509936 [label=ConvolutionBackward0]
	1816584509744 -> 1816584509936
	1816584509744 [label=ReluBackward0]
	1816584509360 -> 1816584509744
	1816584509360 [label=CudnnBatchNormBackward0]
	1816584509600 -> 1816584509360
	1816584509600 [label=ConvolutionBackward0]
	1816584510944 -> 1816584509600
	1816584510944 [label=ReluBackward0]
	1816584508880 -> 1816584510944
	1816584508880 [label=AddBackward0]
	1816584509120 -> 1816584508880
	1816584509120 [label=CudnnBatchNormBackward0]
	1816584508784 -> 1816584509120
	1816584508784 [label=ConvolutionBackward0]
	1816584508496 -> 1816584508784
	1816584508496 [label=ReluBackward0]
	1816584508544 -> 1816584508496
	1816584508544 [label=CudnnBatchNormBackward0]
	1816584508256 -> 1816584508544
	1816584508256 [label=ConvolutionBackward0]
	1816584508160 -> 1816584508256
	1816584508160 [label=ReluBackward0]
	1816584507824 -> 1816584508160
	1816584507824 [label=CudnnBatchNormBackward0]
	1816584507632 -> 1816584507824
	1816584507632 [label=ConvolutionBackward0]
	1816584507488 -> 1816584507632
	1816584507488 [label=ReluBackward0]
	1816584507296 -> 1816584507488
	1816584507296 [label=AddBackward0]
	1816584506960 -> 1816584507296
	1816584506960 [label=CudnnBatchNormBackward0]
	1816584507008 -> 1816584506960
	1816584507008 [label=ConvolutionBackward0]
	1816584506672 -> 1816584507008
	1816584506672 [label=ReluBackward0]
	1816584506720 -> 1816584506672
	1816584506720 [label=CudnnBatchNormBackward0]
	1816584506624 -> 1816584506720
	1816584506624 [label=ConvolutionBackward0]
	1816584506000 -> 1816584506624
	1816584506000 [label=ReluBackward0]
	1816584506048 -> 1816584506000
	1816584506048 [label=CudnnBatchNormBackward0]
	1816584505904 -> 1816584506048
	1816584505904 [label=ConvolutionBackward0]
	1816584507152 -> 1816584505904
	1816584507152 [label=ReluBackward0]
	1816584505568 -> 1816584507152
	1816584505568 [label=AddBackward0]
	1816584505424 -> 1816584505568
	1816584505424 [label=CudnnBatchNormBackward0]
	1816584505040 -> 1816584505424
	1816584505040 [label=ConvolutionBackward0]
	1816584505184 -> 1816584505040
	1816584505184 [label=ReluBackward0]
	1816584504752 -> 1816584505184
	1816584504752 [label=CudnnBatchNormBackward0]
	1816584504656 -> 1816584504752
	1816584504656 [label=ConvolutionBackward0]
	1816584504464 -> 1816584504656
	1816584504464 [label=ReluBackward0]
	1816584504080 -> 1816584504464
	1816584504080 [label=CudnnBatchNormBackward0]
	1816584504320 -> 1816584504080
	1816584504320 [label=ConvolutionBackward0]
	1816584505664 -> 1816584504320
	1816584505664 [label=ReluBackward0]
	1816584503600 -> 1816584505664
	1816584503600 [label=AddBackward0]
	1816584503840 -> 1816584503600
	1816584503840 [label=CudnnBatchNormBackward0]
	1816584503504 -> 1816584503840
	1816584503504 [label=ConvolutionBackward0]
	1816584515984 -> 1816584503504
	1816584515984 [label=ReluBackward0]
	1816584516320 -> 1816584515984
	1816584516320 [label=CudnnBatchNormBackward0]
	1816584516416 -> 1816584516320
	1816584516416 [label=ConvolutionBackward0]
	1816584516608 -> 1816584516416
	1816584516608 [label=ReluBackward0]
	1816584516752 -> 1816584516608
	1816584516752 [label=CudnnBatchNormBackward0]
	1816584516848 -> 1816584516752
	1816584516848 [label=ConvolutionBackward0]
	1816584517040 -> 1816584516848
	1816584517040 [label=MaxPool2DWithIndicesBackward0]
	1816584517184 -> 1816584517040
	1816584517184 [label=ReluBackward0]
	1816584517280 -> 1816584517184
	1816584517280 [label=CudnnBatchNormBackward0]
	1816584517376 -> 1816584517280
	1816584517376 [label=ConvolutionBackward0]
	1816584517568 -> 1816584517376
	1816588255792 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1816588255792 -> 1816584517568
	1816584517568 [label=AccumulateGrad]
	1816584517328 -> 1816584517280
	1816610673904 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1816610673904 -> 1816584517328
	1816584517328 [label=AccumulateGrad]
	1816584517088 -> 1816584517280
	1816610673616 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1816610673616 -> 1816584517088
	1816584517088 [label=AccumulateGrad]
	1816584516992 -> 1816584516848
	1816610673520 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1816610673520 -> 1816584516992
	1816584516992 [label=AccumulateGrad]
	1816584516800 -> 1816584516752
	1816610673424 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1816610673424 -> 1816584516800
	1816584516800 [label=AccumulateGrad]
	1816584516656 -> 1816584516752
	1816610672656 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1816610672656 -> 1816584516656
	1816584516656 [label=AccumulateGrad]
	1816584516560 -> 1816584516416
	1816610672752 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1816610672752 -> 1816584516560
	1816584516560 [label=AccumulateGrad]
	1816584516368 -> 1816584516320
	1816610672848 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1816610672848 -> 1816584516368
	1816584516368 [label=AccumulateGrad]
	1816584516224 -> 1816584516320
	1816610672944 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1816610672944 -> 1816584516224
	1816584516224 [label=AccumulateGrad]
	1816584516128 -> 1816584503504
	1816610673232 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1816610673232 -> 1816584516128
	1816584516128 [label=AccumulateGrad]
	1816584503744 -> 1816584503840
	1816610672272 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1816610672272 -> 1816584503744
	1816584503744 [label=AccumulateGrad]
	1816584503648 -> 1816584503840
	1816610672176 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1816610672176 -> 1816584503648
	1816584503648 [label=AccumulateGrad]
	1816584503696 -> 1816584503600
	1816584503696 [label=CudnnBatchNormBackward0]
	1816584516512 -> 1816584503696
	1816584516512 [label=ConvolutionBackward0]
	1816584517040 -> 1816584516512
	1816584516896 -> 1816584516512
	1816610674096 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1816610674096 -> 1816584516896
	1816584516896 [label=AccumulateGrad]
	1816584503360 -> 1816584503696
	1816610674192 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1816610674192 -> 1816584503360
	1816584503360 [label=AccumulateGrad]
	1816584503456 -> 1816584503696
	1816610674288 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1816610674288 -> 1816584503456
	1816584503456 [label=AccumulateGrad]
	1816584503936 -> 1816584504320
	1816610669488 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1816610669488 -> 1816584503936
	1816584503936 [label=AccumulateGrad]
	1816584504176 -> 1816584504080
	1816610669680 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1816610669680 -> 1816584504176
	1816584504176 [label=AccumulateGrad]
	1816584504416 -> 1816584504080
	1816610671312 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1816610671312 -> 1816584504416
	1816584504416 [label=AccumulateGrad]
	1816584504704 -> 1816584504656
	1816610672080 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1816610672080 -> 1816584504704
	1816584504704 [label=AccumulateGrad]
	1816584504560 -> 1816584504752
	1816610671984 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1816610671984 -> 1816584504560
	1816584504560 [label=AccumulateGrad]
	1816584504944 -> 1816584504752
	1816588834800 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1816588834800 -> 1816584504944
	1816584504944 [label=AccumulateGrad]
	1816584505088 -> 1816584505040
	1816588835184 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1816588835184 -> 1816584505088
	1816584505088 [label=AccumulateGrad]
	1816584505232 -> 1816584505424
	1816588835280 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1816588835280 -> 1816584505232
	1816584505232 [label=AccumulateGrad]
	1816584505376 -> 1816584505424
	1816588835376 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1816588835376 -> 1816584505376
	1816584505376 [label=AccumulateGrad]
	1816584505664 -> 1816584505568
	1816584505616 -> 1816584505904
	1816588835760 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1816588835760 -> 1816584505616
	1816584505616 [label=AccumulateGrad]
	1816584506144 -> 1816584506048
	1816588835856 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1816588835856 -> 1816584506144
	1816584506144 [label=AccumulateGrad]
	1816584506096 -> 1816584506048
	1816588835952 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1816588835952 -> 1816584506096
	1816584506096 [label=AccumulateGrad]
	1816584506192 -> 1816584506624
	1816588836336 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1816588836336 -> 1816584506192
	1816584506192 [label=AccumulateGrad]
	1816584506528 -> 1816584506720
	1816588836432 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1816588836432 -> 1816584506528
	1816584506528 [label=AccumulateGrad]
	1816584506480 -> 1816584506720
	1816588836528 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1816588836528 -> 1816584506480
	1816584506480 [label=AccumulateGrad]
	1816584506816 -> 1816584507008
	1816588836912 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1816588836912 -> 1816584506816
	1816584506816 [label=AccumulateGrad]
	1816584507200 -> 1816584506960
	1816588837008 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1816588837008 -> 1816584507200
	1816584507200 [label=AccumulateGrad]
	1816584507056 -> 1816584506960
	1816588837104 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1816588837104 -> 1816584507056
	1816584507056 [label=AccumulateGrad]
	1816584507152 -> 1816584507296
	1816584507680 -> 1816584507632
	1816588838064 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1816588838064 -> 1816584507680
	1816584507680 [label=AccumulateGrad]
	1816584507776 -> 1816584507824
	1816588838160 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1816588838160 -> 1816584507776
	1816584507776 [label=AccumulateGrad]
	1816584507968 -> 1816584507824
	1816588838256 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1816588838256 -> 1816584507968
	1816584507968 [label=AccumulateGrad]
	1816584508016 -> 1816584508256
	1816588838640 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1816588838640 -> 1816584508016
	1816584508016 [label=AccumulateGrad]
	1816584508304 -> 1816584508544
	1816588838736 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1816588838736 -> 1816584508304
	1816584508304 [label=AccumulateGrad]
	1816584508640 -> 1816584508544
	1816588838832 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1816588838832 -> 1816584508640
	1816584508640 [label=AccumulateGrad]
	1816584508400 -> 1816584508784
	1816588839216 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1816588839216 -> 1816584508400
	1816584508400 [label=AccumulateGrad]
	1816584509024 -> 1816584509120
	1816588839312 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1816588839312 -> 1816584509024
	1816584509024 [label=AccumulateGrad]
	1816584508928 -> 1816584509120
	1816588839408 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1816588839408 -> 1816584508928
	1816584508928 [label=AccumulateGrad]
	1816584508976 -> 1816584508880
	1816584508976 [label=CudnnBatchNormBackward0]
	1816584507920 -> 1816584508976
	1816584507920 [label=ConvolutionBackward0]
	1816584507488 -> 1816584507920
	1816584507440 -> 1816584507920
	1816588837488 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1816588837488 -> 1816584507440
	1816584507440 [label=AccumulateGrad]
	1816584508592 -> 1816584508976
	1816588837584 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1816588837584 -> 1816584508592
	1816584508592 [label=AccumulateGrad]
	1816584508736 -> 1816584508976
	1816588837680 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1816588837680 -> 1816584508736
	1816584508736 [label=AccumulateGrad]
	1816584509216 -> 1816584509600
	1816588839792 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1816588839792 -> 1816584509216
	1816584509216 [label=AccumulateGrad]
	1816584509456 -> 1816584509360
	1816588839888 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1816588839888 -> 1816584509456
	1816584509456 [label=AccumulateGrad]
	1816584509696 -> 1816584509360
	1816588839984 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1816588839984 -> 1816584509696
	1816584509696 [label=AccumulateGrad]
	1816584509984 -> 1816584509936
	1816588840368 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1816588840368 -> 1816584509984
	1816584509984 [label=AccumulateGrad]
	1816584509840 -> 1816584510032
	1816588840464 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1816588840464 -> 1816584509840
	1816584509840 [label=AccumulateGrad]
	1816584510224 -> 1816584510032
	1816588840560 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1816588840560 -> 1816584510224
	1816584510224 [label=AccumulateGrad]
	1816584510368 -> 1816584510320
	1816588840944 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1816588840944 -> 1816584510368
	1816584510368 [label=AccumulateGrad]
	1816584510512 -> 1816584510704
	1816588841040 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1816588841040 -> 1816584510512
	1816584510512 [label=AccumulateGrad]
	1816584510656 -> 1816584510704
	1816588841136 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1816588841136 -> 1816584510656
	1816584510656 [label=AccumulateGrad]
	1816584510944 -> 1816584510848
	1816584510896 -> 1816584511184
	1816588841520 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1816588841520 -> 1816584510896
	1816584510896 [label=AccumulateGrad]
	1816584511424 -> 1816584511328
	1816588841616 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1816588841616 -> 1816584511424
	1816584511424 [label=AccumulateGrad]
	1816584511376 -> 1816584511328
	1816588841712 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1816588841712 -> 1816584511376
	1816584511376 [label=AccumulateGrad]
	1816584511472 -> 1816584511904
	1816588842096 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1816588842096 -> 1816584511472
	1816584511472 [label=AccumulateGrad]
	1816584511808 -> 1816584512000
	1816588842192 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1816588842192 -> 1816584511808
	1816584511808 [label=AccumulateGrad]
	1816584511760 -> 1816584512000
	1816588842288 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1816588842288 -> 1816584511760
	1816584511760 [label=AccumulateGrad]
	1816584512096 -> 1816584512288
	1816588842672 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1816588842672 -> 1816584512096
	1816584512096 [label=AccumulateGrad]
	1816584512480 -> 1816584512240
	1816588842768 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1816588842768 -> 1816584512480
	1816584512480 [label=AccumulateGrad]
	1816584512336 -> 1816584512240
	1816588842864 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1816588842864 -> 1816584512336
	1816584512336 [label=AccumulateGrad]
	1816584512432 -> 1816584512576
	1816584512864 -> 1816584512720
	1816588843248 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1816588843248 -> 1816584512864
	1816584512864 [label=AccumulateGrad]
	1816584512912 -> 1816584513056
	1816588843344 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1816588843344 -> 1816584512912
	1816584512912 [label=AccumulateGrad]
	1816584513344 -> 1816584513056
	1816588843440 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1816588843440 -> 1816584513344
	1816584513344 [label=AccumulateGrad]
	1816584513440 -> 1816584513392
	1816588843824 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1816588843824 -> 1816584513440
	1816584513440 [label=AccumulateGrad]
	1816584513536 -> 1816584513584
	1816588843920 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1816588843920 -> 1816584513536
	1816584513536 [label=AccumulateGrad]
	1816584513728 -> 1816584513584
	1816588844016 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1816588844016 -> 1816584513728
	1816584513728 [label=AccumulateGrad]
	1816584513776 -> 1816584514016
	1816588844400 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1816588844400 -> 1816584513776
	1816584513776 [label=AccumulateGrad]
	1816584514064 -> 1816584514208
	1816588844496 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1816588844496 -> 1816584514064
	1816584514064 [label=AccumulateGrad]
	1816584514304 -> 1816584514208
	1816588844592 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1816588844592 -> 1816584514304
	1816584514304 [label=AccumulateGrad]
	1816584514400 -> 1816584514256
	1816584514544 -> 1816584514880
	1816588534320 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1816588534320 -> 1816584514544
	1816584514544 [label=AccumulateGrad]
	1816584514736 -> 1816584514640
	1816588534416 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1816588534416 -> 1816584514736
	1816584514736 [label=AccumulateGrad]
	1816584514976 -> 1816584514640
	1816588534512 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1816588534512 -> 1816584514976
	1816584514976 [label=AccumulateGrad]
	1816584515264 -> 1816584515216
	1816588534896 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1816588534896 -> 1816584515264
	1816584515264 [label=AccumulateGrad]
	1816584515120 -> 1816584515312
	1816588534992 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1816588534992 -> 1816584515120
	1816584515120 [label=AccumulateGrad]
	1816584515504 -> 1816584515312
	1816588535088 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1816588535088 -> 1816584515504
	1816584515504 [label=AccumulateGrad]
	1816584503408 -> 1816584504512
	1816588535472 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1816588535472 -> 1816584503408
	1816584503408 [label=AccumulateGrad]
	1816584504368 -> 1816584504848
	1816588535568 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1816588535568 -> 1816584504368
	1816584504368 [label=AccumulateGrad]
	1816584504992 -> 1816584504848
	1816588535664 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1816588535664 -> 1816584504992
	1816584504992 [label=AccumulateGrad]
	1816584505472 -> 1816584505328
	1816584505472 [label=CudnnBatchNormBackward0]
	1816584515168 -> 1816584505472
	1816584515168 [label=ConvolutionBackward0]
	1816584514496 -> 1816584515168
	1816584514688 -> 1816584515168
	1816588844976 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1816588844976 -> 1816584514688
	1816584514688 [label=AccumulateGrad]
	1816584504032 -> 1816584505472
	1816588533840 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1816588533840 -> 1816584504032
	1816584504032 [label=AccumulateGrad]
	1816584503888 -> 1816584505472
	1816588533936 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1816588533936 -> 1816584503888
	1816584503888 [label=AccumulateGrad]
	1816584505808 -> 1816584506768
	1816588536048 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1816588536048 -> 1816584505808
	1816584505808 [label=AccumulateGrad]
	1816584507392 -> 1816584507248
	1816588536144 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1816588536144 -> 1816584507392
	1816584507392 [label=AccumulateGrad]
	1816584507728 -> 1816584507248
	1816588536240 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1816588536240 -> 1816584507728
	1816584507728 [label=AccumulateGrad]
	1816584508208 -> 1816584509312
	1816588536624 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1816588536624 -> 1816584508208
	1816584508208 [label=AccumulateGrad]
	1816584509168 -> 1816584509792
	1816588536720 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1816588536720 -> 1816584509168
	1816584509168 [label=AccumulateGrad]
	1816584510272 -> 1816584509792
	1816588536816 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1816588536816 -> 1816584510272
	1816584510272 [label=AccumulateGrad]
	1816584510752 -> 1816584511088
	1816588537200 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1816588537200 -> 1816584510752
	1816584510752 [label=AccumulateGrad]
	1816584511712 -> 1816584512192
	1816588537296 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1816588537296 -> 1816584511712
	1816584511712 [label=AccumulateGrad]
	1816584511568 -> 1816584512192
	1816588537392 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1816588537392 -> 1816584511568
	1816584511568 [label=AccumulateGrad]
	1816584512048 -> 1816584512672
	1816584513152 -> 1816584514112
	1816588537776 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1816588537776 -> 1816584513152
	1816584513152 [label=AccumulateGrad]
	1816584513968 -> 1816584514592
	1816588537872 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1816588537872 -> 1816584513968
	1816584513968 [label=AccumulateGrad]
	1816584515072 -> 1816584514592
	1816588537968 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1816588537968 -> 1816584515072
	1816584515072 [label=AccumulateGrad]
	1816584515408 -> 1816626032320
	1816588538352 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1816588538352 -> 1816584515408
	1816584515408 [label=AccumulateGrad]
	1816626032512 -> 1816626031936
	1816588538448 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1816588538448 -> 1816626032512
	1816626032512 [label=AccumulateGrad]
	1816626026848 -> 1816626031936
	1816588538544 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1816588538544 -> 1816626026848
	1816626026848 [label=AccumulateGrad]
	1816626025168 -> 1816626027808
	1816588538928 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1816588538928 -> 1816626025168
	1816626025168 [label=AccumulateGrad]
	1816626031600 -> 1816626026368
	1816588539024 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1816588539024 -> 1816626031600
	1816626031600 [label=AccumulateGrad]
	1816626029680 -> 1816626026368
	1816588539120 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1816588539120 -> 1816626029680
	1816626029680 [label=AccumulateGrad]
	1816626029728 -> 1816626023392
	1816626025792 -> 1816626031504
	1816588539504 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1816588539504 -> 1816626025792
	1816626025792 [label=AccumulateGrad]
	1816626028048 -> 1816626020464
	1816588539600 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1816588539600 -> 1816626028048
	1816626028048 [label=AccumulateGrad]
	1816626026608 -> 1816626020464
	1816588539696 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1816588539696 -> 1816626026608
	1816626026608 [label=AccumulateGrad]
	1816626020752 -> 1816626021280
	1816588540080 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1816588540080 -> 1816626020752
	1816626020752 [label=AccumulateGrad]
	1816626028720 -> 1816626028336
	1816588540176 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1816588540176 -> 1816626028720
	1816626028720 [label=AccumulateGrad]
	1816626027136 -> 1816626028336
	1816588540272 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1816588540272 -> 1816626027136
	1816626027136 [label=AccumulateGrad]
	1816626028576 -> 1816626030112
	1816588540656 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1816588540656 -> 1816626028576
	1816626028576 [label=AccumulateGrad]
	1816626027280 -> 1816626027088
	1816588540752 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1816588540752 -> 1816626027280
	1816626027280 [label=AccumulateGrad]
	1816626021520 -> 1816626027088
	1816588540848 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1816588540848 -> 1816626021520
	1816626021520 [label=AccumulateGrad]
	1816626024928 -> 1816626025216
	1816626024880 -> 1816626032464
	1816588541232 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1816588541232 -> 1816626024880
	1816626024880 [label=AccumulateGrad]
	1816626022192 -> 1816626032128
	1816588541328 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1816588541328 -> 1816626022192
	1816626022192 [label=AccumulateGrad]
	1816626022528 -> 1816626032128
	1816588541424 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1816588541424 -> 1816626022528
	1816626022528 [label=AccumulateGrad]
	1816626029392 -> 1816626030400
	1816588541808 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1816588541808 -> 1816626029392
	1816626029392 [label=AccumulateGrad]
	1816626028816 -> 1816626023920
	1816588541904 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1816588541904 -> 1816626028816
	1816626028816 [label=AccumulateGrad]
	1816626023056 -> 1816626023920
	1816588542000 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1816588542000 -> 1816626023056
	1816626023056 [label=AccumulateGrad]
	1816626031648 -> 1816626024832
	1816588542384 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1816588542384 -> 1816626031648
	1816626031648 [label=AccumulateGrad]
	1816626030784 -> 1816626025552
	1816588542480 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1816588542480 -> 1816626030784
	1816626030784 [label=AccumulateGrad]
	1816626030208 -> 1816626025552
	1816588542576 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1816588542576 -> 1816626030208
	1816626030208 [label=AccumulateGrad]
	1816626025600 -> 1816626029056
	1816626025888 -> 1816626025456
	1816588542960 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1816588542960 -> 1816626025888
	1816626025888 [label=AccumulateGrad]
	1816626032080 -> 1816626031216
	1816588543056 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1816588543056 -> 1816626032080
	1816626032080 [label=AccumulateGrad]
	1816626030016 -> 1816626031216
	1816588543152 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1816588543152 -> 1816626030016
	1816626030016 [label=AccumulateGrad]
	1816626029536 -> 1816626023968
	1816588543536 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1816588543536 -> 1816626029536
	1816626029536 [label=AccumulateGrad]
	1816626031168 -> 1816626022480
	1816588543632 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1816588543632 -> 1816626031168
	1816626031168 [label=AccumulateGrad]
	1816626029920 -> 1816626022480
	1816588543728 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1816588543728 -> 1816626029920
	1816626029920 [label=AccumulateGrad]
	1816626023248 -> 1816626022816
	1816588544112 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1816588544112 -> 1816626023248
	1816626023248 [label=AccumulateGrad]
	1816626027616 -> 1816626026080
	1816588544208 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1816588544208 -> 1816626027616
	1816626027616 [label=AccumulateGrad]
	1816626022336 -> 1816626026080
	1816588544304 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1816588544304 -> 1816626022336
	1816626022336 [label=AccumulateGrad]
	1816626029872 -> 1816626022864
	1816626024208 -> 1816626026896
	1816588545264 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1816588545264 -> 1816626024208
	1816626024208 [label=AccumulateGrad]
	1816626027424 -> 1816626031456
	1816588545360 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1816588545360 -> 1816626027424
	1816626027424 [label=AccumulateGrad]
	1816626032416 -> 1816626031456
	1816588545456 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1816588545456 -> 1816626032416
	1816626032416 [label=AccumulateGrad]
	1816626028384 -> 1816626023872
	1816588545840 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1816588545840 -> 1816626028384
	1816626028384 [label=AccumulateGrad]
	1816626030160 -> 1816626032560
	1816588545936 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1816588545936 -> 1816626030160
	1816626030160 [label=AccumulateGrad]
	1816626032704 -> 1816626032560
	1816588546032 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1816588546032 -> 1816626032704
	1816626032704 [label=AccumulateGrad]
	1816626032752 -> 1816626032992
	1816588546416 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1816588546416 -> 1816626032752
	1816626032752 [label=AccumulateGrad]
	1816626033040 -> 1816626033184
	1816588546512 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1816588546512 -> 1816626033040
	1816626033040 [label=AccumulateGrad]
	1816626033280 -> 1816626033184
	1816588546608 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1816588546608 -> 1816626033280
	1816626033280 [label=AccumulateGrad]
	1816626033376 -> 1816626033232
	1816626033376 [label=CudnnBatchNormBackward0]
	1816668178992 -> 1816626033376
	1816668178992 [label=ConvolutionBackward0]
	1816626023584 -> 1816668178992
	1816626020704 -> 1816668178992
	1816588544688 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1816588544688 -> 1816626020704
	1816626020704 [label=AccumulateGrad]
	1816626032656 -> 1816626033376
	1816588544784 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1816588544784 -> 1816626032656
	1816626032656 [label=AccumulateGrad]
	1816626032848 -> 1816626033376
	1816588544880 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1816588544880 -> 1816626032848
	1816626032848 [label=AccumulateGrad]
	1816626033328 -> 1816626033664
	1816588546992 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1816588546992 -> 1816626033328
	1816626033328 [label=AccumulateGrad]
	1816626033856 -> 1816626033712
	1816588547088 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1816588547088 -> 1816626033856
	1816626033856 [label=AccumulateGrad]
	1816626033808 -> 1816626033712
	1816588547184 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1816588547184 -> 1816626033808
	1816626033808 [label=AccumulateGrad]
	1816626034000 -> 1816626034336
	1816588547568 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1816588547568 -> 1816626034000
	1816626034000 [label=AccumulateGrad]
	1816626034192 -> 1816626034096
	1816588547664 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1816588547664 -> 1816626034192
	1816626034192 [label=AccumulateGrad]
	1816626034432 -> 1816626034096
	1816588547760 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1816588547760 -> 1816626034432
	1816626034432 [label=AccumulateGrad]
	1816626034720 -> 1816626034672
	1816588548144 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1816588548144 -> 1816626034720
	1816626034720 [label=AccumulateGrad]
	1816626034576 -> 1816626034912
	1816588548240 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1816588548240 -> 1816626034576
	1816626034576 [label=AccumulateGrad]
	1816626034768 -> 1816626034912
	1816588548336 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1816588548336 -> 1816626034768
	1816626034768 [label=AccumulateGrad]
	1816626034960 -> 1816626035200
	1816626035296 -> 1816626035392
	1816588548720 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1816588548720 -> 1816626035296
	1816626035296 [label=AccumulateGrad]
	1816626035440 -> 1816626035680
	1816588548816 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1816588548816 -> 1816626035440
	1816626035440 [label=AccumulateGrad]
	1816626035776 -> 1816626035680
	1816588548912 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1816588548912 -> 1816626035776
	1816626035776 [label=AccumulateGrad]
	1816626035536 -> 1816626035920
	1816588549296 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1816588549296 -> 1816626035536
	1816626035536 [label=AccumulateGrad]
	1816626036160 -> 1816626036064
	1816588549392 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1816588549392 -> 1816626036160
	1816626036160 [label=AccumulateGrad]
	1816626036112 -> 1816626036064
	1816588549488 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1816588549488 -> 1816626036112
	1816626036112 [label=AccumulateGrad]
	1816626036208 -> 1816626036640
	1816588549872 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1816588549872 -> 1816626036208
	1816626036208 [label=AccumulateGrad]
	1816626036544 -> 1816626036496
	1816588549968 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1816588549968 -> 1816626036544
	1816626036544 [label=AccumulateGrad]
	1816626036592 -> 1816626036496
	1816588550064 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1816588550064 -> 1816626036592
	1816626036592 [label=AccumulateGrad]
	1816626036688 -> 1816626028240
	1816626033088 -> 1816626033904
	1816626033088 [label=TBackward0]
	1816626030592 -> 1816626033088
	1816588255888 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	1816588255888 -> 1816626030592
	1816626030592 [label=AccumulateGrad]
	1816626033904 -> 1816587878576
}
