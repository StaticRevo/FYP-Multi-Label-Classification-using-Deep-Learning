digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2615042149072 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2615063328512 [label=AddmmBackward0]
	2615063327408 -> 2615063328512
	2615063738320 [label="fc.bias
 (19)" fillcolor=lightblue]
	2615063738320 -> 2615063327408
	2615063327408 [label=AccumulateGrad]
	2615063327552 -> 2615063328512
	2615063327552 [label=ViewBackward0]
	2615063327072 -> 2615063327552
	2615063327072 [label=MeanBackward1]
	2615063325968 -> 2615063327072
	2615063325968 [label=ReluBackward0]
	2615063325488 -> 2615063325968
	2615063325488 [label=AddBackward0]
	2615063325008 -> 2615063325488
	2615063325008 [label=CudnnBatchNormBackward0]
	2615063324672 -> 2615063325008
	2615063324672 [label=ConvolutionBackward0]
	2615063323712 -> 2615063324672
	2615063323712 [label=ReluBackward0]
	2615063322608 -> 2615063323712
	2615063322608 [label=CudnnBatchNormBackward0]
	2615063322128 -> 2615063322608
	2615063322128 [label=ConvolutionBackward0]
	2615063325632 -> 2615063322128
	2615063325632 [label=ReluBackward0]
	2615063320688 -> 2615063325632
	2615063320688 [label=AddBackward0]
	2615063320208 -> 2615063320688
	2615063320208 [label=CudnnBatchNormBackward0]
	2615063319872 -> 2615063320208
	2615063319872 [label=ConvolutionBackward0]
	2615063318912 -> 2615063319872
	2615063318912 [label=ReluBackward0]
	2615063317808 -> 2615063318912
	2615063317808 [label=CudnnBatchNormBackward0]
	2615063317328 -> 2615063317808
	2615063317328 [label=ConvolutionBackward0]
	2615063316848 -> 2615063317328
	2615063316848 [label=ReluBackward0]
	2615063316080 -> 2615063316848
	2615063316080 [label=AddBackward0]
	2615063330384 -> 2615063316080
	2615063330384 [label=CudnnBatchNormBackward0]
	2615063330000 -> 2615063330384
	2615063330000 [label=ConvolutionBackward0]
	2615063330144 -> 2615063330000
	2615063330144 [label=ReluBackward0]
	2615063329712 -> 2615063330144
	2615063329712 [label=CudnnBatchNormBackward0]
	2615063329616 -> 2615063329712
	2615063329616 [label=ConvolutionBackward0]
	2615063330672 -> 2615063329616
	2615063330672 [label=ReluBackward0]
	2615063329232 -> 2615063330672
	2615063329232 [label=AddBackward0]
	2615063329136 -> 2615063329232
	2615063329136 [label=CudnnBatchNormBackward0]
	2615063329184 -> 2615063329136
	2615063329184 [label=ConvolutionBackward0]
	2615063328560 -> 2615063329184
	2615063328560 [label=ReluBackward0]
	2615063328608 -> 2615063328560
	2615063328608 [label=CudnnBatchNormBackward0]
	2615063328464 -> 2615063328608
	2615063328464 [label=ConvolutionBackward0]
	2615063328176 -> 2615063328464
	2615063328176 [label=ReluBackward0]
	2615063328224 -> 2615063328176
	2615063328224 [label=AddBackward0]
	2615063327936 -> 2615063328224
	2615063327936 [label=CudnnBatchNormBackward0]
	2615063327696 -> 2615063327936
	2615063327696 [label=ConvolutionBackward0]
	2615063327504 -> 2615063327696
	2615063327504 [label=ReluBackward0]
	2615063327120 -> 2615063327504
	2615063327120 [label=CudnnBatchNormBackward0]
	2615063327360 -> 2615063327120
	2615063327360 [label=ConvolutionBackward0]
	2615063327984 -> 2615063327360
	2615063327984 [label=ReluBackward0]
	2615063326640 -> 2615063327984
	2615063326640 [label=AddBackward0]
	2615063326880 -> 2615063326640
	2615063326880 [label=CudnnBatchNormBackward0]
	2615063326544 -> 2615063326880
	2615063326544 [label=ConvolutionBackward0]
	2615063326256 -> 2615063326544
	2615063326256 [label=ReluBackward0]
	2615063326304 -> 2615063326256
	2615063326304 [label=CudnnBatchNormBackward0]
	2615063326016 -> 2615063326304
	2615063326016 [label=ConvolutionBackward0]
	2615063325920 -> 2615063326016
	2615063325920 [label=ReluBackward0]
	2615063325584 -> 2615063325920
	2615063325584 [label=AddBackward0]
	2615063325392 -> 2615063325584
	2615063325392 [label=CudnnBatchNormBackward0]
	2615063325440 -> 2615063325392
	2615063325440 [label=ConvolutionBackward0]
	2615063325056 -> 2615063325440
	2615063325056 [label=ReluBackward0]
	2615063324816 -> 2615063325056
	2615063324816 [label=CudnnBatchNormBackward0]
	2615063324768 -> 2615063324816
	2615063324768 [label=ConvolutionBackward0]
	2615063325536 -> 2615063324768
	2615063325536 [label=ReluBackward0]
	2615063324336 -> 2615063325536
	2615063324336 [label=AddBackward0]
	2615063324288 -> 2615063324336
	2615063324288 [label=CudnnBatchNormBackward0]
	2615063324096 -> 2615063324288
	2615063324096 [label=ConvolutionBackward0]
	2615063324000 -> 2615063324096
	2615063324000 [label=ReluBackward0]
	2615063323664 -> 2615063324000
	2615063323664 [label=CudnnBatchNormBackward0]
	2615063323472 -> 2615063323664
	2615063323472 [label=ConvolutionBackward0]
	2615063324480 -> 2615063323472
	2615063324480 [label=MaxPool2DWithIndicesBackward0]
	2615063323184 -> 2615063324480
	2615063323184 [label=ReluBackward0]
	2615063322992 -> 2615063323184
	2615063322992 [label=CudnnBatchNormBackward0]
	2615063322896 -> 2615063322992
	2615063322896 [label=ConvolutionBackward0]
	2615063322704 -> 2615063322896
	2615063738128 [label="conv1.weight
 (64, 4, 7, 7)" fillcolor=lightblue]
	2615063738128 -> 2615063322704
	2615063322704 [label=AccumulateGrad]
	2615063322800 -> 2615063322992
	2615063712912 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2615063712912 -> 2615063322800
	2615063322800 [label=AccumulateGrad]
	2615063323376 -> 2615063322992
	2615063712624 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2615063712624 -> 2615063323376
	2615063323376 [label=AccumulateGrad]
	2615063323328 -> 2615063323472
	2615063713104 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2615063713104 -> 2615063323328
	2615063323328 [label=AccumulateGrad]
	2615063323616 -> 2615063323664
	2615063713200 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2615063713200 -> 2615063323616
	2615063323616 [label=AccumulateGrad]
	2615063323808 -> 2615063323664
	2615063713296 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2615063713296 -> 2615063323808
	2615063323808 [label=AccumulateGrad]
	2615063323856 -> 2615063324096
	2615063712528 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2615063712528 -> 2615063323856
	2615063323856 [label=AccumulateGrad]
	2615063324144 -> 2615063324288
	2615063712432 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2615063712432 -> 2615063324144
	2615063324144 [label=AccumulateGrad]
	2615063324384 -> 2615063324288
	2615063711664 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2615063711664 -> 2615063324384
	2615063324384 [label=AccumulateGrad]
	2615063324480 -> 2615063324336
	2615063324432 -> 2615063324768
	2615063711760 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2615063711760 -> 2615063324432
	2615063324432 [label=AccumulateGrad]
	2615063324960 -> 2615063324816
	2615063711856 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2615063711856 -> 2615063324960
	2615063324960 [label=AccumulateGrad]
	2615063324912 -> 2615063324816
	2615063711952 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2615063711952 -> 2615063324912
	2615063324912 [label=AccumulateGrad]
	2615063325104 -> 2615063325440
	2615063712240 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2615063712240 -> 2615063325104
	2615063325104 [label=AccumulateGrad]
	2615063325296 -> 2615063325392
	2615063711280 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2615063711280 -> 2615063325296
	2615063325296 [label=AccumulateGrad]
	2615063325200 -> 2615063325392
	2615063711184 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2615063711184 -> 2615063325200
	2615063325200 [label=AccumulateGrad]
	2615063325536 -> 2615063325584
	2615063325776 -> 2615063326016
	2615063711088 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2615063711088 -> 2615063325776
	2615063325776 [label=AccumulateGrad]
	2615063326064 -> 2615063326304
	2615063710992 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2615063710992 -> 2615063326064
	2615063326064 [label=AccumulateGrad]
	2615063326400 -> 2615063326304
	2615063730064 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2615063730064 -> 2615063326400
	2615063326400 [label=AccumulateGrad]
	2615063326160 -> 2615063326544
	2615063730448 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2615063730448 -> 2615063326160
	2615063326160 [label=AccumulateGrad]
	2615063326784 -> 2615063326880
	2615063730544 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2615063730544 -> 2615063326784
	2615063326784 [label=AccumulateGrad]
	2615063326688 -> 2615063326880
	2615063730640 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2615063730640 -> 2615063326688
	2615063326688 [label=AccumulateGrad]
	2615063326736 -> 2615063326640
	2615063326736 [label=CudnnBatchNormBackward0]
	2615063325680 -> 2615063326736
	2615063325680 [label=ConvolutionBackward0]
	2615063325920 -> 2615063325680
	2615063325728 -> 2615063325680
	2615063708496 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2615063708496 -> 2615063325728
	2615063325728 [label=AccumulateGrad]
	2615063326352 -> 2615063326736
	2615063708688 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2615063708688 -> 2615063326352
	2615063326352 [label=AccumulateGrad]
	2615063326496 -> 2615063326736
	2615063710320 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2615063710320 -> 2615063326496
	2615063326496 [label=AccumulateGrad]
	2615063326976 -> 2615063327360
	2615063731024 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2615063731024 -> 2615063326976
	2615063326976 [label=AccumulateGrad]
	2615063327216 -> 2615063327120
	2615063731120 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2615063731120 -> 2615063327216
	2615063327216 [label=AccumulateGrad]
	2615063327456 -> 2615063327120
	2615063731216 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2615063731216 -> 2615063327456
	2615063327456 [label=AccumulateGrad]
	2615063327744 -> 2615063327696
	2615063731600 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2615063731600 -> 2615063327744
	2615063327744 [label=AccumulateGrad]
	2615063327600 -> 2615063327936
	2615063731696 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2615063731696 -> 2615063327600
	2615063327600 [label=AccumulateGrad]
	2615063327792 -> 2615063327936
	2615063731792 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2615063731792 -> 2615063327792
	2615063327792 [label=AccumulateGrad]
	2615063327984 -> 2615063328224
	2615063328080 -> 2615063328464
	2615063732752 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2615063732752 -> 2615063328080
	2615063328080 [label=AccumulateGrad]
	2615063328704 -> 2615063328608
	2615063732848 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2615063732848 -> 2615063328704
	2615063328704 [label=AccumulateGrad]
	2615063328656 -> 2615063328608
	2615063732944 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2615063732944 -> 2615063328656
	2615063328656 [label=AccumulateGrad]
	2615063328752 -> 2615063329184
	2615063733328 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2615063733328 -> 2615063328752
	2615063328752 [label=AccumulateGrad]
	2615063329088 -> 2615063329136
	2615063733424 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2615063733424 -> 2615063329088
	2615063329088 [label=AccumulateGrad]
	2615063329280 -> 2615063329136
	2615063733520 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2615063733520 -> 2615063329280
	2615063329280 [label=AccumulateGrad]
	2615063329040 -> 2615063329232
	2615063329040 [label=CudnnBatchNormBackward0]
	2615063328272 -> 2615063329040
	2615063328272 [label=ConvolutionBackward0]
	2615063328176 -> 2615063328272
	2615063328320 -> 2615063328272
	2615063732176 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2615063732176 -> 2615063328320
	2615063328320 [label=AccumulateGrad]
	2615063328896 -> 2615063329040
	2615063732272 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2615063732272 -> 2615063328896
	2615063328896 [label=AccumulateGrad]
	2615063328944 -> 2615063329040
	2615063732368 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2615063732368 -> 2615063328944
	2615063328944 [label=AccumulateGrad]
	2615063329424 -> 2615063329616
	2615063733904 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2615063733904 -> 2615063329424
	2615063329424 [label=AccumulateGrad]
	2615063329520 -> 2615063329712
	2615063734000 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2615063734000 -> 2615063329520
	2615063329520 [label=AccumulateGrad]
	2615063329904 -> 2615063329712
	2615063734096 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2615063734096 -> 2615063329904
	2615063329904 [label=AccumulateGrad]
	2615063330048 -> 2615063330000
	2615063734480 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2615063734480 -> 2615063330048
	2615063330048 [label=AccumulateGrad]
	2615063330192 -> 2615063330384
	2615063734576 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2615063734576 -> 2615063330192
	2615063330192 [label=AccumulateGrad]
	2615063330336 -> 2615063330384
	2615063734672 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2615063734672 -> 2615063330336
	2615063330336 [label=AccumulateGrad]
	2615063330672 -> 2615063316080
	2615063316128 -> 2615063317328
	2615063735632 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2615063735632 -> 2615063316128
	2615063316128 [label=AccumulateGrad]
	2615063317952 -> 2615063317808
	2615063735728 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2615063735728 -> 2615063317952
	2615063317952 [label=AccumulateGrad]
	2615063318288 -> 2615063317808
	2615063735824 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2615063735824 -> 2615063318288
	2615063318288 [label=AccumulateGrad]
	2615063318768 -> 2615063319872
	2615063736208 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2615063736208 -> 2615063318768
	2615063318768 [label=AccumulateGrad]
	2615063319728 -> 2615063320208
	2615063736304 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2615063736304 -> 2615063319728
	2615063319728 [label=AccumulateGrad]
	2615063320352 -> 2615063320208
	2615063736400 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2615063736400 -> 2615063320352
	2615063320352 [label=AccumulateGrad]
	2615063320832 -> 2615063320688
	2615063320832 [label=CudnnBatchNormBackward0]
	2615063316416 -> 2615063320832
	2615063316416 [label=ConvolutionBackward0]
	2615063316848 -> 2615063316416
	2615063315072 -> 2615063316416
	2615063735056 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2615063735056 -> 2615063315072
	2615063315072 [label=AccumulateGrad]
	2615063319392 -> 2615063320832
	2615063735152 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2615063735152 -> 2615063319392
	2615063319392 [label=AccumulateGrad]
	2615063319248 -> 2615063320832
	2615063735248 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2615063735248 -> 2615063319248
	2615063319248 [label=AccumulateGrad]
	2615063321168 -> 2615063322128
	2615063736784 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2615063736784 -> 2615063321168
	2615063321168 [label=AccumulateGrad]
	2615063322752 -> 2615063322608
	2615063736880 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2615063736880 -> 2615063322752
	2615063322752 [label=AccumulateGrad]
	2615063323088 -> 2615063322608
	2615063736976 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2615063736976 -> 2615063323088
	2615063323088 [label=AccumulateGrad]
	2615063323568 -> 2615063324672
	2615063737360 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2615063737360 -> 2615063323568
	2615063323568 [label=AccumulateGrad]
	2615063324528 -> 2615063325008
	2615063737456 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2615063737456 -> 2615063324528
	2615063324528 [label=AccumulateGrad]
	2615063325152 -> 2615063325008
	2615063737552 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2615063737552 -> 2615063325152
	2615063325152 [label=AccumulateGrad]
	2615063325632 -> 2615063325488
	2615063326928 -> 2615063328512
	2615063326928 [label=TBackward0]
	2615063326112 -> 2615063326928
	2615063738224 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2615063738224 -> 2615063326112
	2615063326112 [label=AccumulateGrad]
	2615063328512 -> 2615042149072
}
