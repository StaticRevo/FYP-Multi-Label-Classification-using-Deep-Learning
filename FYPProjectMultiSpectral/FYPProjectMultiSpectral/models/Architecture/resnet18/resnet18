digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2660458925520 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2660467881392 [label=AddmmBackward0]
	2660467881056 -> 2660467881392
	2660467784688 [label="fc.bias
 (19)" fillcolor=lightblue]
	2660467784688 -> 2660467881056
	2660467881056 [label=AccumulateGrad]
	2660467880432 -> 2660467881392
	2660467880432 [label=ViewBackward0]
	2660467879952 -> 2660467880432
	2660467879952 [label=MeanBackward1]
	2660467879616 -> 2660467879952
	2660467879616 [label=ReluBackward0]
	2660467879136 -> 2660467879616
	2660467879136 [label=AddBackward0]
	2660467878656 -> 2660467879136
	2660467878656 [label=CudnnBatchNormBackward0]
	2660467877552 -> 2660467878656
	2660467877552 [label=ConvolutionBackward0]
	2660467876592 -> 2660467877552
	2660467876592 [label=ReluBackward0]
	2660467876256 -> 2660467876592
	2660467876256 [label=CudnnBatchNormBackward0]
	2660467875776 -> 2660467876256
	2660467875776 [label=ConvolutionBackward0]
	2660467878512 -> 2660467875776
	2660467878512 [label=ReluBackward0]
	2660467874336 -> 2660467878512
	2660467874336 [label=AddBackward0]
	2660467873856 -> 2660467874336
	2660467873856 [label=CudnnBatchNormBackward0]
	2660467872752 -> 2660467873856
	2660467872752 [label=ConvolutionBackward0]
	2660467871792 -> 2660467872752
	2660467871792 [label=ReluBackward0]
	2660467871456 -> 2660467871792
	2660467871456 [label=CudnnBatchNormBackward0]
	2660467870976 -> 2660467871456
	2660467870976 [label=ConvolutionBackward0]
	2660467868720 -> 2660467870976
	2660467868720 [label=ReluBackward0]
	2660467883984 -> 2660467868720
	2660467883984 [label=AddBackward0]
	2660467883552 -> 2660467883984
	2660467883552 [label=CudnnBatchNormBackward0]
	2660467883360 -> 2660467883552
	2660467883360 [label=ConvolutionBackward0]
	2660467883264 -> 2660467883360
	2660467883264 [label=ReluBackward0]
	2660467882928 -> 2660467883264
	2660467882928 [label=CudnnBatchNormBackward0]
	2660467882736 -> 2660467882928
	2660467882736 [label=ConvolutionBackward0]
	2660467883744 -> 2660467882736
	2660467883744 [label=ReluBackward0]
	2660467882448 -> 2660467883744
	2660467882448 [label=AddBackward0]
	2660467882256 -> 2660467882448
	2660467882256 [label=CudnnBatchNormBackward0]
	2660467882304 -> 2660467882256
	2660467882304 [label=ConvolutionBackward0]
	2660467881920 -> 2660467882304
	2660467881920 [label=ReluBackward0]
	2660467881680 -> 2660467881920
	2660467881680 [label=CudnnBatchNormBackward0]
	2660467881632 -> 2660467881680
	2660467881632 [label=ConvolutionBackward0]
	2660467881296 -> 2660467881632
	2660467881296 [label=ReluBackward0]
	2660467881344 -> 2660467881296
	2660467881344 [label=AddBackward0]
	2660467881248 -> 2660467881344
	2660467881248 [label=CudnnBatchNormBackward0]
	2660467880816 -> 2660467881248
	2660467880816 [label=ConvolutionBackward0]
	2660467880672 -> 2660467880816
	2660467880672 [label=ReluBackward0]
	2660467880480 -> 2660467880672
	2660467880480 [label=CudnnBatchNormBackward0]
	2660467880144 -> 2660467880480
	2660467880144 [label=ConvolutionBackward0]
	2660467881152 -> 2660467880144
	2660467881152 [label=ReluBackward0]
	2660467880000 -> 2660467881152
	2660467880000 [label=AddBackward0]
	2660467879664 -> 2660467880000
	2660467879664 [label=CudnnBatchNormBackward0]
	2660467879712 -> 2660467879664
	2660467879712 [label=ConvolutionBackward0]
	2660467879376 -> 2660467879712
	2660467879376 [label=ReluBackward0]
	2660467879424 -> 2660467879376
	2660467879424 [label=CudnnBatchNormBackward0]
	2660467879328 -> 2660467879424
	2660467879328 [label=ConvolutionBackward0]
	2660467878704 -> 2660467879328
	2660467878704 [label=ReluBackward0]
	2660467878752 -> 2660467878704
	2660467878752 [label=AddBackward0]
	2660467878608 -> 2660467878752
	2660467878608 [label=CudnnBatchNormBackward0]
	2660467878224 -> 2660467878608
	2660467878224 [label=ConvolutionBackward0]
	2660467878368 -> 2660467878224
	2660467878368 [label=ReluBackward0]
	2660467877936 -> 2660467878368
	2660467877936 [label=CudnnBatchNormBackward0]
	2660467877840 -> 2660467877936
	2660467877840 [label=ConvolutionBackward0]
	2660467878848 -> 2660467877840
	2660467878848 [label=ReluBackward0]
	2660467877456 -> 2660467878848
	2660467877456 [label=AddBackward0]
	2660467877360 -> 2660467877456
	2660467877360 [label=CudnnBatchNormBackward0]
	2660467877408 -> 2660467877360
	2660467877408 [label=ConvolutionBackward0]
	2660467876784 -> 2660467877408
	2660467876784 [label=ReluBackward0]
	2660467876832 -> 2660467876784
	2660467876832 [label=CudnnBatchNormBackward0]
	2660467876688 -> 2660467876832
	2660467876688 [label=ConvolutionBackward0]
	2660467877264 -> 2660467876688
	2660467877264 [label=MaxPool2DWithIndicesBackward0]
	2660467876352 -> 2660467877264
	2660467876352 [label=ReluBackward0]
	2660467876208 -> 2660467876352
	2660467876208 [label=CudnnBatchNormBackward0]
	2660467876016 -> 2660467876208
	2660467876016 [label=ConvolutionBackward0]
	2660467875872 -> 2660467876016
	2660467784496 [label="conv1.weight
 (64, 5, 7, 7)" fillcolor=lightblue]
	2660467784496 -> 2660467875872
	2660467875872 [label=AccumulateGrad]
	2660467876160 -> 2660467876208
	2660467890352 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2660467890352 -> 2660467876160
	2660467876160 [label=AccumulateGrad]
	2660467876496 -> 2660467876208
	2660467890064 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2660467890064 -> 2660467876496
	2660467876496 [label=AccumulateGrad]
	2660467876400 -> 2660467876688
	2660467890544 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2660467890544 -> 2660467876400
	2660467876400 [label=AccumulateGrad]
	2660467876928 -> 2660467876832
	2660467890640 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2660467890640 -> 2660467876928
	2660467876928 [label=AccumulateGrad]
	2660467876880 -> 2660467876832
	2660467890736 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2660467890736 -> 2660467876880
	2660467876880 [label=AccumulateGrad]
	2660467876976 -> 2660467877408
	2660467889968 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2660467889968 -> 2660467876976
	2660467876976 [label=AccumulateGrad]
	2660467877312 -> 2660467877360
	2660467889872 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2660467889872 -> 2660467877312
	2660467877312 [label=AccumulateGrad]
	2660467877504 -> 2660467877360
	2660467889104 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2660467889104 -> 2660467877504
	2660467877504 [label=AccumulateGrad]
	2660467877264 -> 2660467877456
	2660467877648 -> 2660467877840
	2660467889200 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2660467889200 -> 2660467877648
	2660467877648 [label=AccumulateGrad]
	2660467877744 -> 2660467877936
	2660467889296 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2660467889296 -> 2660467877744
	2660467877744 [label=AccumulateGrad]
	2660467878128 -> 2660467877936
	2660467889392 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2660467889392 -> 2660467878128
	2660467878128 [label=AccumulateGrad]
	2660467878272 -> 2660467878224
	2660467889680 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2660467889680 -> 2660467878272
	2660467878272 [label=AccumulateGrad]
	2660467878416 -> 2660467878608
	2660467888720 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2660467888720 -> 2660467878416
	2660467878416 [label=AccumulateGrad]
	2660467878560 -> 2660467878608
	2660467888624 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2660467888624 -> 2660467878560
	2660467878560 [label=AccumulateGrad]
	2660467878848 -> 2660467878752
	2660467878896 -> 2660467879328
	2660467888528 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2660467888528 -> 2660467878896
	2660467878896 [label=AccumulateGrad]
	2660467879232 -> 2660467879424
	2660467888432 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2660467888432 -> 2660467879232
	2660467879232 [label=AccumulateGrad]
	2660467879184 -> 2660467879424
	2660467776432 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2660467776432 -> 2660467879184
	2660467879184 [label=AccumulateGrad]
	2660467879520 -> 2660467879712
	2660467776816 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2660467776816 -> 2660467879520
	2660467879520 [label=AccumulateGrad]
	2660467879904 -> 2660467879664
	2660467776912 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2660467776912 -> 2660467879904
	2660467879904 [label=AccumulateGrad]
	2660467879760 -> 2660467879664
	2660467777008 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2660467777008 -> 2660467879760
	2660467879760 [label=AccumulateGrad]
	2660467879856 -> 2660467880000
	2660467879856 [label=CudnnBatchNormBackward0]
	2660467879040 -> 2660467879856
	2660467879040 [label=ConvolutionBackward0]
	2660467878704 -> 2660467879040
	2660467878800 -> 2660467879040
	2660467885936 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2660467885936 -> 2660467878800
	2660467878800 [label=AccumulateGrad]
	2660467879568 -> 2660467879856
	2660467886128 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2660467886128 -> 2660467879568
	2660467879568 [label=AccumulateGrad]
	2660467879808 -> 2660467879856
	2660467887760 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2660467887760 -> 2660467879808
	2660467879808 [label=AccumulateGrad]
	2660467880288 -> 2660467880144
	2660467777392 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2660467777392 -> 2660467880288
	2660467880288 [label=AccumulateGrad]
	2660467880336 -> 2660467880480
	2660467777488 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2660467777488 -> 2660467880336
	2660467880336 [label=AccumulateGrad]
	2660467880768 -> 2660467880480
	2660467777584 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2660467777584 -> 2660467880768
	2660467880768 [label=AccumulateGrad]
	2660467880864 -> 2660467880816
	2660467777968 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2660467777968 -> 2660467880864
	2660467880864 [label=AccumulateGrad]
	2660467880960 -> 2660467881248
	2660467778064 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2660467778064 -> 2660467880960
	2660467880960 [label=AccumulateGrad]
	2660467881008 -> 2660467881248
	2660467778160 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2660467778160 -> 2660467881008
	2660467881008 [label=AccumulateGrad]
	2660467881152 -> 2660467881344
	2660467881440 -> 2660467881632
	2660467779120 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2660467779120 -> 2660467881440
	2660467881440 [label=AccumulateGrad]
	2660467881824 -> 2660467881680
	2660467779216 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2660467779216 -> 2660467881824
	2660467881824 [label=AccumulateGrad]
	2660467881776 -> 2660467881680
	2660467779312 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2660467779312 -> 2660467881776
	2660467881776 [label=AccumulateGrad]
	2660467881968 -> 2660467882304
	2660467779696 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2660467779696 -> 2660467881968
	2660467881968 [label=AccumulateGrad]
	2660467882160 -> 2660467882256
	2660467779792 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2660467779792 -> 2660467882160
	2660467882160 [label=AccumulateGrad]
	2660467882064 -> 2660467882256
	2660467779888 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2660467779888 -> 2660467882064
	2660467882064 [label=AccumulateGrad]
	2660467882400 -> 2660467882448
	2660467882400 [label=CudnnBatchNormBackward0]
	2660467881488 -> 2660467882400
	2660467881488 [label=ConvolutionBackward0]
	2660467881296 -> 2660467881488
	2660467881104 -> 2660467881488
	2660467778544 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2660467778544 -> 2660467881104
	2660467881104 [label=AccumulateGrad]
	2660467882208 -> 2660467882400
	2660467778640 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2660467778640 -> 2660467882208
	2660467882208 [label=AccumulateGrad]
	2660467882112 -> 2660467882400
	2660467778736 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2660467778736 -> 2660467882112
	2660467882112 [label=AccumulateGrad]
	2660467882592 -> 2660467882736
	2660467780272 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2660467780272 -> 2660467882592
	2660467882592 [label=AccumulateGrad]
	2660467882880 -> 2660467882928
	2660467780368 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2660467780368 -> 2660467882880
	2660467882880 [label=AccumulateGrad]
	2660467883072 -> 2660467882928
	2660467780464 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2660467780464 -> 2660467883072
	2660467883072 [label=AccumulateGrad]
	2660467883120 -> 2660467883360
	2660467780848 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2660467780848 -> 2660467883120
	2660467883120 [label=AccumulateGrad]
	2660467883408 -> 2660467883552
	2660467780944 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2660467780944 -> 2660467883408
	2660467883408 [label=AccumulateGrad]
	2660467883648 -> 2660467883552
	2660467781040 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2660467781040 -> 2660467883648
	2660467883648 [label=AccumulateGrad]
	2660467883744 -> 2660467883984
	2660467868240 -> 2660467870976
	2660467782000 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2660467782000 -> 2660467868240
	2660467868240 [label=AccumulateGrad]
	2660467870832 -> 2660467871456
	2660467782096 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2660467782096 -> 2660467870832
	2660467870832 [label=AccumulateGrad]
	2660467871936 -> 2660467871456
	2660467782192 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2660467782192 -> 2660467871936
	2660467871936 [label=AccumulateGrad]
	2660467872416 -> 2660467872752
	2660467782576 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2660467782576 -> 2660467872416
	2660467872416 [label=AccumulateGrad]
	2660467873376 -> 2660467873856
	2660467782672 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2660467782672 -> 2660467873376
	2660467873376 [label=AccumulateGrad]
	2660467873232 -> 2660467873856
	2660467782768 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2660467782768 -> 2660467873232
	2660467873232 [label=AccumulateGrad]
	2660467873712 -> 2660467874336
	2660467873712 [label=CudnnBatchNormBackward0]
	2660467870256 -> 2660467873712
	2660467870256 [label=ConvolutionBackward0]
	2660467868720 -> 2660467870256
	2660467869104 -> 2660467870256
	2660467781424 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2660467781424 -> 2660467869104
	2660467869104 [label=AccumulateGrad]
	2660467872272 -> 2660467873712
	2660467781520 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2660467781520 -> 2660467872272
	2660467872272 [label=AccumulateGrad]
	2660467872896 -> 2660467873712
	2660467781616 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2660467781616 -> 2660467872896
	2660467872896 [label=AccumulateGrad]
	2660467874816 -> 2660467875776
	2660467783152 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2660467783152 -> 2660467874816
	2660467874816 [label=AccumulateGrad]
	2660467875632 -> 2660467876256
	2660467783248 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2660467783248 -> 2660467875632
	2660467875632 [label=AccumulateGrad]
	2660467876736 -> 2660467876256
	2660467783344 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2660467783344 -> 2660467876736
	2660467876736 [label=AccumulateGrad]
	2660467877216 -> 2660467877552
	2660467783728 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2660467783728 -> 2660467877216
	2660467877216 [label=AccumulateGrad]
	2660467878176 -> 2660467878656
	2660467783824 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2660467783824 -> 2660467878176
	2660467878176 [label=AccumulateGrad]
	2660467878032 -> 2660467878656
	2660467783920 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2660467783920 -> 2660467878032
	2660467878032 [label=AccumulateGrad]
	2660467878512 -> 2660467879136
	2660467880576 -> 2660467881392
	2660467880576 [label=TBackward0]
	2660467878992 -> 2660467880576
	2660467784592 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2660467784592 -> 2660467878992
	2660467878992 [label=AccumulateGrad]
	2660467881392 -> 2660458925520
}
