digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1730050027024 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1730138797296 [label=AddmmBackward0]
	1730138796960 -> 1730138797296
	1730056401264 [label="fc.bias
 (19)" fillcolor=lightblue]
	1730056401264 -> 1730138796960
	1730138796960 [label=AccumulateGrad]
	1730138796336 -> 1730138797296
	1730138796336 [label=ViewBackward0]
	1730138795856 -> 1730138796336
	1730138795856 [label=MeanBackward1]
	1730138795520 -> 1730138795856
	1730138795520 [label=ReluBackward0]
	1730138795040 -> 1730138795520
	1730138795040 [label=AddBackward0]
	1730138794560 -> 1730138795040
	1730138794560 [label=CudnnBatchNormBackward0]
	1730138793456 -> 1730138794560
	1730138793456 [label=ConvolutionBackward0]
	1730138792496 -> 1730138793456
	1730138792496 [label=ReluBackward0]
	1730138792160 -> 1730138792496
	1730138792160 [label=CudnnBatchNormBackward0]
	1730138785104 -> 1730138792160
	1730138785104 [label=ConvolutionBackward0]
	1730138789040 -> 1730138785104
	1730138789040 [label=ReluBackward0]
	1730138799984 -> 1730138789040
	1730138799984 [label=CudnnBatchNormBackward0]
	1730138800032 -> 1730138799984
	1730138800032 [label=ConvolutionBackward0]
	1730138794416 -> 1730138800032
	1730138794416 [label=ReluBackward0]
	1730138799648 -> 1730138794416
	1730138799648 [label=AddBackward0]
	1730138799552 -> 1730138799648
	1730138799552 [label=CudnnBatchNormBackward0]
	1730138799120 -> 1730138799552
	1730138799120 [label=ConvolutionBackward0]
	1730138798976 -> 1730138799120
	1730138798976 [label=ReluBackward0]
	1730138798784 -> 1730138798976
	1730138798784 [label=CudnnBatchNormBackward0]
	1730138798448 -> 1730138798784
	1730138798448 [label=ConvolutionBackward0]
	1730138798592 -> 1730138798448
	1730138798592 [label=ReluBackward0]
	1730138798160 -> 1730138798592
	1730138798160 [label=CudnnBatchNormBackward0]
	1730138798064 -> 1730138798160
	1730138798064 [label=ConvolutionBackward0]
	1730138799456 -> 1730138798064
	1730138799456 [label=ReluBackward0]
	1730138797680 -> 1730138799456
	1730138797680 [label=AddBackward0]
	1730138797584 -> 1730138797680
	1730138797584 [label=CudnnBatchNormBackward0]
	1730138797632 -> 1730138797584
	1730138797632 [label=ConvolutionBackward0]
	1730138797008 -> 1730138797632
	1730138797008 [label=ReluBackward0]
	1730138797056 -> 1730138797008
	1730138797056 [label=CudnnBatchNormBackward0]
	1730138796912 -> 1730138797056
	1730138796912 [label=ConvolutionBackward0]
	1730138796624 -> 1730138796912
	1730138796624 [label=ReluBackward0]
	1730138796672 -> 1730138796624
	1730138796672 [label=CudnnBatchNormBackward0]
	1730138796384 -> 1730138796672
	1730138796384 [label=ConvolutionBackward0]
	1730138796288 -> 1730138796384
	1730138796288 [label=ReluBackward0]
	1730138795952 -> 1730138796288
	1730138795952 [label=AddBackward0]
	1730138795760 -> 1730138795952
	1730138795760 [label=CudnnBatchNormBackward0]
	1730138795808 -> 1730138795760
	1730138795808 [label=ConvolutionBackward0]
	1730138795424 -> 1730138795808
	1730138795424 [label=ReluBackward0]
	1730138795184 -> 1730138795424
	1730138795184 [label=CudnnBatchNormBackward0]
	1730138795136 -> 1730138795184
	1730138795136 [label=ConvolutionBackward0]
	1730138794800 -> 1730138795136
	1730138794800 [label=ReluBackward0]
	1730138794848 -> 1730138794800
	1730138794848 [label=CudnnBatchNormBackward0]
	1730138794752 -> 1730138794848
	1730138794752 [label=ConvolutionBackward0]
	1730138795904 -> 1730138794752
	1730138795904 [label=ReluBackward0]
	1730138794368 -> 1730138795904
	1730138794368 [label=AddBackward0]
	1730138794272 -> 1730138794368
	1730138794272 [label=CudnnBatchNormBackward0]
	1730138793840 -> 1730138794272
	1730138793840 [label=ConvolutionBackward0]
	1730138793696 -> 1730138793840
	1730138793696 [label=ReluBackward0]
	1730138793504 -> 1730138793696
	1730138793504 [label=CudnnBatchNormBackward0]
	1730138793168 -> 1730138793504
	1730138793168 [label=ConvolutionBackward0]
	1730138793312 -> 1730138793168
	1730138793312 [label=ReluBackward0]
	1730138792880 -> 1730138793312
	1730138792880 [label=CudnnBatchNormBackward0]
	1730138792784 -> 1730138792880
	1730138792784 [label=ConvolutionBackward0]
	1730138794176 -> 1730138792784
	1730138794176 [label=ReluBackward0]
	1730138792400 -> 1730138794176
	1730138792400 [label=AddBackward0]
	1730138792304 -> 1730138792400
	1730138792304 [label=CudnnBatchNormBackward0]
	1730138792352 -> 1730138792304
	1730138792352 [label=ConvolutionBackward0]
	1730138791344 -> 1730138792352
	1730138791344 [label=ReluBackward0]
	1730138791776 -> 1730138791344
	1730138791776 [label=CudnnBatchNormBackward0]
	1730138787216 -> 1730138791776
	1730138787216 [label=ConvolutionBackward0]
	1730138790480 -> 1730138787216
	1730138790480 [label=ReluBackward0]
	1730138786688 -> 1730138790480
	1730138786688 [label=CudnnBatchNormBackward0]
	1730138788320 -> 1730138786688
	1730138788320 [label=ConvolutionBackward0]
	1730138792208 -> 1730138788320
	1730138792208 [label=ReluBackward0]
	1730138789232 -> 1730138792208
	1730138789232 [label=AddBackward0]
	1730138790768 -> 1730138789232
	1730138790768 [label=CudnnBatchNormBackward0]
	1730138788704 -> 1730138790768
	1730138788704 [label=ConvolutionBackward0]
	1730138791680 -> 1730138788704
	1730138791680 [label=ReluBackward0]
	1730138789616 -> 1730138791680
	1730138789616 [label=CudnnBatchNormBackward0]
	1730138786544 -> 1730138789616
	1730138786544 [label=ConvolutionBackward0]
	1730138791200 -> 1730138786544
	1730138791200 [label=ReluBackward0]
	1730138789808 -> 1730138791200
	1730138789808 [label=CudnnBatchNormBackward0]
	1730138784288 -> 1730138789808
	1730138784288 [label=ConvolutionBackward0]
	1730138790144 -> 1730138784288
	1730138790144 [label=ReluBackward0]
	1730138787072 -> 1730138790144
	1730138787072 [label=AddBackward0]
	1730138790576 -> 1730138787072
	1730138790576 [label=CudnnBatchNormBackward0]
	1730138790864 -> 1730138790576
	1730138790864 [label=ConvolutionBackward0]
	1730138791584 -> 1730138790864
	1730138791584 [label=ReluBackward0]
	1730138789952 -> 1730138791584
	1730138789952 [label=CudnnBatchNormBackward0]
	1730138786256 -> 1730138789952
	1730138786256 [label=ConvolutionBackward0]
	1730138787600 -> 1730138786256
	1730138787600 [label=ReluBackward0]
	1730138788848 -> 1730138787600
	1730138788848 [label=CudnnBatchNormBackward0]
	1730138788560 -> 1730138788848
	1730138788560 [label=ConvolutionBackward0]
	1730138789856 -> 1730138788560
	1730138789856 [label=ReluBackward0]
	1730138787888 -> 1730138789856
	1730138787888 [label=AddBackward0]
	1730138786016 -> 1730138787888
	1730138786016 [label=CudnnBatchNormBackward0]
	1730138789424 -> 1730138786016
	1730138789424 [label=ConvolutionBackward0]
	1730138787408 -> 1730138789424
	1730138787408 [label=ReluBackward0]
	1730138786496 -> 1730138787408
	1730138786496 [label=CudnnBatchNormBackward0]
	1730138789904 -> 1730138786496
	1730138789904 [label=ConvolutionBackward0]
	1730055027296 -> 1730138789904
	1730055027296 [label=ReluBackward0]
	1730055028016 -> 1730055027296
	1730055028016 [label=CudnnBatchNormBackward0]
	1730055026864 -> 1730055028016
	1730055026864 [label=ConvolutionBackward0]
	1730055028256 -> 1730055026864
	1730055028256 [label=ReluBackward0]
	1730055028448 -> 1730055028256
	1730055028448 [label=AddBackward0]
	1730055026480 -> 1730055028448
	1730055026480 [label=CudnnBatchNormBackward0]
	1730055028160 -> 1730055026480
	1730055028160 [label=ConvolutionBackward0]
	1730055028208 -> 1730055028160
	1730055028208 [label=ReluBackward0]
	1730055027104 -> 1730055028208
	1730055027104 [label=CudnnBatchNormBackward0]
	1730055027632 -> 1730055027104
	1730055027632 [label=ConvolutionBackward0]
	1730138827472 -> 1730055027632
	1730138827472 [label=ReluBackward0]
	1730050383136 -> 1730138827472
	1730050383136 [label=CudnnBatchNormBackward0]
	1730050382656 -> 1730050383136
	1730050382656 [label=ConvolutionBackward0]
	1730055026912 -> 1730050382656
	1730055026912 [label=ReluBackward0]
	1730050381216 -> 1730055026912
	1730050381216 [label=AddBackward0]
	1730050380736 -> 1730050381216
	1730050380736 [label=CudnnBatchNormBackward0]
	1730050379632 -> 1730050380736
	1730050379632 [label=ConvolutionBackward0]
	1730050378672 -> 1730050379632
	1730050378672 [label=ReluBackward0]
	1730050378336 -> 1730050378672
	1730050378336 [label=CudnnBatchNormBackward0]
	1730050377856 -> 1730050378336
	1730050377856 [label=ConvolutionBackward0]
	1730050376896 -> 1730050377856
	1730050376896 [label=ReluBackward0]
	1730050375792 -> 1730050376896
	1730050375792 [label=CudnnBatchNormBackward0]
	1730050383568 -> 1730050375792
	1730050383568 [label=ConvolutionBackward0]
	1730050380592 -> 1730050383568
	1730050380592 [label=ReluBackward0]
	1730050383232 -> 1730050380592
	1730050383232 [label=AddBackward0]
	1730050383088 -> 1730050383232
	1730050383088 [label=CudnnBatchNormBackward0]
	1730050382704 -> 1730050383088
	1730050382704 [label=ConvolutionBackward0]
	1730050382848 -> 1730050382704
	1730050382848 [label=ReluBackward0]
	1730050382416 -> 1730050382848
	1730050382416 [label=CudnnBatchNormBackward0]
	1730050382320 -> 1730050382416
	1730050382320 [label=ConvolutionBackward0]
	1730050382128 -> 1730050382320
	1730050382128 [label=ReluBackward0]
	1730050381744 -> 1730050382128
	1730050381744 [label=CudnnBatchNormBackward0]
	1730050381984 -> 1730050381744
	1730050381984 [label=ConvolutionBackward0]
	1730050383328 -> 1730050381984
	1730050383328 [label=ReluBackward0]
	1730050381264 -> 1730050383328
	1730050381264 [label=AddBackward0]
	1730050381504 -> 1730050381264
	1730050381504 [label=CudnnBatchNormBackward0]
	1730050381168 -> 1730050381504
	1730050381168 [label=ConvolutionBackward0]
	1730050380880 -> 1730050381168
	1730050380880 [label=ReluBackward0]
	1730050380928 -> 1730050380880
	1730050380928 [label=CudnnBatchNormBackward0]
	1730050380640 -> 1730050380928
	1730050380640 [label=ConvolutionBackward0]
	1730050380544 -> 1730050380640
	1730050380544 [label=ReluBackward0]
	1730050380208 -> 1730050380544
	1730050380208 [label=CudnnBatchNormBackward0]
	1730050380016 -> 1730050380208
	1730050380016 [label=ConvolutionBackward0]
	1730050379872 -> 1730050380016
	1730050379872 [label=ReluBackward0]
	1730050379680 -> 1730050379872
	1730050379680 [label=AddBackward0]
	1730050379344 -> 1730050379680
	1730050379344 [label=CudnnBatchNormBackward0]
	1730050379392 -> 1730050379344
	1730050379392 [label=ConvolutionBackward0]
	1730050379056 -> 1730050379392
	1730050379056 [label=ReluBackward0]
	1730050379104 -> 1730050379056
	1730050379104 [label=CudnnBatchNormBackward0]
	1730050379008 -> 1730050379104
	1730050379008 [label=ConvolutionBackward0]
	1730050378384 -> 1730050379008
	1730050378384 [label=ReluBackward0]
	1730050378432 -> 1730050378384
	1730050378432 [label=CudnnBatchNormBackward0]
	1730050378288 -> 1730050378432
	1730050378288 [label=ConvolutionBackward0]
	1730050379536 -> 1730050378288
	1730050379536 [label=ReluBackward0]
	1730050377952 -> 1730050379536
	1730050377952 [label=AddBackward0]
	1730050377808 -> 1730050377952
	1730050377808 [label=CudnnBatchNormBackward0]
	1730050377424 -> 1730050377808
	1730050377424 [label=ConvolutionBackward0]
	1730050377568 -> 1730050377424
	1730050377568 [label=ReluBackward0]
	1730050377136 -> 1730050377568
	1730050377136 [label=CudnnBatchNormBackward0]
	1730050377040 -> 1730050377136
	1730050377040 [label=ConvolutionBackward0]
	1730050376848 -> 1730050377040
	1730050376848 [label=ReluBackward0]
	1730050376464 -> 1730050376848
	1730050376464 [label=CudnnBatchNormBackward0]
	1730050376704 -> 1730050376464
	1730050376704 [label=ConvolutionBackward0]
	1730050378048 -> 1730050376704
	1730050378048 [label=ReluBackward0]
	1730050375984 -> 1730050378048
	1730050375984 [label=AddBackward0]
	1730050376224 -> 1730050375984
	1730050376224 [label=CudnnBatchNormBackward0]
	1730050375888 -> 1730050376224
	1730050375888 [label=ConvolutionBackward0]
	1730050384048 -> 1730050375888
	1730050384048 [label=ReluBackward0]
	1730050384384 -> 1730050384048
	1730050384384 [label=CudnnBatchNormBackward0]
	1730050384480 -> 1730050384384
	1730050384480 [label=ConvolutionBackward0]
	1730050384672 -> 1730050384480
	1730050384672 [label=ReluBackward0]
	1730050384816 -> 1730050384672
	1730050384816 [label=CudnnBatchNormBackward0]
	1730050384912 -> 1730050384816
	1730050384912 [label=ConvolutionBackward0]
	1730050385104 -> 1730050384912
	1730050385104 [label=MaxPool2DWithIndicesBackward0]
	1730050385248 -> 1730050385104
	1730050385248 [label=ReluBackward0]
	1730050385344 -> 1730050385248
	1730050385344 [label=CudnnBatchNormBackward0]
	1730050385440 -> 1730050385344
	1730050385440 [label=ConvolutionBackward0]
	1730050385632 -> 1730050385440
	1730056401072 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1730056401072 -> 1730050385632
	1730050385632 [label=AccumulateGrad]
	1730050385392 -> 1730050385344
	1730113879216 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1730113879216 -> 1730050385392
	1730050385392 [label=AccumulateGrad]
	1730050385152 -> 1730050385344
	1730113878928 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1730113878928 -> 1730050385152
	1730050385152 [label=AccumulateGrad]
	1730050385056 -> 1730050384912
	1730113878832 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1730113878832 -> 1730050385056
	1730050385056 [label=AccumulateGrad]
	1730050384864 -> 1730050384816
	1730113878736 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1730113878736 -> 1730050384864
	1730050384864 [label=AccumulateGrad]
	1730050384720 -> 1730050384816
	1730113877968 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1730113877968 -> 1730050384720
	1730050384720 [label=AccumulateGrad]
	1730050384624 -> 1730050384480
	1730113878064 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1730113878064 -> 1730050384624
	1730050384624 [label=AccumulateGrad]
	1730050384432 -> 1730050384384
	1730113878160 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1730113878160 -> 1730050384432
	1730050384432 [label=AccumulateGrad]
	1730050384288 -> 1730050384384
	1730113878256 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1730113878256 -> 1730050384288
	1730050384288 [label=AccumulateGrad]
	1730050384192 -> 1730050375888
	1730113878544 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1730113878544 -> 1730050384192
	1730050384192 [label=AccumulateGrad]
	1730050376128 -> 1730050376224
	1730113877584 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1730113877584 -> 1730050376128
	1730050376128 [label=AccumulateGrad]
	1730050376032 -> 1730050376224
	1730113877488 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1730113877488 -> 1730050376032
	1730050376032 [label=AccumulateGrad]
	1730050376080 -> 1730050375984
	1730050376080 [label=CudnnBatchNormBackward0]
	1730050384576 -> 1730050376080
	1730050384576 [label=ConvolutionBackward0]
	1730050385104 -> 1730050384576
	1730050384960 -> 1730050384576
	1730113879408 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1730113879408 -> 1730050384960
	1730050384960 [label=AccumulateGrad]
	1730050375744 -> 1730050376080
	1730113879504 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1730113879504 -> 1730050375744
	1730050375744 [label=AccumulateGrad]
	1730050375840 -> 1730050376080
	1730113879600 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1730113879600 -> 1730050375840
	1730050375840 [label=AccumulateGrad]
	1730050376320 -> 1730050376704
	1730113877296 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1730113877296 -> 1730050376320
	1730050376320 [label=AccumulateGrad]
	1730050376560 -> 1730050376464
	1730113876816 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1730113876816 -> 1730050376560
	1730050376560 [label=AccumulateGrad]
	1730050376800 -> 1730050376464
	1730113875088 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1730113875088 -> 1730050376800
	1730050376800 [label=AccumulateGrad]
	1730050377088 -> 1730050377040
	1730113877392 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1730113877392 -> 1730050377088
	1730050377088 [label=AccumulateGrad]
	1730050376944 -> 1730050377136
	1730113877200 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1730113877200 -> 1730050376944
	1730050376944 [label=AccumulateGrad]
	1730050377328 -> 1730050377136
	1730056734384 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1730056734384 -> 1730050377328
	1730050377328 [label=AccumulateGrad]
	1730050377472 -> 1730050377424
	1730056734768 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1730056734768 -> 1730050377472
	1730050377472 [label=AccumulateGrad]
	1730050377616 -> 1730050377808
	1730056734864 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1730056734864 -> 1730050377616
	1730050377616 [label=AccumulateGrad]
	1730050377760 -> 1730050377808
	1730056734960 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1730056734960 -> 1730050377760
	1730050377760 [label=AccumulateGrad]
	1730050378048 -> 1730050377952
	1730050378000 -> 1730050378288
	1730056735344 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1730056735344 -> 1730050378000
	1730050378000 [label=AccumulateGrad]
	1730050378528 -> 1730050378432
	1730056735440 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1730056735440 -> 1730050378528
	1730050378528 [label=AccumulateGrad]
	1730050378480 -> 1730050378432
	1730056735536 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1730056735536 -> 1730050378480
	1730050378480 [label=AccumulateGrad]
	1730050378576 -> 1730050379008
	1730056735920 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1730056735920 -> 1730050378576
	1730050378576 [label=AccumulateGrad]
	1730050378912 -> 1730050379104
	1730056736016 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1730056736016 -> 1730050378912
	1730050378912 [label=AccumulateGrad]
	1730050378864 -> 1730050379104
	1730056736112 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1730056736112 -> 1730050378864
	1730050378864 [label=AccumulateGrad]
	1730050379200 -> 1730050379392
	1730056736496 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1730056736496 -> 1730050379200
	1730050379200 [label=AccumulateGrad]
	1730050379584 -> 1730050379344
	1730056736592 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1730056736592 -> 1730050379584
	1730050379584 [label=AccumulateGrad]
	1730050379440 -> 1730050379344
	1730056736688 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1730056736688 -> 1730050379440
	1730050379440 [label=AccumulateGrad]
	1730050379536 -> 1730050379680
	1730050380064 -> 1730050380016
	1730056737648 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1730056737648 -> 1730050380064
	1730050380064 [label=AccumulateGrad]
	1730050380160 -> 1730050380208
	1730056737744 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1730056737744 -> 1730050380160
	1730050380160 [label=AccumulateGrad]
	1730050380352 -> 1730050380208
	1730056737840 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1730056737840 -> 1730050380352
	1730050380352 [label=AccumulateGrad]
	1730050380400 -> 1730050380640
	1730056738224 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1730056738224 -> 1730050380400
	1730050380400 [label=AccumulateGrad]
	1730050380688 -> 1730050380928
	1730056738320 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1730056738320 -> 1730050380688
	1730050380688 [label=AccumulateGrad]
	1730050381024 -> 1730050380928
	1730056738416 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1730056738416 -> 1730050381024
	1730050381024 [label=AccumulateGrad]
	1730050380784 -> 1730050381168
	1730056738800 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1730056738800 -> 1730050380784
	1730050380784 [label=AccumulateGrad]
	1730050381408 -> 1730050381504
	1730056738896 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1730056738896 -> 1730050381408
	1730050381408 [label=AccumulateGrad]
	1730050381312 -> 1730050381504
	1730056738992 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1730056738992 -> 1730050381312
	1730050381312 [label=AccumulateGrad]
	1730050381360 -> 1730050381264
	1730050381360 [label=CudnnBatchNormBackward0]
	1730050380304 -> 1730050381360
	1730050380304 [label=ConvolutionBackward0]
	1730050379872 -> 1730050380304
	1730050379824 -> 1730050380304
	1730056737072 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1730056737072 -> 1730050379824
	1730050379824 [label=AccumulateGrad]
	1730050380976 -> 1730050381360
	1730056737168 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1730056737168 -> 1730050380976
	1730050380976 [label=AccumulateGrad]
	1730050381120 -> 1730050381360
	1730056737264 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1730056737264 -> 1730050381120
	1730050381120 [label=AccumulateGrad]
	1730050381600 -> 1730050381984
	1730056739376 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1730056739376 -> 1730050381600
	1730050381600 [label=AccumulateGrad]
	1730050381840 -> 1730050381744
	1730056739472 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1730056739472 -> 1730050381840
	1730050381840 [label=AccumulateGrad]
	1730050382080 -> 1730050381744
	1730056739568 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1730056739568 -> 1730050382080
	1730050382080 [label=AccumulateGrad]
	1730050382368 -> 1730050382320
	1730056739952 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1730056739952 -> 1730050382368
	1730050382368 [label=AccumulateGrad]
	1730050382224 -> 1730050382416
	1730056740048 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1730056740048 -> 1730050382224
	1730050382224 [label=AccumulateGrad]
	1730050382608 -> 1730050382416
	1730056740144 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1730056740144 -> 1730050382608
	1730050382608 [label=AccumulateGrad]
	1730050382752 -> 1730050382704
	1730056740528 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1730056740528 -> 1730050382752
	1730050382752 [label=AccumulateGrad]
	1730050382896 -> 1730050383088
	1730056740624 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1730056740624 -> 1730050382896
	1730050382896 [label=AccumulateGrad]
	1730050383040 -> 1730050383088
	1730056740720 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1730056740720 -> 1730050383040
	1730050383040 [label=AccumulateGrad]
	1730050383328 -> 1730050383232
	1730050383280 -> 1730050383568
	1730056741104 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1730056741104 -> 1730050383280
	1730050383280 [label=AccumulateGrad]
	1730050375936 -> 1730050375792
	1730056741200 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1730056741200 -> 1730050375936
	1730050375936 [label=AccumulateGrad]
	1730050376272 -> 1730050375792
	1730056741296 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1730056741296 -> 1730050376272
	1730050376272 [label=AccumulateGrad]
	1730050376752 -> 1730050377856
	1730056741680 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1730056741680 -> 1730050376752
	1730050376752 [label=AccumulateGrad]
	1730050377712 -> 1730050378336
	1730056741776 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1730056741776 -> 1730050377712
	1730050377712 [label=AccumulateGrad]
	1730050378816 -> 1730050378336
	1730056741872 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1730056741872 -> 1730050378816
	1730050378816 [label=AccumulateGrad]
	1730050379296 -> 1730050379632
	1730056742256 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1730056742256 -> 1730050379296
	1730050379296 [label=AccumulateGrad]
	1730050380256 -> 1730050380736
	1730056742352 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1730056742352 -> 1730050380256
	1730050380256 [label=AccumulateGrad]
	1730050380112 -> 1730050380736
	1730056742448 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1730056742448 -> 1730050380112
	1730050380112 [label=AccumulateGrad]
	1730050380592 -> 1730050381216
	1730050381696 -> 1730050382656
	1730056742832 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1730056742832 -> 1730050381696
	1730050381696 [label=AccumulateGrad]
	1730050382512 -> 1730050383136
	1730056742928 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1730056742928 -> 1730050382512
	1730050382512 [label=AccumulateGrad]
	1730050383472 -> 1730050383136
	1730056743024 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1730056743024 -> 1730050383472
	1730050383472 [label=AccumulateGrad]
	1730055026288 -> 1730055027632
	1730056743408 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1730056743408 -> 1730055026288
	1730055026288 [label=AccumulateGrad]
	1730055026576 -> 1730055027104
	1730056743504 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1730056743504 -> 1730055026576
	1730055026576 [label=AccumulateGrad]
	1730055027584 -> 1730055027104
	1730056743600 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1730056743600 -> 1730055027584
	1730055027584 [label=AccumulateGrad]
	1730055028640 -> 1730055028160
	1730056743984 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1730056743984 -> 1730055028640
	1730055028640 [label=AccumulateGrad]
	1730055026624 -> 1730055026480
	1730056744080 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1730056744080 -> 1730055026624
	1730055026624 [label=AccumulateGrad]
	1730055028592 -> 1730055026480
	1730056744176 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1730056744176 -> 1730055028592
	1730055028592 [label=AccumulateGrad]
	1730055026912 -> 1730055028448
	1730055026048 -> 1730055026864
	1730056745136 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1730056745136 -> 1730055026048
	1730055026048 [label=AccumulateGrad]
	1730055026192 -> 1730055028016
	1730056745232 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1730056745232 -> 1730055026192
	1730055026192 [label=AccumulateGrad]
	1730055025472 -> 1730055028016
	1730056745328 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1730056745328 -> 1730055025472
	1730055025472 [label=AccumulateGrad]
	1730055026960 -> 1730138789904
	1730056745712 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1730056745712 -> 1730055026960
	1730055026960 [label=AccumulateGrad]
	1730055025808 -> 1730138786496
	1730056745808 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1730056745808 -> 1730055025808
	1730055025808 [label=AccumulateGrad]
	1730055027248 -> 1730138786496
	1730056745904 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1730056745904 -> 1730055027248
	1730055027248 [label=AccumulateGrad]
	1730138787456 -> 1730138789424
	1730056746288 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1730056746288 -> 1730138787456
	1730138787456 [label=AccumulateGrad]
	1730138789328 -> 1730138786016
	1730056746384 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1730056746384 -> 1730138789328
	1730138789328 [label=AccumulateGrad]
	1730138789568 -> 1730138786016
	1730056746480 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1730056746480 -> 1730138789568
	1730138789568 [label=AccumulateGrad]
	1730138785968 -> 1730138787888
	1730138785968 [label=CudnnBatchNormBackward0]
	1730138789088 -> 1730138785968
	1730138789088 [label=ConvolutionBackward0]
	1730055028256 -> 1730138789088
	1730055027920 -> 1730138789088
	1730056744560 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1730056744560 -> 1730055027920
	1730055027920 [label=AccumulateGrad]
	1730138785536 -> 1730138785968
	1730056744656 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1730056744656 -> 1730138785536
	1730138785536 [label=AccumulateGrad]
	1730138789664 -> 1730138785968
	1730056744752 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1730056744752 -> 1730138789664
	1730138789664 [label=AccumulateGrad]
	1730138791440 -> 1730138788560
	1730056746864 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1730056746864 -> 1730138791440
	1730138791440 [label=AccumulateGrad]
	1730138788656 -> 1730138788848
	1730056746960 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1730056746960 -> 1730138788656
	1730138788656 [label=AccumulateGrad]
	1730138789520 -> 1730138788848
	1730056747056 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1730056747056 -> 1730138789520
	1730138789520 [label=AccumulateGrad]
	1730138790096 -> 1730138786256
	1730056747440 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1730056747440 -> 1730138790096
	1730138790096 [label=AccumulateGrad]
	1730138784384 -> 1730138789952
	1730056747536 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1730056747536 -> 1730138784384
	1730138784384 [label=AccumulateGrad]
	1730138788176 -> 1730138789952
	1730056747632 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1730056747632 -> 1730138788176
	1730138788176 [label=AccumulateGrad]
	1730138790336 -> 1730138790864
	1730056748016 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1730056748016 -> 1730138790336
	1730138790336 [label=AccumulateGrad]
	1730138787840 -> 1730138790576
	1730056748112 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1730056748112 -> 1730138787840
	1730138787840 [label=AccumulateGrad]
	1730138788464 -> 1730138790576
	1730056748208 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1730056748208 -> 1730138788464
	1730138788464 [label=AccumulateGrad]
	1730138789856 -> 1730138787072
	1730138786592 -> 1730138784288
	1730056748592 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1730056748592 -> 1730138786592
	1730138786592 [label=AccumulateGrad]
	1730138791392 -> 1730138789808
	1730056748688 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1730056748688 -> 1730138791392
	1730138791392 [label=AccumulateGrad]
	1730138784576 -> 1730138789808
	1730056748784 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1730056748784 -> 1730138784576
	1730138784576 [label=AccumulateGrad]
	1730138791536 -> 1730138786544
	1730056388784 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1730056388784 -> 1730138791536
	1730138791536 [label=AccumulateGrad]
	1730138786304 -> 1730138789616
	1730056388880 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1730056388880 -> 1730138786304
	1730138786304 [label=AccumulateGrad]
	1730138784864 -> 1730138789616
	1730056388976 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1730056388976 -> 1730138784864
	1730138784864 [label=AccumulateGrad]
	1730138784432 -> 1730138788704
	1730056389360 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1730056389360 -> 1730138784432
	1730138784432 [label=AccumulateGrad]
	1730138790288 -> 1730138790768
	1730056389456 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1730056389456 -> 1730138790288
	1730138790288 [label=AccumulateGrad]
	1730138784720 -> 1730138790768
	1730056389552 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1730056389552 -> 1730138784720
	1730138784720 [label=AccumulateGrad]
	1730138790144 -> 1730138789232
	1730138784816 -> 1730138788320
	1730056389936 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1730056389936 -> 1730138784816
	1730138784816 [label=AccumulateGrad]
	1730138787696 -> 1730138786688
	1730056390032 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1730056390032 -> 1730138787696
	1730138787696 [label=AccumulateGrad]
	1730138788944 -> 1730138786688
	1730056390128 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1730056390128 -> 1730138788944
	1730138788944 [label=AccumulateGrad]
	1730138788224 -> 1730138787216
	1730056390512 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1730056390512 -> 1730138788224
	1730138788224 [label=AccumulateGrad]
	1730138791872 -> 1730138791776
	1730056390608 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1730056390608 -> 1730138791872
	1730138791872 [label=AccumulateGrad]
	1730138791824 -> 1730138791776
	1730056390704 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1730056390704 -> 1730138791824
	1730138791824 [label=AccumulateGrad]
	1730138791920 -> 1730138792352
	1730056391088 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1730056391088 -> 1730138791920
	1730138791920 [label=AccumulateGrad]
	1730138792256 -> 1730138792304
	1730056391184 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1730056391184 -> 1730138792256
	1730138792256 [label=AccumulateGrad]
	1730138792448 -> 1730138792304
	1730056391280 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1730056391280 -> 1730138792448
	1730138792448 [label=AccumulateGrad]
	1730138792208 -> 1730138792400
	1730138792592 -> 1730138792784
	1730056391664 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1730056391664 -> 1730138792592
	1730138792592 [label=AccumulateGrad]
	1730138792688 -> 1730138792880
	1730056391760 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1730056391760 -> 1730138792688
	1730138792688 [label=AccumulateGrad]
	1730138793072 -> 1730138792880
	1730056391856 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1730056391856 -> 1730138793072
	1730138793072 [label=AccumulateGrad]
	1730138793216 -> 1730138793168
	1730056392240 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1730056392240 -> 1730138793216
	1730138793216 [label=AccumulateGrad]
	1730138793360 -> 1730138793504
	1730056392336 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1730056392336 -> 1730138793360
	1730138793360 [label=AccumulateGrad]
	1730138793792 -> 1730138793504
	1730056392432 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1730056392432 -> 1730138793792
	1730138793792 [label=AccumulateGrad]
	1730138793888 -> 1730138793840
	1730056392816 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1730056392816 -> 1730138793888
	1730138793888 [label=AccumulateGrad]
	1730138793984 -> 1730138794272
	1730056392912 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1730056392912 -> 1730138793984
	1730138793984 [label=AccumulateGrad]
	1730138794032 -> 1730138794272
	1730056393008 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1730056393008 -> 1730138794032
	1730138794032 [label=AccumulateGrad]
	1730138794176 -> 1730138794368
	1730138794128 -> 1730138794752
	1730056393392 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1730056393392 -> 1730138794128
	1730138794128 [label=AccumulateGrad]
	1730138794656 -> 1730138794848
	1730056393488 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1730056393488 -> 1730138794656
	1730138794656 [label=AccumulateGrad]
	1730138794608 -> 1730138794848
	1730056393584 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1730056393584 -> 1730138794608
	1730138794608 [label=AccumulateGrad]
	1730138794944 -> 1730138795136
	1730056393968 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1730056393968 -> 1730138794944
	1730138794944 [label=AccumulateGrad]
	1730138795328 -> 1730138795184
	1730056394064 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1730056394064 -> 1730138795328
	1730138795328 [label=AccumulateGrad]
	1730138795280 -> 1730138795184
	1730056394160 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1730056394160 -> 1730138795280
	1730138795280 [label=AccumulateGrad]
	1730138795472 -> 1730138795808
	1730056394544 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1730056394544 -> 1730138795472
	1730138795472 [label=AccumulateGrad]
	1730138795664 -> 1730138795760
	1730056394640 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1730056394640 -> 1730138795664
	1730138795664 [label=AccumulateGrad]
	1730138795568 -> 1730138795760
	1730056394736 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1730056394736 -> 1730138795568
	1730138795568 [label=AccumulateGrad]
	1730138795904 -> 1730138795952
	1730138796144 -> 1730138796384
	1730056395696 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1730056395696 -> 1730138796144
	1730138796144 [label=AccumulateGrad]
	1730138796432 -> 1730138796672
	1730056395792 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1730056395792 -> 1730138796432
	1730138796432 [label=AccumulateGrad]
	1730138796768 -> 1730138796672
	1730056395888 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1730056395888 -> 1730138796768
	1730138796768 [label=AccumulateGrad]
	1730138796528 -> 1730138796912
	1730056396272 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1730056396272 -> 1730138796528
	1730138796528 [label=AccumulateGrad]
	1730138797152 -> 1730138797056
	1730056396368 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1730056396368 -> 1730138797152
	1730138797152 [label=AccumulateGrad]
	1730138797104 -> 1730138797056
	1730056396464 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1730056396464 -> 1730138797104
	1730138797104 [label=AccumulateGrad]
	1730138797200 -> 1730138797632
	1730056396848 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1730056396848 -> 1730138797200
	1730138797200 [label=AccumulateGrad]
	1730138797536 -> 1730138797584
	1730056396944 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1730056396944 -> 1730138797536
	1730138797536 [label=AccumulateGrad]
	1730138797728 -> 1730138797584
	1730056397040 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1730056397040 -> 1730138797728
	1730138797728 [label=AccumulateGrad]
	1730138797488 -> 1730138797680
	1730138797488 [label=CudnnBatchNormBackward0]
	1730138796720 -> 1730138797488
	1730138796720 [label=ConvolutionBackward0]
	1730138796288 -> 1730138796720
	1730138796240 -> 1730138796720
	1730056395120 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1730056395120 -> 1730138796240
	1730138796240 [label=AccumulateGrad]
	1730138797344 -> 1730138797488
	1730056395216 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1730056395216 -> 1730138797344
	1730138797344 [label=AccumulateGrad]
	1730138797392 -> 1730138797488
	1730056395312 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1730056395312 -> 1730138797392
	1730138797392 [label=AccumulateGrad]
	1730138797872 -> 1730138798064
	1730056397424 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1730056397424 -> 1730138797872
	1730138797872 [label=AccumulateGrad]
	1730138797968 -> 1730138798160
	1730056397520 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1730056397520 -> 1730138797968
	1730138797968 [label=AccumulateGrad]
	1730138798352 -> 1730138798160
	1730056397616 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1730056397616 -> 1730138798352
	1730138798352 [label=AccumulateGrad]
	1730138798496 -> 1730138798448
	1730056398000 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1730056398000 -> 1730138798496
	1730138798496 [label=AccumulateGrad]
	1730138798640 -> 1730138798784
	1730056398096 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1730056398096 -> 1730138798640
	1730138798640 [label=AccumulateGrad]
	1730138799072 -> 1730138798784
	1730056398192 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1730056398192 -> 1730138799072
	1730138799072 [label=AccumulateGrad]
	1730138799168 -> 1730138799120
	1730056398576 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1730056398576 -> 1730138799168
	1730138799168 [label=AccumulateGrad]
	1730138799264 -> 1730138799552
	1730056398672 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1730056398672 -> 1730138799264
	1730138799264 [label=AccumulateGrad]
	1730138799312 -> 1730138799552
	1730056398768 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1730056398768 -> 1730138799312
	1730138799312 [label=AccumulateGrad]
	1730138799456 -> 1730138799648
	1730138799408 -> 1730138800032
	1730056399152 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1730056399152 -> 1730138799408
	1730138799408 [label=AccumulateGrad]
	1730138799936 -> 1730138799984
	1730056399248 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1730056399248 -> 1730138799936
	1730138799936 [label=AccumulateGrad]
	1730138800080 -> 1730138799984
	1730056399344 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1730056399344 -> 1730138800080
	1730138800080 [label=AccumulateGrad]
	1730138784000 -> 1730138785104
	1730056399728 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1730056399728 -> 1730138784000
	1730138784000 [label=AccumulateGrad]
	1730138787120 -> 1730138792160
	1730056399824 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1730056399824 -> 1730138787120
	1730138787120 [label=AccumulateGrad]
	1730138792640 -> 1730138792160
	1730056399920 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1730056399920 -> 1730138792640
	1730138792640 [label=AccumulateGrad]
	1730138793120 -> 1730138793456
	1730056400304 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1730056400304 -> 1730138793120
	1730138793120 [label=AccumulateGrad]
	1730138794080 -> 1730138794560
	1730056400400 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1730056400400 -> 1730138794080
	1730138794080 [label=AccumulateGrad]
	1730138793936 -> 1730138794560
	1730056400496 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1730056400496 -> 1730138793936
	1730138793936 [label=AccumulateGrad]
	1730138794416 -> 1730138795040
	1730138796480 -> 1730138797296
	1730138796480 [label=TBackward0]
	1730138794896 -> 1730138796480
	1730056401168 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	1730056401168 -> 1730138794896
	1730138794896 [label=AccumulateGrad]
	1730138797296 -> 1730050027024
}
