digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2767345956720 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2767283119216 [label=AddmmBackward0]
	2767283114080 -> 2767283119216
	2767345753200 [label="fc.bias
 (19)" fillcolor=lightblue]
	2767345753200 -> 2767283114080
	2767283114080 [label=AccumulateGrad]
	2767283119456 -> 2767283119216
	2767283119456 [label=ViewBackward0]
	2767283117056 -> 2767283119456
	2767283117056 [label=MeanBackward1]
	2767283115088 -> 2767283117056
	2767283115088 [label=ReluBackward0]
	2767283125504 -> 2767283115088
	2767283125504 [label=AddBackward0]
	2767283113552 -> 2767283125504
	2767283113552 [label=CudnnBatchNormBackward0]
	2767283118880 -> 2767283113552
	2767283118880 [label=ConvolutionBackward0]
	2767283119936 -> 2767283118880
	2767283119936 [label=ReluBackward0]
	2767283117728 -> 2767283119936
	2767283117728 [label=CudnnBatchNormBackward0]
	2767283117200 -> 2767283117728
	2767283117200 [label=ConvolutionBackward0]
	2767283120224 -> 2767283117200
	2767283120224 [label=ReluBackward0]
	2767283123392 -> 2767283120224
	2767283123392 [label=AddBackward0]
	2767283125696 -> 2767283123392
	2767283125696 [label=CudnnBatchNormBackward0]
	2767283124400 -> 2767283125696
	2767283124400 [label=ConvolutionBackward0]
	2767283123200 -> 2767283124400
	2767283123200 [label=ReluBackward0]
	2767283115520 -> 2767283123200
	2767283115520 [label=CudnnBatchNormBackward0]
	2767283121232 -> 2767283115520
	2767283121232 [label=ConvolutionBackward0]
	2767283113744 -> 2767283121232
	2767283113744 [label=ReluBackward0]
	2767283120944 -> 2767283113744
	2767283120944 [label=AddBackward0]
	2767283114032 -> 2767283120944
	2767283114032 [label=CudnnBatchNormBackward0]
	2767283120656 -> 2767283114032
	2767283120656 [label=ConvolutionBackward0]
	2767283124592 -> 2767283120656
	2767283124592 [label=ReluBackward0]
	2767283116384 -> 2767283124592
	2767283116384 [label=CudnnBatchNormBackward0]
	2767283113600 -> 2767283116384
	2767283113600 [label=ConvolutionBackward0]
	2767283116768 -> 2767283113600
	2767283116768 [label=ReluBackward0]
	2767283118208 -> 2767283116768
	2767283118208 [label=AddBackward0]
	2767283114704 -> 2767283118208
	2767283114704 [label=CudnnBatchNormBackward0]
	2767283118784 -> 2767283114704
	2767283118784 [label=ConvolutionBackward0]
	2767283124448 -> 2767283118784
	2767283124448 [label=ReluBackward0]
	2767283126080 -> 2767283124448
	2767283126080 [label=CudnnBatchNormBackward0]
	2767283118832 -> 2767283126080
	2767283118832 [label=ConvolutionBackward0]
	2767418263056 -> 2767283118832
	2767418263056 [label=ReluBackward0]
	2767314788432 -> 2767418263056
	2767314788432 [label=AddBackward0]
	2767314788480 -> 2767314788432
	2767314788480 [label=CudnnBatchNormBackward0]
	2767314788576 -> 2767314788480
	2767314788576 [label=ConvolutionBackward0]
	2767314793952 -> 2767314788576
	2767314793952 [label=ReluBackward0]
	2767314780848 -> 2767314793952
	2767314780848 [label=CudnnBatchNormBackward0]
	2767314780272 -> 2767314780848
	2767314780272 [label=ConvolutionBackward0]
	2767314784016 -> 2767314780272
	2767314784016 [label=ReluBackward0]
	2767314785216 -> 2767314784016
	2767314785216 [label=AddBackward0]
	2767314784112 -> 2767314785216
	2767314784112 [label=CudnnBatchNormBackward0]
	2767314782720 -> 2767314784112
	2767314782720 [label=ConvolutionBackward0]
	2767314783488 -> 2767314782720
	2767314783488 [label=ReluBackward0]
	2767314792560 -> 2767314783488
	2767314792560 [label=CudnnBatchNormBackward0]
	2767314789728 -> 2767314792560
	2767314789728 [label=ConvolutionBackward0]
	2767314780608 -> 2767314789728
	2767314780608 [label=ReluBackward0]
	2767314780656 -> 2767314780608
	2767314780656 [label=AddBackward0]
	2767314781040 -> 2767314780656
	2767314781040 [label=CudnnBatchNormBackward0]
	2767314789488 -> 2767314781040
	2767314789488 [label=ConvolutionBackward0]
	2767314786896 -> 2767314789488
	2767314786896 [label=ReluBackward0]
	2767314785120 -> 2767314786896
	2767314785120 [label=CudnnBatchNormBackward0]
	2767314786224 -> 2767314785120
	2767314786224 [label=ConvolutionBackward0]
	2767314791840 -> 2767314786224
	2767314791840 [label=ReluBackward0]
	2767314783872 -> 2767314791840
	2767314783872 [label=AddBackward0]
	2767314783104 -> 2767314783872
	2767314783104 [label=CudnnBatchNormBackward0]
	2767314787040 -> 2767314783104
	2767314787040 [label=ConvolutionBackward0]
	2767314788672 -> 2767314787040
	2767314788672 [label=ReluBackward0]
	2767314791888 -> 2767314788672
	2767314791888 [label=CudnnBatchNormBackward0]
	2767314789248 -> 2767314791888
	2767314789248 [label=ConvolutionBackward0]
	2767314784592 -> 2767314789248
	2767314784592 [label=MaxPool2DWithIndicesBackward0]
	2767314791936 -> 2767314784592
	2767314791936 [label=ReluBackward0]
	2767314795488 -> 2767314791936
	2767314795488 [label=CudnnBatchNormBackward0]
	2767314784736 -> 2767314795488
	2767314784736 [label=ConvolutionBackward0]
	2767314781184 -> 2767314784736
	2767345753008 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2767345753008 -> 2767314781184
	2767314781184 [label=AccumulateGrad]
	2767314795440 -> 2767314795488
	2767318838416 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2767318838416 -> 2767314795440
	2767314795440 [label=AccumulateGrad]
	2767314785024 -> 2767314795488
	2767318838128 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2767318838128 -> 2767314785024
	2767314785024 [label=AccumulateGrad]
	2767314791504 -> 2767314789248
	2767318838608 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2767318838608 -> 2767314791504
	2767314791504 [label=AccumulateGrad]
	2767314781280 -> 2767314791888
	2767318838704 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2767318838704 -> 2767314781280
	2767314781280 [label=AccumulateGrad]
	2767314785360 -> 2767314791888
	2767318838800 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2767318838800 -> 2767314785360
	2767314785360 [label=AccumulateGrad]
	2767314785312 -> 2767314787040
	2767318838032 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2767318838032 -> 2767314785312
	2767314785312 [label=AccumulateGrad]
	2767314787664 -> 2767314783104
	2767318837936 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2767318837936 -> 2767314787664
	2767314787664 [label=AccumulateGrad]
	2767314782192 -> 2767314783104
	2767318837168 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2767318837168 -> 2767314782192
	2767314782192 [label=AccumulateGrad]
	2767314784592 -> 2767314783872
	2767314784448 -> 2767314786224
	2767318837264 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2767318837264 -> 2767314784448
	2767314784448 [label=AccumulateGrad]
	2767314786512 -> 2767314785120
	2767318837360 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2767318837360 -> 2767314786512
	2767314786512 [label=AccumulateGrad]
	2767314785648 -> 2767314785120
	2767318837456 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2767318837456 -> 2767314785648
	2767314785648 [label=AccumulateGrad]
	2767314786272 -> 2767314789488
	2767318837744 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2767318837744 -> 2767314786272
	2767314786272 [label=AccumulateGrad]
	2767314790208 -> 2767314781040
	2767318836784 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2767318836784 -> 2767314790208
	2767314790208 [label=AccumulateGrad]
	2767314790352 -> 2767314781040
	2767318836688 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2767318836688 -> 2767314790352
	2767314790352 [label=AccumulateGrad]
	2767314791840 -> 2767314780656
	2767314782240 -> 2767314789728
	2767318836592 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2767318836592 -> 2767314782240
	2767314782240 [label=AccumulateGrad]
	2767314780464 -> 2767314792560
	2767318836496 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2767318836496 -> 2767314780464
	2767314780464 [label=AccumulateGrad]
	2767314782144 -> 2767314792560
	2767345597424 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2767345597424 -> 2767314782144
	2767314782144 [label=AccumulateGrad]
	2767314789824 -> 2767314782720
	2767345597808 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2767345597808 -> 2767314789824
	2767314789824 [label=AccumulateGrad]
	2767314788336 -> 2767314784112
	2767345597904 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2767345597904 -> 2767314788336
	2767314788336 [label=AccumulateGrad]
	2767314783584 -> 2767314784112
	2767345598000 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2767345598000 -> 2767314783584
	2767314783584 [label=AccumulateGrad]
	2767314784976 -> 2767314785216
	2767314784976 [label=CudnnBatchNormBackward0]
	2767314786800 -> 2767314784976
	2767314786800 [label=ConvolutionBackward0]
	2767314780608 -> 2767314786800
	2767314781712 -> 2767314786800
	2767318834000 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2767318834000 -> 2767314781712
	2767314781712 [label=AccumulateGrad]
	2767314783200 -> 2767314784976
	2767318834192 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2767318834192 -> 2767314783200
	2767314783200 [label=AccumulateGrad]
	2767314784160 -> 2767314784976
	2767318835824 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2767318835824 -> 2767314784160
	2767314784160 [label=AccumulateGrad]
	2767314784208 -> 2767314780272
	2767345598384 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2767345598384 -> 2767314784208
	2767314784208 [label=AccumulateGrad]
	2767314780752 -> 2767314780848
	2767345746000 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2767345746000 -> 2767314780752
	2767314780752 [label=AccumulateGrad]
	2767314781232 -> 2767314780848
	2767345746096 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2767345746096 -> 2767314781232
	2767314781232 [label=AccumulateGrad]
	2767314788720 -> 2767314788576
	2767345746480 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2767345746480 -> 2767314788720
	2767314788720 [label=AccumulateGrad]
	2767314786416 -> 2767314788480
	2767345746576 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2767345746576 -> 2767314786416
	2767314786416 [label=AccumulateGrad]
	2767314789152 -> 2767314788480
	2767345746672 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2767345746672 -> 2767314789152
	2767314789152 [label=AccumulateGrad]
	2767314784016 -> 2767314788432
	2767283124016 -> 2767283118832
	2767345747632 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2767345747632 -> 2767283124016
	2767283124016 [label=AccumulateGrad]
	2767283122000 -> 2767283126080
	2767345747728 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2767345747728 -> 2767283122000
	2767283122000 [label=AccumulateGrad]
	2767283114320 -> 2767283126080
	2767345747824 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2767345747824 -> 2767283114320
	2767283114320 [label=AccumulateGrad]
	2767283121808 -> 2767283118784
	2767345748208 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2767345748208 -> 2767283121808
	2767283121808 [label=AccumulateGrad]
	2767283125936 -> 2767283114704
	2767345748304 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2767345748304 -> 2767283125936
	2767283125936 [label=AccumulateGrad]
	2767283118256 -> 2767283114704
	2767345748400 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2767345748400 -> 2767283118256
	2767283118256 [label=AccumulateGrad]
	2767283125648 -> 2767283118208
	2767283125648 [label=CudnnBatchNormBackward0]
	2767283113264 -> 2767283125648
	2767283113264 [label=ConvolutionBackward0]
	2767418263056 -> 2767283113264
	2767314781760 -> 2767283113264
	2767345747056 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2767345747056 -> 2767314781760
	2767314781760 [label=AccumulateGrad]
	2767283113792 -> 2767283125648
	2767345747152 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2767345747152 -> 2767283113792
	2767283113792 [label=AccumulateGrad]
	2767283118064 -> 2767283125648
	2767345747248 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2767345747248 -> 2767283118064
	2767283118064 [label=AccumulateGrad]
	2767283117680 -> 2767283113600
	2767345748784 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2767345748784 -> 2767283117680
	2767283117680 [label=AccumulateGrad]
	2767283117536 -> 2767283116384
	2767345748880 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2767345748880 -> 2767283117536
	2767283117536 [label=AccumulateGrad]
	2767283123920 -> 2767283116384
	2767345748976 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2767345748976 -> 2767283123920
	2767283123920 [label=AccumulateGrad]
	2767283114416 -> 2767283120656
	2767345749360 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2767345749360 -> 2767283114416
	2767283114416 [label=AccumulateGrad]
	2767283118352 -> 2767283114032
	2767345749456 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2767345749456 -> 2767283118352
	2767283118352 [label=AccumulateGrad]
	2767283117152 -> 2767283114032
	2767345749552 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2767345749552 -> 2767283117152
	2767283117152 [label=AccumulateGrad]
	2767283116768 -> 2767283120944
	2767283119888 -> 2767283121232
	2767345750512 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2767345750512 -> 2767283119888
	2767283119888 [label=AccumulateGrad]
	2767283124352 -> 2767283115520
	2767345750608 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2767345750608 -> 2767283124352
	2767283124352 [label=AccumulateGrad]
	2767283120080 -> 2767283115520
	2767345750704 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2767345750704 -> 2767283120080
	2767283120080 [label=AccumulateGrad]
	2767283115040 -> 2767283124400
	2767345751088 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2767345751088 -> 2767283115040
	2767283115040 [label=AccumulateGrad]
	2767283122528 -> 2767283125696
	2767345751184 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2767345751184 -> 2767283122528
	2767283122528 [label=AccumulateGrad]
	2767283121664 -> 2767283125696
	2767345751280 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2767345751280 -> 2767283121664
	2767283121664 [label=AccumulateGrad]
	2767283117920 -> 2767283123392
	2767283117920 [label=CudnnBatchNormBackward0]
	2767283114608 -> 2767283117920
	2767283114608 [label=ConvolutionBackward0]
	2767283113744 -> 2767283114608
	2767283123248 -> 2767283114608
	2767345749936 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2767345749936 -> 2767283123248
	2767283123248 [label=AccumulateGrad]
	2767283118016 -> 2767283117920
	2767345750032 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2767345750032 -> 2767283118016
	2767283118016 [label=AccumulateGrad]
	2767283121952 -> 2767283117920
	2767345750128 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2767345750128 -> 2767283121952
	2767283121952 [label=AccumulateGrad]
	2767283116720 -> 2767283117200
	2767345751664 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2767345751664 -> 2767283116720
	2767283116720 [label=AccumulateGrad]
	2767283125360 -> 2767283117728
	2767345751760 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2767345751760 -> 2767283125360
	2767283125360 [label=AccumulateGrad]
	2767283119648 -> 2767283117728
	2767345751856 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2767345751856 -> 2767283119648
	2767283119648 [label=AccumulateGrad]
	2767283123728 -> 2767283118880
	2767345752240 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2767345752240 -> 2767283123728
	2767283123728 [label=AccumulateGrad]
	2767283120320 -> 2767283113552
	2767345752336 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2767345752336 -> 2767283120320
	2767283120320 [label=AccumulateGrad]
	2767283125552 -> 2767283113552
	2767345752432 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2767345752432 -> 2767283125552
	2767283125552 [label=AccumulateGrad]
	2767283120224 -> 2767283125504
	2767283123152 -> 2767283119216
	2767283123152 [label=TBackward0]
	2767283116816 -> 2767283123152
	2767345753104 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2767345753104 -> 2767283116816
	2767283116816 [label=AccumulateGrad]
	2767283119216 -> 2767345956720
}
