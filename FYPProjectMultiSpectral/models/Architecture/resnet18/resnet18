digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2878739053168 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2878645799808 [label=AddmmBackward0]
	2878645802496 -> 2878645799808
	2878739152816 [label="fc.bias
 (19)" fillcolor=lightblue]
	2878739152816 -> 2878645802496
	2878645802496 [label=AccumulateGrad]
	2878645803840 -> 2878645799808
	2878645803840 [label=ViewBackward0]
	2878645803120 -> 2878645803840
	2878645803120 [label=MeanBackward1]
	2878645801728 -> 2878645803120
	2878645801728 [label=ReluBackward0]
	2878645800960 -> 2878645801728
	2878645800960 [label=AddBackward0]
	2878645804032 -> 2878645800960
	2878645804032 [label=CudnnBatchNormBackward0]
	2878645805904 -> 2878645804032
	2878645805904 [label=ConvolutionBackward0]
	2878645798992 -> 2878645805904
	2878645798992 [label=ReluBackward0]
	2878645799376 -> 2878645798992
	2878645799376 [label=CudnnBatchNormBackward0]
	2878645801200 -> 2878645799376
	2878645801200 [label=ConvolutionBackward0]
	2878645804896 -> 2878645801200
	2878645804896 [label=ReluBackward0]
	2878764861712 -> 2878645804896
	2878764861712 [label=AddBackward0]
	2878764868336 -> 2878764861712
	2878764868336 [label=CudnnBatchNormBackward0]
	2878764865120 -> 2878764868336
	2878764865120 [label=ConvolutionBackward0]
	2878764870640 -> 2878764865120
	2878764870640 [label=ReluBackward0]
	2878764864640 -> 2878764870640
	2878764864640 [label=CudnnBatchNormBackward0]
	2878764868720 -> 2878764864640
	2878764868720 [label=ConvolutionBackward0]
	2878764866512 -> 2878764868720
	2878764866512 [label=ReluBackward0]
	2878764861856 -> 2878764866512
	2878764861856 [label=AddBackward0]
	2878764862480 -> 2878764861856
	2878764862480 [label=CudnnBatchNormBackward0]
	2878764871504 -> 2878764862480
	2878764871504 [label=ConvolutionBackward0]
	2878764867664 -> 2878764871504
	2878764867664 [label=ReluBackward0]
	2878764870304 -> 2878764867664
	2878764870304 [label=CudnnBatchNormBackward0]
	2878764861616 -> 2878764870304
	2878764861616 [label=ConvolutionBackward0]
	2878764862912 -> 2878764861616
	2878764862912 [label=ReluBackward0]
	2878764870256 -> 2878764862912
	2878764870256 [label=AddBackward0]
	2878764862336 -> 2878764870256
	2878764862336 [label=CudnnBatchNormBackward0]
	2878764865552 -> 2878764862336
	2878764865552 [label=ConvolutionBackward0]
	2878764867136 -> 2878764865552
	2878764867136 [label=ReluBackward0]
	2878764868096 -> 2878764867136
	2878764868096 [label=CudnnBatchNormBackward0]
	2878764867568 -> 2878764868096
	2878764867568 [label=ConvolutionBackward0]
	2878764868816 -> 2878764867568
	2878764868816 [label=ReluBackward0]
	2878764869824 -> 2878764868816
	2878764869824 [label=AddBackward0]
	2878764864352 -> 2878764869824
	2878764864352 [label=CudnnBatchNormBackward0]
	2878764860272 -> 2878764864352
	2878764860272 [label=ConvolutionBackward0]
	2878764870208 -> 2878764860272
	2878764870208 [label=ReluBackward0]
	2878764869872 -> 2878764870208
	2878764869872 [label=CudnnBatchNormBackward0]
	2878764871552 -> 2878764869872
	2878764871552 [label=ConvolutionBackward0]
	2878764870400 -> 2878764871552
	2878764870400 [label=ReluBackward0]
	2878764864880 -> 2878764870400
	2878764864880 [label=AddBackward0]
	2878764869488 -> 2878764864880
	2878764869488 [label=CudnnBatchNormBackward0]
	2878764862672 -> 2878764869488
	2878764862672 [label=ConvolutionBackward0]
	2878764860080 -> 2878764862672
	2878764860080 [label=ReluBackward0]
	2878764859648 -> 2878764860080
	2878764859648 [label=CudnnBatchNormBackward0]
	2878764869200 -> 2878764859648
	2878764869200 [label=ConvolutionBackward0]
	2878764870448 -> 2878764869200
	2878764870448 [label=ReluBackward0]
	2878764870928 -> 2878764870448
	2878764870928 [label=AddBackward0]
	2878764869536 -> 2878764870928
	2878764869536 [label=CudnnBatchNormBackward0]
	2878764868912 -> 2878764869536
	2878764868912 [label=ConvolutionBackward0]
	2878764869056 -> 2878764868912
	2878764869056 [label=ReluBackward0]
	2878764862144 -> 2878764869056
	2878764862144 [label=CudnnBatchNormBackward0]
	2878764870352 -> 2878764862144
	2878764870352 [label=ConvolutionBackward0]
	2878764870592 -> 2878764870352
	2878764870592 [label=ReluBackward0]
	2878764869632 -> 2878764870592
	2878764869632 [label=AddBackward0]
	2878764869008 -> 2878764869632
	2878764869008 [label=CudnnBatchNormBackward0]
	2878764868576 -> 2878764869008
	2878764868576 [label=ConvolutionBackward0]
	2878764860752 -> 2878764868576
	2878764860752 [label=ReluBackward0]
	2878764861424 -> 2878764860752
	2878764861424 [label=CudnnBatchNormBackward0]
	2878764866368 -> 2878764861424
	2878764866368 [label=ConvolutionBackward0]
	2878764868768 -> 2878764866368
	2878764868768 [label=MaxPool2DWithIndicesBackward0]
	2878764863776 -> 2878764868768
	2878764863776 [label=ReluBackward0]
	2878764860896 -> 2878764863776
	2878764860896 [label=CudnnBatchNormBackward0]
	2878764860224 -> 2878764860896
	2878764860224 [label=ConvolutionBackward0]
	2878764860704 -> 2878764860224
	2878739444368 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2878739444368 -> 2878764860704
	2878764860704 [label=AccumulateGrad]
	2878764869968 -> 2878764860896
	2878757523280 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2878757523280 -> 2878764869968
	2878764869968 [label=AccumulateGrad]
	2878764864832 -> 2878764860896
	2878757522992 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2878757522992 -> 2878764864832
	2878764864832 [label=AccumulateGrad]
	2878764871264 -> 2878764866368
	2878757523472 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2878757523472 -> 2878764871264
	2878764871264 [label=AccumulateGrad]
	2878764859600 -> 2878764861424
	2878757523568 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2878757523568 -> 2878764859600
	2878764859600 [label=AccumulateGrad]
	2878764867040 -> 2878764861424
	2878757523664 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2878757523664 -> 2878764867040
	2878764867040 [label=AccumulateGrad]
	2878764866896 -> 2878764868576
	2878757522896 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2878757522896 -> 2878764866896
	2878764866896 [label=AccumulateGrad]
	2878764871456 -> 2878764869008
	2878757522800 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2878757522800 -> 2878764871456
	2878764871456 [label=AccumulateGrad]
	2878764862240 -> 2878764869008
	2878757522032 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2878757522032 -> 2878764862240
	2878764862240 [label=AccumulateGrad]
	2878764868768 -> 2878764869632
	2878764861808 -> 2878764870352
	2878757522128 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2878757522128 -> 2878764861808
	2878764861808 [label=AccumulateGrad]
	2878764860848 -> 2878764862144
	2878757522224 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2878757522224 -> 2878764860848
	2878764860848 [label=AccumulateGrad]
	2878764864304 -> 2878764862144
	2878757522320 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2878757522320 -> 2878764864304
	2878764864304 [label=AccumulateGrad]
	2878764864256 -> 2878764868912
	2878757522608 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2878757522608 -> 2878764864256
	2878764864256 [label=AccumulateGrad]
	2878764865840 -> 2878764869536
	2878757521648 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2878757521648 -> 2878764865840
	2878764865840 [label=AccumulateGrad]
	2878764866080 -> 2878764869536
	2878757521552 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2878757521552 -> 2878764866080
	2878764866080 [label=AccumulateGrad]
	2878764870592 -> 2878764870928
	2878764861472 -> 2878764869200
	2878757521456 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2878757521456 -> 2878764861472
	2878764861472 [label=AccumulateGrad]
	2878764870736 -> 2878764859648
	2878757521360 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2878757521360 -> 2878764870736
	2878764870736 [label=AccumulateGrad]
	2878764866272 -> 2878764859648
	2878739436112 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2878739436112 -> 2878764866272
	2878764866272 [label=AccumulateGrad]
	2878764865408 -> 2878764862672
	2878739436496 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2878739436496 -> 2878764865408
	2878764865408 [label=AccumulateGrad]
	2878764861520 -> 2878764869488
	2878739436592 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2878739436592 -> 2878764861520
	2878764861520 [label=AccumulateGrad]
	2878764861664 -> 2878764869488
	2878739436688 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2878739436688 -> 2878764861664
	2878764861664 [label=AccumulateGrad]
	2878764866848 -> 2878764864880
	2878764866848 [label=CudnnBatchNormBackward0]
	2878764862384 -> 2878764866848
	2878764862384 [label=ConvolutionBackward0]
	2878764870448 -> 2878764862384
	2878764865168 -> 2878764862384
	2878757520016 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2878757520016 -> 2878764865168
	2878764865168 [label=AccumulateGrad]
	2878764871168 -> 2878764866848
	2878757520112 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2878757520112 -> 2878764871168
	2878764871168 [label=AccumulateGrad]
	2878764868480 -> 2878764866848
	2878757520688 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2878757520688 -> 2878764868480
	2878764868480 [label=AccumulateGrad]
	2878764859696 -> 2878764871552
	2878739437072 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2878739437072 -> 2878764859696
	2878764859696 [label=AccumulateGrad]
	2878764864400 -> 2878764869872
	2878739437168 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2878739437168 -> 2878764864400
	2878764864400 [label=AccumulateGrad]
	2878764871408 -> 2878764869872
	2878739437264 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2878739437264 -> 2878764871408
	2878764871408 [label=AccumulateGrad]
	2878764865984 -> 2878764860272
	2878739437648 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2878739437648 -> 2878764865984
	2878764865984 [label=AccumulateGrad]
	2878764870496 -> 2878764864352
	2878739437744 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2878739437744 -> 2878764870496
	2878764870496 [label=AccumulateGrad]
	2878764865264 -> 2878764864352
	2878739437840 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2878739437840 -> 2878764865264
	2878764865264 [label=AccumulateGrad]
	2878764870400 -> 2878764869824
	2878764862576 -> 2878764867568
	2878739438800 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2878739438800 -> 2878764862576
	2878764862576 [label=AccumulateGrad]
	2878764860368 -> 2878764868096
	2878739438896 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2878739438896 -> 2878764860368
	2878764860368 [label=AccumulateGrad]
	2878764865072 -> 2878764868096
	2878739438992 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2878739438992 -> 2878764865072
	2878764865072 [label=AccumulateGrad]
	2878764862864 -> 2878764865552
	2878739439376 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2878739439376 -> 2878764862864
	2878764862864 [label=AccumulateGrad]
	2878764869104 -> 2878764862336
	2878739439472 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2878739439472 -> 2878764869104
	2878764869104 [label=AccumulateGrad]
	2878764866560 -> 2878764862336
	2878739439568 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2878739439568 -> 2878764866560
	2878764866560 [label=AccumulateGrad]
	2878764868192 -> 2878764870256
	2878764868192 [label=CudnnBatchNormBackward0]
	2878764868144 -> 2878764868192
	2878764868144 [label=ConvolutionBackward0]
	2878764868816 -> 2878764868144
	2878764859840 -> 2878764868144
	2878739438224 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2878739438224 -> 2878764859840
	2878764859840 [label=AccumulateGrad]
	2878764864544 -> 2878764868192
	2878739438320 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2878739438320 -> 2878764864544
	2878764864544 [label=AccumulateGrad]
	2878764868432 -> 2878764868192
	2878739438416 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2878739438416 -> 2878764868432
	2878764868432 [label=AccumulateGrad]
	2878764869152 -> 2878764861616
	2878739439952 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2878739439952 -> 2878764869152
	2878764869152 [label=AccumulateGrad]
	2878764865936 -> 2878764870304
	2878739440048 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2878739440048 -> 2878764865936
	2878764865936 [label=AccumulateGrad]
	2878764870880 -> 2878764870304
	2878739440144 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2878739440144 -> 2878764870880
	2878764870880 [label=AccumulateGrad]
	2878764859744 -> 2878764871504
	2878739440528 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2878739440528 -> 2878764859744
	2878764859744 [label=AccumulateGrad]
	2878764870688 -> 2878764862480
	2878739440624 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2878739440624 -> 2878764870688
	2878764870688 [label=AccumulateGrad]
	2878764868672 -> 2878764862480
	2878739440720 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2878739440720 -> 2878764868672
	2878764868672 [label=AccumulateGrad]
	2878764862912 -> 2878764861856
	2878764864496 -> 2878764868720
	2878739441680 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2878739441680 -> 2878764864496
	2878764864496 [label=AccumulateGrad]
	2878764869584 -> 2878764864640
	2878739441776 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2878739441776 -> 2878764869584
	2878764869584 [label=AccumulateGrad]
	2878764863872 -> 2878764864640
	2878739441872 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2878739441872 -> 2878764863872
	2878764863872 [label=AccumulateGrad]
	2878764862624 -> 2878764865120
	2878739442256 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2878739442256 -> 2878764862624
	2878764862624 [label=AccumulateGrad]
	2878764867424 -> 2878764868336
	2878739442352 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2878739442352 -> 2878764867424
	2878764867424 [label=AccumulateGrad]
	2878764868000 -> 2878764868336
	2878739442448 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2878739442448 -> 2878764868000
	2878764868000 [label=AccumulateGrad]
	2878764866128 -> 2878764861712
	2878764866128 [label=CudnnBatchNormBackward0]
	2878764869728 -> 2878764866128
	2878764869728 [label=ConvolutionBackward0]
	2878764866512 -> 2878764869728
	2878764863584 -> 2878764869728
	2878739441104 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2878739441104 -> 2878764863584
	2878764863584 [label=AccumulateGrad]
	2878764865456 -> 2878764866128
	2878739441200 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2878739441200 -> 2878764865456
	2878764865456 [label=AccumulateGrad]
	2878764870112 -> 2878764866128
	2878739441296 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2878739441296 -> 2878764870112
	2878764870112 [label=AccumulateGrad]
	2878764862192 -> 2878645801200
	2878739442832 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2878739442832 -> 2878764862192
	2878764862192 [label=AccumulateGrad]
	2878645807344 -> 2878645799376
	2878739442928 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2878739442928 -> 2878645807344
	2878645807344 [label=AccumulateGrad]
	2878764865648 -> 2878645799376
	2878739443024 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2878739443024 -> 2878764865648
	2878764865648 [label=AccumulateGrad]
	2878645799952 -> 2878645805904
	2878739443408 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2878739443408 -> 2878645799952
	2878645799952 [label=AccumulateGrad]
	2878645800768 -> 2878645804032
	2878739443504 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2878739443504 -> 2878645800768
	2878645800768 [label=AccumulateGrad]
	2878645810800 -> 2878645804032
	2878739443600 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2878739443600 -> 2878645810800
	2878645810800 [label=AccumulateGrad]
	2878645804896 -> 2878645800960
	2878645804944 -> 2878645799808
	2878645804944 [label=TBackward0]
	2878645802832 -> 2878645804944
	2878739444272 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2878739444272 -> 2878645802832
	2878645802832 [label=AccumulateGrad]
	2878645799808 -> 2878739053168
}
