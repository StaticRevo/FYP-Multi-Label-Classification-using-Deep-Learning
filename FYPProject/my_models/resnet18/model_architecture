digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1925612504464 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1925245977168 [label=SigmoidBackward0]
	1925245990176 -> 1925245977168
	1925245990176 [label=AddmmBackward0]
	1925245982640 -> 1925245990176
	1925246079088 [label="model.fc.bias
 (19)" fillcolor=lightblue]
	1925246079088 -> 1925245982640
	1925245982640 [label=AccumulateGrad]
	1925245988976 -> 1925245990176
	1925245988976 [label=ViewBackward0]
	1925245982976 -> 1925245988976
	1925245982976 [label=MeanBackward1]
	1925245982496 -> 1925245982976
	1925245982496 [label=ReluBackward0]
	1925245982256 -> 1925245982496
	1925245982256 [label=AddBackward0]
	1925245977648 -> 1925245982256
	1925245977648 [label=CudnnBatchNormBackward0]
	1925245981248 -> 1925245977648
	1925245981248 [label=ConvolutionBackward0]
	1925245981296 -> 1925245981248
	1925245981296 [label=ReluBackward0]
	1925245982400 -> 1925245981296
	1925245982400 [label=CudnnBatchNormBackward0]
	1925245990512 -> 1925245982400
	1925245990512 [label=ConvolutionBackward0]
	1925245981920 -> 1925245990512
	1925245981920 [label=ReluBackward0]
	1925245983552 -> 1925245981920
	1925245983552 [label=AddBackward0]
	1925245983600 -> 1925245983552
	1925245983600 [label=CudnnBatchNormBackward0]
	1925245984032 -> 1925245983600
	1925245984032 [label=ConvolutionBackward0]
	1925245984608 -> 1925245984032
	1925245984608 [label=ReluBackward0]
	1925245984368 -> 1925245984608
	1925245984368 [label=CudnnBatchNormBackward0]
	1925245984512 -> 1925245984368
	1925245984512 [label=ConvolutionBackward0]
	1925245984848 -> 1925245984512
	1925245984848 [label=ReluBackward0]
	1925245985328 -> 1925245984848
	1925245985328 [label=AddBackward0]
	1925245985136 -> 1925245985328
	1925245985136 [label=CudnnBatchNormBackward0]
	1925245989456 -> 1925245985136
	1925245989456 [label=ConvolutionBackward0]
	1925245989072 -> 1925245989456
	1925245989072 [label=ReluBackward0]
	1925245980528 -> 1925245989072
	1925245980528 [label=CudnnBatchNormBackward0]
	1925245980336 -> 1925245980528
	1925245980336 [label=ConvolutionBackward0]
	1925245988784 -> 1925245980336
	1925245988784 [label=ReluBackward0]
	1925245978272 -> 1925245988784
	1925245978272 [label=AddBackward0]
	1925245978176 -> 1925245978272
	1925245978176 [label=CudnnBatchNormBackward0]
	1925245978608 -> 1925245978176
	1925245978608 [label=ConvolutionBackward0]
	1925245990320 -> 1925245978608
	1925245990320 [label=ReluBackward0]
	1925245989792 -> 1925245990320
	1925245989792 [label=CudnnBatchNormBackward0]
	1925245982448 -> 1925245989792
	1925245982448 [label=ConvolutionBackward0]
	1925245983888 -> 1925245982448
	1925245983888 [label=ReluBackward0]
	1925245977408 -> 1925245983888
	1925245977408 [label=AddBackward0]
	1925245977360 -> 1925245977408
	1925245977360 [label=CudnnBatchNormBackward0]
	1925245976928 -> 1925245977360
	1925245976928 [label=ConvolutionBackward0]
	1925245976736 -> 1925245976928
	1925245976736 [label=ReluBackward0]
	1925245981680 -> 1925245976736
	1925245981680 [label=CudnnBatchNormBackward0]
	1925245981584 -> 1925245981680
	1925245981584 [label=ConvolutionBackward0]
	1925245977456 -> 1925245981584
	1925245977456 [label=ReluBackward0]
	1925245990800 -> 1925245977456
	1925245990800 [label=AddBackward0]
	1925245991376 -> 1925245990800
	1925245991376 [label=CudnnBatchNormBackward0]
	1925245991520 -> 1925245991376
	1925245991520 [label=ConvolutionBackward0]
	1925245991712 -> 1925245991520
	1925245991712 [label=ReluBackward0]
	1925245991856 -> 1925245991712
	1925245991856 [label=CudnnBatchNormBackward0]
	1925245991952 -> 1925245991856
	1925245991952 [label=ConvolutionBackward0]
	1925245992144 -> 1925245991952
	1925245992144 [label=ReluBackward0]
	1925245992288 -> 1925245992144
	1925245992288 [label=AddBackward0]
	1925245992384 -> 1925245992288
	1925245992384 [label=CudnnBatchNormBackward0]
	1925245992528 -> 1925245992384
	1925245992528 [label=ConvolutionBackward0]
	1925245992720 -> 1925245992528
	1925245992720 [label=ReluBackward0]
	1925245992864 -> 1925245992720
	1925245992864 [label=CudnnBatchNormBackward0]
	1925245992768 -> 1925245992864
	1925245992768 [label=ConvolutionBackward0]
	1925245992336 -> 1925245992768
	1925245992336 [label=ReluBackward0]
	1925240721984 -> 1925245992336
	1925240721984 [label=AddBackward0]
	1925240722032 -> 1925240721984
	1925240722032 [label=CudnnBatchNormBackward0]
	1925240720880 -> 1925240722032
	1925240720880 [label=ConvolutionBackward0]
	1925240726592 -> 1925240720880
	1925240726592 [label=ReluBackward0]
	1925240719152 -> 1925240726592
	1925240719152 [label=CudnnBatchNormBackward0]
	1925240731056 -> 1925240719152
	1925240731056 [label=ConvolutionBackward0]
	1925240719248 -> 1925240731056
	1925240719248 [label=MaxPool2DWithIndicesBackward0]
	1925240725440 -> 1925240719248
	1925240725440 [label=ReluBackward0]
	1925240725296 -> 1925240725440
	1925240725296 [label=CudnnBatchNormBackward0]
	1925240724864 -> 1925240725296
	1925240724864 [label=ConvolutionBackward0]
	1925240725824 -> 1925240724864
	1925212403184 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1925212403184 -> 1925240725824
	1925240725824 [label=AccumulateGrad]
	1925240731968 -> 1925240725296
	1924898015664 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	1924898015664 -> 1925240731968
	1925240731968 [label=AccumulateGrad]
	1925240732064 -> 1925240725296
	1924898019600 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	1924898019600 -> 1925240732064
	1925240732064 [label=AccumulateGrad]
	1925240726352 -> 1925240731056
	1925245885936 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1925245885936 -> 1925240726352
	1925240726352 [label=AccumulateGrad]
	1925240732832 -> 1925240719152
	1925245886032 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1925245886032 -> 1925240732832
	1925240732832 [label=AccumulateGrad]
	1925240731104 -> 1925240719152
	1925245886128 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1925245886128 -> 1925240731104
	1925240731104 [label=AccumulateGrad]
	1925240721120 -> 1925240720880
	1925245886512 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1925245886512 -> 1925240721120
	1925240721120 [label=AccumulateGrad]
	1925240722224 -> 1925240722032
	1925245886608 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1925245886608 -> 1925240722224
	1925240722224 [label=AccumulateGrad]
	1925240722176 -> 1925240722032
	1925245886704 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1925245886704 -> 1925240722176
	1925240722176 [label=AccumulateGrad]
	1925240719248 -> 1925240721984
	1925240724576 -> 1925245992768
	1925245888816 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1925245888816 -> 1925240724576
	1925240724576 [label=AccumulateGrad]
	1925240733264 -> 1925245992864
	1925245888912 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1925245888912 -> 1925240733264
	1925240733264 [label=AccumulateGrad]
	1925240725200 -> 1925245992864
	1925245889008 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1925245889008 -> 1925240725200
	1925240725200 [label=AccumulateGrad]
	1925245992672 -> 1925245992528
	1925245889392 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1925245889392 -> 1925245992672
	1925245992672 [label=AccumulateGrad]
	1925245992480 -> 1925245992384
	1925245889488 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1925245889488 -> 1925245992480
	1925245992480 [label=AccumulateGrad]
	1925245992432 -> 1925245992384
	1925245889584 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1925245889584 -> 1925245992432
	1925245992432 [label=AccumulateGrad]
	1925245992336 -> 1925245992288
	1925245992096 -> 1925245991952
	1925245890544 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1925245890544 -> 1925245992096
	1925245992096 [label=AccumulateGrad]
	1925245991904 -> 1925245991856
	1925245890640 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1925245890640 -> 1925245991904
	1925245991904 [label=AccumulateGrad]
	1925245991760 -> 1925245991856
	1925245890736 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1925245890736 -> 1925245991760
	1925245991760 [label=AccumulateGrad]
	1925245991664 -> 1925245991520
	1925245891120 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1925245891120 -> 1925245991664
	1925245991664 [label=AccumulateGrad]
	1925245991472 -> 1925245991376
	1925245891216 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1925245891216 -> 1925245991472
	1925245991472 [label=AccumulateGrad]
	1925245991424 -> 1925245991376
	1925245891312 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1925245891312 -> 1925245991424
	1925245991424 [label=AccumulateGrad]
	1925245991328 -> 1925245990800
	1925245991328 [label=CudnnBatchNormBackward0]
	1925245992048 -> 1925245991328
	1925245992048 [label=ConvolutionBackward0]
	1925245992144 -> 1925245992048
	1925245992192 -> 1925245992048
	1925245889968 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1925245889968 -> 1925245992192
	1925245992192 [label=AccumulateGrad]
	1925245991616 -> 1925245991328
	1925245890064 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1925245890064 -> 1925245991616
	1925245991616 [label=AccumulateGrad]
	1925245991568 -> 1925245991328
	1925245890160 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1925245890160 -> 1925245991568
	1925245991568 [label=AccumulateGrad]
	1925245990944 -> 1925245981584
	1925245891696 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1925245891696 -> 1925245990944
	1925245990944 [label=AccumulateGrad]
	1925245981632 -> 1925245981680
	1925245891792 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1925245891792 -> 1925245981632
	1925245981632 [label=AccumulateGrad]
	1925245976880 -> 1925245981680
	1925245891888 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1925245891888 -> 1925245976880
	1925245976880 [label=AccumulateGrad]
	1925245976784 -> 1925245976928
	1925245892272 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1925245892272 -> 1925245976784
	1925245976784 [label=AccumulateGrad]
	1925245976976 -> 1925245977360
	1925245892368 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1925245892368 -> 1925245976976
	1925245976976 [label=AccumulateGrad]
	1925245977216 -> 1925245977360
	1925245892464 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1925245892464 -> 1925245977216
	1925245977216 [label=AccumulateGrad]
	1925245977456 -> 1925245977408
	1925245990464 -> 1925245982448
	1925245893424 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1925245893424 -> 1925245990464
	1925245990464 [label=AccumulateGrad]
	1925245988832 -> 1925245989792
	1925245893520 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1925245893520 -> 1925245988832
	1925245988832 [label=AccumulateGrad]
	1925245990128 -> 1925245989792
	1925245893616 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1925245893616 -> 1925245990128
	1925245990128 [label=AccumulateGrad]
	1925245990272 -> 1925245978608
	1925245893904 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1925245893904 -> 1925245990272
	1925245990272 [label=AccumulateGrad]
	1925245978464 -> 1925245978176
	1925245894000 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1925245894000 -> 1925245978464
	1925245978464 [label=AccumulateGrad]
	1925245978512 -> 1925245978176
	1925245894096 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1925245894096 -> 1925245978512
	1925245978512 [label=AccumulateGrad]
	1925245978224 -> 1925245978272
	1925245978224 [label=CudnnBatchNormBackward0]
	1925245984224 -> 1925245978224
	1925245984224 [label=ConvolutionBackward0]
	1925245983888 -> 1925245984224
	1925245984704 -> 1925245984224
	1925245892848 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1925245892848 -> 1925245984704
	1925245984704 [label=AccumulateGrad]
	1925245981008 -> 1925245978224
	1925245892944 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1925245892944 -> 1925245981008
	1925245981008 [label=AccumulateGrad]
	1925245977792 -> 1925245978224
	1925245893040 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1925245893040 -> 1925245977792
	1925245977792 [label=AccumulateGrad]
	1925245980288 -> 1925245980336
	1925245894480 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1925245894480 -> 1925245980288
	1925245980288 [label=AccumulateGrad]
	1925245980384 -> 1925245980528
	1925245894576 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1925245894576 -> 1925245980384
	1925245980384 [label=AccumulateGrad]
	1925245988928 -> 1925245980528
	1925246074960 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1925246074960 -> 1925245988928
	1925245988928 [label=AccumulateGrad]
	1925245987440 -> 1925245989456
	1925246075344 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1925246075344 -> 1925245987440
	1925245987440 [label=AccumulateGrad]
	1925245989648 -> 1925245985136
	1925246075440 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1925246075440 -> 1925245989648
	1925245989648 [label=AccumulateGrad]
	1925245989120 -> 1925245985136
	1925246075536 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1925246075536 -> 1925245989120
	1925245989120 [label=AccumulateGrad]
	1925245988784 -> 1925245985328
	1925245984992 -> 1925245984512
	1925246076496 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1925246076496 -> 1925245984992
	1925245984992 [label=AccumulateGrad]
	1925245984416 -> 1925245984368
	1925246076592 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1925246076592 -> 1925245984416
	1925245984416 [label=AccumulateGrad]
	1925245984464 -> 1925245984368
	1925246076688 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1925246076688 -> 1925245984464
	1925245984464 [label=AccumulateGrad]
	1925245984560 -> 1925245984032
	1925246077072 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1925246077072 -> 1925245984560
	1925245984560 [label=AccumulateGrad]
	1925245983984 -> 1925245983600
	1925246077168 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1925246077168 -> 1925245983984
	1925245983984 [label=AccumulateGrad]
	1925245983936 -> 1925245983600
	1925246077264 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1925246077264 -> 1925245983936
	1925245983936 [label=AccumulateGrad]
	1925245983456 -> 1925245983552
	1925245983456 [label=CudnnBatchNormBackward0]
	1925245984944 -> 1925245983456
	1925245984944 [label=ConvolutionBackward0]
	1925245984848 -> 1925245984944
	1925245984896 -> 1925245984944
	1925246075920 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1925246075920 -> 1925245984896
	1925245984896 [label=AccumulateGrad]
	1925245984272 -> 1925245983456
	1925246076016 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1925246076016 -> 1925245984272
	1925245984272 [label=AccumulateGrad]
	1925245984128 -> 1925245983456
	1925246076112 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1925246076112 -> 1925245984128
	1925245984128 [label=AccumulateGrad]
	1925245983648 -> 1925245990512
	1925246077648 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1925246077648 -> 1925245983648
	1925245983648 [label=AccumulateGrad]
	1925245983696 -> 1925245982400
	1925246077744 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1925246077744 -> 1925245983696
	1925245983696 [label=AccumulateGrad]
	1925245981488 -> 1925245982400
	1925246077840 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1925246077840 -> 1925245981488
	1925245981488 [label=AccumulateGrad]
	1925245981440 -> 1925245981248
	1925246078224 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1925246078224 -> 1925245981440
	1925245981440 [label=AccumulateGrad]
	1925245977744 -> 1925245977648
	1925246078320 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1925246078320 -> 1925245977744
	1925245977744 [label=AccumulateGrad]
	1925245977696 -> 1925245977648
	1925246078416 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1925246078416 -> 1925245977696
	1925245977696 [label=AccumulateGrad]
	1925245981920 -> 1925245982256
	1925245982736 -> 1925245990176
	1925245982736 [label=TBackward0]
	1925245982352 -> 1925245982736
	1925246078992 [label="model.fc.weight
 (19, 512)" fillcolor=lightblue]
	1925246078992 -> 1925245982352
	1925245982352 [label=AccumulateGrad]
	1925245977168 -> 1925612504464
}
