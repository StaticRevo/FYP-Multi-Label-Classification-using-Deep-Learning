digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2237410016432 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2237530870048 [label=AddmmBackward0]
	2237530870192 -> 2237530870048
	2237512462416 [label="fc.bias
 (19)" fillcolor=lightblue]
	2237512462416 -> 2237530870192
	2237530870192 [label=AccumulateGrad]
	2237530870240 -> 2237530870048
	2237530870240 [label=ViewBackward0]
	2237530870336 -> 2237530870240
	2237530870336 [label=MeanBackward1]
	2237530870480 -> 2237530870336
	2237530870480 [label=ReluBackward0]
	2237530870576 -> 2237530870480
	2237530870576 [label=AddBackward0]
	2237530870672 -> 2237530870576
	2237530870672 [label=CudnnBatchNormBackward0]
	2237530870816 -> 2237530870672
	2237530870816 [label=ConvolutionBackward0]
	2237530871008 -> 2237530870816
	2237530871008 [label=ReluBackward0]
	2237530871152 -> 2237530871008
	2237530871152 [label=CudnnBatchNormBackward0]
	2237530871248 -> 2237530871152
	2237530871248 [label=ConvolutionBackward0]
	2237530870624 -> 2237530871248
	2237530870624 [label=ReluBackward0]
	2237530871392 -> 2237530870624
	2237530871392 [label=AddBackward0]
	2237530871488 -> 2237530871392
	2237530871488 [label=CudnnBatchNormBackward0]
	2237530871632 -> 2237530871488
	2237530871632 [label=ConvolutionBackward0]
	2237530871824 -> 2237530871632
	2237530871824 [label=ReluBackward0]
	2237530871968 -> 2237530871824
	2237530871968 [label=CudnnBatchNormBackward0]
	2237530872064 -> 2237530871968
	2237530872064 [label=ConvolutionBackward0]
	2237530872256 -> 2237530872064
	2237530872256 [label=ReluBackward0]
	2237530872400 -> 2237530872256
	2237530872400 [label=AddBackward0]
	2237530872496 -> 2237530872400
	2237530872496 [label=CudnnBatchNormBackward0]
	2237530872640 -> 2237530872496
	2237530872640 [label=ConvolutionBackward0]
	2237530872832 -> 2237530872640
	2237530872832 [label=ReluBackward0]
	2237530872976 -> 2237530872832
	2237530872976 [label=CudnnBatchNormBackward0]
	2237530873072 -> 2237530872976
	2237530873072 [label=ConvolutionBackward0]
	2237530872448 -> 2237530873072
	2237530872448 [label=ReluBackward0]
	2237530873360 -> 2237530872448
	2237530873360 [label=AddBackward0]
	2237530873456 -> 2237530873360
	2237530873456 [label=CudnnBatchNormBackward0]
	2237530873600 -> 2237530873456
	2237530873600 [label=ConvolutionBackward0]
	2237530873792 -> 2237530873600
	2237530873792 [label=ReluBackward0]
	2237530873936 -> 2237530873792
	2237530873936 [label=CudnnBatchNormBackward0]
	2237530874032 -> 2237530873936
	2237530874032 [label=ConvolutionBackward0]
	2237530874224 -> 2237530874032
	2237530874224 [label=ReluBackward0]
	2237530874368 -> 2237530874224
	2237530874368 [label=AddBackward0]
	2237530874464 -> 2237530874368
	2237530874464 [label=CudnnBatchNormBackward0]
	2237530874608 -> 2237530874464
	2237530874608 [label=ConvolutionBackward0]
	2237530874800 -> 2237530874608
	2237530874800 [label=ReluBackward0]
	2237530874944 -> 2237530874800
	2237530874944 [label=CudnnBatchNormBackward0]
	2237530875040 -> 2237530874944
	2237530875040 [label=ConvolutionBackward0]
	2237530874416 -> 2237530875040
	2237530874416 [label=ReluBackward0]
	2237530875328 -> 2237530874416
	2237530875328 [label=AddBackward0]
	2237530875424 -> 2237530875328
	2237530875424 [label=CudnnBatchNormBackward0]
	2237530875568 -> 2237530875424
	2237530875568 [label=ConvolutionBackward0]
	2237530875760 -> 2237530875568
	2237530875760 [label=ReluBackward0]
	2237530875904 -> 2237530875760
	2237530875904 [label=CudnnBatchNormBackward0]
	2237530876000 -> 2237530875904
	2237530876000 [label=ConvolutionBackward0]
	2237530876192 -> 2237530876000
	2237530876192 [label=ReluBackward0]
	2237530876336 -> 2237530876192
	2237530876336 [label=AddBackward0]
	2237530876432 -> 2237530876336
	2237530876432 [label=CudnnBatchNormBackward0]
	2237530876576 -> 2237530876432
	2237530876576 [label=ConvolutionBackward0]
	2237530876768 -> 2237530876576
	2237530876768 [label=ReluBackward0]
	2237530876912 -> 2237530876768
	2237530876912 [label=CudnnBatchNormBackward0]
	2237530877008 -> 2237530876912
	2237530877008 [label=ConvolutionBackward0]
	2237530876384 -> 2237530877008
	2237530876384 [label=ReluBackward0]
	2237530877296 -> 2237530876384
	2237530877296 [label=AddBackward0]
	2237530877392 -> 2237530877296
	2237530877392 [label=CudnnBatchNormBackward0]
	2237530877536 -> 2237530877392
	2237530877536 [label=ConvolutionBackward0]
	2237530877728 -> 2237530877536
	2237530877728 [label=ReluBackward0]
	2237530877872 -> 2237530877728
	2237530877872 [label=CudnnBatchNormBackward0]
	2237530877968 -> 2237530877872
	2237530877968 [label=ConvolutionBackward0]
	2237530877344 -> 2237530877968
	2237530877344 [label=MaxPool2DWithIndicesBackward0]
	2237530878256 -> 2237530877344
	2237530878256 [label=ReluBackward0]
	2237530878352 -> 2237530878256
	2237530878352 [label=CudnnBatchNormBackward0]
	2237530878448 -> 2237530878352
	2237530878448 [label=ConvolutionBackward0]
	2237530878640 -> 2237530878448
	2237511869712 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2237511869712 -> 2237530878640
	2237530878640 [label=AccumulateGrad]
	2237530878400 -> 2237530878352
	2237530538160 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2237530538160 -> 2237530878400
	2237530878400 [label=AccumulateGrad]
	2237530878064 -> 2237530878352
	2237530538256 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2237530538256 -> 2237530878064
	2237530878064 [label=AccumulateGrad]
	2237530878160 -> 2237530877968
	2237530538640 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2237530538640 -> 2237530878160
	2237530878160 [label=AccumulateGrad]
	2237530877920 -> 2237530877872
	2237530538736 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2237530538736 -> 2237530877920
	2237530877920 [label=AccumulateGrad]
	2237530877776 -> 2237530877872
	2237530538832 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2237530538832 -> 2237530877776
	2237530877776 [label=AccumulateGrad]
	2237530877680 -> 2237530877536
	2237530539216 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2237530539216 -> 2237530877680
	2237530877680 [label=AccumulateGrad]
	2237530877488 -> 2237530877392
	2237530539504 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2237530539504 -> 2237530877488
	2237530877488 [label=AccumulateGrad]
	2237530877440 -> 2237530877392
	2237512891280 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2237512891280 -> 2237530877440
	2237530877440 [label=AccumulateGrad]
	2237530877344 -> 2237530877296
	2237530877200 -> 2237530877008
	2237512891664 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2237512891664 -> 2237530877200
	2237530877200 [label=AccumulateGrad]
	2237530876960 -> 2237530876912
	2237512891760 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2237512891760 -> 2237530876960
	2237530876960 [label=AccumulateGrad]
	2237530876816 -> 2237530876912
	2237512891856 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2237512891856 -> 2237530876816
	2237530876816 [label=AccumulateGrad]
	2237530876720 -> 2237530876576
	2237512892240 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2237512892240 -> 2237530876720
	2237530876720 [label=AccumulateGrad]
	2237530876528 -> 2237530876432
	2237512892336 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2237512892336 -> 2237530876528
	2237530876528 [label=AccumulateGrad]
	2237530876480 -> 2237530876432
	2237511860304 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2237511860304 -> 2237530876480
	2237530876480 [label=AccumulateGrad]
	2237530876384 -> 2237530876336
	2237530876144 -> 2237530876000
	2237511861264 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2237511861264 -> 2237530876144
	2237530876144 [label=AccumulateGrad]
	2237530875952 -> 2237530875904
	2237511861360 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2237511861360 -> 2237530875952
	2237530875952 [label=AccumulateGrad]
	2237530875808 -> 2237530875904
	2237511861456 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2237511861456 -> 2237530875808
	2237530875808 [label=AccumulateGrad]
	2237530875712 -> 2237530875568
	2237511861840 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2237511861840 -> 2237530875712
	2237530875712 [label=AccumulateGrad]
	2237530875520 -> 2237530875424
	2237511861936 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2237511861936 -> 2237530875520
	2237530875520 [label=AccumulateGrad]
	2237530875472 -> 2237530875424
	2237511862032 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2237511862032 -> 2237530875472
	2237530875472 [label=AccumulateGrad]
	2237530875376 -> 2237530875328
	2237530875376 [label=CudnnBatchNormBackward0]
	2237530876096 -> 2237530875376
	2237530876096 [label=ConvolutionBackward0]
	2237530876192 -> 2237530876096
	2237530876240 -> 2237530876096
	2237511860688 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2237511860688 -> 2237530876240
	2237530876240 [label=AccumulateGrad]
	2237530875664 -> 2237530875376
	2237511860784 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2237511860784 -> 2237530875664
	2237530875664 [label=AccumulateGrad]
	2237530875616 -> 2237530875376
	2237511860880 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2237511860880 -> 2237530875616
	2237530875616 [label=AccumulateGrad]
	2237530875232 -> 2237530875040
	2237511862416 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2237511862416 -> 2237530875232
	2237530875232 [label=AccumulateGrad]
	2237530874992 -> 2237530874944
	2237511862512 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2237511862512 -> 2237530874992
	2237530874992 [label=AccumulateGrad]
	2237530874848 -> 2237530874944
	2237511862608 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2237511862608 -> 2237530874848
	2237530874848 [label=AccumulateGrad]
	2237530874752 -> 2237530874608
	2237511862992 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2237511862992 -> 2237530874752
	2237530874752 [label=AccumulateGrad]
	2237530874560 -> 2237530874464
	2237511863088 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2237511863088 -> 2237530874560
	2237530874560 [label=AccumulateGrad]
	2237530874512 -> 2237530874464
	2237511863184 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2237511863184 -> 2237530874512
	2237530874512 [label=AccumulateGrad]
	2237530874416 -> 2237530874368
	2237530874176 -> 2237530874032
	2237511864144 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2237511864144 -> 2237530874176
	2237530874176 [label=AccumulateGrad]
	2237530873984 -> 2237530873936
	2237511864240 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2237511864240 -> 2237530873984
	2237530873984 [label=AccumulateGrad]
	2237530873840 -> 2237530873936
	2237511864336 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2237511864336 -> 2237530873840
	2237530873840 [label=AccumulateGrad]
	2237530873744 -> 2237530873600
	2237511864720 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2237511864720 -> 2237530873744
	2237530873744 [label=AccumulateGrad]
	2237530873552 -> 2237530873456
	2237511864816 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2237511864816 -> 2237530873552
	2237530873552 [label=AccumulateGrad]
	2237530873504 -> 2237530873456
	2237511864912 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2237511864912 -> 2237530873504
	2237530873504 [label=AccumulateGrad]
	2237530873408 -> 2237530873360
	2237530873408 [label=CudnnBatchNormBackward0]
	2237530874128 -> 2237530873408
	2237530874128 [label=ConvolutionBackward0]
	2237530874224 -> 2237530874128
	2237530874272 -> 2237530874128
	2237511863568 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2237511863568 -> 2237530874272
	2237530874272 [label=AccumulateGrad]
	2237530873696 -> 2237530873408
	2237511863664 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2237511863664 -> 2237530873696
	2237530873696 [label=AccumulateGrad]
	2237530873648 -> 2237530873408
	2237511863760 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2237511863760 -> 2237530873648
	2237530873648 [label=AccumulateGrad]
	2237530873264 -> 2237530873072
	2237511865296 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2237511865296 -> 2237530873264
	2237530873264 [label=AccumulateGrad]
	2237530873024 -> 2237530872976
	2237511865392 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2237511865392 -> 2237530873024
	2237530873024 [label=AccumulateGrad]
	2237530872880 -> 2237530872976
	2237511865488 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2237511865488 -> 2237530872880
	2237530872880 [label=AccumulateGrad]
	2237530872784 -> 2237530872640
	2237511865872 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2237511865872 -> 2237530872784
	2237530872784 [label=AccumulateGrad]
	2237530872592 -> 2237530872496
	2237511865968 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2237511865968 -> 2237530872592
	2237530872592 [label=AccumulateGrad]
	2237530872544 -> 2237530872496
	2237511866064 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2237511866064 -> 2237530872544
	2237530872544 [label=AccumulateGrad]
	2237530872448 -> 2237530872400
	2237530872208 -> 2237530872064
	2237511867024 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2237511867024 -> 2237530872208
	2237530872208 [label=AccumulateGrad]
	2237530872016 -> 2237530871968
	2237511867120 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2237511867120 -> 2237530872016
	2237530872016 [label=AccumulateGrad]
	2237530871872 -> 2237530871968
	2237511867216 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2237511867216 -> 2237530871872
	2237530871872 [label=AccumulateGrad]
	2237530871776 -> 2237530871632
	2237511867600 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2237511867600 -> 2237530871776
	2237530871776 [label=AccumulateGrad]
	2237530871584 -> 2237530871488
	2237511867696 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2237511867696 -> 2237530871584
	2237530871584 [label=AccumulateGrad]
	2237530871536 -> 2237530871488
	2237511867792 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2237511867792 -> 2237530871536
	2237530871536 [label=AccumulateGrad]
	2237530871440 -> 2237530871392
	2237530871440 [label=CudnnBatchNormBackward0]
	2237530872160 -> 2237530871440
	2237530872160 [label=ConvolutionBackward0]
	2237530872256 -> 2237530872160
	2237530872304 -> 2237530872160
	2237511866448 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2237511866448 -> 2237530872304
	2237530872304 [label=AccumulateGrad]
	2237530871728 -> 2237530871440
	2237511866544 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2237511866544 -> 2237530871728
	2237530871728 [label=AccumulateGrad]
	2237530871680 -> 2237530871440
	2237511866640 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2237511866640 -> 2237530871680
	2237530871680 [label=AccumulateGrad]
	2237530869472 -> 2237530871248
	2237511868176 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2237511868176 -> 2237530869472
	2237530869472 [label=AccumulateGrad]
	2237530871200 -> 2237530871152
	2237511868272 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2237511868272 -> 2237530871200
	2237530871200 [label=AccumulateGrad]
	2237530871056 -> 2237530871152
	2237511868368 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2237511868368 -> 2237530871056
	2237530871056 [label=AccumulateGrad]
	2237530870960 -> 2237530870816
	2237511868752 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2237511868752 -> 2237530870960
	2237530870960 [label=AccumulateGrad]
	2237530870768 -> 2237530870672
	2237511868848 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2237511868848 -> 2237530870768
	2237530870768 [label=AccumulateGrad]
	2237530870720 -> 2237530870672
	2237511868944 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2237511868944 -> 2237530870720
	2237530870720 [label=AccumulateGrad]
	2237530870624 -> 2237530870576
	2237530870288 -> 2237530870048
	2237530870288 [label=TBackward0]
	2237530870528 -> 2237530870288
	2237511869616 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2237511869616 -> 2237530870528
	2237530870528 [label=AccumulateGrad]
	2237530870048 -> 2237410016432
}
