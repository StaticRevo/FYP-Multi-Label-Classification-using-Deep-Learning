digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2647634242896 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2647553062368 [label=AddmmBackward0]
	2647553062032 -> 2647553062368
	2647638487120 [label="fc.bias
 (19)" fillcolor=lightblue]
	2647638487120 -> 2647553062032
	2647553062032 [label=AccumulateGrad]
	2647553061408 -> 2647553062368
	2647553061408 [label=ViewBackward0]
	2647553060928 -> 2647553061408
	2647553060928 [label=MeanBackward1]
	2647553060592 -> 2647553060928
	2647553060592 [label=ReluBackward0]
	2647553060112 -> 2647553060592
	2647553060112 [label=AddBackward0]
	2647553059632 -> 2647553060112
	2647553059632 [label=CudnnBatchNormBackward0]
	2647553058528 -> 2647553059632
	2647553058528 [label=ConvolutionBackward0]
	2647553057568 -> 2647553058528
	2647553057568 [label=ReluBackward0]
	2647553053488 -> 2647553057568
	2647553053488 [label=CudnnBatchNormBackward0]
	2647553055552 -> 2647553053488
	2647553055552 [label=ConvolutionBackward0]
	2647553064816 -> 2647553055552
	2647553064816 [label=ReluBackward0]
	2647553064576 -> 2647553064816
	2647553064576 [label=CudnnBatchNormBackward0]
	2647553064528 -> 2647553064576
	2647553064528 [label=ConvolutionBackward0]
	2647553059488 -> 2647553064528
	2647553059488 [label=ReluBackward0]
	2647553064096 -> 2647553059488
	2647553064096 [label=AddBackward0]
	2647553064048 -> 2647553064096
	2647553064048 [label=CudnnBatchNormBackward0]
	2647553063856 -> 2647553064048
	2647553063856 [label=ConvolutionBackward0]
	2647553063760 -> 2647553063856
	2647553063760 [label=ReluBackward0]
	2647553063424 -> 2647553063760
	2647553063424 [label=CudnnBatchNormBackward0]
	2647553063232 -> 2647553063424
	2647553063232 [label=ConvolutionBackward0]
	2647553063088 -> 2647553063232
	2647553063088 [label=ReluBackward0]
	2647553062896 -> 2647553063088
	2647553062896 [label=CudnnBatchNormBackward0]
	2647553062560 -> 2647553062896
	2647553062560 [label=ConvolutionBackward0]
	2647553064240 -> 2647553062560
	2647553064240 [label=ReluBackward0]
	2647553062416 -> 2647553064240
	2647553062416 [label=AddBackward0]
	2647553062080 -> 2647553062416
	2647553062080 [label=CudnnBatchNormBackward0]
	2647553062128 -> 2647553062080
	2647553062128 [label=ConvolutionBackward0]
	2647553061792 -> 2647553062128
	2647553061792 [label=ReluBackward0]
	2647553061840 -> 2647553061792
	2647553061840 [label=CudnnBatchNormBackward0]
	2647553061744 -> 2647553061840
	2647553061744 [label=ConvolutionBackward0]
	2647553061120 -> 2647553061744
	2647553061120 [label=ReluBackward0]
	2647553061168 -> 2647553061120
	2647553061168 [label=CudnnBatchNormBackward0]
	2647553061024 -> 2647553061168
	2647553061024 [label=ConvolutionBackward0]
	2647553060736 -> 2647553061024
	2647553060736 [label=ReluBackward0]
	2647553060784 -> 2647553060736
	2647553060784 [label=AddBackward0]
	2647553060496 -> 2647553060784
	2647553060496 [label=CudnnBatchNormBackward0]
	2647553060256 -> 2647553060496
	2647553060256 [label=ConvolutionBackward0]
	2647553060064 -> 2647553060256
	2647553060064 [label=ReluBackward0]
	2647553059680 -> 2647553060064
	2647553059680 [label=CudnnBatchNormBackward0]
	2647553059920 -> 2647553059680
	2647553059920 [label=ConvolutionBackward0]
	2647553059536 -> 2647553059920
	2647553059536 [label=ReluBackward0]
	2647553059296 -> 2647553059536
	2647553059296 [label=CudnnBatchNormBackward0]
	2647553059248 -> 2647553059296
	2647553059248 [label=ConvolutionBackward0]
	2647553060544 -> 2647553059248
	2647553060544 [label=ReluBackward0]
	2647553058816 -> 2647553060544
	2647553058816 [label=AddBackward0]
	2647553058768 -> 2647553058816
	2647553058768 [label=CudnnBatchNormBackward0]
	2647553058576 -> 2647553058768
	2647553058576 [label=ConvolutionBackward0]
	2647553058480 -> 2647553058576
	2647553058480 [label=ReluBackward0]
	2647553058144 -> 2647553058480
	2647553058144 [label=CudnnBatchNormBackward0]
	2647553057952 -> 2647553058144
	2647553057952 [label=ConvolutionBackward0]
	2647553057808 -> 2647553057952
	2647553057808 [label=ReluBackward0]
	2647553057616 -> 2647553057808
	2647553057616 [label=CudnnBatchNormBackward0]
	2647553057280 -> 2647553057616
	2647553057280 [label=ConvolutionBackward0]
	2647553058960 -> 2647553057280
	2647553058960 [label=ReluBackward0]
	2647553053680 -> 2647553058960
	2647553053680 [label=AddBackward0]
	2647553052480 -> 2647553053680
	2647553052480 [label=CudnnBatchNormBackward0]
	2647553053728 -> 2647553052480
	2647553053728 [label=ConvolutionBackward0]
	2647553054208 -> 2647553053728
	2647553054208 [label=ReluBackward0]
	2647553056512 -> 2647553054208
	2647553056512 [label=CudnnBatchNormBackward0]
	2647553056128 -> 2647553056512
	2647553056128 [label=ConvolutionBackward0]
	2647553050320 -> 2647553056128
	2647553050320 [label=ReluBackward0]
	2647553049264 -> 2647553050320
	2647553049264 [label=CudnnBatchNormBackward0]
	2647553054016 -> 2647553049264
	2647553054016 [label=ConvolutionBackward0]
	2647553053584 -> 2647553054016
	2647553053584 [label=ReluBackward0]
	2647553051952 -> 2647553053584
	2647553051952 [label=AddBackward0]
	2647553054160 -> 2647553051952
	2647553054160 [label=CudnnBatchNormBackward0]
	2647553057184 -> 2647553054160
	2647553057184 [label=ConvolutionBackward0]
	2647553053824 -> 2647553057184
	2647553053824 [label=ReluBackward0]
	2647553056608 -> 2647553053824
	2647553056608 [label=CudnnBatchNormBackward0]
	2647553055840 -> 2647553056608
	2647553055840 [label=ConvolutionBackward0]
	2647553055072 -> 2647553055840
	2647553055072 [label=ReluBackward0]
	2647553056752 -> 2647553055072
	2647553056752 [label=CudnnBatchNormBackward0]
	2647553052048 -> 2647553056752
	2647553052048 [label=ConvolutionBackward0]
	2647553049600 -> 2647553052048
	2647553049600 [label=ReluBackward0]
	2647553051760 -> 2647553049600
	2647553051760 [label=AddBackward0]
	2647553054592 -> 2647553051760
	2647553054592 [label=CudnnBatchNormBackward0]
	2647553051712 -> 2647553054592
	2647553051712 [label=ConvolutionBackward0]
	2647553056416 -> 2647553051712
	2647553056416 [label=ReluBackward0]
	2647553052768 -> 2647553056416
	2647553052768 [label=CudnnBatchNormBackward0]
	2647553050080 -> 2647553052768
	2647553050080 [label=ConvolutionBackward0]
	2647553049840 -> 2647553050080
	2647553049840 [label=ReluBackward0]
	2647553054688 -> 2647553049840
	2647553054688 [label=CudnnBatchNormBackward0]
	2647553054640 -> 2647553054688
	2647553054640 [label=ConvolutionBackward0]
	2647553053296 -> 2647553054640
	2647553053296 [label=ReluBackward0]
	2647553050656 -> 2647553053296
	2647553050656 [label=AddBackward0]
	2647553053248 -> 2647553050656
	2647553053248 [label=CudnnBatchNormBackward0]
	2647553053920 -> 2647553053248
	2647553053920 [label=ConvolutionBackward0]
	2647553048976 -> 2647553053920
	2647553048976 [label=ReluBackward0]
	2647641372416 -> 2647553048976
	2647641372416 [label=CudnnBatchNormBackward0]
	2647641374624 -> 2647641372416
	2647641374624 [label=ConvolutionBackward0]
	2647641373616 -> 2647641374624
	2647641373616 [label=ReluBackward0]
	2647641374336 -> 2647641373616
	2647641374336 [label=CudnnBatchNormBackward0]
	2647641373904 -> 2647641374336
	2647641373904 [label=ConvolutionBackward0]
	2647641373328 -> 2647641373904
	2647641373328 [label=ReluBackward0]
	2647641373856 -> 2647641373328
	2647641373856 [label=AddBackward0]
	2647553001648 -> 2647641373856
	2647553001648 [label=CudnnBatchNormBackward0]
	2647634681696 -> 2647553001648
	2647634681696 [label=ConvolutionBackward0]
	2647634680736 -> 2647634681696
	2647634680736 [label=ReluBackward0]
	2647634680400 -> 2647634680736
	2647634680400 [label=CudnnBatchNormBackward0]
	2647634679920 -> 2647634680400
	2647634679920 [label=ConvolutionBackward0]
	2647634678960 -> 2647634679920
	2647634678960 [label=ReluBackward0]
	2647634677856 -> 2647634678960
	2647634677856 [label=CudnnBatchNormBackward0]
	2647634677376 -> 2647634677856
	2647634677376 [label=ConvolutionBackward0]
	2647641373664 -> 2647634677376
	2647641373664 [label=ReluBackward0]
	2647634675936 -> 2647641373664
	2647634675936 [label=AddBackward0]
	2647634675456 -> 2647634675936
	2647634675456 [label=CudnnBatchNormBackward0]
	2647634675120 -> 2647634675456
	2647634675120 [label=ConvolutionBackward0]
	2647634674160 -> 2647634675120
	2647634674160 [label=ReluBackward0]
	2647634682080 -> 2647634674160
	2647634682080 [label=CudnnBatchNormBackward0]
	2647634681984 -> 2647634682080
	2647634681984 [label=ConvolutionBackward0]
	2647634681792 -> 2647634681984
	2647634681792 [label=ReluBackward0]
	2647634681408 -> 2647634681792
	2647634681408 [label=CudnnBatchNormBackward0]
	2647634681648 -> 2647634681408
	2647634681648 [label=ConvolutionBackward0]
	2647634676080 -> 2647634681648
	2647634676080 [label=ReluBackward0]
	2647634680928 -> 2647634676080
	2647634680928 [label=AddBackward0]
	2647634681168 -> 2647634680928
	2647634681168 [label=CudnnBatchNormBackward0]
	2647634680832 -> 2647634681168
	2647634680832 [label=ConvolutionBackward0]
	2647634680544 -> 2647634680832
	2647634680544 [label=ReluBackward0]
	2647634680592 -> 2647634680544
	2647634680592 [label=CudnnBatchNormBackward0]
	2647634680304 -> 2647634680592
	2647634680304 [label=ConvolutionBackward0]
	2647634680208 -> 2647634680304
	2647634680208 [label=ReluBackward0]
	2647634679872 -> 2647634680208
	2647634679872 [label=CudnnBatchNormBackward0]
	2647634679680 -> 2647634679872
	2647634679680 [label=ConvolutionBackward0]
	2647634681024 -> 2647634679680
	2647634681024 [label=ReluBackward0]
	2647634679392 -> 2647634681024
	2647634679392 [label=AddBackward0]
	2647634679200 -> 2647634679392
	2647634679200 [label=CudnnBatchNormBackward0]
	2647634679248 -> 2647634679200
	2647634679248 [label=ConvolutionBackward0]
	2647634678864 -> 2647634679248
	2647634678864 [label=ReluBackward0]
	2647634678624 -> 2647634678864
	2647634678624 [label=CudnnBatchNormBackward0]
	2647634678576 -> 2647634678624
	2647634678576 [label=ConvolutionBackward0]
	2647634678240 -> 2647634678576
	2647634678240 [label=ReluBackward0]
	2647634678288 -> 2647634678240
	2647634678288 [label=CudnnBatchNormBackward0]
	2647634678192 -> 2647634678288
	2647634678192 [label=ConvolutionBackward0]
	2647634677568 -> 2647634678192
	2647634677568 [label=ReluBackward0]
	2647634677616 -> 2647634677568
	2647634677616 [label=AddBackward0]
	2647634677472 -> 2647634677616
	2647634677472 [label=CudnnBatchNormBackward0]
	2647634677088 -> 2647634677472
	2647634677088 [label=ConvolutionBackward0]
	2647634677232 -> 2647634677088
	2647634677232 [label=ReluBackward0]
	2647634676800 -> 2647634677232
	2647634676800 [label=CudnnBatchNormBackward0]
	2647634676704 -> 2647634676800
	2647634676704 [label=ConvolutionBackward0]
	2647634676512 -> 2647634676704
	2647634676512 [label=ReluBackward0]
	2647634676128 -> 2647634676512
	2647634676128 [label=CudnnBatchNormBackward0]
	2647634676368 -> 2647634676128
	2647634676368 [label=ConvolutionBackward0]
	2647634677712 -> 2647634676368
	2647634677712 [label=ReluBackward0]
	2647634675648 -> 2647634677712
	2647634675648 [label=AddBackward0]
	2647634675888 -> 2647634675648
	2647634675888 [label=CudnnBatchNormBackward0]
	2647634675552 -> 2647634675888
	2647634675552 [label=ConvolutionBackward0]
	2647634675264 -> 2647634675552
	2647634675264 [label=ReluBackward0]
	2647634675312 -> 2647634675264
	2647634675312 [label=CudnnBatchNormBackward0]
	2647634675024 -> 2647634675312
	2647634675024 [label=ConvolutionBackward0]
	2647634674928 -> 2647634675024
	2647634674928 [label=ReluBackward0]
	2647634674592 -> 2647634674928
	2647634674592 [label=CudnnBatchNormBackward0]
	2647634674400 -> 2647634674592
	2647634674400 [label=ConvolutionBackward0]
	2647634675744 -> 2647634674400
	2647634675744 [label=ReluBackward0]
	2647634674112 -> 2647634675744
	2647634674112 [label=AddBackward0]
	2647634673920 -> 2647634674112
	2647634673920 [label=CudnnBatchNormBackward0]
	2647634673968 -> 2647634673920
	2647634673968 [label=ConvolutionBackward0]
	2647634682752 -> 2647634673968
	2647634682752 [label=ReluBackward0]
	2647634683088 -> 2647634682752
	2647634683088 [label=CudnnBatchNormBackward0]
	2647634683184 -> 2647634683088
	2647634683184 [label=ConvolutionBackward0]
	2647634683376 -> 2647634683184
	2647634683376 [label=ReluBackward0]
	2647634683520 -> 2647634683376
	2647634683520 [label=CudnnBatchNormBackward0]
	2647634683616 -> 2647634683520
	2647634683616 [label=ConvolutionBackward0]
	2647634683808 -> 2647634683616
	2647634683808 [label=MaxPool2DWithIndicesBackward0]
	2647634683952 -> 2647634683808
	2647634683952 [label=ReluBackward0]
	2647634684048 -> 2647634683952
	2647634684048 [label=CudnnBatchNormBackward0]
	2647634684144 -> 2647634684048
	2647634684144 [label=ConvolutionBackward0]
	2647634684336 -> 2647634684144
	2647638486928 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2647638486928 -> 2647634684336
	2647634684336 [label=AccumulateGrad]
	2647634684096 -> 2647634684048
	2647550131184 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2647550131184 -> 2647634684096
	2647634684096 [label=AccumulateGrad]
	2647634683856 -> 2647634684048
	2647550130896 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2647550130896 -> 2647634683856
	2647634683856 [label=AccumulateGrad]
	2647634683760 -> 2647634683616
	2647550131760 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2647550131760 -> 2647634683760
	2647634683760 [label=AccumulateGrad]
	2647634683568 -> 2647634683520
	2647550130800 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2647550130800 -> 2647634683568
	2647634683568 [label=AccumulateGrad]
	2647634683424 -> 2647634683520
	2647550130704 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2647550130704 -> 2647634683424
	2647634683424 [label=AccumulateGrad]
	2647634683328 -> 2647634683184
	2647550129840 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2647550129840 -> 2647634683328
	2647634683328 [label=AccumulateGrad]
	2647634683136 -> 2647634683088
	2647550130032 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2647550130032 -> 2647634683136
	2647634683136 [label=AccumulateGrad]
	2647634682992 -> 2647634683088
	2647550130128 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2647550130128 -> 2647634682992
	2647634682992 [label=AccumulateGrad]
	2647634682896 -> 2647634673968
	2647550130608 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2647550130608 -> 2647634682896
	2647634682896 [label=AccumulateGrad]
	2647634673824 -> 2647634673920
	2647550130512 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2647550130512 -> 2647634673824
	2647634673824 [label=AccumulateGrad]
	2647634673728 -> 2647634673920
	2647550129552 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2647550129552 -> 2647634673728
	2647634673728 [label=AccumulateGrad]
	2647634674064 -> 2647634674112
	2647634674064 [label=CudnnBatchNormBackward0]
	2647634683280 -> 2647634674064
	2647634683280 [label=ConvolutionBackward0]
	2647634683808 -> 2647634683280
	2647634683664 -> 2647634683280
	2647550131280 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2647550131280 -> 2647634683664
	2647634683664 [label=AccumulateGrad]
	2647634673872 -> 2647634674064
	2647550131376 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2647550131376 -> 2647634673872
	2647634673872 [label=AccumulateGrad]
	2647634673776 -> 2647634674064
	2647550131472 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2647550131472 -> 2647634673776
	2647634673776 [label=AccumulateGrad]
	2647634674256 -> 2647634674400
	2647550129168 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2647550129168 -> 2647634674256
	2647634674256 [label=AccumulateGrad]
	2647634674544 -> 2647634674592
	2647550126768 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2647550126768 -> 2647634674544
	2647634674544 [label=AccumulateGrad]
	2647634674736 -> 2647634674592
	2647550126960 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2647550126960 -> 2647634674736
	2647634674736 [label=AccumulateGrad]
	2647634674784 -> 2647634675024
	2647550129072 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2647550129072 -> 2647634674784
	2647634674784 [label=AccumulateGrad]
	2647634675072 -> 2647634675312
	2647550129360 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2647550129360 -> 2647634675072
	2647634675072 [label=AccumulateGrad]
	2647634675408 -> 2647634675312
	2647550129264 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2647550129264 -> 2647634675408
	2647634675408 [label=AccumulateGrad]
	2647634675168 -> 2647634675552
	2647638853392 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2647638853392 -> 2647634675168
	2647634675168 [label=AccumulateGrad]
	2647634675792 -> 2647634675888
	2647638853488 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2647638853488 -> 2647634675792
	2647634675792 [label=AccumulateGrad]
	2647634675696 -> 2647634675888
	2647638853584 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2647638853584 -> 2647634675696
	2647634675696 [label=AccumulateGrad]
	2647634675744 -> 2647634675648
	2647634675984 -> 2647634676368
	2647638853968 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2647638853968 -> 2647634675984
	2647634675984 [label=AccumulateGrad]
	2647634676224 -> 2647634676128
	2647638854064 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2647638854064 -> 2647634676224
	2647634676224 [label=AccumulateGrad]
	2647634676464 -> 2647634676128
	2647638854160 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2647638854160 -> 2647634676464
	2647634676464 [label=AccumulateGrad]
	2647634676752 -> 2647634676704
	2647638854544 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2647638854544 -> 2647634676752
	2647634676752 [label=AccumulateGrad]
	2647634676608 -> 2647634676800
	2647638854640 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2647638854640 -> 2647634676608
	2647634676608 [label=AccumulateGrad]
	2647634676992 -> 2647634676800
	2647638854736 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2647638854736 -> 2647634676992
	2647634676992 [label=AccumulateGrad]
	2647634677136 -> 2647634677088
	2647638855120 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2647638855120 -> 2647634677136
	2647634677136 [label=AccumulateGrad]
	2647634677280 -> 2647634677472
	2647638855216 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2647638855216 -> 2647634677280
	2647634677280 [label=AccumulateGrad]
	2647634677424 -> 2647634677472
	2647638855312 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2647638855312 -> 2647634677424
	2647634677424 [label=AccumulateGrad]
	2647634677712 -> 2647634677616
	2647634677760 -> 2647634678192
	2647638856272 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2647638856272 -> 2647634677760
	2647634677760 [label=AccumulateGrad]
	2647634678096 -> 2647634678288
	2647638856368 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2647638856368 -> 2647634678096
	2647634678096 [label=AccumulateGrad]
	2647634678048 -> 2647634678288
	2647638856464 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2647638856464 -> 2647634678048
	2647634678048 [label=AccumulateGrad]
	2647634678384 -> 2647634678576
	2647638856848 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2647638856848 -> 2647634678384
	2647634678384 [label=AccumulateGrad]
	2647634678768 -> 2647634678624
	2647638856944 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2647638856944 -> 2647634678768
	2647634678768 [label=AccumulateGrad]
	2647634678720 -> 2647634678624
	2647638857040 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2647638857040 -> 2647634678720
	2647634678720 [label=AccumulateGrad]
	2647634678912 -> 2647634679248
	2647638857424 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2647638857424 -> 2647634678912
	2647634678912 [label=AccumulateGrad]
	2647634679104 -> 2647634679200
	2647638857520 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2647638857520 -> 2647634679104
	2647634679104 [label=AccumulateGrad]
	2647634679008 -> 2647634679200
	2647638857616 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2647638857616 -> 2647634679008
	2647634679008 [label=AccumulateGrad]
	2647634679344 -> 2647634679392
	2647634679344 [label=CudnnBatchNormBackward0]
	2647634678432 -> 2647634679344
	2647634678432 [label=ConvolutionBackward0]
	2647634677568 -> 2647634678432
	2647634677952 -> 2647634678432
	2647638855696 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2647638855696 -> 2647634677952
	2647634677952 [label=AccumulateGrad]
	2647634679152 -> 2647634679344
	2647638855792 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2647638855792 -> 2647634679152
	2647634679152 [label=AccumulateGrad]
	2647634679056 -> 2647634679344
	2647638855888 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2647638855888 -> 2647634679056
	2647634679056 [label=AccumulateGrad]
	2647634679536 -> 2647634679680
	2647638858000 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2647638858000 -> 2647634679536
	2647634679536 [label=AccumulateGrad]
	2647634679824 -> 2647634679872
	2647638858096 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2647638858096 -> 2647634679824
	2647634679824 [label=AccumulateGrad]
	2647634680016 -> 2647634679872
	2647638858192 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2647638858192 -> 2647634680016
	2647634680016 [label=AccumulateGrad]
	2647634680064 -> 2647634680304
	2647638858576 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2647638858576 -> 2647634680064
	2647634680064 [label=AccumulateGrad]
	2647634680352 -> 2647634680592
	2647638858672 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2647638858672 -> 2647634680352
	2647634680352 [label=AccumulateGrad]
	2647634680688 -> 2647634680592
	2647638858768 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2647638858768 -> 2647634680688
	2647634680688 [label=AccumulateGrad]
	2647634680448 -> 2647634680832
	2647638859152 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2647638859152 -> 2647634680448
	2647634680448 [label=AccumulateGrad]
	2647634681072 -> 2647634681168
	2647638859248 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2647638859248 -> 2647634681072
	2647634681072 [label=AccumulateGrad]
	2647634680976 -> 2647634681168
	2647638859344 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2647638859344 -> 2647634680976
	2647634680976 [label=AccumulateGrad]
	2647634681024 -> 2647634680928
	2647634681264 -> 2647634681648
	2647638859728 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2647638859728 -> 2647634681264
	2647634681264 [label=AccumulateGrad]
	2647634681504 -> 2647634681408
	2647638859824 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2647638859824 -> 2647634681504
	2647634681504 [label=AccumulateGrad]
	2647634681744 -> 2647634681408
	2647638859920 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2647638859920 -> 2647634681744
	2647634681744 [label=AccumulateGrad]
	2647634682032 -> 2647634681984
	2647638860304 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2647638860304 -> 2647634682032
	2647634682032 [label=AccumulateGrad]
	2647634681888 -> 2647634682080
	2647638860400 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2647638860400 -> 2647634681888
	2647634681888 [label=AccumulateGrad]
	2647634682272 -> 2647634682080
	2647638860496 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2647638860496 -> 2647634682272
	2647634682272 [label=AccumulateGrad]
	2647634674016 -> 2647634675120
	2647638860880 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2647638860880 -> 2647634674016
	2647634674016 [label=AccumulateGrad]
	2647634674976 -> 2647634675456
	2647638860976 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2647638860976 -> 2647634674976
	2647634674976 [label=AccumulateGrad]
	2647634675600 -> 2647634675456
	2647638861072 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2647638861072 -> 2647634675600
	2647634675600 [label=AccumulateGrad]
	2647634676080 -> 2647634675936
	2647634676416 -> 2647634677376
	2647638861456 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2647638861456 -> 2647634676416
	2647634676416 [label=AccumulateGrad]
	2647634678000 -> 2647634677856
	2647638861552 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2647638861552 -> 2647634678000
	2647634678000 [label=AccumulateGrad]
	2647634678336 -> 2647634677856
	2647638861648 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2647638861648 -> 2647634678336
	2647634678336 [label=AccumulateGrad]
	2647634678816 -> 2647634679920
	2647638862032 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2647638862032 -> 2647634678816
	2647634678816 [label=AccumulateGrad]
	2647634679776 -> 2647634680400
	2647638862128 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2647638862128 -> 2647634679776
	2647634679776 [label=AccumulateGrad]
	2647634680880 -> 2647634680400
	2647638862224 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2647638862224 -> 2647634680880
	2647634680880 [label=AccumulateGrad]
	2647634681360 -> 2647634681696
	2647638862608 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2647638862608 -> 2647634681360
	2647634681360 [label=AccumulateGrad]
	2647634682176 -> 2647553001648
	2647638862704 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2647638862704 -> 2647634682176
	2647634682176 [label=AccumulateGrad]
	2647634682320 -> 2647553001648
	2647638862800 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2647638862800 -> 2647634682320
	2647634682320 [label=AccumulateGrad]
	2647641373664 -> 2647641373856
	2647641372800 -> 2647641373904
	2647638863760 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2647638863760 -> 2647641372800
	2647641372800 [label=AccumulateGrad]
	2647641374048 -> 2647641374336
	2647638863856 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2647638863856 -> 2647641374048
	2647641374048 [label=AccumulateGrad]
	2647641373232 -> 2647641374336
	2647638863952 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2647638863952 -> 2647641373232
	2647641373232 [label=AccumulateGrad]
	2647641373712 -> 2647641374624
	2647638864336 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2647638864336 -> 2647641373712
	2647641373712 [label=AccumulateGrad]
	2647641372512 -> 2647641372416
	2647638864432 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2647638864432 -> 2647641372512
	2647641372512 [label=AccumulateGrad]
	2647641374240 -> 2647641372416
	2647638864528 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2647638864528 -> 2647641374240
	2647641374240 [label=AccumulateGrad]
	2647641373760 -> 2647553053920
	2647638864912 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2647638864912 -> 2647641373760
	2647641373760 [label=AccumulateGrad]
	2647553048688 -> 2647553053248
	2647638865008 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2647638865008 -> 2647553048688
	2647553048688 [label=AccumulateGrad]
	2647553056800 -> 2647553053248
	2647638865104 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2647638865104 -> 2647553056800
	2647553056800 [label=AccumulateGrad]
	2647553054736 -> 2647553050656
	2647553054736 [label=CudnnBatchNormBackward0]
	2647553054448 -> 2647553054736
	2647553054448 [label=ConvolutionBackward0]
	2647641373328 -> 2647553054448
	2647641374144 -> 2647553054448
	2647638863184 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2647638863184 -> 2647641374144
	2647641374144 [label=AccumulateGrad]
	2647641372848 -> 2647553054736
	2647638863280 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2647638863280 -> 2647641372848
	2647641372848 [label=AccumulateGrad]
	2647641374288 -> 2647553054736
	2647638863376 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2647638863376 -> 2647641374288
	2647641374288 [label=AccumulateGrad]
	2647553056656 -> 2647553054640
	2647638865488 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2647638865488 -> 2647553056656
	2647553056656 [label=AccumulateGrad]
	2647553051328 -> 2647553054688
	2647638865584 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2647638865584 -> 2647553051328
	2647553051328 [label=AccumulateGrad]
	2647553053152 -> 2647553054688
	2647638865680 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2647638865680 -> 2647553053152
	2647553053152 [label=AccumulateGrad]
	2647553050416 -> 2647553050080
	2647638866064 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2647638866064 -> 2647553050416
	2647553050416 [label=AccumulateGrad]
	2647553050848 -> 2647553052768
	2647638866160 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2647638866160 -> 2647553050848
	2647553050848 [label=AccumulateGrad]
	2647553055168 -> 2647553052768
	2647638866256 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2647638866256 -> 2647553055168
	2647553055168 [label=AccumulateGrad]
	2647553052336 -> 2647553051712
	2647638866640 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2647638866640 -> 2647553052336
	2647553052336 [label=AccumulateGrad]
	2647553054352 -> 2647553054592
	2647638866736 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2647638866736 -> 2647553054352
	2647553054352 [label=AccumulateGrad]
	2647553052672 -> 2647553054592
	2647638866832 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2647638866832 -> 2647553052672
	2647553052672 [label=AccumulateGrad]
	2647553053296 -> 2647553051760
	2647553052384 -> 2647553052048
	2647638867216 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2647638867216 -> 2647553052384
	2647553052384 [label=AccumulateGrad]
	2647553050800 -> 2647553056752
	2647638867312 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2647638867312 -> 2647553050800
	2647553050800 [label=AccumulateGrad]
	2647553055600 -> 2647553056752
	2647638867408 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2647638867408 -> 2647553055600
	2647553055600 [label=AccumulateGrad]
	2647553053104 -> 2647553055840
	2647638867792 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2647638867792 -> 2647553053104
	2647553053104 [label=AccumulateGrad]
	2647553055648 -> 2647553056608
	2647638867888 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2647638867888 -> 2647553055648
	2647553055648 [label=AccumulateGrad]
	2647553050032 -> 2647553056608
	2647638474832 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2647638474832 -> 2647553050032
	2647553050032 [label=AccumulateGrad]
	2647553055024 -> 2647553057184
	2647638475216 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2647638475216 -> 2647553055024
	2647553055024 [label=AccumulateGrad]
	2647553052864 -> 2647553054160
	2647638475312 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2647638475312 -> 2647553052864
	2647553052864 [label=AccumulateGrad]
	2647553050272 -> 2647553054160
	2647638475408 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2647638475408 -> 2647553050272
	2647553050272 [label=AccumulateGrad]
	2647553049600 -> 2647553051952
	2647553055456 -> 2647553054016
	2647638475792 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2647638475792 -> 2647553055456
	2647553055456 [label=AccumulateGrad]
	2647553056272 -> 2647553049264
	2647638475888 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2647638475888 -> 2647553056272
	2647553056272 [label=AccumulateGrad]
	2647553054880 -> 2647553049264
	2647638475984 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2647638475984 -> 2647553054880
	2647553054880 [label=AccumulateGrad]
	2647553057040 -> 2647553056128
	2647638476368 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2647638476368 -> 2647553057040
	2647553057040 [label=AccumulateGrad]
	2647553051904 -> 2647553056512
	2647638476464 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2647638476464 -> 2647553051904
	2647553051904 [label=AccumulateGrad]
	2647553051472 -> 2647553056512
	2647638476560 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2647638476560 -> 2647553051472
	2647553051472 [label=AccumulateGrad]
	2647553052624 -> 2647553053728
	2647638476944 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2647638476944 -> 2647553052624
	2647553052624 [label=AccumulateGrad]
	2647553054976 -> 2647553052480
	2647638477040 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2647638477040 -> 2647553054976
	2647553054976 [label=AccumulateGrad]
	2647553053632 -> 2647553052480
	2647638477136 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2647638477136 -> 2647553053632
	2647553053632 [label=AccumulateGrad]
	2647553053584 -> 2647553053680
	2647553057424 -> 2647553057280
	2647638477520 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2647638477520 -> 2647553057424
	2647553057424 [label=AccumulateGrad]
	2647553057472 -> 2647553057616
	2647638477616 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2647638477616 -> 2647553057472
	2647553057472 [label=AccumulateGrad]
	2647553057904 -> 2647553057616
	2647638477712 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2647638477712 -> 2647553057904
	2647553057904 [label=AccumulateGrad]
	2647553058000 -> 2647553057952
	2647638478096 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2647638478096 -> 2647553058000
	2647553058000 [label=AccumulateGrad]
	2647553058096 -> 2647553058144
	2647638478192 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2647638478192 -> 2647553058096
	2647553058096 [label=AccumulateGrad]
	2647553058288 -> 2647553058144
	2647638478288 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2647638478288 -> 2647553058288
	2647553058288 [label=AccumulateGrad]
	2647553058336 -> 2647553058576
	2647638478672 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2647638478672 -> 2647553058336
	2647553058336 [label=AccumulateGrad]
	2647553058624 -> 2647553058768
	2647638478768 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2647638478768 -> 2647553058624
	2647553058624 [label=AccumulateGrad]
	2647553058864 -> 2647553058768
	2647638478864 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2647638478864 -> 2647553058864
	2647553058864 [label=AccumulateGrad]
	2647553058960 -> 2647553058816
	2647553058912 -> 2647553059248
	2647638479248 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2647638479248 -> 2647553058912
	2647553058912 [label=AccumulateGrad]
	2647553059440 -> 2647553059296
	2647638479344 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2647638479344 -> 2647553059440
	2647553059440 [label=AccumulateGrad]
	2647553059392 -> 2647553059296
	2647638479440 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2647638479440 -> 2647553059392
	2647553059392 [label=AccumulateGrad]
	2647553059584 -> 2647553059920
	2647638479824 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2647638479824 -> 2647553059584
	2647553059584 [label=AccumulateGrad]
	2647553059776 -> 2647553059680
	2647638479920 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2647638479920 -> 2647553059776
	2647553059776 [label=AccumulateGrad]
	2647553060016 -> 2647553059680
	2647638480016 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2647638480016 -> 2647553060016
	2647553060016 [label=AccumulateGrad]
	2647553060304 -> 2647553060256
	2647638480400 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2647638480400 -> 2647553060304
	2647553060304 [label=AccumulateGrad]
	2647553060160 -> 2647553060496
	2647638480496 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2647638480496 -> 2647553060160
	2647553060160 [label=AccumulateGrad]
	2647553060352 -> 2647553060496
	2647638480592 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2647638480592 -> 2647553060352
	2647553060352 [label=AccumulateGrad]
	2647553060544 -> 2647553060784
	2647553060640 -> 2647553061024
	2647638481552 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2647638481552 -> 2647553060640
	2647553060640 [label=AccumulateGrad]
	2647553061264 -> 2647553061168
	2647638481648 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2647638481648 -> 2647553061264
	2647553061264 [label=AccumulateGrad]
	2647553061216 -> 2647553061168
	2647638481744 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2647638481744 -> 2647553061216
	2647553061216 [label=AccumulateGrad]
	2647553061312 -> 2647553061744
	2647638482128 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2647638482128 -> 2647553061312
	2647553061312 [label=AccumulateGrad]
	2647553061648 -> 2647553061840
	2647638482224 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2647638482224 -> 2647553061648
	2647553061648 [label=AccumulateGrad]
	2647553061600 -> 2647553061840
	2647638482320 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2647638482320 -> 2647553061600
	2647553061600 [label=AccumulateGrad]
	2647553061936 -> 2647553062128
	2647638482704 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2647638482704 -> 2647553061936
	2647553061936 [label=AccumulateGrad]
	2647553062320 -> 2647553062080
	2647638482800 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2647638482800 -> 2647553062320
	2647553062320 [label=AccumulateGrad]
	2647553062176 -> 2647553062080
	2647638482896 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2647638482896 -> 2647553062176
	2647553062176 [label=AccumulateGrad]
	2647553062272 -> 2647553062416
	2647553062272 [label=CudnnBatchNormBackward0]
	2647553061456 -> 2647553062272
	2647553061456 [label=ConvolutionBackward0]
	2647553060736 -> 2647553061456
	2647553060976 -> 2647553061456
	2647638480976 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2647638480976 -> 2647553060976
	2647553060976 [label=AccumulateGrad]
	2647553061984 -> 2647553062272
	2647638481072 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2647638481072 -> 2647553061984
	2647553061984 [label=AccumulateGrad]
	2647553062224 -> 2647553062272
	2647638481168 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2647638481168 -> 2647553062224
	2647553062224 [label=AccumulateGrad]
	2647553062704 -> 2647553062560
	2647638483280 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2647638483280 -> 2647553062704
	2647553062704 [label=AccumulateGrad]
	2647553062752 -> 2647553062896
	2647638483376 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2647638483376 -> 2647553062752
	2647553062752 [label=AccumulateGrad]
	2647553063184 -> 2647553062896
	2647638483472 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2647638483472 -> 2647553063184
	2647553063184 [label=AccumulateGrad]
	2647553063280 -> 2647553063232
	2647638483856 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2647638483856 -> 2647553063280
	2647553063280 [label=AccumulateGrad]
	2647553063376 -> 2647553063424
	2647638483952 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2647638483952 -> 2647553063376
	2647553063376 [label=AccumulateGrad]
	2647553063568 -> 2647553063424
	2647638484048 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2647638484048 -> 2647553063568
	2647553063568 [label=AccumulateGrad]
	2647553063616 -> 2647553063856
	2647638484432 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2647638484432 -> 2647553063616
	2647553063616 [label=AccumulateGrad]
	2647553063904 -> 2647553064048
	2647638484528 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2647638484528 -> 2647553063904
	2647553063904 [label=AccumulateGrad]
	2647553064144 -> 2647553064048
	2647638484624 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2647638484624 -> 2647553064144
	2647553064144 [label=AccumulateGrad]
	2647553064240 -> 2647553064096
	2647553064192 -> 2647553064528
	2647638485008 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2647638485008 -> 2647553064192
	2647553064192 [label=AccumulateGrad]
	2647553064720 -> 2647553064576
	2647638485104 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2647638485104 -> 2647553064720
	2647553064720 [label=AccumulateGrad]
	2647553064672 -> 2647553064576
	2647638485200 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2647638485200 -> 2647553064672
	2647553064672 [label=AccumulateGrad]
	2647553064864 -> 2647553055552
	2647638485584 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2647638485584 -> 2647553064864
	2647553064864 [label=AccumulateGrad]
	2647553050560 -> 2647553053488
	2647638485680 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2647638485680 -> 2647553050560
	2647553050560 [label=AccumulateGrad]
	2647553057712 -> 2647553053488
	2647638485776 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2647638485776 -> 2647553057712
	2647553057712 [label=AccumulateGrad]
	2647553058192 -> 2647553058528
	2647638486160 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2647638486160 -> 2647553058192
	2647553058192 [label=AccumulateGrad]
	2647553059152 -> 2647553059632
	2647638486256 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2647638486256 -> 2647553059152
	2647553059152 [label=AccumulateGrad]
	2647553059008 -> 2647553059632
	2647638486352 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2647638486352 -> 2647553059008
	2647553059008 [label=AccumulateGrad]
	2647553059488 -> 2647553060112
	2647553061552 -> 2647553062368
	2647553061552 [label=TBackward0]
	2647553059968 -> 2647553061552
	2647638487024 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2647638487024 -> 2647553059968
	2647553059968 [label=AccumulateGrad]
	2647553062368 -> 2647634242896
}
