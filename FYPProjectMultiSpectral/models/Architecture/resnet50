digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2046982088336 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2046981109504 [label=AddmmBackward0]
	2046981109648 -> 2046981109504
	2046988025488 [label="fc.bias
 (19)" fillcolor=lightblue]
	2046988025488 -> 2046981109648
	2046981109648 [label=AccumulateGrad]
	2046981109696 -> 2046981109504
	2046981109696 [label=ViewBackward0]
	2046981109792 -> 2046981109696
	2046981109792 [label=MeanBackward1]
	2046981109936 -> 2046981109792
	2046981109936 [label=ReluBackward0]
	2046981110032 -> 2046981109936
	2046981110032 [label=AddBackward0]
	2046981110128 -> 2046981110032
	2046981110128 [label=CudnnBatchNormBackward0]
	2046981110272 -> 2046981110128
	2046981110272 [label=ConvolutionBackward0]
	2046981110464 -> 2046981110272
	2046981110464 [label=ReluBackward0]
	2046981110608 -> 2046981110464
	2046981110608 [label=CudnnBatchNormBackward0]
	2046981110704 -> 2046981110608
	2046981110704 [label=ConvolutionBackward0]
	2046981110896 -> 2046981110704
	2046981110896 [label=ReluBackward0]
	2046981111040 -> 2046981110896
	2046981111040 [label=CudnnBatchNormBackward0]
	2046981111136 -> 2046981111040
	2046981111136 [label=ConvolutionBackward0]
	2046981110080 -> 2046981111136
	2046981110080 [label=ReluBackward0]
	2046981111424 -> 2046981110080
	2046981111424 [label=AddBackward0]
	2046981111520 -> 2046981111424
	2046981111520 [label=CudnnBatchNormBackward0]
	2046981111664 -> 2046981111520
	2046981111664 [label=ConvolutionBackward0]
	2046981108928 -> 2046981111664
	2046981108928 [label=ReluBackward0]
	2046977671664 -> 2046981108928
	2046977671664 [label=CudnnBatchNormBackward0]
	2046981108976 -> 2046977671664
	2046981108976 [label=ConvolutionBackward0]
	2046976901424 -> 2046981108976
	2046976901424 [label=ReluBackward0]
	2046976901568 -> 2046976901424
	2046976901568 [label=CudnnBatchNormBackward0]
	2046976901664 -> 2046976901568
	2046976901664 [label=ConvolutionBackward0]
	2046981111472 -> 2046976901664
	2046981111472 [label=ReluBackward0]
	2046976901952 -> 2046981111472
	2046976901952 [label=AddBackward0]
	2046976902048 -> 2046976901952
	2046976902048 [label=CudnnBatchNormBackward0]
	2046976902192 -> 2046976902048
	2046976902192 [label=ConvolutionBackward0]
	2046976902384 -> 2046976902192
	2046976902384 [label=ReluBackward0]
	2046976902528 -> 2046976902384
	2046976902528 [label=CudnnBatchNormBackward0]
	2046976902624 -> 2046976902528
	2046976902624 [label=ConvolutionBackward0]
	2046976902816 -> 2046976902624
	2046976902816 [label=ReluBackward0]
	2046976902960 -> 2046976902816
	2046976902960 [label=CudnnBatchNormBackward0]
	2046976903056 -> 2046976902960
	2046976903056 [label=ConvolutionBackward0]
	2046976903248 -> 2046976903056
	2046976903248 [label=ReluBackward0]
	2046976903392 -> 2046976903248
	2046976903392 [label=AddBackward0]
	2046976903488 -> 2046976903392
	2046976903488 [label=CudnnBatchNormBackward0]
	2046976903632 -> 2046976903488
	2046976903632 [label=ConvolutionBackward0]
	2046976903824 -> 2046976903632
	2046976903824 [label=ReluBackward0]
	2046976903968 -> 2046976903824
	2046976903968 [label=CudnnBatchNormBackward0]
	2046976904064 -> 2046976903968
	2046976904064 [label=ConvolutionBackward0]
	2046976904256 -> 2046976904064
	2046976904256 [label=ReluBackward0]
	2046976904400 -> 2046976904256
	2046976904400 [label=CudnnBatchNormBackward0]
	2046976904496 -> 2046976904400
	2046976904496 [label=ConvolutionBackward0]
	2046976903440 -> 2046976904496
	2046976903440 [label=ReluBackward0]
	2046976904784 -> 2046976903440
	2046976904784 [label=AddBackward0]
	2046976904880 -> 2046976904784
	2046976904880 [label=CudnnBatchNormBackward0]
	2046976905024 -> 2046976904880
	2046976905024 [label=ConvolutionBackward0]
	2046976905216 -> 2046976905024
	2046976905216 [label=ReluBackward0]
	2046976905360 -> 2046976905216
	2046976905360 [label=CudnnBatchNormBackward0]
	2046976905456 -> 2046976905360
	2046976905456 [label=ConvolutionBackward0]
	2046976905648 -> 2046976905456
	2046976905648 [label=ReluBackward0]
	2046976905792 -> 2046976905648
	2046976905792 [label=CudnnBatchNormBackward0]
	2046976905888 -> 2046976905792
	2046976905888 [label=ConvolutionBackward0]
	2046976904832 -> 2046976905888
	2046976904832 [label=ReluBackward0]
	2046976906176 -> 2046976904832
	2046976906176 [label=AddBackward0]
	2046976906272 -> 2046976906176
	2046976906272 [label=CudnnBatchNormBackward0]
	2046976906416 -> 2046976906272
	2046976906416 [label=ConvolutionBackward0]
	2046976906608 -> 2046976906416
	2046976906608 [label=ReluBackward0]
	2046976906752 -> 2046976906608
	2046976906752 [label=CudnnBatchNormBackward0]
	2046976906848 -> 2046976906752
	2046976906848 [label=ConvolutionBackward0]
	2046976907040 -> 2046976906848
	2046976907040 [label=ReluBackward0]
	2046976907184 -> 2046976907040
	2046976907184 [label=CudnnBatchNormBackward0]
	2046976907280 -> 2046976907184
	2046976907280 [label=ConvolutionBackward0]
	2046976906224 -> 2046976907280
	2046976906224 [label=ReluBackward0]
	2046976907568 -> 2046976906224
	2046976907568 [label=AddBackward0]
	2046976907664 -> 2046976907568
	2046976907664 [label=CudnnBatchNormBackward0]
	2046976907808 -> 2046976907664
	2046976907808 [label=ConvolutionBackward0]
	2046976908000 -> 2046976907808
	2046976908000 [label=ReluBackward0]
	2046976908144 -> 2046976908000
	2046976908144 [label=CudnnBatchNormBackward0]
	2046976908240 -> 2046976908144
	2046976908240 [label=ConvolutionBackward0]
	2046976908576 -> 2046976908240
	2046976908576 [label=ReluBackward0]
	2046984479856 -> 2046976908576
	2046984479856 [label=CudnnBatchNormBackward0]
	2046984479952 -> 2046984479856
	2046984479952 [label=ConvolutionBackward0]
	2046976907616 -> 2046984479952
	2046976907616 [label=ReluBackward0]
	2046984480240 -> 2046976907616
	2046984480240 [label=AddBackward0]
	2046984480336 -> 2046984480240
	2046984480336 [label=CudnnBatchNormBackward0]
	2046984480480 -> 2046984480336
	2046984480480 [label=ConvolutionBackward0]
	2046984480672 -> 2046984480480
	2046984480672 [label=ReluBackward0]
	2046984480816 -> 2046984480672
	2046984480816 [label=CudnnBatchNormBackward0]
	2046984480912 -> 2046984480816
	2046984480912 [label=ConvolutionBackward0]
	2046984481104 -> 2046984480912
	2046984481104 [label=ReluBackward0]
	2046984481248 -> 2046984481104
	2046984481248 [label=CudnnBatchNormBackward0]
	2046984481344 -> 2046984481248
	2046984481344 [label=ConvolutionBackward0]
	2046984480288 -> 2046984481344
	2046984480288 [label=ReluBackward0]
	2046984481632 -> 2046984480288
	2046984481632 [label=AddBackward0]
	2046984481728 -> 2046984481632
	2046984481728 [label=CudnnBatchNormBackward0]
	2046984481872 -> 2046984481728
	2046984481872 [label=ConvolutionBackward0]
	2046984482064 -> 2046984481872
	2046984482064 [label=ReluBackward0]
	2046984482208 -> 2046984482064
	2046984482208 [label=CudnnBatchNormBackward0]
	2046984482304 -> 2046984482208
	2046984482304 [label=ConvolutionBackward0]
	2046984482496 -> 2046984482304
	2046984482496 [label=ReluBackward0]
	2046984482640 -> 2046984482496
	2046984482640 [label=CudnnBatchNormBackward0]
	2046984482736 -> 2046984482640
	2046984482736 [label=ConvolutionBackward0]
	2046984482928 -> 2046984482736
	2046984482928 [label=ReluBackward0]
	2046984483072 -> 2046984482928
	2046984483072 [label=AddBackward0]
	2046984483168 -> 2046984483072
	2046984483168 [label=CudnnBatchNormBackward0]
	2046984483312 -> 2046984483168
	2046984483312 [label=ConvolutionBackward0]
	2046984483504 -> 2046984483312
	2046984483504 [label=ReluBackward0]
	2046984483648 -> 2046984483504
	2046984483648 [label=CudnnBatchNormBackward0]
	2046984483744 -> 2046984483648
	2046984483744 [label=ConvolutionBackward0]
	2046984483936 -> 2046984483744
	2046984483936 [label=ReluBackward0]
	2046984484080 -> 2046984483936
	2046984484080 [label=CudnnBatchNormBackward0]
	2046984484176 -> 2046984484080
	2046984484176 [label=ConvolutionBackward0]
	2046984483120 -> 2046984484176
	2046984483120 [label=ReluBackward0]
	2046984484464 -> 2046984483120
	2046984484464 [label=AddBackward0]
	2046984484560 -> 2046984484464
	2046984484560 [label=CudnnBatchNormBackward0]
	2046984484704 -> 2046984484560
	2046984484704 [label=ConvolutionBackward0]
	2046984484896 -> 2046984484704
	2046984484896 [label=ReluBackward0]
	2046984485040 -> 2046984484896
	2046984485040 [label=CudnnBatchNormBackward0]
	2046984485136 -> 2046984485040
	2046984485136 [label=ConvolutionBackward0]
	2046984485328 -> 2046984485136
	2046984485328 [label=ReluBackward0]
	2046984485472 -> 2046984485328
	2046984485472 [label=CudnnBatchNormBackward0]
	2046984485568 -> 2046984485472
	2046984485568 [label=ConvolutionBackward0]
	2046984484512 -> 2046984485568
	2046984484512 [label=ReluBackward0]
	2046984485856 -> 2046984484512
	2046984485856 [label=AddBackward0]
	2046984485952 -> 2046984485856
	2046984485952 [label=CudnnBatchNormBackward0]
	2046984486096 -> 2046984485952
	2046984486096 [label=ConvolutionBackward0]
	2046984486288 -> 2046984486096
	2046984486288 [label=ReluBackward0]
	2046984486432 -> 2046984486288
	2046984486432 [label=CudnnBatchNormBackward0]
	2046984486528 -> 2046984486432
	2046984486528 [label=ConvolutionBackward0]
	2046984486720 -> 2046984486528
	2046984486720 [label=ReluBackward0]
	2046984486864 -> 2046984486720
	2046984486864 [label=CudnnBatchNormBackward0]
	2046984486768 -> 2046984486864
	2046984486768 [label=ConvolutionBackward0]
	2046984485904 -> 2046984486768
	2046984485904 [label=ReluBackward0]
	2046976115088 -> 2046984485904
	2046976115088 [label=AddBackward0]
	2046976115184 -> 2046976115088
	2046976115184 [label=CudnnBatchNormBackward0]
	2046976115328 -> 2046976115184
	2046976115328 [label=ConvolutionBackward0]
	2046976115520 -> 2046976115328
	2046976115520 [label=ReluBackward0]
	2046976115664 -> 2046976115520
	2046976115664 [label=CudnnBatchNormBackward0]
	2046976115760 -> 2046976115664
	2046976115760 [label=ConvolutionBackward0]
	2046976115952 -> 2046976115760
	2046976115952 [label=ReluBackward0]
	2046976116096 -> 2046976115952
	2046976116096 [label=CudnnBatchNormBackward0]
	2046976116192 -> 2046976116096
	2046976116192 [label=ConvolutionBackward0]
	2046976116384 -> 2046976116192
	2046976116384 [label=ReluBackward0]
	2046976116528 -> 2046976116384
	2046976116528 [label=AddBackward0]
	2046976116624 -> 2046976116528
	2046976116624 [label=CudnnBatchNormBackward0]
	2046976116768 -> 2046976116624
	2046976116768 [label=ConvolutionBackward0]
	2046976116960 -> 2046976116768
	2046976116960 [label=ReluBackward0]
	2046976117104 -> 2046976116960
	2046976117104 [label=CudnnBatchNormBackward0]
	2046976117200 -> 2046976117104
	2046976117200 [label=ConvolutionBackward0]
	2046976117392 -> 2046976117200
	2046976117392 [label=ReluBackward0]
	2046976117536 -> 2046976117392
	2046976117536 [label=CudnnBatchNormBackward0]
	2046976117632 -> 2046976117536
	2046976117632 [label=ConvolutionBackward0]
	2046976116576 -> 2046976117632
	2046976116576 [label=ReluBackward0]
	2046976117920 -> 2046976116576
	2046976117920 [label=AddBackward0]
	2046976118016 -> 2046976117920
	2046976118016 [label=CudnnBatchNormBackward0]
	2046976118160 -> 2046976118016
	2046976118160 [label=ConvolutionBackward0]
	2046976118352 -> 2046976118160
	2046976118352 [label=ReluBackward0]
	2046976118496 -> 2046976118352
	2046976118496 [label=CudnnBatchNormBackward0]
	2046976118592 -> 2046976118496
	2046976118592 [label=ConvolutionBackward0]
	2046976118784 -> 2046976118592
	2046976118784 [label=ReluBackward0]
	2046976118928 -> 2046976118784
	2046976118928 [label=CudnnBatchNormBackward0]
	2046976119024 -> 2046976118928
	2046976119024 [label=ConvolutionBackward0]
	2046976117968 -> 2046976119024
	2046976117968 [label=ReluBackward0]
	2046976119312 -> 2046976117968
	2046976119312 [label=AddBackward0]
	2046976119408 -> 2046976119312
	2046976119408 [label=CudnnBatchNormBackward0]
	2046976119552 -> 2046976119408
	2046976119552 [label=ConvolutionBackward0]
	2046976119744 -> 2046976119552
	2046976119744 [label=ReluBackward0]
	2046976119888 -> 2046976119744
	2046976119888 [label=CudnnBatchNormBackward0]
	2046976119984 -> 2046976119888
	2046976119984 [label=ConvolutionBackward0]
	2046976120176 -> 2046976119984
	2046976120176 [label=ReluBackward0]
	2046976120320 -> 2046976120176
	2046976120320 [label=CudnnBatchNormBackward0]
	2046976120416 -> 2046976120320
	2046976120416 [label=ConvolutionBackward0]
	2046976120608 -> 2046976120416
	2046976120608 [label=MaxPool2DWithIndicesBackward0]
	2046976120752 -> 2046976120608
	2046976120752 [label=ReluBackward0]
	2046976120848 -> 2046976120752
	2046976120848 [label=CudnnBatchNormBackward0]
	2046976120944 -> 2046976120848
	2046976120944 [label=ConvolutionBackward0]
	2046976121136 -> 2046976120944
	2046988025680 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2046988025680 -> 2046976121136
	2046976121136 [label=AccumulateGrad]
	2046976120896 -> 2046976120848
	2046866524208 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2046866524208 -> 2046976120896
	2046976120896 [label=AccumulateGrad]
	2046976120656 -> 2046976120848
	2046866524304 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2046866524304 -> 2046976120656
	2046976120656 [label=AccumulateGrad]
	2046976120560 -> 2046976120416
	2046866525264 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2046866525264 -> 2046976120560
	2046976120560 [label=AccumulateGrad]
	2046976120368 -> 2046976120320
	2046866525360 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2046866525360 -> 2046976120368
	2046976120368 [label=AccumulateGrad]
	2046976120224 -> 2046976120320
	2046866525456 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2046866525456 -> 2046976120224
	2046976120224 [label=AccumulateGrad]
	2046976120128 -> 2046976119984
	2046866525840 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2046866525840 -> 2046976120128
	2046976120128 [label=AccumulateGrad]
	2046976119936 -> 2046976119888
	2046866525936 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2046866525936 -> 2046976119936
	2046976119936 [label=AccumulateGrad]
	2046976119792 -> 2046976119888
	2046866526032 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2046866526032 -> 2046976119792
	2046976119792 [label=AccumulateGrad]
	2046976119696 -> 2046976119552
	2046866526416 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2046866526416 -> 2046976119696
	2046976119696 [label=AccumulateGrad]
	2046976119504 -> 2046976119408
	2046866526512 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2046866526512 -> 2046976119504
	2046976119504 [label=AccumulateGrad]
	2046976119456 -> 2046976119408
	2046866526608 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2046866526608 -> 2046976119456
	2046976119456 [label=AccumulateGrad]
	2046976119360 -> 2046976119312
	2046976119360 [label=CudnnBatchNormBackward0]
	2046976120080 -> 2046976119360
	2046976120080 [label=ConvolutionBackward0]
	2046976120608 -> 2046976120080
	2046976120464 -> 2046976120080
	2046866524688 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2046866524688 -> 2046976120464
	2046976120464 [label=AccumulateGrad]
	2046976119648 -> 2046976119360
	2046866524784 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2046866524784 -> 2046976119648
	2046976119648 [label=AccumulateGrad]
	2046976119600 -> 2046976119360
	2046866524880 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2046866524880 -> 2046976119600
	2046976119600 [label=AccumulateGrad]
	2046976119216 -> 2046976119024
	2046866526992 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2046866526992 -> 2046976119216
	2046976119216 [label=AccumulateGrad]
	2046976118976 -> 2046976118928
	2046866527088 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2046866527088 -> 2046976118976
	2046976118976 [label=AccumulateGrad]
	2046976118832 -> 2046976118928
	2046866527184 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2046866527184 -> 2046976118832
	2046976118832 [label=AccumulateGrad]
	2046976118736 -> 2046976118592
	2046866527568 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2046866527568 -> 2046976118736
	2046976118736 [label=AccumulateGrad]
	2046976118544 -> 2046976118496
	2046866527664 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2046866527664 -> 2046976118544
	2046976118544 [label=AccumulateGrad]
	2046976118400 -> 2046976118496
	2046866527760 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2046866527760 -> 2046976118400
	2046976118400 [label=AccumulateGrad]
	2046976118304 -> 2046976118160
	2046866528144 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2046866528144 -> 2046976118304
	2046976118304 [label=AccumulateGrad]
	2046976118112 -> 2046976118016
	2046866528240 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2046866528240 -> 2046976118112
	2046976118112 [label=AccumulateGrad]
	2046976118064 -> 2046976118016
	2046866528336 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2046866528336 -> 2046976118064
	2046976118064 [label=AccumulateGrad]
	2046976117968 -> 2046976117920
	2046976117824 -> 2046976117632
	2046866528720 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2046866528720 -> 2046976117824
	2046976117824 [label=AccumulateGrad]
	2046976117584 -> 2046976117536
	2046866528816 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2046866528816 -> 2046976117584
	2046976117584 [label=AccumulateGrad]
	2046976117440 -> 2046976117536
	2046866528912 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2046866528912 -> 2046976117440
	2046976117440 [label=AccumulateGrad]
	2046976117344 -> 2046976117200
	2046866529296 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2046866529296 -> 2046976117344
	2046976117344 [label=AccumulateGrad]
	2046976117152 -> 2046976117104
	2046866529392 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2046866529392 -> 2046976117152
	2046976117152 [label=AccumulateGrad]
	2046976117008 -> 2046976117104
	2046866529488 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2046866529488 -> 2046976117008
	2046976117008 [label=AccumulateGrad]
	2046976116912 -> 2046976116768
	2046866529872 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2046866529872 -> 2046976116912
	2046976116912 [label=AccumulateGrad]
	2046976116720 -> 2046976116624
	2046866529968 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2046866529968 -> 2046976116720
	2046976116720 [label=AccumulateGrad]
	2046976116672 -> 2046976116624
	2046866530064 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2046866530064 -> 2046976116672
	2046976116672 [label=AccumulateGrad]
	2046976116576 -> 2046976116528
	2046976116336 -> 2046976116192
	2046866531024 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2046866531024 -> 2046976116336
	2046976116336 [label=AccumulateGrad]
	2046976116144 -> 2046976116096
	2046866531120 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2046866531120 -> 2046976116144
	2046976116144 [label=AccumulateGrad]
	2046976116000 -> 2046976116096
	2046866531216 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2046866531216 -> 2046976116000
	2046976116000 [label=AccumulateGrad]
	2046976115904 -> 2046976115760
	2046866531600 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2046866531600 -> 2046976115904
	2046976115904 [label=AccumulateGrad]
	2046976115712 -> 2046976115664
	2046866531696 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2046866531696 -> 2046976115712
	2046976115712 [label=AccumulateGrad]
	2046976115568 -> 2046976115664
	2046866531792 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2046866531792 -> 2046976115568
	2046976115568 [label=AccumulateGrad]
	2046976115472 -> 2046976115328
	2046866532176 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2046866532176 -> 2046976115472
	2046976115472 [label=AccumulateGrad]
	2046976115280 -> 2046976115184
	2046866532272 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2046866532272 -> 2046976115280
	2046976115280 [label=AccumulateGrad]
	2046976115232 -> 2046976115184
	2046866532368 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2046866532368 -> 2046976115232
	2046976115232 [label=AccumulateGrad]
	2046976115136 -> 2046976115088
	2046976115136 [label=CudnnBatchNormBackward0]
	2046976115856 -> 2046976115136
	2046976115856 [label=ConvolutionBackward0]
	2046976116384 -> 2046976115856
	2046976116240 -> 2046976115856
	2046866530448 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2046866530448 -> 2046976116240
	2046976116240 [label=AccumulateGrad]
	2046976115424 -> 2046976115136
	2046866530544 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2046866530544 -> 2046976115424
	2046976115424 [label=AccumulateGrad]
	2046976115376 -> 2046976115136
	2046866530640 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2046866530640 -> 2046976115376
	2046976115376 [label=AccumulateGrad]
	2046976114992 -> 2046984486768
	2046866532752 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2046866532752 -> 2046976114992
	2046976114992 [label=AccumulateGrad]
	2046976114800 -> 2046984486864
	2046866532848 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2046866532848 -> 2046976114800
	2046976114800 [label=AccumulateGrad]
	2046976114752 -> 2046984486864
	2046866532944 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2046866532944 -> 2046976114752
	2046976114752 [label=AccumulateGrad]
	2046984486672 -> 2046984486528
	2046866533328 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2046866533328 -> 2046984486672
	2046984486672 [label=AccumulateGrad]
	2046984486480 -> 2046984486432
	2046866533424 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2046866533424 -> 2046984486480
	2046984486480 [label=AccumulateGrad]
	2046984486336 -> 2046984486432
	2046866533520 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2046866533520 -> 2046984486336
	2046984486336 [label=AccumulateGrad]
	2046984486240 -> 2046984486096
	2046866533904 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2046866533904 -> 2046984486240
	2046984486240 [label=AccumulateGrad]
	2046984486048 -> 2046984485952
	2046866534000 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2046866534000 -> 2046984486048
	2046984486048 [label=AccumulateGrad]
	2046984486000 -> 2046984485952
	2046866534096 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2046866534096 -> 2046984486000
	2046984486000 [label=AccumulateGrad]
	2046984485904 -> 2046984485856
	2046984485760 -> 2046984485568
	2046866534480 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2046866534480 -> 2046984485760
	2046984485760 [label=AccumulateGrad]
	2046984485520 -> 2046984485472
	2046866534576 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2046866534576 -> 2046984485520
	2046984485520 [label=AccumulateGrad]
	2046984485376 -> 2046984485472
	2046866534672 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2046866534672 -> 2046984485376
	2046984485376 [label=AccumulateGrad]
	2046984485280 -> 2046984485136
	2046866535056 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2046866535056 -> 2046984485280
	2046984485280 [label=AccumulateGrad]
	2046984485088 -> 2046984485040
	2046866535152 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2046866535152 -> 2046984485088
	2046984485088 [label=AccumulateGrad]
	2046984484944 -> 2046984485040
	2046866535248 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2046866535248 -> 2046984484944
	2046984484944 [label=AccumulateGrad]
	2046984484848 -> 2046984484704
	2046866535632 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2046866535632 -> 2046984484848
	2046984484848 [label=AccumulateGrad]
	2046984484656 -> 2046984484560
	2046866535728 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2046866535728 -> 2046984484656
	2046984484656 [label=AccumulateGrad]
	2046984484608 -> 2046984484560
	2046866535824 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2046866535824 -> 2046984484608
	2046984484608 [label=AccumulateGrad]
	2046984484512 -> 2046984484464
	2046984484368 -> 2046984484176
	2046866536208 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2046866536208 -> 2046984484368
	2046984484368 [label=AccumulateGrad]
	2046984484128 -> 2046984484080
	2046866536304 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2046866536304 -> 2046984484128
	2046984484128 [label=AccumulateGrad]
	2046984483984 -> 2046984484080
	2046866536400 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2046866536400 -> 2046984483984
	2046984483984 [label=AccumulateGrad]
	2046984483888 -> 2046984483744
	2046866536784 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2046866536784 -> 2046984483888
	2046984483888 [label=AccumulateGrad]
	2046984483696 -> 2046984483648
	2046866536880 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2046866536880 -> 2046984483696
	2046984483696 [label=AccumulateGrad]
	2046984483552 -> 2046984483648
	2046866536976 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2046866536976 -> 2046984483552
	2046984483552 [label=AccumulateGrad]
	2046984483456 -> 2046984483312
	2046866537360 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2046866537360 -> 2046984483456
	2046984483456 [label=AccumulateGrad]
	2046984483264 -> 2046984483168
	2046866537456 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2046866537456 -> 2046984483264
	2046984483264 [label=AccumulateGrad]
	2046984483216 -> 2046984483168
	2046866537552 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2046866537552 -> 2046984483216
	2046984483216 [label=AccumulateGrad]
	2046984483120 -> 2046984483072
	2046984482880 -> 2046984482736
	2046977622096 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2046977622096 -> 2046984482880
	2046984482880 [label=AccumulateGrad]
	2046984482688 -> 2046984482640
	2046977622192 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2046977622192 -> 2046984482688
	2046984482688 [label=AccumulateGrad]
	2046984482544 -> 2046984482640
	2046977622288 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2046977622288 -> 2046984482544
	2046984482544 [label=AccumulateGrad]
	2046984482448 -> 2046984482304
	2046977622672 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2046977622672 -> 2046984482448
	2046984482448 [label=AccumulateGrad]
	2046984482256 -> 2046984482208
	2046977622768 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2046977622768 -> 2046984482256
	2046984482256 [label=AccumulateGrad]
	2046984482112 -> 2046984482208
	2046977622864 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2046977622864 -> 2046984482112
	2046984482112 [label=AccumulateGrad]
	2046984482016 -> 2046984481872
	2046977623248 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2046977623248 -> 2046984482016
	2046984482016 [label=AccumulateGrad]
	2046984481824 -> 2046984481728
	2046977623344 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2046977623344 -> 2046984481824
	2046984481824 [label=AccumulateGrad]
	2046984481776 -> 2046984481728
	2046977623440 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2046977623440 -> 2046984481776
	2046984481776 [label=AccumulateGrad]
	2046984481680 -> 2046984481632
	2046984481680 [label=CudnnBatchNormBackward0]
	2046984482400 -> 2046984481680
	2046984482400 [label=ConvolutionBackward0]
	2046984482928 -> 2046984482400
	2046984482784 -> 2046984482400
	2046866537936 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2046866537936 -> 2046984482784
	2046984482784 [label=AccumulateGrad]
	2046984481968 -> 2046984481680
	2046866538032 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2046866538032 -> 2046984481968
	2046984481968 [label=AccumulateGrad]
	2046984481920 -> 2046984481680
	2046866538128 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2046866538128 -> 2046984481920
	2046984481920 [label=AccumulateGrad]
	2046984481536 -> 2046984481344
	2046977623824 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2046977623824 -> 2046984481536
	2046984481536 [label=AccumulateGrad]
	2046984481296 -> 2046984481248
	2046977623920 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2046977623920 -> 2046984481296
	2046984481296 [label=AccumulateGrad]
	2046984481152 -> 2046984481248
	2046977624016 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2046977624016 -> 2046984481152
	2046984481152 [label=AccumulateGrad]
	2046984481056 -> 2046984480912
	2046977624400 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2046977624400 -> 2046984481056
	2046984481056 [label=AccumulateGrad]
	2046984480864 -> 2046984480816
	2046977624496 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2046977624496 -> 2046984480864
	2046984480864 [label=AccumulateGrad]
	2046984480720 -> 2046984480816
	2046977624592 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2046977624592 -> 2046984480720
	2046984480720 [label=AccumulateGrad]
	2046984480624 -> 2046984480480
	2046977624976 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2046977624976 -> 2046984480624
	2046984480624 [label=AccumulateGrad]
	2046984480432 -> 2046984480336
	2046977625072 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2046977625072 -> 2046984480432
	2046984480432 [label=AccumulateGrad]
	2046984480384 -> 2046984480336
	2046977625168 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2046977625168 -> 2046984480384
	2046984480384 [label=AccumulateGrad]
	2046984480288 -> 2046984480240
	2046984480144 -> 2046984479952
	2046977625552 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2046977625552 -> 2046984480144
	2046984480144 [label=AccumulateGrad]
	2046984479904 -> 2046984479856
	2046977625648 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2046977625648 -> 2046984479904
	2046984479904 [label=AccumulateGrad]
	2046984479760 -> 2046984479856
	2046977625744 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2046977625744 -> 2046984479760
	2046984479760 [label=AccumulateGrad]
	2046984479712 -> 2046976908240
	2046977626128 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2046977626128 -> 2046984479712
	2046984479712 [label=AccumulateGrad]
	2046976908192 -> 2046976908144
	2046977626224 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2046977626224 -> 2046976908192
	2046976908192 [label=AccumulateGrad]
	2046976908048 -> 2046976908144
	2046977626320 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2046977626320 -> 2046976908048
	2046976908048 [label=AccumulateGrad]
	2046976907952 -> 2046976907808
	2046977626704 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2046977626704 -> 2046976907952
	2046976907952 [label=AccumulateGrad]
	2046976907760 -> 2046976907664
	2046977626800 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2046977626800 -> 2046976907760
	2046976907760 [label=AccumulateGrad]
	2046976907712 -> 2046976907664
	2046977626896 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2046977626896 -> 2046976907712
	2046976907712 [label=AccumulateGrad]
	2046976907616 -> 2046976907568
	2046976907472 -> 2046976907280
	2046977627280 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2046977627280 -> 2046976907472
	2046976907472 [label=AccumulateGrad]
	2046976907232 -> 2046976907184
	2046977627376 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2046977627376 -> 2046976907232
	2046976907232 [label=AccumulateGrad]
	2046976907088 -> 2046976907184
	2046977627472 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2046977627472 -> 2046976907088
	2046976907088 [label=AccumulateGrad]
	2046976906992 -> 2046976906848
	2046977627856 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2046977627856 -> 2046976906992
	2046976906992 [label=AccumulateGrad]
	2046976906800 -> 2046976906752
	2046977627952 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2046977627952 -> 2046976906800
	2046976906800 [label=AccumulateGrad]
	2046976906656 -> 2046976906752
	2046977628048 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2046977628048 -> 2046976906656
	2046976906656 [label=AccumulateGrad]
	2046976906560 -> 2046976906416
	2046977628432 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2046977628432 -> 2046976906560
	2046976906560 [label=AccumulateGrad]
	2046976906368 -> 2046976906272
	2046977628528 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2046977628528 -> 2046976906368
	2046976906368 [label=AccumulateGrad]
	2046976906320 -> 2046976906272
	2046977628624 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2046977628624 -> 2046976906320
	2046976906320 [label=AccumulateGrad]
	2046976906224 -> 2046976906176
	2046976906080 -> 2046976905888
	2046977629008 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2046977629008 -> 2046976906080
	2046976906080 [label=AccumulateGrad]
	2046976905840 -> 2046976905792
	2046977629104 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2046977629104 -> 2046976905840
	2046976905840 [label=AccumulateGrad]
	2046976905696 -> 2046976905792
	2046977629200 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2046977629200 -> 2046976905696
	2046976905696 [label=AccumulateGrad]
	2046976905600 -> 2046976905456
	2046977629584 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2046977629584 -> 2046976905600
	2046976905600 [label=AccumulateGrad]
	2046976905408 -> 2046976905360
	2046977629680 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2046977629680 -> 2046976905408
	2046976905408 [label=AccumulateGrad]
	2046976905264 -> 2046976905360
	2046977629776 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2046977629776 -> 2046976905264
	2046976905264 [label=AccumulateGrad]
	2046976905168 -> 2046976905024
	2046977630160 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2046977630160 -> 2046976905168
	2046976905168 [label=AccumulateGrad]
	2046976904976 -> 2046976904880
	2046977630256 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2046977630256 -> 2046976904976
	2046976904976 [label=AccumulateGrad]
	2046976904928 -> 2046976904880
	2046977630352 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2046977630352 -> 2046976904928
	2046976904928 [label=AccumulateGrad]
	2046976904832 -> 2046976904784
	2046976904688 -> 2046976904496
	2046977630736 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2046977630736 -> 2046976904688
	2046976904688 [label=AccumulateGrad]
	2046976904448 -> 2046976904400
	2046977630832 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2046977630832 -> 2046976904448
	2046976904448 [label=AccumulateGrad]
	2046976904304 -> 2046976904400
	2046977630928 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2046977630928 -> 2046976904304
	2046976904304 [label=AccumulateGrad]
	2046976904208 -> 2046976904064
	2046977631312 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2046977631312 -> 2046976904208
	2046976904208 [label=AccumulateGrad]
	2046976904016 -> 2046976903968
	2046977631408 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2046977631408 -> 2046976904016
	2046976904016 [label=AccumulateGrad]
	2046976903872 -> 2046976903968
	2046977631504 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2046977631504 -> 2046976903872
	2046976903872 [label=AccumulateGrad]
	2046976903776 -> 2046976903632
	2046977631888 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2046977631888 -> 2046976903776
	2046976903776 [label=AccumulateGrad]
	2046976903584 -> 2046976903488
	2046977631984 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2046977631984 -> 2046976903584
	2046976903584 [label=AccumulateGrad]
	2046976903536 -> 2046976903488
	2046977632080 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2046977632080 -> 2046976903536
	2046976903536 [label=AccumulateGrad]
	2046976903440 -> 2046976903392
	2046976903200 -> 2046976903056
	2046977633040 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2046977633040 -> 2046976903200
	2046976903200 [label=AccumulateGrad]
	2046976903008 -> 2046976902960
	2046977633136 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2046977633136 -> 2046976903008
	2046976903008 [label=AccumulateGrad]
	2046976902864 -> 2046976902960
	2046977633232 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2046977633232 -> 2046976902864
	2046976902864 [label=AccumulateGrad]
	2046976902768 -> 2046976902624
	2046977633616 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2046977633616 -> 2046976902768
	2046976902768 [label=AccumulateGrad]
	2046976902576 -> 2046976902528
	2046977633712 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2046977633712 -> 2046976902576
	2046976902576 [label=AccumulateGrad]
	2046976902432 -> 2046976902528
	2046977633808 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2046977633808 -> 2046976902432
	2046976902432 [label=AccumulateGrad]
	2046976902336 -> 2046976902192
	2046977634192 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2046977634192 -> 2046976902336
	2046976902336 [label=AccumulateGrad]
	2046976902144 -> 2046976902048
	2046977634288 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2046977634288 -> 2046976902144
	2046976902144 [label=AccumulateGrad]
	2046976902096 -> 2046976902048
	2046977634384 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2046977634384 -> 2046976902096
	2046976902096 [label=AccumulateGrad]
	2046976902000 -> 2046976901952
	2046976902000 [label=CudnnBatchNormBackward0]
	2046976902720 -> 2046976902000
	2046976902720 [label=ConvolutionBackward0]
	2046976903248 -> 2046976902720
	2046976903104 -> 2046976902720
	2046977632464 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2046977632464 -> 2046976903104
	2046976903104 [label=AccumulateGrad]
	2046976902288 -> 2046976902000
	2046977632560 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2046977632560 -> 2046976902288
	2046976902288 [label=AccumulateGrad]
	2046976902240 -> 2046976902000
	2046977632656 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2046977632656 -> 2046976902240
	2046976902240 [label=AccumulateGrad]
	2046976901856 -> 2046976901664
	2046977634768 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2046977634768 -> 2046976901856
	2046976901856 [label=AccumulateGrad]
	2046976901616 -> 2046976901568
	2046977634864 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2046977634864 -> 2046976901616
	2046976901616 [label=AccumulateGrad]
	2046976901472 -> 2046976901568
	2046977634960 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2046977634960 -> 2046976901472
	2046976901472 [label=AccumulateGrad]
	2046976901376 -> 2046981108976
	2046977635344 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2046977635344 -> 2046976901376
	2046976901376 [label=AccumulateGrad]
	2046976901232 -> 2046977671664
	2046977635440 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2046977635440 -> 2046976901232
	2046976901232 [label=AccumulateGrad]
	2046976901184 -> 2046977671664
	2046977635536 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2046977635536 -> 2046976901184
	2046976901184 [label=AccumulateGrad]
	2046981108832 -> 2046981111664
	2046977635920 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2046977635920 -> 2046981108832
	2046981108832 [label=AccumulateGrad]
	2046981111616 -> 2046981111520
	2046977636016 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2046977636016 -> 2046981111616
	2046981111616 [label=AccumulateGrad]
	2046981111568 -> 2046981111520
	2046977636112 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2046977636112 -> 2046981111568
	2046981111568 [label=AccumulateGrad]
	2046981111472 -> 2046981111424
	2046981111328 -> 2046981111136
	2046977636496 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2046977636496 -> 2046981111328
	2046981111328 [label=AccumulateGrad]
	2046981111088 -> 2046981111040
	2046977636592 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2046977636592 -> 2046981111088
	2046981111088 [label=AccumulateGrad]
	2046981110944 -> 2046981111040
	2046977636688 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2046977636688 -> 2046981110944
	2046981110944 [label=AccumulateGrad]
	2046981110848 -> 2046981110704
	2046977637072 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2046977637072 -> 2046981110848
	2046981110848 [label=AccumulateGrad]
	2046981110656 -> 2046981110608
	2046977637168 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2046977637168 -> 2046981110656
	2046981110656 [label=AccumulateGrad]
	2046981110512 -> 2046981110608
	2046977637264 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2046977637264 -> 2046981110512
	2046981110512 [label=AccumulateGrad]
	2046981110416 -> 2046981110272
	2046981158000 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2046981158000 -> 2046981110416
	2046981110416 [label=AccumulateGrad]
	2046981110224 -> 2046981110128
	2046981158096 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2046981158096 -> 2046981110224
	2046981110224 [label=AccumulateGrad]
	2046981110176 -> 2046981110128
	2046981158192 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2046981158192 -> 2046981110176
	2046981110176 [label=AccumulateGrad]
	2046981110080 -> 2046981110032
	2046981109744 -> 2046981109504
	2046981109744 [label=TBackward0]
	2046981109984 -> 2046981109744
	2046988025584 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2046988025584 -> 2046981109984
	2046981109984 [label=AccumulateGrad]
	2046981109504 -> 2046982088336
}
