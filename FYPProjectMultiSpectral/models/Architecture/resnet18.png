digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1450752774160 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1450745048720 [label=AddmmBackward0]
	1450745048864 -> 1450745048720
	1450738313328 [label="fc.bias
 (19)" fillcolor=lightblue]
	1450738313328 -> 1450745048864
	1450745048864 [label=AccumulateGrad]
	1450745048912 -> 1450745048720
	1450745048912 [label=ViewBackward0]
	1450745049008 -> 1450745048912
	1450745049008 [label=MeanBackward1]
	1450745049152 -> 1450745049008
	1450745049152 [label=ReluBackward0]
	1450745049248 -> 1450745049152
	1450745049248 [label=AddBackward0]
	1450745049344 -> 1450745049248
	1450745049344 [label=CudnnBatchNormBackward0]
	1450745049488 -> 1450745049344
	1450745049488 [label=ConvolutionBackward0]
	1450745049680 -> 1450745049488
	1450745049680 [label=ReluBackward0]
	1450745049824 -> 1450745049680
	1450745049824 [label=CudnnBatchNormBackward0]
	1450745049920 -> 1450745049824
	1450745049920 [label=ConvolutionBackward0]
	1450745049296 -> 1450745049920
	1450745049296 [label=ReluBackward0]
	1450745050064 -> 1450745049296
	1450745050064 [label=AddBackward0]
	1450745050160 -> 1450745050064
	1450745050160 [label=CudnnBatchNormBackward0]
	1450745050304 -> 1450745050160
	1450745050304 [label=ConvolutionBackward0]
	1450745050496 -> 1450745050304
	1450745050496 [label=ReluBackward0]
	1450745050640 -> 1450745050496
	1450745050640 [label=CudnnBatchNormBackward0]
	1450745050736 -> 1450745050640
	1450745050736 [label=ConvolutionBackward0]
	1450745050928 -> 1450745050736
	1450745050928 [label=ReluBackward0]
	1450745051072 -> 1450745050928
	1450745051072 [label=AddBackward0]
	1450745051168 -> 1450745051072
	1450745051168 [label=CudnnBatchNormBackward0]
	1450745051312 -> 1450745051168
	1450745051312 [label=ConvolutionBackward0]
	1450745051504 -> 1450745051312
	1450745051504 [label=ReluBackward0]
	1450745051648 -> 1450745051504
	1450745051648 [label=CudnnBatchNormBackward0]
	1450745051744 -> 1450745051648
	1450745051744 [label=ConvolutionBackward0]
	1450745051120 -> 1450745051744
	1450745051120 [label=ReluBackward0]
	1450745052032 -> 1450745051120
	1450745052032 [label=AddBackward0]
	1450745052128 -> 1450745052032
	1450745052128 [label=CudnnBatchNormBackward0]
	1450745052272 -> 1450745052128
	1450745052272 [label=ConvolutionBackward0]
	1450745052464 -> 1450745052272
	1450745052464 [label=ReluBackward0]
	1450745052608 -> 1450745052464
	1450745052608 [label=CudnnBatchNormBackward0]
	1450745052704 -> 1450745052608
	1450745052704 [label=ConvolutionBackward0]
	1450745052896 -> 1450745052704
	1450745052896 [label=ReluBackward0]
	1450745053040 -> 1450745052896
	1450745053040 [label=AddBackward0]
	1450745053136 -> 1450745053040
	1450745053136 [label=CudnnBatchNormBackward0]
	1450735599776 -> 1450745053136
	1450735599776 [label=ConvolutionBackward0]
	1450735599968 -> 1450735599776
	1450735599968 [label=ReluBackward0]
	1450735600112 -> 1450735599968
	1450735600112 [label=CudnnBatchNormBackward0]
	1450735600208 -> 1450735600112
	1450735600208 [label=ConvolutionBackward0]
	1450745053088 -> 1450735600208
	1450745053088 [label=ReluBackward0]
	1450735600496 -> 1450745053088
	1450735600496 [label=AddBackward0]
	1450735600592 -> 1450735600496
	1450735600592 [label=CudnnBatchNormBackward0]
	1450735600736 -> 1450735600592
	1450735600736 [label=ConvolutionBackward0]
	1450735600928 -> 1450735600736
	1450735600928 [label=ReluBackward0]
	1450735601072 -> 1450735600928
	1450735601072 [label=CudnnBatchNormBackward0]
	1450735601168 -> 1450735601072
	1450735601168 [label=ConvolutionBackward0]
	1450735601360 -> 1450735601168
	1450735601360 [label=ReluBackward0]
	1450735601504 -> 1450735601360
	1450735601504 [label=AddBackward0]
	1450735601600 -> 1450735601504
	1450735601600 [label=CudnnBatchNormBackward0]
	1450735601744 -> 1450735601600
	1450735601744 [label=ConvolutionBackward0]
	1450735601936 -> 1450735601744
	1450735601936 [label=ReluBackward0]
	1450735602080 -> 1450735601936
	1450735602080 [label=CudnnBatchNormBackward0]
	1450735602176 -> 1450735602080
	1450735602176 [label=ConvolutionBackward0]
	1450735601552 -> 1450735602176
	1450735601552 [label=ReluBackward0]
	1450735602464 -> 1450735601552
	1450735602464 [label=AddBackward0]
	1450735602560 -> 1450735602464
	1450735602560 [label=CudnnBatchNormBackward0]
	1450735602704 -> 1450735602560
	1450735602704 [label=ConvolutionBackward0]
	1450735602896 -> 1450735602704
	1450735602896 [label=ReluBackward0]
	1450735603040 -> 1450735602896
	1450735603040 [label=CudnnBatchNormBackward0]
	1450735603136 -> 1450735603040
	1450735603136 [label=ConvolutionBackward0]
	1450735602512 -> 1450735603136
	1450735602512 [label=MaxPool2DWithIndicesBackward0]
	1450735603424 -> 1450735602512
	1450735603424 [label=ReluBackward0]
	1450735603520 -> 1450735603424
	1450735603520 [label=CudnnBatchNormBackward0]
	1450735603616 -> 1450735603520
	1450735603616 [label=ConvolutionBackward0]
	1450735603808 -> 1450735603616
	1450738313232 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1450738313232 -> 1450735603808
	1450735603808 [label=AccumulateGrad]
	1450735603568 -> 1450735603520
	1450731549232 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1450731549232 -> 1450735603568
	1450735603568 [label=AccumulateGrad]
	1450735603232 -> 1450735603520
	1450731551536 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1450731551536 -> 1450735603232
	1450735603232 [label=AccumulateGrad]
	1450735603328 -> 1450735603136
	1450731551824 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1450731551824 -> 1450735603328
	1450735603328 [label=AccumulateGrad]
	1450735603088 -> 1450735603040
	1450731551920 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1450731551920 -> 1450735603088
	1450735603088 [label=AccumulateGrad]
	1450735602944 -> 1450735603040
	1450731552016 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1450731552016 -> 1450735602944
	1450735602944 [label=AccumulateGrad]
	1450735602848 -> 1450735602704
	1450731552400 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1450731552400 -> 1450735602848
	1450735602848 [label=AccumulateGrad]
	1450735602656 -> 1450735602560
	1450731552496 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1450731552496 -> 1450735602656
	1450735602656 [label=AccumulateGrad]
	1450735602608 -> 1450735602560
	1450731552592 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1450731552592 -> 1450735602608
	1450735602608 [label=AccumulateGrad]
	1450735602512 -> 1450735602464
	1450735602368 -> 1450735602176
	1450738303248 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1450738303248 -> 1450735602368
	1450735602368 [label=AccumulateGrad]
	1450735602128 -> 1450735602080
	1450738303344 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1450738303344 -> 1450735602128
	1450735602128 [label=AccumulateGrad]
	1450735601984 -> 1450735602080
	1450738303440 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1450738303440 -> 1450735601984
	1450735601984 [label=AccumulateGrad]
	1450735601888 -> 1450735601744
	1450738303824 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1450738303824 -> 1450735601888
	1450735601888 [label=AccumulateGrad]
	1450735601696 -> 1450735601600
	1450738303920 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1450738303920 -> 1450735601696
	1450735601696 [label=AccumulateGrad]
	1450735601648 -> 1450735601600
	1450738304016 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1450738304016 -> 1450735601648
	1450735601648 [label=AccumulateGrad]
	1450735601552 -> 1450735601504
	1450735601312 -> 1450735601168
	1450738304976 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1450738304976 -> 1450735601312
	1450735601312 [label=AccumulateGrad]
	1450735601120 -> 1450735601072
	1450738305072 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1450738305072 -> 1450735601120
	1450735601120 [label=AccumulateGrad]
	1450735600976 -> 1450735601072
	1450738305168 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1450738305168 -> 1450735600976
	1450735600976 [label=AccumulateGrad]
	1450735600880 -> 1450735600736
	1450738305552 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1450738305552 -> 1450735600880
	1450735600880 [label=AccumulateGrad]
	1450735600688 -> 1450735600592
	1450738305648 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1450738305648 -> 1450735600688
	1450735600688 [label=AccumulateGrad]
	1450735600640 -> 1450735600592
	1450738305744 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1450738305744 -> 1450735600640
	1450735600640 [label=AccumulateGrad]
	1450735600544 -> 1450735600496
	1450735600544 [label=CudnnBatchNormBackward0]
	1450735601264 -> 1450735600544
	1450735601264 [label=ConvolutionBackward0]
	1450735601360 -> 1450735601264
	1450735601408 -> 1450735601264
	1450738304400 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1450738304400 -> 1450735601408
	1450735601408 [label=AccumulateGrad]
	1450735600832 -> 1450735600544
	1450738304496 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1450738304496 -> 1450735600832
	1450735600832 [label=AccumulateGrad]
	1450735600784 -> 1450735600544
	1450738304592 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1450738304592 -> 1450735600784
	1450735600784 [label=AccumulateGrad]
	1450735600400 -> 1450735600208
	1450738306128 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1450738306128 -> 1450735600400
	1450735600400 [label=AccumulateGrad]
	1450735600160 -> 1450735600112
	1450738306224 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1450738306224 -> 1450735600160
	1450735600160 [label=AccumulateGrad]
	1450735600016 -> 1450735600112
	1450738306320 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1450738306320 -> 1450735600016
	1450735600016 [label=AccumulateGrad]
	1450735599920 -> 1450735599776
	1450738306704 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1450738306704 -> 1450735599920
	1450735599920 [label=AccumulateGrad]
	1450735599728 -> 1450745053136
	1450738306800 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1450738306800 -> 1450735599728
	1450735599728 [label=AccumulateGrad]
	1450735599680 -> 1450745053136
	1450738306896 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1450738306896 -> 1450735599680
	1450735599680 [label=AccumulateGrad]
	1450745053088 -> 1450745053040
	1450745052848 -> 1450745052704
	1450738307856 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1450738307856 -> 1450745052848
	1450745052848 [label=AccumulateGrad]
	1450745052656 -> 1450745052608
	1450738307952 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1450738307952 -> 1450745052656
	1450745052656 [label=AccumulateGrad]
	1450745052512 -> 1450745052608
	1450738308048 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1450738308048 -> 1450745052512
	1450745052512 [label=AccumulateGrad]
	1450745052416 -> 1450745052272
	1450738308432 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1450738308432 -> 1450745052416
	1450745052416 [label=AccumulateGrad]
	1450745052224 -> 1450745052128
	1450738308528 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1450738308528 -> 1450745052224
	1450745052224 [label=AccumulateGrad]
	1450745052176 -> 1450745052128
	1450738308624 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1450738308624 -> 1450745052176
	1450745052176 [label=AccumulateGrad]
	1450745052080 -> 1450745052032
	1450745052080 [label=CudnnBatchNormBackward0]
	1450745052800 -> 1450745052080
	1450745052800 [label=ConvolutionBackward0]
	1450745052896 -> 1450745052800
	1450745052992 -> 1450745052800
	1450738307280 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1450738307280 -> 1450745052992
	1450745052992 [label=AccumulateGrad]
	1450745052368 -> 1450745052080
	1450738307376 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1450738307376 -> 1450745052368
	1450745052368 [label=AccumulateGrad]
	1450745052320 -> 1450745052080
	1450738307472 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1450738307472 -> 1450745052320
	1450745052320 [label=AccumulateGrad]
	1450745051936 -> 1450745051744
	1450738309008 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1450738309008 -> 1450745051936
	1450745051936 [label=AccumulateGrad]
	1450745051696 -> 1450745051648
	1450738309104 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1450738309104 -> 1450745051696
	1450745051696 [label=AccumulateGrad]
	1450745051552 -> 1450745051648
	1450738309200 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1450738309200 -> 1450745051552
	1450745051552 [label=AccumulateGrad]
	1450745051456 -> 1450745051312
	1450738309584 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1450738309584 -> 1450745051456
	1450745051456 [label=AccumulateGrad]
	1450745051264 -> 1450745051168
	1450738309680 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1450738309680 -> 1450745051264
	1450745051264 [label=AccumulateGrad]
	1450745051216 -> 1450745051168
	1450738309776 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1450738309776 -> 1450745051216
	1450745051216 [label=AccumulateGrad]
	1450745051120 -> 1450745051072
	1450745050880 -> 1450745050736
	1450738310736 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1450738310736 -> 1450745050880
	1450745050880 [label=AccumulateGrad]
	1450745050688 -> 1450745050640
	1450738310832 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1450738310832 -> 1450745050688
	1450745050688 [label=AccumulateGrad]
	1450745050544 -> 1450745050640
	1450738310928 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1450738310928 -> 1450745050544
	1450745050544 [label=AccumulateGrad]
	1450745050448 -> 1450745050304
	1450738311312 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1450738311312 -> 1450745050448
	1450745050448 [label=AccumulateGrad]
	1450745050256 -> 1450745050160
	1450738311408 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1450738311408 -> 1450745050256
	1450745050256 [label=AccumulateGrad]
	1450745050208 -> 1450745050160
	1450738311504 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1450738311504 -> 1450745050208
	1450745050208 [label=AccumulateGrad]
	1450745050112 -> 1450745050064
	1450745050112 [label=CudnnBatchNormBackward0]
	1450745050832 -> 1450745050112
	1450745050832 [label=ConvolutionBackward0]
	1450745050928 -> 1450745050832
	1450745050976 -> 1450745050832
	1450738310160 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1450738310160 -> 1450745050976
	1450745050976 [label=AccumulateGrad]
	1450745050400 -> 1450745050112
	1450738310256 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1450738310256 -> 1450745050400
	1450745050400 [label=AccumulateGrad]
	1450745050352 -> 1450745050112
	1450738310352 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1450738310352 -> 1450745050352
	1450745050352 [label=AccumulateGrad]
	1450745048144 -> 1450745049920
	1450738311888 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1450738311888 -> 1450745048144
	1450745048144 [label=AccumulateGrad]
	1450745049872 -> 1450745049824
	1450738311984 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1450738311984 -> 1450745049872
	1450745049872 [label=AccumulateGrad]
	1450745049728 -> 1450745049824
	1450738312080 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1450738312080 -> 1450745049728
	1450745049728 [label=AccumulateGrad]
	1450745049632 -> 1450745049488
	1450738312464 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1450738312464 -> 1450745049632
	1450745049632 [label=AccumulateGrad]
	1450745049440 -> 1450745049344
	1450738312560 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1450738312560 -> 1450745049440
	1450745049440 [label=AccumulateGrad]
	1450745049392 -> 1450745049344
	1450738312656 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1450738312656 -> 1450745049392
	1450745049392 [label=AccumulateGrad]
	1450745049296 -> 1450745049248
	1450745048960 -> 1450745048720
	1450745048960 [label=TBackward0]
	1450745049200 -> 1450745048960
	1450738313424 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1450738313424 -> 1450745049200
	1450745049200 [label=AccumulateGrad]
	1450745048720 -> 1450752774160
}
