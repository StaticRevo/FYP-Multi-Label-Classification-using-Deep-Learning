digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1920340002000 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1920332234416 [label=AddmmBackward0]
	1920332234560 -> 1920332234416
	1920325513008 [label="fc.bias
 (19)" fillcolor=lightblue]
	1920325513008 -> 1920332234560
	1920332234560 [label=AccumulateGrad]
	1920332234608 -> 1920332234416
	1920332234608 [label=ViewBackward0]
	1920332234704 -> 1920332234608
	1920332234704 [label=MeanBackward1]
	1920332233888 -> 1920332234704
	1920332233888 [label=ReluBackward0]
	1920332233744 -> 1920332233888
	1920332233744 [label=AddBackward0]
	1920318833040 -> 1920332233744
	1920318833040 [label=CudnnBatchNormBackward0]
	1920318833520 -> 1920318833040
	1920318833520 [label=ConvolutionBackward0]
	1920318833760 -> 1920318833520
	1920318833760 [label=ReluBackward0]
	1920318833904 -> 1920318833760
	1920318833904 [label=CudnnBatchNormBackward0]
	1920318834000 -> 1920318833904
	1920318834000 [label=ConvolutionBackward0]
	1920318833568 -> 1920318834000
	1920318833568 [label=ReluBackward0]
	1920318834288 -> 1920318833568
	1920318834288 [label=AddBackward0]
	1920318834384 -> 1920318834288
	1920318834384 [label=CudnnBatchNormBackward0]
	1920318834528 -> 1920318834384
	1920318834528 [label=ConvolutionBackward0]
	1920318834720 -> 1920318834528
	1920318834720 [label=ReluBackward0]
	1920318834864 -> 1920318834720
	1920318834864 [label=CudnnBatchNormBackward0]
	1920318834960 -> 1920318834864
	1920318834960 [label=ConvolutionBackward0]
	1920318835152 -> 1920318834960
	1920318835152 [label=ReluBackward0]
	1920318835296 -> 1920318835152
	1920318835296 [label=AddBackward0]
	1920318835392 -> 1920318835296
	1920318835392 [label=CudnnBatchNormBackward0]
	1920318835536 -> 1920318835392
	1920318835536 [label=ConvolutionBackward0]
	1920318835728 -> 1920318835536
	1920318835728 [label=ReluBackward0]
	1920318835872 -> 1920318835728
	1920318835872 [label=CudnnBatchNormBackward0]
	1920318835968 -> 1920318835872
	1920318835968 [label=ConvolutionBackward0]
	1920318835344 -> 1920318835968
	1920318835344 [label=ReluBackward0]
	1920318836256 -> 1920318835344
	1920318836256 [label=AddBackward0]
	1920318836352 -> 1920318836256
	1920318836352 [label=CudnnBatchNormBackward0]
	1920318836496 -> 1920318836352
	1920318836496 [label=ConvolutionBackward0]
	1920318836688 -> 1920318836496
	1920318836688 [label=ReluBackward0]
	1920318836832 -> 1920318836688
	1920318836832 [label=CudnnBatchNormBackward0]
	1920318836928 -> 1920318836832
	1920318836928 [label=ConvolutionBackward0]
	1920318837120 -> 1920318836928
	1920318837120 [label=ReluBackward0]
	1920318837264 -> 1920318837120
	1920318837264 [label=AddBackward0]
	1920318837360 -> 1920318837264
	1920318837360 [label=CudnnBatchNormBackward0]
	1920318837504 -> 1920318837360
	1920318837504 [label=ConvolutionBackward0]
	1920318837696 -> 1920318837504
	1920318837696 [label=ReluBackward0]
	1920318837840 -> 1920318837696
	1920318837840 [label=CudnnBatchNormBackward0]
	1920318837936 -> 1920318837840
	1920318837936 [label=ConvolutionBackward0]
	1920318837312 -> 1920318837936
	1920318837312 [label=ReluBackward0]
	1920318838224 -> 1920318837312
	1920318838224 [label=AddBackward0]
	1920318838320 -> 1920318838224
	1920318838320 [label=CudnnBatchNormBackward0]
	1920318838464 -> 1920318838320
	1920318838464 [label=ConvolutionBackward0]
	1920318838656 -> 1920318838464
	1920318838656 [label=ReluBackward0]
	1920318838800 -> 1920318838656
	1920318838800 [label=CudnnBatchNormBackward0]
	1920318838896 -> 1920318838800
	1920318838896 [label=ConvolutionBackward0]
	1920318839088 -> 1920318838896
	1920318839088 [label=ReluBackward0]
	1920318839232 -> 1920318839088
	1920318839232 [label=AddBackward0]
	1920318839328 -> 1920318839232
	1920318839328 [label=CudnnBatchNormBackward0]
	1920318839472 -> 1920318839328
	1920318839472 [label=ConvolutionBackward0]
	1920318839664 -> 1920318839472
	1920318839664 [label=ReluBackward0]
	1920318839808 -> 1920318839664
	1920318839808 [label=CudnnBatchNormBackward0]
	1920318839904 -> 1920318839808
	1920318839904 [label=ConvolutionBackward0]
	1920318839280 -> 1920318839904
	1920318839280 [label=ReluBackward0]
	1920318840192 -> 1920318839280
	1920318840192 [label=AddBackward0]
	1920318840288 -> 1920318840192
	1920318840288 [label=CudnnBatchNormBackward0]
	1920318840432 -> 1920318840288
	1920318840432 [label=ConvolutionBackward0]
	1920318840624 -> 1920318840432
	1920318840624 [label=ReluBackward0]
	1920318840768 -> 1920318840624
	1920318840768 [label=CudnnBatchNormBackward0]
	1920318840864 -> 1920318840768
	1920318840864 [label=ConvolutionBackward0]
	1920318840240 -> 1920318840864
	1920318840240 [label=MaxPool2DWithIndicesBackward0]
	1920318841152 -> 1920318840240
	1920318841152 [label=ReluBackward0]
	1920318841248 -> 1920318841152
	1920318841248 [label=CudnnBatchNormBackward0]
	1920318841344 -> 1920318841248
	1920318841344 [label=ConvolutionBackward0]
	1920318841536 -> 1920318841344
	1920325512912 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1920325512912 -> 1920318841536
	1920318841536 [label=AccumulateGrad]
	1920318841296 -> 1920318841248
	1920325501296 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1920325501296 -> 1920318841296
	1920318841296 [label=AccumulateGrad]
	1920318840960 -> 1920318841248
	1920325501392 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1920325501392 -> 1920318840960
	1920318840960 [label=AccumulateGrad]
	1920318841056 -> 1920318840864
	1920325501776 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1920325501776 -> 1920318841056
	1920318841056 [label=AccumulateGrad]
	1920318840816 -> 1920318840768
	1920325501872 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1920325501872 -> 1920318840816
	1920318840816 [label=AccumulateGrad]
	1920318840672 -> 1920318840768
	1920325501968 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1920325501968 -> 1920318840672
	1920318840672 [label=AccumulateGrad]
	1920318840576 -> 1920318840432
	1920325502352 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1920325502352 -> 1920318840576
	1920318840576 [label=AccumulateGrad]
	1920318840384 -> 1920318840288
	1920325502448 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1920325502448 -> 1920318840384
	1920318840384 [label=AccumulateGrad]
	1920318840336 -> 1920318840288
	1920325502544 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1920325502544 -> 1920318840336
	1920318840336 [label=AccumulateGrad]
	1920318840240 -> 1920318840192
	1920318840096 -> 1920318839904
	1920325502928 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1920325502928 -> 1920318840096
	1920318840096 [label=AccumulateGrad]
	1920318839856 -> 1920318839808
	1920325503024 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1920325503024 -> 1920318839856
	1920318839856 [label=AccumulateGrad]
	1920318839712 -> 1920318839808
	1920325503120 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1920325503120 -> 1920318839712
	1920318839712 [label=AccumulateGrad]
	1920318839616 -> 1920318839472
	1920325503504 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1920325503504 -> 1920318839616
	1920318839616 [label=AccumulateGrad]
	1920318839424 -> 1920318839328
	1920325503600 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1920325503600 -> 1920318839424
	1920318839424 [label=AccumulateGrad]
	1920318839376 -> 1920318839328
	1920325503696 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1920325503696 -> 1920318839376
	1920318839376 [label=AccumulateGrad]
	1920318839280 -> 1920318839232
	1920318839040 -> 1920318838896
	1920325504656 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1920325504656 -> 1920318839040
	1920318839040 [label=AccumulateGrad]
	1920318838848 -> 1920318838800
	1920325504752 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1920325504752 -> 1920318838848
	1920318838848 [label=AccumulateGrad]
	1920318838704 -> 1920318838800
	1920325504848 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1920325504848 -> 1920318838704
	1920318838704 [label=AccumulateGrad]
	1920318838608 -> 1920318838464
	1920325505232 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1920325505232 -> 1920318838608
	1920318838608 [label=AccumulateGrad]
	1920318838416 -> 1920318838320
	1920325505328 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1920325505328 -> 1920318838416
	1920318838416 [label=AccumulateGrad]
	1920318838368 -> 1920318838320
	1920325505424 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1920325505424 -> 1920318838368
	1920318838368 [label=AccumulateGrad]
	1920318838272 -> 1920318838224
	1920318838272 [label=CudnnBatchNormBackward0]
	1920318838992 -> 1920318838272
	1920318838992 [label=ConvolutionBackward0]
	1920318839088 -> 1920318838992
	1920318839136 -> 1920318838992
	1920325504080 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1920325504080 -> 1920318839136
	1920318839136 [label=AccumulateGrad]
	1920318838560 -> 1920318838272
	1920325504176 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1920325504176 -> 1920318838560
	1920318838560 [label=AccumulateGrad]
	1920318838512 -> 1920318838272
	1920325504272 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1920325504272 -> 1920318838512
	1920318838512 [label=AccumulateGrad]
	1920318838128 -> 1920318837936
	1920325505808 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1920325505808 -> 1920318838128
	1920318838128 [label=AccumulateGrad]
	1920318837888 -> 1920318837840
	1920325505904 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1920325505904 -> 1920318837888
	1920318837888 [label=AccumulateGrad]
	1920318837744 -> 1920318837840
	1920325506000 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1920325506000 -> 1920318837744
	1920318837744 [label=AccumulateGrad]
	1920318837648 -> 1920318837504
	1920325506384 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1920325506384 -> 1920318837648
	1920318837648 [label=AccumulateGrad]
	1920318837456 -> 1920318837360
	1920325506480 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1920325506480 -> 1920318837456
	1920318837456 [label=AccumulateGrad]
	1920318837408 -> 1920318837360
	1920325506576 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1920325506576 -> 1920318837408
	1920318837408 [label=AccumulateGrad]
	1920318837312 -> 1920318837264
	1920318837072 -> 1920318836928
	1920325507536 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1920325507536 -> 1920318837072
	1920318837072 [label=AccumulateGrad]
	1920318836880 -> 1920318836832
	1920325507632 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1920325507632 -> 1920318836880
	1920318836880 [label=AccumulateGrad]
	1920318836736 -> 1920318836832
	1920325507728 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1920325507728 -> 1920318836736
	1920318836736 [label=AccumulateGrad]
	1920318836640 -> 1920318836496
	1920325508112 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1920325508112 -> 1920318836640
	1920318836640 [label=AccumulateGrad]
	1920318836448 -> 1920318836352
	1920325508208 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1920325508208 -> 1920318836448
	1920318836448 [label=AccumulateGrad]
	1920318836400 -> 1920318836352
	1920325508304 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1920325508304 -> 1920318836400
	1920318836400 [label=AccumulateGrad]
	1920318836304 -> 1920318836256
	1920318836304 [label=CudnnBatchNormBackward0]
	1920318837024 -> 1920318836304
	1920318837024 [label=ConvolutionBackward0]
	1920318837120 -> 1920318837024
	1920318837168 -> 1920318837024
	1920325506960 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1920325506960 -> 1920318837168
	1920318837168 [label=AccumulateGrad]
	1920318836592 -> 1920318836304
	1920325507056 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1920325507056 -> 1920318836592
	1920318836592 [label=AccumulateGrad]
	1920318836544 -> 1920318836304
	1920325507152 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1920325507152 -> 1920318836544
	1920318836544 [label=AccumulateGrad]
	1920318836160 -> 1920318835968
	1920325508688 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1920325508688 -> 1920318836160
	1920318836160 [label=AccumulateGrad]
	1920318835920 -> 1920318835872
	1920325508784 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1920325508784 -> 1920318835920
	1920318835920 [label=AccumulateGrad]
	1920318835776 -> 1920318835872
	1920325508880 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1920325508880 -> 1920318835776
	1920318835776 [label=AccumulateGrad]
	1920318835680 -> 1920318835536
	1920325509264 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1920325509264 -> 1920318835680
	1920318835680 [label=AccumulateGrad]
	1920318835488 -> 1920318835392
	1920325509360 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1920325509360 -> 1920318835488
	1920318835488 [label=AccumulateGrad]
	1920318835440 -> 1920318835392
	1920325509456 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1920325509456 -> 1920318835440
	1920318835440 [label=AccumulateGrad]
	1920318835344 -> 1920318835296
	1920318835104 -> 1920318834960
	1920325510416 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1920325510416 -> 1920318835104
	1920318835104 [label=AccumulateGrad]
	1920318834912 -> 1920318834864
	1920325510512 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1920325510512 -> 1920318834912
	1920318834912 [label=AccumulateGrad]
	1920318834768 -> 1920318834864
	1920325510608 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1920325510608 -> 1920318834768
	1920318834768 [label=AccumulateGrad]
	1920318834672 -> 1920318834528
	1920325510992 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1920325510992 -> 1920318834672
	1920318834672 [label=AccumulateGrad]
	1920318834480 -> 1920318834384
	1920325511088 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1920325511088 -> 1920318834480
	1920318834480 [label=AccumulateGrad]
	1920318834432 -> 1920318834384
	1920325511184 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1920325511184 -> 1920318834432
	1920318834432 [label=AccumulateGrad]
	1920318834336 -> 1920318834288
	1920318834336 [label=CudnnBatchNormBackward0]
	1920318835056 -> 1920318834336
	1920318835056 [label=ConvolutionBackward0]
	1920318835152 -> 1920318835056
	1920318835200 -> 1920318835056
	1920325509840 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1920325509840 -> 1920318835200
	1920318835200 [label=AccumulateGrad]
	1920318834624 -> 1920318834336
	1920325509936 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1920325509936 -> 1920318834624
	1920318834624 [label=AccumulateGrad]
	1920318834576 -> 1920318834336
	1920325510032 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1920325510032 -> 1920318834576
	1920318834576 [label=AccumulateGrad]
	1920318834192 -> 1920318834000
	1920325511568 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1920325511568 -> 1920318834192
	1920318834192 [label=AccumulateGrad]
	1920318833952 -> 1920318833904
	1920325511664 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1920325511664 -> 1920318833952
	1920318833952 [label=AccumulateGrad]
	1920318833808 -> 1920318833904
	1920325511760 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1920325511760 -> 1920318833808
	1920318833808 [label=AccumulateGrad]
	1920318833712 -> 1920318833520
	1920325512144 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1920325512144 -> 1920318833712
	1920318833712 [label=AccumulateGrad]
	1920318833472 -> 1920318833040
	1920325512240 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1920325512240 -> 1920318833472
	1920318833472 [label=AccumulateGrad]
	1920318833328 -> 1920318833040
	1920325512336 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1920325512336 -> 1920318833328
	1920318833328 [label=AccumulateGrad]
	1920318833568 -> 1920332233744
	1920332234656 -> 1920332234416
	1920332234656 [label=TBackward0]
	1920332233840 -> 1920332234656
	1920325513104 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1920325513104 -> 1920332233840
	1920332233840 [label=AccumulateGrad]
	1920332234416 -> 1920340002000
}
