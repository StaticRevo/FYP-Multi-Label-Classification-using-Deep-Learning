digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1408461930576 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1408457837872 [label=SigmoidBackward0]
	1408457845216 -> 1408457837872
	1408457845216 [label=AddmmBackward0]
	1408457838256 -> 1408457845216
	1408461757264 [label="model.fc.bias
 (19)" fillcolor=lightblue]
	1408461757264 -> 1408457838256
	1408457838256 [label=AccumulateGrad]
	1408457837392 -> 1408457845216
	1408457837392 [label=ViewBackward0]
	1408457841712 -> 1408457837392
	1408457841712 [label=MeanBackward1]
	1408457841088 -> 1408457841712
	1408457841088 [label=ReluBackward0]
	1408457841808 -> 1408457841088
	1408457841808 [label=AddBackward0]
	1408457839312 -> 1408457841808
	1408457839312 [label=CudnnBatchNormBackward0]
	1408457841328 -> 1408457839312
	1408457841328 [label=ConvolutionBackward0]
	1408457842528 -> 1408457841328
	1408457842528 [label=ReluBackward0]
	1408457842912 -> 1408457842528
	1408457842912 [label=CudnnBatchNormBackward0]
	1408457849248 -> 1408457842912
	1408457849248 [label=ConvolutionBackward0]
	1408457836576 -> 1408457849248
	1408457836576 [label=ReluBackward0]
	1408457845552 -> 1408457836576
	1408457845552 [label=AddBackward0]
	1408457840272 -> 1408457845552
	1408457840272 [label=CudnnBatchNormBackward0]
	1408457845360 -> 1408457840272
	1408457845360 [label=ConvolutionBackward0]
	1408457850256 -> 1408457845360
	1408457850256 [label=ReluBackward0]
	1408457850784 -> 1408457850256
	1408457850784 [label=CudnnBatchNormBackward0]
	1408457848240 -> 1408457850784
	1408457848240 [label=ConvolutionBackward0]
	1408457848288 -> 1408457848240
	1408457848288 [label=ReluBackward0]
	1408457845936 -> 1408457848288
	1408457845936 [label=AddBackward0]
	1408457838304 -> 1408457845936
	1408457838304 [label=CudnnBatchNormBackward0]
	1408457842864 -> 1408457838304
	1408457842864 [label=ConvolutionBackward0]
	1408457849152 -> 1408457842864
	1408457849152 [label=ReluBackward0]
	1408457839120 -> 1408457849152
	1408457839120 [label=CudnnBatchNormBackward0]
	1408457839456 -> 1408457839120
	1408457839456 [label=ConvolutionBackward0]
	1408457844208 -> 1408457839456
	1408457844208 [label=ReluBackward0]
	1408457848720 -> 1408457844208
	1408457848720 [label=AddBackward0]
	1408457845312 -> 1408457848720
	1408457845312 [label=CudnnBatchNormBackward0]
	1408457845600 -> 1408457845312
	1408457845600 [label=ConvolutionBackward0]
	1408457844736 -> 1408457845600
	1408457844736 [label=ReluBackward0]
	1408457842576 -> 1408457844736
	1408457842576 [label=CudnnBatchNormBackward0]
	1408457841424 -> 1408457842576
	1408457841424 [label=ConvolutionBackward0]
	1408457841520 -> 1408457841424
	1408457841520 [label=ReluBackward0]
	1408457846848 -> 1408457841520
	1408457846848 [label=AddBackward0]
	1408457846224 -> 1408457846848
	1408457846224 [label=CudnnBatchNormBackward0]
	1408457850736 -> 1408457846224
	1408457850736 [label=ConvolutionBackward0]
	1408457848960 -> 1408457850736
	1408457848960 [label=ReluBackward0]
	1408457845744 -> 1408457848960
	1408457845744 [label=CudnnBatchNormBackward0]
	1408457843488 -> 1408457845744
	1408457843488 [label=ConvolutionBackward0]
	1408457848864 -> 1408457843488
	1408457848864 [label=ReluBackward0]
	1408457849536 -> 1408457848864
	1408457849536 [label=AddBackward0]
	1408457847952 -> 1408457849536
	1408457847952 [label=CudnnBatchNormBackward0]
	1408457847280 -> 1408457847952
	1408457847280 [label=ConvolutionBackward0]
	1408458810528 -> 1408457847280
	1408458810528 [label=ReluBackward0]
	1408458812112 -> 1408458810528
	1408458812112 [label=CudnnBatchNormBackward0]
	1408458806448 -> 1408458812112
	1408458806448 [label=ConvolutionBackward0]
	1408458813936 -> 1408458806448
	1408458813936 [label=ReluBackward0]
	1408458813792 -> 1408458813936
	1408458813792 [label=AddBackward0]
	1408458813696 -> 1408458813792
	1408458813696 [label=CudnnBatchNormBackward0]
	1408458813552 -> 1408458813696
	1408458813552 [label=ConvolutionBackward0]
	1408458813360 -> 1408458813552
	1408458813360 [label=ReluBackward0]
	1408458814128 -> 1408458813360
	1408458814128 [label=CudnnBatchNormBackward0]
	1408458814224 -> 1408458814128
	1408458814224 [label=ConvolutionBackward0]
	1408458813744 -> 1408458814224
	1408458813744 [label=ReluBackward0]
	1408458814512 -> 1408458813744
	1408458814512 [label=AddBackward0]
	1408458814608 -> 1408458814512
	1408458814608 [label=CudnnBatchNormBackward0]
	1408458814752 -> 1408458814608
	1408458814752 [label=ConvolutionBackward0]
	1408458814944 -> 1408458814752
	1408458814944 [label=ReluBackward0]
	1408458815088 -> 1408458814944
	1408458815088 [label=CudnnBatchNormBackward0]
	1408458815184 -> 1408458815088
	1408458815184 [label=ConvolutionBackward0]
	1408458814560 -> 1408458815184
	1408458814560 [label=MaxPool2DWithIndicesBackward0]
	1408458802224 -> 1408458814560
	1408458802224 [label=ReluBackward0]
	1408458805344 -> 1408458802224
	1408458805344 [label=CudnnBatchNormBackward0]
	1408458808944 -> 1408458805344
	1408458808944 [label=ConvolutionBackward0]
	1408458815376 -> 1408458808944
	1408461079952 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1408461079952 -> 1408458815376
	1408458815376 [label=AccumulateGrad]
	1408458803184 -> 1408458805344
	1408461580880 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	1408461580880 -> 1408458803184
	1408458803184 [label=AccumulateGrad]
	1408458815520 -> 1408458805344
	1408461581648 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	1408461581648 -> 1408458815520
	1408458815520 [label=AccumulateGrad]
	1408458811584 -> 1408458815184
	1408461581552 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1408461581552 -> 1408458811584
	1408458811584 [label=AccumulateGrad]
	1408458815136 -> 1408458815088
	1408461582032 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1408461582032 -> 1408458815136
	1408458815136 [label=AccumulateGrad]
	1408458814992 -> 1408458815088
	1408461582320 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1408461582320 -> 1408458814992
	1408458814992 [label=AccumulateGrad]
	1408458814896 -> 1408458814752
	1408461582704 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1408461582704 -> 1408458814896
	1408458814896 [label=AccumulateGrad]
	1408458814704 -> 1408458814608
	1408461582800 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1408461582800 -> 1408458814704
	1408458814704 [label=AccumulateGrad]
	1408458814656 -> 1408458814608
	1408461582896 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1408461582896 -> 1408458814656
	1408458814656 [label=AccumulateGrad]
	1408458814560 -> 1408458814512
	1408458814416 -> 1408458814224
	1408461583280 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1408461583280 -> 1408458814416
	1408458814416 [label=AccumulateGrad]
	1408458814176 -> 1408458814128
	1408461583376 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1408461583376 -> 1408458814176
	1408458814176 [label=AccumulateGrad]
	1408458813312 -> 1408458814128
	1408461583472 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1408461583472 -> 1408458813312
	1408458813312 [label=AccumulateGrad]
	1408458813408 -> 1408458813552
	1408461583856 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1408461583856 -> 1408458813408
	1408458813408 [label=AccumulateGrad]
	1408458813600 -> 1408458813696
	1408461583952 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1408461583952 -> 1408458813600
	1408458813600 [label=AccumulateGrad]
	1408458813648 -> 1408458813696
	1408461584048 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1408461584048 -> 1408458813648
	1408458813648 [label=AccumulateGrad]
	1408458813744 -> 1408458813792
	1408458813984 -> 1408458806448
	1408461585008 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1408461585008 -> 1408458813984
	1408458813984 [label=AccumulateGrad]
	1408458810336 -> 1408458812112
	1408461585104 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1408461585104 -> 1408458810336
	1408458810336 [label=AccumulateGrad]
	1408458803088 -> 1408458812112
	1408461585200 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1408461585200 -> 1408458803088
	1408458803088 [label=AccumulateGrad]
	1408458803808 -> 1408457847280
	1408461585584 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1408461585584 -> 1408458803808
	1408458803808 [label=AccumulateGrad]
	1408457842240 -> 1408457847952
	1408461585680 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1408461585680 -> 1408457842240
	1408457842240 [label=AccumulateGrad]
	1408458806160 -> 1408457847952
	1408461585776 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1408461585776 -> 1408458806160
	1408458806160 [label=AccumulateGrad]
	1408457841616 -> 1408457849536
	1408457841616 [label=CudnnBatchNormBackward0]
	1408458814032 -> 1408457841616
	1408458814032 [label=ConvolutionBackward0]
	1408458813936 -> 1408458814032
	1408458813888 -> 1408458814032
	1408461584432 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1408461584432 -> 1408458813888
	1408458813888 [label=AccumulateGrad]
	1408458809232 -> 1408457841616
	1408461584528 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1408461584528 -> 1408458809232
	1408458809232 [label=AccumulateGrad]
	1408458811104 -> 1408457841616
	1408461584624 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1408461584624 -> 1408458811104
	1408458811104 [label=AccumulateGrad]
	1408457840176 -> 1408457843488
	1408461586160 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1408461586160 -> 1408457840176
	1408457840176 [label=AccumulateGrad]
	1408457845072 -> 1408457845744
	1408461586256 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1408461586256 -> 1408457845072
	1408457845072 [label=AccumulateGrad]
	1408457849872 -> 1408457845744
	1408461586352 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1408461586352 -> 1408457849872
	1408457849872 [label=AccumulateGrad]
	1408457850640 -> 1408457850736
	1408461750640 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1408461750640 -> 1408457850640
	1408457850640 [label=AccumulateGrad]
	1408457849680 -> 1408457846224
	1408461750736 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1408461750736 -> 1408457849680
	1408457849680 [label=AccumulateGrad]
	1408457850592 -> 1408457846224
	1408461750832 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1408461750832 -> 1408457850592
	1408457850592 [label=AccumulateGrad]
	1408457848864 -> 1408457846848
	1408457837008 -> 1408457841424
	1408461751792 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1408461751792 -> 1408457837008
	1408457837008 [label=AccumulateGrad]
	1408457840992 -> 1408457842576
	1408461751888 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1408461751888 -> 1408457840992
	1408457840992 [label=AccumulateGrad]
	1408457844112 -> 1408457842576
	1408461751984 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1408461751984 -> 1408457844112
	1408457844112 [label=AccumulateGrad]
	1408457849008 -> 1408457845600
	1408461752368 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1408461752368 -> 1408457849008
	1408457849008 [label=AccumulateGrad]
	1408457844016 -> 1408457845312
	1408461752464 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1408461752464 -> 1408457844016
	1408457844016 [label=AccumulateGrad]
	1408457846128 -> 1408457845312
	1408461752560 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1408461752560 -> 1408457846128
	1408457846128 [label=AccumulateGrad]
	1408457847376 -> 1408457848720
	1408457847376 [label=CudnnBatchNormBackward0]
	1408457849200 -> 1408457847376
	1408457849200 [label=ConvolutionBackward0]
	1408457841520 -> 1408457849200
	1408457847808 -> 1408457849200
	1408461751216 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1408461751216 -> 1408457847808
	1408457847808 [label=AccumulateGrad]
	1408457846704 -> 1408457847376
	1408461751312 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1408461751312 -> 1408457846704
	1408457846704 [label=AccumulateGrad]
	1408457839168 -> 1408457847376
	1408461751408 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1408461751408 -> 1408457839168
	1408457839168 [label=AccumulateGrad]
	1408457840128 -> 1408457839456
	1408461752944 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1408461752944 -> 1408457840128
	1408457840128 [label=AccumulateGrad]
	1408457841904 -> 1408457839120
	1408461753040 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1408461753040 -> 1408457841904
	1408457841904 [label=AccumulateGrad]
	1408457845792 -> 1408457839120
	1408461753136 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1408461753136 -> 1408457845792
	1408457845792 [label=AccumulateGrad]
	1408457840752 -> 1408457842864
	1408461753520 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1408461753520 -> 1408457840752
	1408457840752 [label=AccumulateGrad]
	1408457844976 -> 1408457838304
	1408461753616 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1408461753616 -> 1408457844976
	1408457844976 [label=AccumulateGrad]
	1408457850208 -> 1408457838304
	1408461753712 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1408461753712 -> 1408457850208
	1408457850208 [label=AccumulateGrad]
	1408457844208 -> 1408457845936
	1408457838976 -> 1408457848240
	1408461754672 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1408461754672 -> 1408457838976
	1408457838976 [label=AccumulateGrad]
	1408457846416 -> 1408457850784
	1408461754768 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1408461754768 -> 1408457846416
	1408457846416 [label=AccumulateGrad]
	1408457843440 -> 1408457850784
	1408461754864 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1408461754864 -> 1408457843440
	1408457843440 [label=AccumulateGrad]
	1408457849824 -> 1408457845360
	1408461755248 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1408461755248 -> 1408457849824
	1408457849824 [label=AccumulateGrad]
	1408457844784 -> 1408457840272
	1408461755344 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1408461755344 -> 1408457844784
	1408457844784 [label=AccumulateGrad]
	1408457846368 -> 1408457840272
	1408461755440 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1408461755440 -> 1408457846368
	1408457846368 [label=AccumulateGrad]
	1408457847568 -> 1408457845552
	1408457847568 [label=CudnnBatchNormBackward0]
	1408457850544 -> 1408457847568
	1408457850544 [label=ConvolutionBackward0]
	1408457848288 -> 1408457850544
	1408457841280 -> 1408457850544
	1408461754096 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1408461754096 -> 1408457841280
	1408457841280 [label=AccumulateGrad]
	1408457838832 -> 1408457847568
	1408461754192 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1408461754192 -> 1408457838832
	1408457838832 [label=AccumulateGrad]
	1408457849728 -> 1408457847568
	1408461754288 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1408461754288 -> 1408457849728
	1408457849728 [label=AccumulateGrad]
	1408457848528 -> 1408457849248
	1408461755824 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1408461755824 -> 1408457848528
	1408457848528 [label=AccumulateGrad]
	1408457847664 -> 1408457842912
	1408461755920 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1408461755920 -> 1408457847664
	1408457847664 [label=AccumulateGrad]
	1408457848048 -> 1408457842912
	1408461756016 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1408461756016 -> 1408457848048
	1408457848048 [label=AccumulateGrad]
	1408457836672 -> 1408457841328
	1408461756400 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1408461756400 -> 1408457836672
	1408457836672 [label=AccumulateGrad]
	1408457844160 -> 1408457839312
	1408461756496 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1408461756496 -> 1408457844160
	1408457844160 [label=AccumulateGrad]
	1408457844448 -> 1408457839312
	1408461756592 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1408461756592 -> 1408457844448
	1408457844448 [label=AccumulateGrad]
	1408457836576 -> 1408457841808
	1408457837728 -> 1408457845216
	1408457837728 [label=TBackward0]
	1408457840608 -> 1408457837728
	1408461757168 [label="model.fc.weight
 (19, 512)" fillcolor=lightblue]
	1408461757168 -> 1408457840608
	1408457840608 [label=AccumulateGrad]
	1408457837872 -> 1408461930576
}
