digraph {
	graph [size="86.7,86.7"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1688802740080 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1689025418848 [label=AddmmBackward0]
	1689025423360 -> 1689025418848
	1688758316656 [label="29.bias
 (19)" fillcolor=lightblue]
	1688758316656 -> 1689025423360
	1689025423360 [label=AccumulateGrad]
	1689025419424 -> 1689025418848
	1689025419424 [label=NativeDropoutBackward0]
	1689025419712 -> 1689025419424
	1689025419712 [label=ViewBackward0]
	1689025425616 -> 1689025419712
	1689025425616 [label=MeanBackward1]
	1689025417168 -> 1689025425616
	1689025417168 [label=MulBackward0]
	1689025414864 -> 1689025417168
	1689025414864 [label=MulBackward0]
	1689025416256 -> 1689025414864
	1689025416256 [label=ReluBackward0]
	1689025417888 -> 1689025416256
	1689025417888 [label=AddBackward0]
	1689025428352 -> 1689025417888
	1689025428352 [label=CudnnBatchNormBackward0]
	1689025414960 -> 1689025428352
	1689025414960 [label=ConvolutionBackward0]
	1689025417600 -> 1689025414960
	1689025417600 [label=ReluBackward0]
	1689025425568 -> 1689025417600
	1689025425568 [label=CudnnBatchNormBackward0]
	1689025427344 -> 1689025425568
	1689025427344 [label=ConvolutionBackward0]
	1689025425472 -> 1689025427344
	1689025425472 [label=ReluBackward0]
	1689025420432 -> 1689025425472
	1689025420432 [label=CudnnBatchNormBackward0]
	1689025416544 -> 1689025420432
	1689025416544 [label=ConvolutionBackward0]
	1689025418416 -> 1689025416544
	1689025418416 [label=ConvolutionBackward0]
	1689025413472 -> 1689025418416
	1689025413472 [label=MulBackward0]
	1689025423168 -> 1689025413472
	1689025423168 [label=ReluBackward0]
	1689025420912 -> 1689025423168
	1689025420912 [label=AddBackward0]
	1689025422208 -> 1689025420912
	1689025422208 [label=CudnnBatchNormBackward0]
	1689025426096 -> 1689025422208
	1689025426096 [label=ConvolutionBackward0]
	1689025419184 -> 1689025426096
	1689025419184 [label=ReluBackward0]
	1689025428208 -> 1689025419184
	1689025428208 [label=CudnnBatchNormBackward0]
	1689025416640 -> 1689025428208
	1689025416640 [label=ConvolutionBackward0]
	1689025417840 -> 1689025416640
	1689025417840 [label=ConvolutionBackward0]
	1689025427968 -> 1689025417840
	1689025427968 [label=CatBackward0]
	1689025421824 -> 1689025427968
	1689025421824 [label=ConvolutionBackward0]
	1689025429360 -> 1689025421824
	1689025429360 [label=ReluBackward0]
	1689025424464 -> 1689025429360
	1689025424464 [label=CudnnBatchNormBackward0]
	1689025427440 -> 1689025424464
	1689025427440 [label=ConvolutionBackward0]
	1689025413664 -> 1689025427440
	1689025413664 [label=ConvolutionBackward0]
	1689025423024 -> 1689025413664
	1689025423024 [label=MulBackward0]
	1689025420384 -> 1689025423024
	1689025420384 [label=ReluBackward0]
	1689025421248 -> 1689025420384
	1689025421248 [label=AddBackward0]
	1689025422928 -> 1689025421248
	1689025422928 [label=CudnnBatchNormBackward0]
	1689025421536 -> 1689025422928
	1689025421536 [label=ConvolutionBackward0]
	1689025428832 -> 1689025421536
	1689025428832 [label=ReluBackward0]
	1689025422592 -> 1689025428832
	1689025422592 [label=CudnnBatchNormBackward0]
	1689025426144 -> 1689025422592
	1689025426144 [label=ConvolutionBackward0]
	1689025418368 -> 1689025426144
	1689025418368 [label=ConvolutionBackward0]
	1689025414768 -> 1689025418368
	1689025414768 [label=CatBackward0]
	1689025419616 -> 1689025414768
	1689025419616 [label=ConvolutionBackward0]
	1689025414672 -> 1689025419616
	1689025414672 [label=ReluBackward0]
	1689025416352 -> 1689025414672
	1689025416352 [label=CudnnBatchNormBackward0]
	1689025419328 -> 1689025416352
	1689025419328 [label=ConvolutionBackward0]
	1689025417504 -> 1689025419328
	1689025417504 [label=ConvolutionBackward0]
	1689025428160 -> 1689025417504
	1689025428160 [label=MulBackward0]
	1689025424080 -> 1689025428160
	1689025424080 [label=MulBackward0]
	1689025424992 -> 1689025424080
	1689025424992 [label=MulBackward0]
	1689025418176 -> 1689025424992
	1689025418176 [label=ReluBackward0]
	1689025413904 -> 1689025418176
	1689025413904 [label=AddBackward0]
	1689025427296 -> 1689025413904
	1689025427296 [label=CudnnBatchNormBackward0]
	1689025420816 -> 1689025427296
	1689025420816 [label=ConvolutionBackward0]
	1689025419664 -> 1689025420816
	1689025419664 [label=ReluBackward0]
	1689025425040 -> 1689025419664
	1689025425040 [label=CudnnBatchNormBackward0]
	1689025414000 -> 1689025425040
	1689025414000 [label=ConvolutionBackward0]
	1689025421728 -> 1689025414000
	1689025421728 [label=ReluBackward0]
	1689025428112 -> 1689025421728
	1689025428112 [label=CudnnBatchNormBackward0]
	1689025417792 -> 1689025428112
	1689025417792 [label=ConvolutionBackward0]
	1689025428736 -> 1689025417792
	1689025428736 [label=ConvolutionBackward0]
	1689025420720 -> 1689025428736
	1689025420720 [label=ReluBackward0]
	1689025424416 -> 1689025420720
	1689025424416 [label=CudnnBatchNormBackward0]
	1689025428976 -> 1689025424416
	1689025428976 [label=ConvolutionBackward0]
	1689025418224 -> 1689025428976
	1688712169136 [label="0.weight
 (16, 12, 1, 1)" fillcolor=lightblue]
	1688712169136 -> 1689025418224
	1689025418224 [label=AccumulateGrad]
	1689025422880 -> 1689025424416
	1688712169040 [label="1.weight
 (16)" fillcolor=lightblue]
	1688712169040 -> 1689025422880
	1689025422880 [label=AccumulateGrad]
	1689025419904 -> 1689025424416
	1688712168272 [label="1.bias
 (16)" fillcolor=lightblue]
	1688712168272 -> 1689025419904
	1689025419904 [label=AccumulateGrad]
	1689025413328 -> 1689025428736
	1688712168368 [label="3.depthwise.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	1688712168368 -> 1689025413328
	1689025413328 [label=AccumulateGrad]
	1689025417024 -> 1689025417792
	1688712168464 [label="3.pointwise.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	1688712168464 -> 1689025417024
	1689025417024 [label=AccumulateGrad]
	1689025427104 -> 1689025428112
	1688712168560 [label="4.weight
 (32)" fillcolor=lightblue]
	1688712168560 -> 1689025427104
	1689025427104 [label=AccumulateGrad]
	1689025423840 -> 1689025428112
	1688712168656 [label="4.bias
 (32)" fillcolor=lightblue]
	1688712168656 -> 1689025423840
	1689025423840 [label=AccumulateGrad]
	1689025415440 -> 1689025414000
	1688712167888 [label="6.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1688712167888 -> 1689025415440
	1689025415440 [label=AccumulateGrad]
	1689025421488 -> 1689025425040
	1688712167792 [label="6.bn1.weight
 (32)" fillcolor=lightblue]
	1688712167792 -> 1689025421488
	1689025421488 [label=AccumulateGrad]
	1689025415488 -> 1689025425040
	1688712167504 [label="6.bn1.bias
 (32)" fillcolor=lightblue]
	1688712167504 -> 1689025415488
	1689025415488 [label=AccumulateGrad]
	1689025417744 -> 1689025420816
	1688719015184 [label="6.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1688719015184 -> 1689025417744
	1689025417744 [label=AccumulateGrad]
	1689025422640 -> 1689025427296
	1688719015280 [label="6.bn2.weight
 (32)" fillcolor=lightblue]
	1688719015280 -> 1689025422640
	1689025422640 [label=AccumulateGrad]
	1689025426192 -> 1689025427296
	1688719015376 [label="6.bn2.bias
 (32)" fillcolor=lightblue]
	1688719015376 -> 1689025426192
	1689025426192 [label=AccumulateGrad]
	1689025421728 -> 1689025413904
	1689025429408 -> 1689025424992
	1689025429408 [label=ViewBackward0]
	1689025414288 -> 1689025429408
	1689025414288 [label=SigmoidBackward0]
	1689025424176 -> 1689025414288
	1689025424176 [label=AddmmBackward0]
	1689025423744 -> 1689025424176
	1688719014224 [label="7.fc2.bias
 (32)" fillcolor=lightblue]
	1688719014224 -> 1689025423744
	1689025423744 [label=AccumulateGrad]
	1689025429312 -> 1689025424176
	1689025429312 [label=ReluBackward0]
	1689025416400 -> 1689025429312
	1689025416400 [label=AddmmBackward0]
	1689025418128 -> 1689025416400
	1688719015088 [label="7.fc1.bias
 (2)" fillcolor=lightblue]
	1688719015088 -> 1689025418128
	1689025418128 [label=AccumulateGrad]
	1689025417264 -> 1689025416400
	1689025417264 [label=MeanBackward1]
	1689025426240 -> 1689025417264
	1689025426240 [label=ViewBackward0]
	1689025418176 -> 1689025426240
	1689025426000 -> 1689025416400
	1689025426000 [label=TBackward0]
	1689025415728 -> 1689025426000
	1688719015856 [label="7.fc1.weight
 (2, 32)" fillcolor=lightblue]
	1688719015856 -> 1689025415728
	1689025415728 [label=AccumulateGrad]
	1689025425856 -> 1689025424176
	1689025425856 [label=TBackward0]
	1689025413808 -> 1689025425856
	1688719014992 [label="7.fc2.weight
 (32, 2)" fillcolor=lightblue]
	1688719014992 -> 1689025413808
	1689025413808 [label=AccumulateGrad]
	1689025417648 -> 1689025424080
	1689025417648 [label=SigmoidBackward0]
	1689025415968 -> 1689025417648
	1689025415968 [label=ConvolutionBackward0]
	1689025415008 -> 1689025415968
	1689025415008 [label=SplitWithSizesBackward0]
	1689025415776 -> 1689025415008
	1689025415776 [label=ReluBackward0]
	1689025420576 -> 1689025415776
	1689025420576 [label=CudnnBatchNormBackward0]
	1689025419088 -> 1689025420576
	1689025419088 [label=ConvolutionBackward0]
	1689025420048 -> 1689025419088
	1689025420048 [label=CatBackward0]
	1689025428304 -> 1689025420048
	1689025428304 [label=AdaptiveAvgPool2DBackward0]
	1689025424992 -> 1689025428304
	1689025423408 -> 1689025420048
	1689025423408 [label=PermuteBackward0]
	1689025427536 -> 1689025423408
	1689025427536 [label=AdaptiveAvgPool2DBackward0]
	1689025424992 -> 1689025427536
	1689025413376 -> 1689025419088
	1688719013840 [label="8.conv1.weight
 (8, 32, 1, 1)" fillcolor=lightblue]
	1688719013840 -> 1689025413376
	1689025413376 [label=AccumulateGrad]
	1689025424224 -> 1689025420576
	1688719014704 [label="8.bn1.weight
 (8)" fillcolor=lightblue]
	1688719014704 -> 1689025424224
	1689025424224 [label=AccumulateGrad]
	1689025427392 -> 1689025420576
	1688719012304 [label="8.bn1.bias
 (8)" fillcolor=lightblue]
	1688719012304 -> 1689025427392
	1689025427392 [label=AccumulateGrad]
	1689025414096 -> 1689025415968
	1688719014512 [label="8.conv_h.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1688719014512 -> 1689025414096
	1689025414096 [label=AccumulateGrad]
	1689025420336 -> 1689025428160
	1689025420336 [label=PermuteBackward0]
	1689025424368 -> 1689025420336
	1689025424368 [label=SigmoidBackward0]
	1689025425280 -> 1689025424368
	1689025425280 [label=ConvolutionBackward0]
	1689025415008 -> 1689025425280
	1689025414720 -> 1689025425280
	1688719014608 [label="8.conv_w.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1688719014608 -> 1689025414720
	1689025414720 [label=AccumulateGrad]
	1689025429264 -> 1689025417504
	1688719014896 [label="9.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	1688719014896 -> 1689025429264
	1689025429264 [label=AccumulateGrad]
	1689025413952 -> 1689025419328
	1688719014800 [label="9.pointwise.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	1688719014800 -> 1689025413952
	1689025413952 [label=AccumulateGrad]
	1689025428784 -> 1689025416352
	1688758308784 [label="10.weight
 (64)" fillcolor=lightblue]
	1688758308784 -> 1689025428784
	1689025428784 [label=AccumulateGrad]
	1689025413424 -> 1689025416352
	1688758308880 [label="10.bias
 (64)" fillcolor=lightblue]
	1688758308880 -> 1689025413424
	1689025413424 [label=AccumulateGrad]
	1689025416688 -> 1689025419616
	1688758309264 [label="12.conv_dil1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1688758309264 -> 1689025416688
	1689025416688 [label=AccumulateGrad]
	1689025418656 -> 1689025419616
	1688758309360 [label="12.conv_dil1.bias
 (64)" fillcolor=lightblue]
	1688758309360 -> 1689025418656
	1689025418656 [label=AccumulateGrad]
	1689025424560 -> 1689025414768
	1689025424560 [label=ConvolutionBackward0]
	1689025414672 -> 1689025424560
	1689025416448 -> 1689025424560
	1688758309456 [label="12.conv_dil2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1688758309456 -> 1689025416448
	1689025416448 [label=AccumulateGrad]
	1689025419136 -> 1689025424560
	1688758309552 [label="12.conv_dil2.bias
 (64)" fillcolor=lightblue]
	1688758309552 -> 1689025419136
	1689025419136 [label=AccumulateGrad]
	1689025422160 -> 1689025414768
	1689025422160 [label=ConvolutionBackward0]
	1689025414672 -> 1689025422160
	1689025421776 -> 1689025422160
	1688758309648 [label="12.conv_dil3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1688758309648 -> 1689025421776
	1689025421776 [label=AccumulateGrad]
	1689025418080 -> 1689025422160
	1688758309744 [label="12.conv_dil3.bias
 (64)" fillcolor=lightblue]
	1688758309744 -> 1689025418080
	1689025418080 [label=AccumulateGrad]
	1689025416496 -> 1689025418368
	1688758309840 [label="12.fuse.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	1688758309840 -> 1689025416496
	1689025416496 [label=AccumulateGrad]
	1689025421680 -> 1689025418368
	1688758309936 [label="12.fuse.bias
 (64)" fillcolor=lightblue]
	1688758309936 -> 1689025421680
	1689025421680 [label=AccumulateGrad]
	1689025422400 -> 1689025426144
	1688758310032 [label="13.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1688758310032 -> 1689025422400
	1689025422400 [label=AccumulateGrad]
	1689025413760 -> 1689025422592
	1688758310128 [label="13.bn1.weight
 (64)" fillcolor=lightblue]
	1688758310128 -> 1689025413760
	1689025413760 [label=AccumulateGrad]
	1689025415584 -> 1689025422592
	1688758310224 [label="13.bn1.bias
 (64)" fillcolor=lightblue]
	1688758310224 -> 1689025415584
	1689025415584 [label=AccumulateGrad]
	1689025414240 -> 1689025421536
	1688758310608 [label="13.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1688758310608 -> 1689025414240
	1689025414240 [label=AccumulateGrad]
	1689025419280 -> 1689025422928
	1688758310704 [label="13.bn2.weight
 (64)" fillcolor=lightblue]
	1688758310704 -> 1689025419280
	1689025419280 [label=AccumulateGrad]
	1689025424656 -> 1689025422928
	1688758310800 [label="13.bn2.bias
 (64)" fillcolor=lightblue]
	1688758310800 -> 1689025424656
	1689025424656 [label=AccumulateGrad]
	1689025418368 -> 1689025421248
	1689025429072 -> 1689025423024
	1689025429072 [label=UnsqueezeBackward0]
	1689025427776 -> 1689025429072
	1689025427776 [label=UnsqueezeBackward0]
	1689025428064 -> 1689025427776
	1689025428064 [label=SigmoidBackward0]
	1689025424512 -> 1689025428064
	1689025424512 [label=SqueezeBackward1]
	1689025426480 -> 1689025424512
	1689025426480 [label=ConvolutionBackward0]
	1689025427584 -> 1689025426480
	1689025427584 [label=UnsqueezeBackward0]
	1689025427680 -> 1689025427584
	1689025427680 [label=SqueezeBackward1]
	1689025426816 -> 1689025427680
	1689025426816 [label=SqueezeBackward1]
	1689025420000 -> 1689025426816
	1689025420000 [label=MeanBackward1]
	1689025420384 -> 1689025420000
	1689025415104 -> 1689025426480
	1688758311184 [label="14.conv.weight
 (1, 1, 3)" fillcolor=lightblue]
	1688758311184 -> 1689025415104
	1689025415104 [label=AccumulateGrad]
	1689025428016 -> 1689025413664
	1688758311280 [label="15.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	1688758311280 -> 1689025428016
	1689025428016 [label=AccumulateGrad]
	1689025414912 -> 1689025427440
	1688758311376 [label="15.pointwise.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1688758311376 -> 1689025414912
	1689025414912 [label=AccumulateGrad]
	1689025422112 -> 1689025424464
	1688758311472 [label="16.weight
 (128)" fillcolor=lightblue]
	1688758311472 -> 1689025422112
	1689025422112 [label=AccumulateGrad]
	1689025424128 -> 1689025424464
	1688758311568 [label="16.bias
 (128)" fillcolor=lightblue]
	1688758311568 -> 1689025424128
	1689025424128 [label=AccumulateGrad]
	1689025419232 -> 1689025421824
	1688758311952 [label="18.conv_dil1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1688758311952 -> 1689025419232
	1689025419232 [label=AccumulateGrad]
	1689025416928 -> 1689025421824
	1688758312048 [label="18.conv_dil1.bias
 (128)" fillcolor=lightblue]
	1688758312048 -> 1689025416928
	1689025416928 [label=AccumulateGrad]
	1689025417312 -> 1689025427968
	1689025417312 [label=ConvolutionBackward0]
	1689025429360 -> 1689025417312
	1689025423264 -> 1689025417312
	1688758312144 [label="18.conv_dil2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1688758312144 -> 1689025423264
	1689025423264 [label=AccumulateGrad]
	1689025416112 -> 1689025417312
	1688758312240 [label="18.conv_dil2.bias
 (128)" fillcolor=lightblue]
	1688758312240 -> 1689025416112
	1689025416112 [label=AccumulateGrad]
	1689025425712 -> 1689025427968
	1689025425712 [label=ConvolutionBackward0]
	1689025429360 -> 1689025425712
	1689025416064 -> 1689025425712
	1688758312336 [label="18.conv_dil3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1688758312336 -> 1689025416064
	1689025416064 [label=AccumulateGrad]
	1689025417456 -> 1689025425712
	1688758312432 [label="18.conv_dil3.bias
 (128)" fillcolor=lightblue]
	1688758312432 -> 1689025417456
	1689025417456 [label=AccumulateGrad]
	1689025425088 -> 1689025417840
	1688758312528 [label="18.fuse.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	1688758312528 -> 1689025425088
	1689025425088 [label=AccumulateGrad]
	1689025426720 -> 1689025417840
	1688758312624 [label="18.fuse.bias
 (128)" fillcolor=lightblue]
	1688758312624 -> 1689025426720
	1689025426720 [label=AccumulateGrad]
	1689025422832 -> 1689025416640
	1688758312720 [label="19.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1688758312720 -> 1689025422832
	1689025422832 [label=AccumulateGrad]
	1689025426048 -> 1689025428208
	1688758312816 [label="19.bn1.weight
 (128)" fillcolor=lightblue]
	1688758312816 -> 1689025426048
	1689025426048 [label=AccumulateGrad]
	1689025416976 -> 1689025428208
	1688758312912 [label="19.bn1.bias
 (128)" fillcolor=lightblue]
	1688758312912 -> 1689025416976
	1689025416976 [label=AccumulateGrad]
	1689025413568 -> 1689025426096
	1688758313296 [label="19.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1688758313296 -> 1689025413568
	1689025413568 [label=AccumulateGrad]
	1689025426960 -> 1689025422208
	1688758313392 [label="19.bn2.weight
 (128)" fillcolor=lightblue]
	1688758313392 -> 1689025426960
	1689025426960 [label=AccumulateGrad]
	1689025428496 -> 1689025422208
	1688758313488 [label="19.bn2.bias
 (128)" fillcolor=lightblue]
	1688758313488 -> 1689025428496
	1689025428496 [label=AccumulateGrad]
	1689025417840 -> 1689025420912
	1689025424608 -> 1689025413472
	1689025424608 [label=SigmoidBackward0]
	1689025423888 -> 1689025424608
	1689025423888 [label=ConvolutionBackward0]
	1689025420144 -> 1689025423888
	1689025420144 [label=NativeDropoutBackward0]
	1689025416880 -> 1689025420144
	1689025416880 [label=ReluBackward0]
	1689025415152 -> 1689025416880
	1689025415152 [label=ConvolutionBackward0]
	1689025422496 -> 1689025415152
	1689025422496 [label=MeanBackward1]
	1689025423168 -> 1689025422496
	1689025417552 -> 1689025415152
	1688758313872 [label="20.fc1.weight
 (8, 128, 1, 1)" fillcolor=lightblue]
	1688758313872 -> 1689025417552
	1689025417552 [label=AccumulateGrad]
	1689025428688 -> 1689025415152
	1688758313968 [label="20.fc1.bias
 (8)" fillcolor=lightblue]
	1688758313968 -> 1689025428688
	1689025428688 [label=AccumulateGrad]
	1689025420624 -> 1689025423888
	1688758314064 [label="20.fc2.weight
 (128, 8, 1, 1)" fillcolor=lightblue]
	1688758314064 -> 1689025420624
	1689025420624 [label=AccumulateGrad]
	1689025418512 -> 1689025423888
	1688758314160 [label="20.fc2.bias
 (128)" fillcolor=lightblue]
	1688758314160 -> 1689025418512
	1689025418512 [label=AccumulateGrad]
	1689025423456 -> 1689025418416
	1688758314256 [label="21.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	1688758314256 -> 1689025423456
	1689025423456 [label=AccumulateGrad]
	1689025420240 -> 1689025416544
	1688758314352 [label="21.pointwise.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1688758314352 -> 1689025420240
	1689025420240 [label=AccumulateGrad]
	1689025423216 -> 1689025420432
	1688758314448 [label="22.weight
 (256)" fillcolor=lightblue]
	1688758314448 -> 1689025423216
	1689025423216 [label=AccumulateGrad]
	1689025424896 -> 1689025420432
	1688758314544 [label="22.bias
 (256)" fillcolor=lightblue]
	1688758314544 -> 1689025424896
	1689025424896 [label=AccumulateGrad]
	1689025427872 -> 1689025427344
	1688758314928 [label="24.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1688758314928 -> 1689025427872
	1689025427872 [label=AccumulateGrad]
	1689025422016 -> 1689025425568
	1688758315024 [label="24.bn1.weight
 (256)" fillcolor=lightblue]
	1688758315024 -> 1689025422016
	1689025422016 [label=AccumulateGrad]
	1689025415824 -> 1689025425568
	1688758315120 [label="24.bn1.bias
 (256)" fillcolor=lightblue]
	1688758315120 -> 1689025415824
	1689025415824 [label=AccumulateGrad]
	1689025428592 -> 1689025414960
	1688758315504 [label="24.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1688758315504 -> 1689025428592
	1689025428592 [label=AccumulateGrad]
	1689025413616 -> 1689025428352
	1688758315600 [label="24.bn2.weight
 (256)" fillcolor=lightblue]
	1688758315600 -> 1689025413616
	1689025413616 [label=AccumulateGrad]
	1689025427008 -> 1689025428352
	1688758315696 [label="24.bn2.bias
 (256)" fillcolor=lightblue]
	1688758315696 -> 1689025427008
	1689025427008 [label=AccumulateGrad]
	1689025425472 -> 1689025417888
	1689025427824 -> 1689025414864
	1689025427824 [label=ViewBackward0]
	1689025414624 -> 1689025427824
	1689025414624 [label=SigmoidBackward0]
	1689025426288 -> 1689025414624
	1689025426288 [label=AddmmBackward0]
	1689025418464 -> 1689025426288
	1688758316368 [label="25.channel_att.fc2.bias
 (256)" fillcolor=lightblue]
	1688758316368 -> 1689025418464
	1689025418464 [label=AccumulateGrad]
	1689025416736 -> 1689025426288
	1689025416736 [label=ReluBackward0]
	1689025418704 -> 1689025416736
	1689025418704 [label=AddmmBackward0]
	1689025422064 -> 1689025418704
	1688758316176 [label="25.channel_att.fc1.bias
 (16)" fillcolor=lightblue]
	1688758316176 -> 1689025422064
	1689025422064 [label=AccumulateGrad]
	1689025419568 -> 1689025418704
	1689025419568 [label=MeanBackward1]
	1689025424800 -> 1689025419568
	1689025424800 [label=ViewBackward0]
	1689025416256 -> 1689025424800
	1689025427632 -> 1689025418704
	1689025427632 [label=TBackward0]
	1689025424944 -> 1689025427632
	1688758316080 [label="25.channel_att.fc1.weight
 (16, 256)" fillcolor=lightblue]
	1688758316080 -> 1689025424944
	1689025424944 [label=AccumulateGrad]
	1689025415872 -> 1689025426288
	1689025415872 [label=TBackward0]
	1689025416016 -> 1689025415872
	1688758316272 [label="25.channel_att.fc2.weight
 (256, 16)" fillcolor=lightblue]
	1688758316272 -> 1689025416016
	1689025416016 [label=AccumulateGrad]
	1689025416592 -> 1689025417168
	1689025416592 [label=SigmoidBackward0]
	1689025414048 -> 1689025416592
	1689025414048 [label=ConvolutionBackward0]
	1689025416784 -> 1689025414048
	1689025416784 [label=CatBackward0]
	1689025426768 -> 1689025416784
	1689025426768 [label=MeanBackward1]
	1689025414864 -> 1689025426768
	1689025427248 -> 1689025416784
	1689025427248 [label=MaxBackward0]
	1689025414864 -> 1689025427248
	1689025427488 -> 1689025414048
	1688758316464 [label="25.spatial_att.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	1688758316464 -> 1689025427488
	1689025427488 [label=AccumulateGrad]
	1689025425424 -> 1689025418848
	1689025425424 [label=TBackward0]
	1689025419808 -> 1689025425424
	1688758316560 [label="29.weight
 (19, 256)" fillcolor=lightblue]
	1688758316560 -> 1689025419808
	1689025419808 [label=AccumulateGrad]
	1689025418848 -> 1688802740080
}
