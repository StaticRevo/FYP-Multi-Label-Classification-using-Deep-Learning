digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1694097972144 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1694027291696 [label=AddmmBackward0]
	1694027292944 -> 1694027291696
	1694097948592 [label="fc.bias
 (19)" fillcolor=lightblue]
	1694097948592 -> 1694027292944
	1694027292944 [label=AccumulateGrad]
	1694027291264 -> 1694027291696
	1694027291264 [label=ViewBackward0]
	1694027290544 -> 1694027291264
	1694027290544 [label=MeanBackward1]
	1694027288960 -> 1694027290544
	1694027288960 [label=ReluBackward0]
	1694027288240 -> 1694027288960
	1694027288240 [label=AddBackward0]
	1694027285024 -> 1694027288240
	1694027285024 [label=CudnnBatchNormBackward0]
	1694027292320 -> 1694027285024
	1694027292320 [label=ConvolutionBackward0]
	1694027291840 -> 1694027292320
	1694027291840 [label=ReluBackward0]
	1694027290112 -> 1694027291840
	1694027290112 [label=CudnnBatchNormBackward0]
	1694027288096 -> 1694027290112
	1694027288096 [label=ConvolutionBackward0]
	1694027298224 -> 1694027288096
	1694027298224 [label=ReluBackward0]
	1694027294048 -> 1694027298224
	1694027294048 [label=CudnnBatchNormBackward0]
	1694027290400 -> 1694027294048
	1694027290400 [label=ConvolutionBackward0]
	1694027290784 -> 1694027290400
	1694027290784 [label=ReluBackward0]
	1694027286944 -> 1694027290784
	1694027286944 [label=AddBackward0]
	1694027287904 -> 1694027286944
	1694027287904 [label=CudnnBatchNormBackward0]
	1694027287472 -> 1694027287904
	1694027287472 [label=ConvolutionBackward0]
	1694027286512 -> 1694027287472
	1694027286512 [label=ReluBackward0]
	1694027298896 -> 1694027286512
	1694027298896 [label=CudnnBatchNormBackward0]
	1694027296832 -> 1694027298896
	1694027296832 [label=ConvolutionBackward0]
	1694027297120 -> 1694027296832
	1694027297120 [label=ReluBackward0]
	1694027296688 -> 1694027297120
	1694027296688 [label=CudnnBatchNormBackward0]
	1694027296016 -> 1694027296688
	1694027296016 [label=ConvolutionBackward0]
	1694027287856 -> 1694027296016
	1694027287856 [label=ReluBackward0]
	1694027295200 -> 1694027287856
	1694027295200 [label=AddBackward0]
	1694027295248 -> 1694027295200
	1694027295248 [label=CudnnBatchNormBackward0]
	1694027295056 -> 1694027295248
	1694027295056 [label=ConvolutionBackward0]
	1694027294720 -> 1694027295056
	1694027294720 [label=ReluBackward0]
	1694027293808 -> 1694027294720
	1694027293808 [label=CudnnBatchNormBackward0]
	1694027285072 -> 1694027293808
	1694027285072 [label=ConvolutionBackward0]
	1694027285264 -> 1694027285072
	1694027285264 [label=ReluBackward0]
	1694027285168 -> 1694027285264
	1694027285168 [label=CudnnBatchNormBackward0]
	1694027285456 -> 1694027285168
	1694027285456 [label=ConvolutionBackward0]
	1694027286128 -> 1694027285456
	1694027286128 [label=ReluBackward0]
	1694027287664 -> 1694027286128
	1694027287664 [label=AddBackward0]
	1694027287952 -> 1694027287664
	1694027287952 [label=CudnnBatchNormBackward0]
	1694027287328 -> 1694027287952
	1694027287328 [label=ConvolutionBackward0]
	1694027287232 -> 1694027287328
	1694027287232 [label=ReluBackward0]
	1694027288672 -> 1694027287232
	1694027288672 [label=CudnnBatchNormBackward0]
	1694027289584 -> 1694027288672
	1694027289584 [label=ConvolutionBackward0]
	1694027288528 -> 1694027289584
	1694027288528 [label=ReluBackward0]
	1694027288432 -> 1694027288528
	1694027288432 [label=CudnnBatchNormBackward0]
	1694027289776 -> 1694027288432
	1694027289776 [label=ConvolutionBackward0]
	1694027286368 -> 1694027289776
	1694027286368 [label=ReluBackward0]
	1694027290640 -> 1694027286368
	1694027290640 [label=AddBackward0]
	1694027290928 -> 1694027290640
	1694027290928 [label=CudnnBatchNormBackward0]
	1694027291888 -> 1694027290928
	1694027291888 [label=ConvolutionBackward0]
	1694027292464 -> 1694027291888
	1694027292464 [label=ReluBackward0]
	1694027292368 -> 1694027292464
	1694027292368 [label=CudnnBatchNormBackward0]
	1694027292656 -> 1694027292368
	1694027292656 [label=ConvolutionBackward0]
	1694027298416 -> 1694027292656
	1694027298416 [label=ReluBackward0]
	1694027291552 -> 1694027298416
	1694027291552 [label=CudnnBatchNormBackward0]
	1694027298608 -> 1694027291552
	1694027298608 [label=ConvolutionBackward0]
	1694027291312 -> 1694027298608
	1694027291312 [label=ReluBackward0]
	1694027292272 -> 1694027291312
	1694027292272 [label=AddBackward0]
	1694027290880 -> 1694027292272
	1694027290880 [label=CudnnBatchNormBackward0]
	1694027299040 -> 1694027290880
	1694027299040 [label=ConvolutionBackward0]
	1694027292512 -> 1694027299040
	1694027292512 [label=ReluBackward0]
	1694027292800 -> 1694027292512
	1694027292800 [label=CudnnBatchNormBackward0]
	1694027297552 -> 1694027292800
	1694027297552 [label=ConvolutionBackward0]
	1694027287712 -> 1694027297552
	1694027287712 [label=ReluBackward0]
	1694027289824 -> 1694027287712
	1694027289824 [label=CudnnBatchNormBackward0]
	1694027285216 -> 1694027289824
	1694027285216 [label=ConvolutionBackward0]
	1694027298320 -> 1694027285216
	1694027298320 [label=ReluBackward0]
	1694027286800 -> 1694027298320
	1694027286800 [label=AddBackward0]
	1694027286032 -> 1694027286800
	1694027286032 [label=CudnnBatchNormBackward0]
	1694027289008 -> 1694027286032
	1694027289008 [label=ConvolutionBackward0]
	1694027287760 -> 1694027289008
	1694027287760 [label=ReluBackward0]
	1694027284592 -> 1694027287760
	1694027284592 [label=CudnnBatchNormBackward0]
	1694027286224 -> 1694027284592
	1694027286224 [label=ConvolutionBackward0]
	1694027285360 -> 1694027286224
	1694027285360 [label=ReluBackward0]
	1694027293760 -> 1694027285360
	1694027293760 [label=CudnnBatchNormBackward0]
	1694027287568 -> 1694027293760
	1694027287568 [label=ConvolutionBackward0]
	1694027288480 -> 1694027287568
	1694027288480 [label=ReluBackward0]
	1694027293568 -> 1694027288480
	1694027293568 [label=AddBackward0]
	1694027294144 -> 1694027293568
	1694027294144 [label=CudnnBatchNormBackward0]
	1694027297072 -> 1694027294144
	1694027297072 [label=ConvolutionBackward0]
	1694027295344 -> 1694027297072
	1694027295344 [label=ReluBackward0]
	1694027294912 -> 1694027295344
	1694027294912 [label=CudnnBatchNormBackward0]
	1694027293712 -> 1694027294912
	1694027293712 [label=ConvolutionBackward0]
	1694027284880 -> 1694027293712
	1694027284880 [label=ReluBackward0]
	1694027296064 -> 1694027284880
	1694027296064 [label=CudnnBatchNormBackward0]
	1694027296544 -> 1694027296064
	1694027296544 [label=ConvolutionBackward0]
	1694027284688 -> 1694027296544
	1694027284688 [label=ReluBackward0]
	1694027294864 -> 1694027284688
	1694027294864 [label=AddBackward0]
	1694027294768 -> 1694027294864
	1694027294768 [label=CudnnBatchNormBackward0]
	1694027295680 -> 1694027294768
	1694027295680 [label=ConvolutionBackward0]
	1694027299088 -> 1694027295680
	1694027299088 [label=ReluBackward0]
	1694027294096 -> 1694027299088
	1694027294096 [label=CudnnBatchNormBackward0]
	1694027297984 -> 1694027294096
	1694027297984 [label=ConvolutionBackward0]
	1694027296208 -> 1694027297984
	1694027296208 [label=ReluBackward0]
	1694027297744 -> 1694027296208
	1694027297744 [label=CudnnBatchNormBackward0]
	1694027294960 -> 1694027297744
	1694027294960 [label=ConvolutionBackward0]
	1694027297840 -> 1694027294960
	1694027297840 [label=ReluBackward0]
	1694027294192 -> 1694027297840
	1694027294192 [label=AddBackward0]
	1694027299232 -> 1694027294192
	1694027299232 [label=CudnnBatchNormBackward0]
	1694027299376 -> 1694027299232
	1694027299376 [label=ConvolutionBackward0]
	1694027299616 -> 1694027299376
	1694027299616 [label=ReluBackward0]
	1694027299520 -> 1694027299616
	1694027299520 [label=CudnnBatchNormBackward0]
	1694027298080 -> 1694027299520
	1694027298080 [label=ConvolutionBackward0]
	1694027297504 -> 1694027298080
	1694027297504 [label=ReluBackward0]
	1694027293184 -> 1694027297504
	1694027293184 [label=CudnnBatchNormBackward0]
	1694027299952 -> 1694027293184
	1694027299952 [label=ConvolutionBackward0]
	1694027299280 -> 1694027299952
	1694027299280 [label=ReluBackward0]
	1694027299136 -> 1694027299280
	1694027299136 [label=AddBackward0]
	1694027300000 -> 1694027299136
	1694027300000 [label=CudnnBatchNormBackward0]
	1694027300144 -> 1694027300000
	1694027300144 [label=ConvolutionBackward0]
	1694027300336 -> 1694027300144
	1694027300336 [label=ReluBackward0]
	1694027300480 -> 1694027300336
	1694027300480 [label=CudnnBatchNormBackward0]
	1694027300576 -> 1694027300480
	1694027300576 [label=ConvolutionBackward0]
	1694027300768 -> 1694027300576
	1694027300768 [label=ReluBackward0]
	1694027300816 -> 1694027300768
	1694027300816 [label=CudnnBatchNormBackward0]
	1694096228560 -> 1694027300816
	1694096228560 [label=ConvolutionBackward0]
	1694027296352 -> 1694096228560
	1694027296352 [label=ReluBackward0]
	1694096228848 -> 1694027296352
	1694096228848 [label=AddBackward0]
	1694096228944 -> 1694096228848
	1694096228944 [label=CudnnBatchNormBackward0]
	1694096229088 -> 1694096228944
	1694096229088 [label=ConvolutionBackward0]
	1694096229280 -> 1694096229088
	1694096229280 [label=ReluBackward0]
	1694096229424 -> 1694096229280
	1694096229424 [label=CudnnBatchNormBackward0]
	1694096229520 -> 1694096229424
	1694096229520 [label=ConvolutionBackward0]
	1694096229712 -> 1694096229520
	1694096229712 [label=ReluBackward0]
	1694096229856 -> 1694096229712
	1694096229856 [label=CudnnBatchNormBackward0]
	1694096229952 -> 1694096229856
	1694096229952 [label=ConvolutionBackward0]
	1694096228896 -> 1694096229952
	1694096228896 [label=ReluBackward0]
	1694096230240 -> 1694096228896
	1694096230240 [label=AddBackward0]
	1694096230336 -> 1694096230240
	1694096230336 [label=CudnnBatchNormBackward0]
	1694096230480 -> 1694096230336
	1694096230480 [label=ConvolutionBackward0]
	1694096230672 -> 1694096230480
	1694096230672 [label=ReluBackward0]
	1694096230768 -> 1694096230672
	1694096230768 [label=CudnnBatchNormBackward0]
	1694097667024 -> 1694096230768
	1694097667024 [label=ConvolutionBackward0]
	1694097659200 -> 1694097667024
	1694097659200 [label=ReluBackward0]
	1694097654544 -> 1694097659200
	1694097654544 [label=CudnnBatchNormBackward0]
	1694097659824 -> 1694097654544
	1694097659824 [label=ConvolutionBackward0]
	1694135407648 -> 1694097659824
	1694135407648 [label=ReluBackward0]
	1694135407792 -> 1694135407648
	1694135407792 [label=AddBackward0]
	1694135407888 -> 1694135407792
	1694135407888 [label=CudnnBatchNormBackward0]
	1694135408032 -> 1694135407888
	1694135408032 [label=ConvolutionBackward0]
	1694135408224 -> 1694135408032
	1694135408224 [label=ReluBackward0]
	1694135408368 -> 1694135408224
	1694135408368 [label=CudnnBatchNormBackward0]
	1694135408464 -> 1694135408368
	1694135408464 [label=ConvolutionBackward0]
	1694135408656 -> 1694135408464
	1694135408656 [label=ReluBackward0]
	1694135408800 -> 1694135408656
	1694135408800 [label=CudnnBatchNormBackward0]
	1694135408896 -> 1694135408800
	1694135408896 [label=ConvolutionBackward0]
	1694135407840 -> 1694135408896
	1694135407840 [label=ReluBackward0]
	1694135409184 -> 1694135407840
	1694135409184 [label=AddBackward0]
	1694135409280 -> 1694135409184
	1694135409280 [label=CudnnBatchNormBackward0]
	1694145742640 -> 1694135409280
	1694145742640 [label=ConvolutionBackward0]
	1694135409520 -> 1694145742640
	1694135409520 [label=ReluBackward0]
	1694135409664 -> 1694135409520
	1694135409664 [label=CudnnBatchNormBackward0]
	1694135409760 -> 1694135409664
	1694135409760 [label=ConvolutionBackward0]
	1694135409952 -> 1694135409760
	1694135409952 [label=ReluBackward0]
	1694135410096 -> 1694135409952
	1694135410096 [label=CudnnBatchNormBackward0]
	1694135410192 -> 1694135410096
	1694135410192 [label=ConvolutionBackward0]
	1694135409232 -> 1694135410192
	1694135409232 [label=ReluBackward0]
	1694135410480 -> 1694135409232
	1694135410480 [label=AddBackward0]
	1694135410576 -> 1694135410480
	1694135410576 [label=CudnnBatchNormBackward0]
	1694135410720 -> 1694135410576
	1694135410720 [label=ConvolutionBackward0]
	1694135410912 -> 1694135410720
	1694135410912 [label=ReluBackward0]
	1694135411056 -> 1694135410912
	1694135411056 [label=CudnnBatchNormBackward0]
	1694135411152 -> 1694135411056
	1694135411152 [label=ConvolutionBackward0]
	1694135411344 -> 1694135411152
	1694135411344 [label=ReluBackward0]
	1694135411488 -> 1694135411344
	1694135411488 [label=CudnnBatchNormBackward0]
	1694135411584 -> 1694135411488
	1694135411584 [label=ConvolutionBackward0]
	1694135411776 -> 1694135411584
	1694135411776 [label=MaxPool2DWithIndicesBackward0]
	1694135411920 -> 1694135411776
	1694135411920 [label=ReluBackward0]
	1694135412016 -> 1694135411920
	1694135412016 [label=CudnnBatchNormBackward0]
	1694135412112 -> 1694135412016
	1694135412112 [label=ConvolutionBackward0]
	1694135412304 -> 1694135412112
	1694093095184 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1694093095184 -> 1694135412304
	1694135412304 [label=AccumulateGrad]
	1694135412064 -> 1694135412016
	1694096195760 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1694096195760 -> 1694135412064
	1694135412064 [label=AccumulateGrad]
	1694135411824 -> 1694135412016
	1694096195856 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1694096195856 -> 1694135411824
	1694135411824 [label=AccumulateGrad]
	1694135411728 -> 1694135411584
	1694096196912 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1694096196912 -> 1694135411728
	1694135411728 [label=AccumulateGrad]
	1694135411536 -> 1694135411488
	1694120329680 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1694120329680 -> 1694135411536
	1694135411536 [label=AccumulateGrad]
	1694135411392 -> 1694135411488
	1694120329776 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1694120329776 -> 1694135411392
	1694135411392 [label=AccumulateGrad]
	1694135411296 -> 1694135411152
	1694120330160 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1694120330160 -> 1694135411296
	1694135411296 [label=AccumulateGrad]
	1694135411104 -> 1694135411056
	1694120330256 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1694120330256 -> 1694135411104
	1694135411104 [label=AccumulateGrad]
	1694135410960 -> 1694135411056
	1694120330352 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1694120330352 -> 1694135410960
	1694135410960 [label=AccumulateGrad]
	1694135410864 -> 1694135410720
	1694120330736 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1694120330736 -> 1694135410864
	1694135410864 [label=AccumulateGrad]
	1694135410672 -> 1694135410576
	1694120330832 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1694120330832 -> 1694135410672
	1694135410672 [label=AccumulateGrad]
	1694135410624 -> 1694135410576
	1694120330928 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1694120330928 -> 1694135410624
	1694135410624 [label=AccumulateGrad]
	1694135410528 -> 1694135410480
	1694135410528 [label=CudnnBatchNormBackward0]
	1694135411248 -> 1694135410528
	1694135411248 [label=ConvolutionBackward0]
	1694135411776 -> 1694135411248
	1694135411632 -> 1694135411248
	1694096196144 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1694096196144 -> 1694135411632
	1694135411632 [label=AccumulateGrad]
	1694135410816 -> 1694135410528
	1694096196240 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1694096196240 -> 1694135410816
	1694135410816 [label=AccumulateGrad]
	1694135410768 -> 1694135410528
	1694096196336 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1694096196336 -> 1694135410768
	1694135410768 [label=AccumulateGrad]
	1694135410384 -> 1694135410192
	1694120331312 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1694120331312 -> 1694135410384
	1694135410384 [label=AccumulateGrad]
	1694135410144 -> 1694135410096
	1694120331408 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1694120331408 -> 1694135410144
	1694135410144 [label=AccumulateGrad]
	1694135410000 -> 1694135410096
	1694120331504 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1694120331504 -> 1694135410000
	1694135410000 [label=AccumulateGrad]
	1694135409904 -> 1694135409760
	1694120331888 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1694120331888 -> 1694135409904
	1694135409904 [label=AccumulateGrad]
	1694135409712 -> 1694135409664
	1694120331984 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1694120331984 -> 1694135409712
	1694135409712 [label=AccumulateGrad]
	1694135409568 -> 1694135409664
	1694120332080 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1694120332080 -> 1694135409568
	1694135409568 [label=AccumulateGrad]
	1694135409472 -> 1694145742640
	1694120332464 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1694120332464 -> 1694135409472
	1694135409472 [label=AccumulateGrad]
	1694145741344 -> 1694135409280
	1694120332560 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1694120332560 -> 1694145741344
	1694145741344 [label=AccumulateGrad]
	1694135409424 -> 1694135409280
	1694120332656 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1694120332656 -> 1694135409424
	1694135409424 [label=AccumulateGrad]
	1694135409232 -> 1694135409184
	1694135409088 -> 1694135408896
	1694120333040 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1694120333040 -> 1694135409088
	1694135409088 [label=AccumulateGrad]
	1694135408848 -> 1694135408800
	1694120333136 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1694120333136 -> 1694135408848
	1694135408848 [label=AccumulateGrad]
	1694135408704 -> 1694135408800
	1694120333232 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1694120333232 -> 1694135408704
	1694135408704 [label=AccumulateGrad]
	1694135408608 -> 1694135408464
	1694120333616 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1694120333616 -> 1694135408608
	1694135408608 [label=AccumulateGrad]
	1694135408416 -> 1694135408368
	1694120333712 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1694120333712 -> 1694135408416
	1694135408416 [label=AccumulateGrad]
	1694135408272 -> 1694135408368
	1694120333808 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1694120333808 -> 1694135408272
	1694135408272 [label=AccumulateGrad]
	1694135408176 -> 1694135408032
	1694120334192 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1694120334192 -> 1694135408176
	1694135408176 [label=AccumulateGrad]
	1694135407984 -> 1694135407888
	1694120334288 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1694120334288 -> 1694135407984
	1694135407984 [label=AccumulateGrad]
	1694135407936 -> 1694135407888
	1694120334384 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1694120334384 -> 1694135407936
	1694135407936 [label=AccumulateGrad]
	1694135407840 -> 1694135407792
	1694135407600 -> 1694097659824
	1694120335344 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1694120335344 -> 1694135407600
	1694135407600 [label=AccumulateGrad]
	1694097659584 -> 1694097654544
	1694120335440 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1694120335440 -> 1694097659584
	1694097659584 [label=AccumulateGrad]
	1694135407456 -> 1694097654544
	1694120335536 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1694120335536 -> 1694135407456
	1694135407456 [label=AccumulateGrad]
	1694097668800 -> 1694097667024
	1694120335920 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1694120335920 -> 1694097668800
	1694097668800 [label=AccumulateGrad]
	1694097654496 -> 1694096230768
	1694120336016 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1694120336016 -> 1694097654496
	1694097654496 [label=AccumulateGrad]
	1694097668464 -> 1694096230768
	1694120336112 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1694120336112 -> 1694097668464
	1694097668464 [label=AccumulateGrad]
	1694096230624 -> 1694096230480
	1694120336496 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1694120336496 -> 1694096230624
	1694096230624 [label=AccumulateGrad]
	1694096230432 -> 1694096230336
	1694120336592 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1694120336592 -> 1694096230432
	1694096230432 [label=AccumulateGrad]
	1694096230384 -> 1694096230336
	1694120336688 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1694120336688 -> 1694096230384
	1694096230384 [label=AccumulateGrad]
	1694096230288 -> 1694096230240
	1694096230288 [label=CudnnBatchNormBackward0]
	1694096230720 -> 1694096230288
	1694096230720 [label=ConvolutionBackward0]
	1694135407648 -> 1694096230720
	1694097656368 -> 1694096230720
	1694120334768 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1694120334768 -> 1694097656368
	1694097656368 [label=AccumulateGrad]
	1694096230576 -> 1694096230288
	1694120334864 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1694120334864 -> 1694096230576
	1694096230576 [label=AccumulateGrad]
	1694096230528 -> 1694096230288
	1694120334960 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1694120334960 -> 1694096230528
	1694096230528 [label=AccumulateGrad]
	1694096230144 -> 1694096229952
	1694120337072 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1694120337072 -> 1694096230144
	1694096230144 [label=AccumulateGrad]
	1694096229904 -> 1694096229856
	1694120337168 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1694120337168 -> 1694096229904
	1694096229904 [label=AccumulateGrad]
	1694096229760 -> 1694096229856
	1694120337264 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1694120337264 -> 1694096229760
	1694096229760 [label=AccumulateGrad]
	1694096229664 -> 1694096229520
	1694120337648 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1694120337648 -> 1694096229664
	1694096229664 [label=AccumulateGrad]
	1694096229472 -> 1694096229424
	1694120337744 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1694120337744 -> 1694096229472
	1694096229472 [label=AccumulateGrad]
	1694096229328 -> 1694096229424
	1694120337840 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1694120337840 -> 1694096229328
	1694096229328 [label=AccumulateGrad]
	1694096229232 -> 1694096229088
	1694120338224 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1694120338224 -> 1694096229232
	1694096229232 [label=AccumulateGrad]
	1694096229040 -> 1694096228944
	1694120338320 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1694120338320 -> 1694096229040
	1694096229040 [label=AccumulateGrad]
	1694096228992 -> 1694096228944
	1694120338416 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1694120338416 -> 1694096228992
	1694096228992 [label=AccumulateGrad]
	1694096228896 -> 1694096228848
	1694096228752 -> 1694096228560
	1694120338800 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1694120338800 -> 1694096228752
	1694096228752 [label=AccumulateGrad]
	1694096228512 -> 1694027300816
	1694120338896 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1694120338896 -> 1694096228512
	1694096228512 [label=AccumulateGrad]
	1694096228416 -> 1694027300816
	1694120338992 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1694120338992 -> 1694096228416
	1694096228416 [label=AccumulateGrad]
	1694027300720 -> 1694027300576
	1694120339376 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1694120339376 -> 1694027300720
	1694027300720 [label=AccumulateGrad]
	1694027300528 -> 1694027300480
	1694120339472 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1694120339472 -> 1694027300528
	1694027300528 [label=AccumulateGrad]
	1694027300384 -> 1694027300480
	1694120339568 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1694120339568 -> 1694027300384
	1694027300384 [label=AccumulateGrad]
	1694027300288 -> 1694027300144
	1694120339952 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1694120339952 -> 1694027300288
	1694027300288 [label=AccumulateGrad]
	1694027300096 -> 1694027300000
	1694120340048 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1694120340048 -> 1694027300096
	1694027300096 [label=AccumulateGrad]
	1694027300048 -> 1694027300000
	1694120340144 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1694120340144 -> 1694027300048
	1694027300048 [label=AccumulateGrad]
	1694027296352 -> 1694027299136
	1694027298032 -> 1694027299952
	1694120340528 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1694120340528 -> 1694027298032
	1694027298032 [label=AccumulateGrad]
	1694027293232 -> 1694027293184
	1694120340624 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1694120340624 -> 1694027293232
	1694027293232 [label=AccumulateGrad]
	1694027293328 -> 1694027293184
	1694120340720 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1694120340720 -> 1694027293328
	1694027293328 [label=AccumulateGrad]
	1694027297408 -> 1694027298080
	1694120341104 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1694120341104 -> 1694027297408
	1694027297408 [label=AccumulateGrad]
	1694027291168 -> 1694027299520
	1694120341200 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1694120341200 -> 1694027291168
	1694027291168 [label=AccumulateGrad]
	1694027299568 -> 1694027299520
	1694120341296 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1694120341296 -> 1694027299568
	1694027299568 [label=AccumulateGrad]
	1694027295440 -> 1694027299376
	1694120341680 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1694120341680 -> 1694027295440
	1694027295440 [label=AccumulateGrad]
	1694027299328 -> 1694027299232
	1694120341776 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1694120341776 -> 1694027299328
	1694027299328 [label=AccumulateGrad]
	1694027299712 -> 1694027299232
	1694120341872 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1694120341872 -> 1694027299712
	1694027299712 [label=AccumulateGrad]
	1694027299280 -> 1694027294192
	1694027299760 -> 1694027294960
	1694120342832 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1694120342832 -> 1694027299760
	1694027299760 [label=AccumulateGrad]
	1694027297792 -> 1694027297744
	1694120342928 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1694120342928 -> 1694027297792
	1694027297792 [label=AccumulateGrad]
	1694027296160 -> 1694027297744
	1694120343024 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1694120343024 -> 1694027296160
	1694027296160 [label=AccumulateGrad]
	1694027285696 -> 1694027297984
	1694120343408 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1694120343408 -> 1694027285696
	1694027285696 [label=AccumulateGrad]
	1694027298800 -> 1694027294096
	1694120343504 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1694120343504 -> 1694027298800
	1694027298800 [label=AccumulateGrad]
	1694027295872 -> 1694027294096
	1694120343600 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1694120343600 -> 1694027295872
	1694027295872 [label=AccumulateGrad]
	1694027296256 -> 1694027295680
	1694120343984 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1694120343984 -> 1694027296256
	1694027296256 [label=AccumulateGrad]
	1694027295536 -> 1694027294768
	1694120344080 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1694120344080 -> 1694027295536
	1694027295536 [label=AccumulateGrad]
	1694027295632 -> 1694027294768
	1694120344176 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1694120344176 -> 1694027295632
	1694027295632 [label=AccumulateGrad]
	1694027294624 -> 1694027294864
	1694027294624 [label=CudnnBatchNormBackward0]
	1694027294336 -> 1694027294624
	1694027294336 [label=ConvolutionBackward0]
	1694027297840 -> 1694027294336
	1694027298464 -> 1694027294336
	1694120342256 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1694120342256 -> 1694027298464
	1694027298464 [label=AccumulateGrad]
	1694027295920 -> 1694027294624
	1694120342352 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1694120342352 -> 1694027295920
	1694027295920 [label=AccumulateGrad]
	1694027295488 -> 1694027294624
	1694120342448 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1694120342448 -> 1694027295488
	1694027295488 [label=AccumulateGrad]
	1694027295104 -> 1694027296544
	1694120344560 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1694120344560 -> 1694027295104
	1694027295104 [label=AccumulateGrad]
	1694027296448 -> 1694027296064
	1694120344656 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1694120344656 -> 1694027296448
	1694027296448 [label=AccumulateGrad]
	1694027293664 -> 1694027296064
	1694120344752 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1694120344752 -> 1694027293664
	1694027293664 [label=AccumulateGrad]
	1694027286656 -> 1694027293712
	1694120345136 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1694120345136 -> 1694027286656
	1694027286656 [label=AccumulateGrad]
	1694027284976 -> 1694027294912
	1694120345232 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1694120345232 -> 1694027284976
	1694027284976 [label=AccumulateGrad]
	1694027286176 -> 1694027294912
	1694120345328 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1694120345328 -> 1694027286176
	1694027286176 [label=AccumulateGrad]
	1694027299856 -> 1694027297072
	1694098210992 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1694098210992 -> 1694027299856
	1694027299856 [label=AccumulateGrad]
	1694027284784 -> 1694027294144
	1694098211088 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1694098211088 -> 1694027284784
	1694027284784 [label=AccumulateGrad]
	1694027293904 -> 1694027294144
	1694098211184 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1694098211184 -> 1694027293904
	1694027293904 [label=AccumulateGrad]
	1694027284688 -> 1694027293568
	1694027293472 -> 1694027287568
	1694094950320 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1694094950320 -> 1694027293472
	1694027293472 [label=AccumulateGrad]
	1694027296928 -> 1694027293760
	1694094947632 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1694094947632 -> 1694027296928
	1694027296928 [label=AccumulateGrad]
	1694027293616 -> 1694027293760
	1694117608912 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1694117608912 -> 1694027293616
	1694027293616 [label=AccumulateGrad]
	1694027294816 -> 1694027286224
	1694117609296 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1694117609296 -> 1694027294816
	1694027294816 [label=AccumulateGrad]
	1694027286080 -> 1694027284592
	1694117609392 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1694117609392 -> 1694027286080
	1694027286080 [label=AccumulateGrad]
	1694027287616 -> 1694027284592
	1694093082704 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1694093082704 -> 1694027287616
	1694027287616 [label=AccumulateGrad]
	1694027287520 -> 1694027289008
	1694093083088 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1694093083088 -> 1694027287520
	1694027287520 [label=AccumulateGrad]
	1694027290592 -> 1694027286032
	1694093083184 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1694093083184 -> 1694027290592
	1694027290592 [label=AccumulateGrad]
	1694027286560 -> 1694027286032
	1694093083280 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1694093083280 -> 1694027286560
	1694027286560 [label=AccumulateGrad]
	1694027288480 -> 1694027286800
	1694027292032 -> 1694027285216
	1694093083664 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1694093083664 -> 1694027292032
	1694027292032 [label=AccumulateGrad]
	1694027287280 -> 1694027289824
	1694093083760 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1694093083760 -> 1694027287280
	1694027287280 [label=AccumulateGrad]
	1694027293520 -> 1694027289824
	1694093083856 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1694093083856 -> 1694027293520
	1694027293520 [label=AccumulateGrad]
	1694027286464 -> 1694027297552
	1694093084240 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1694093084240 -> 1694027286464
	1694027286464 [label=AccumulateGrad]
	1694027298992 -> 1694027292800
	1694093084336 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1694093084336 -> 1694027298992
	1694027298992 [label=AccumulateGrad]
	1694027291504 -> 1694027292800
	1694093084432 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1694093084432 -> 1694027291504
	1694027291504 [label=AccumulateGrad]
	1694027289440 -> 1694027299040
	1694093084816 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1694093084816 -> 1694027289440
	1694027289440 [label=AccumulateGrad]
	1694027298944 -> 1694027290880
	1694093084912 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1694093084912 -> 1694027298944
	1694027298944 [label=AccumulateGrad]
	1694027298368 -> 1694027290880
	1694093085008 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1694093085008 -> 1694027298368
	1694027298368 [label=AccumulateGrad]
	1694027298320 -> 1694027292272
	1694027297936 -> 1694027298608
	1694093085392 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1694093085392 -> 1694027297936
	1694027297936 [label=AccumulateGrad]
	1694027298848 -> 1694027291552
	1694093085488 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1694093085488 -> 1694027298848
	1694027298848 [label=AccumulateGrad]
	1694027297696 -> 1694027291552
	1694093085584 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1694093085584 -> 1694027297696
	1694027297696 [label=AccumulateGrad]
	1694027298272 -> 1694027292656
	1694093085968 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1694093085968 -> 1694027298272
	1694027298272 [label=AccumulateGrad]
	1694027292896 -> 1694027292368
	1694093086064 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1694093086064 -> 1694027292896
	1694027292896 [label=AccumulateGrad]
	1694027292080 -> 1694027292368
	1694093086160 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1694093086160 -> 1694027292080
	1694027292080 [label=AccumulateGrad]
	1694027291456 -> 1694027291888
	1694093086544 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1694093086544 -> 1694027291456
	1694027291456 [label=AccumulateGrad]
	1694027291216 -> 1694027290928
	1694093086640 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1694093086640 -> 1694027291216
	1694027291216 [label=AccumulateGrad]
	1694027291792 -> 1694027290928
	1694093086736 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1694093086736 -> 1694027291792
	1694027291792 [label=AccumulateGrad]
	1694027291312 -> 1694027290640
	1694027290352 -> 1694027289776
	1694093087120 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1694093087120 -> 1694027290352
	1694027290352 [label=AccumulateGrad]
	1694027290160 -> 1694027288432
	1694093087216 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1694093087216 -> 1694027290160
	1694027290160 [label=AccumulateGrad]
	1694027289296 -> 1694027288432
	1694093087312 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1694093087312 -> 1694027289296
	1694027289296 [label=AccumulateGrad]
	1694027289200 -> 1694027289584
	1694093087696 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1694093087696 -> 1694027289200
	1694027289200 [label=AccumulateGrad]
	1694027288624 -> 1694027288672
	1694093087792 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1694093087792 -> 1694027288624
	1694027288624 [label=AccumulateGrad]
	1694027289056 -> 1694027288672
	1694093087888 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1694093087888 -> 1694027289056
	1694027289056 [label=AccumulateGrad]
	1694027288336 -> 1694027287328
	1694093088272 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1694093088272 -> 1694027288336
	1694027288336 [label=AccumulateGrad]
	1694027286992 -> 1694027287952
	1694093088368 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1694093088368 -> 1694027286992
	1694027286992 [label=AccumulateGrad]
	1694027286896 -> 1694027287952
	1694093088464 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1694093088464 -> 1694027286896
	1694027286896 [label=AccumulateGrad]
	1694027286368 -> 1694027287664
	1694027286608 -> 1694027285456
	1694093089520 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1694093089520 -> 1694027286608
	1694027286608 [label=AccumulateGrad]
	1694027285840 -> 1694027285168
	1694093089616 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1694093089616 -> 1694027285840
	1694027285840 [label=AccumulateGrad]
	1694027284544 -> 1694027285168
	1694093089712 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1694093089712 -> 1694027284544
	1694027284544 [label=AccumulateGrad]
	1694027284928 -> 1694027285072
	1694093090096 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1694093090096 -> 1694027284928
	1694027284928 [label=AccumulateGrad]
	1694027293424 -> 1694027293808
	1694093090192 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1694093090192 -> 1694027293424
	1694027293424 [label=AccumulateGrad]
	1694027294000 -> 1694027293808
	1694093090288 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1694093090288 -> 1694027294000
	1694027294000 [label=AccumulateGrad]
	1694027293856 -> 1694027295056
	1694093090672 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1694093090672 -> 1694027293856
	1694027293856 [label=AccumulateGrad]
	1694027293952 -> 1694027295248
	1694093090768 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1694093090768 -> 1694027293952
	1694027293952 [label=AccumulateGrad]
	1694027295584 -> 1694027295248
	1694093090864 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1694093090864 -> 1694027295584
	1694027295584 [label=AccumulateGrad]
	1694027295824 -> 1694027295200
	1694027295824 [label=CudnnBatchNormBackward0]
	1694027284640 -> 1694027295824
	1694027284640 [label=ConvolutionBackward0]
	1694027286128 -> 1694027284640
	1694027286320 -> 1694027284640
	1694093088944 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1694093088944 -> 1694027286320
	1694027286320 [label=AccumulateGrad]
	1694027295008 -> 1694027295824
	1694093089040 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1694093089040 -> 1694027295008
	1694027295008 [label=AccumulateGrad]
	1694027293376 -> 1694027295824
	1694093089136 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1694093089136 -> 1694027293376
	1694027293376 [label=AccumulateGrad]
	1694027295392 -> 1694027296016
	1694093091248 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1694093091248 -> 1694027295392
	1694027295392 [label=AccumulateGrad]
	1694027297024 -> 1694027296688
	1694093091344 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1694093091344 -> 1694027297024
	1694027297024 [label=AccumulateGrad]
	1694027296640 -> 1694027296688
	1694093091440 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1694093091440 -> 1694027296640
	1694027296640 [label=AccumulateGrad]
	1694027299664 -> 1694027296832
	1694093091824 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1694093091824 -> 1694027299664
	1694027299664 [label=AccumulateGrad]
	1694027297888 -> 1694027298896
	1694093091920 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1694093091920 -> 1694027297888
	1694027297888 [label=AccumulateGrad]
	1694027287136 -> 1694027298896
	1694093092016 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1694093092016 -> 1694027287136
	1694027287136 [label=AccumulateGrad]
	1694027290496 -> 1694027287472
	1694093092400 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1694093092400 -> 1694027290496
	1694027290496 [label=AccumulateGrad]
	1694027290256 -> 1694027287904
	1694093092496 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1694093092496 -> 1694027290256
	1694027290256 [label=AccumulateGrad]
	1694027285600 -> 1694027287904
	1694093092592 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1694093092592 -> 1694027285600
	1694027285600 [label=AccumulateGrad]
	1694027287856 -> 1694027286944
	1694027291984 -> 1694027290400
	1694093092976 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1694093092976 -> 1694027291984
	1694027291984 [label=AccumulateGrad]
	1694027289344 -> 1694027294048
	1694093093072 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1694093093072 -> 1694027289344
	1694027289344 [label=AccumulateGrad]
	1694027288384 -> 1694027294048
	1694093093168 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1694093093168 -> 1694027288384
	1694027288384 [label=AccumulateGrad]
	1694027288192 -> 1694027288096
	1694093093552 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1694093093552 -> 1694027288192
	1694027288192 [label=AccumulateGrad]
	1694027288864 -> 1694027290112
	1694093093648 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1694093093648 -> 1694027288864
	1694027288864 [label=AccumulateGrad]
	1694027289728 -> 1694027290112
	1694093093744 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1694093093744 -> 1694027289728
	1694027289728 [label=AccumulateGrad]
	1694027289680 -> 1694027292320
	1694093094128 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1694093094128 -> 1694027289680
	1694027289680 [label=AccumulateGrad]
	1694027288576 -> 1694027285024
	1694093094224 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1694093094224 -> 1694027288576
	1694027288576 [label=AccumulateGrad]
	1694027292416 -> 1694027285024
	1694093094320 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1694093094320 -> 1694027292416
	1694027292416 [label=AccumulateGrad]
	1694027290784 -> 1694027288240
	1694027292128 -> 1694027291696
	1694027292128 [label=TBackward0]
	1694027292704 -> 1694027292128
	1694093094992 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	1694093094992 -> 1694027292704
	1694027292704 [label=AccumulateGrad]
	1694027291696 -> 1694097972144
}
