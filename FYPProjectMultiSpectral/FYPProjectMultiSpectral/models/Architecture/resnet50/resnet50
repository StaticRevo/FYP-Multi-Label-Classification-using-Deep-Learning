digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2037457899248 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2037346268544 [label=AddmmBackward0]
	2037346268208 -> 2037346268544
	2037458424304 [label="fc.bias
 (19)" fillcolor=lightblue]
	2037458424304 -> 2037346268208
	2037346268208 [label=AccumulateGrad]
	2037346267584 -> 2037346268544
	2037346267584 [label=ViewBackward0]
	2037346267104 -> 2037346267584
	2037346267104 [label=MeanBackward1]
	2037346266768 -> 2037346267104
	2037346266768 [label=ReluBackward0]
	2037346266288 -> 2037346266768
	2037346266288 [label=AddBackward0]
	2037346265808 -> 2037346266288
	2037346265808 [label=CudnnBatchNormBackward0]
	2037346264704 -> 2037346265808
	2037346264704 [label=ConvolutionBackward0]
	2037346263744 -> 2037346264704
	2037346263744 [label=ReluBackward0]
	2037346263408 -> 2037346263744
	2037346263408 [label=CudnnBatchNormBackward0]
	2037346262928 -> 2037346263408
	2037346262928 [label=ConvolutionBackward0]
	2037346261536 -> 2037346262928
	2037346261536 [label=ReluBackward0]
	2037346271040 -> 2037346261536
	2037346271040 [label=CudnnBatchNormBackward0]
	2037346270848 -> 2037346271040
	2037346270848 [label=ConvolutionBackward0]
	2037346265664 -> 2037346270848
	2037346265664 [label=ReluBackward0]
	2037346270560 -> 2037346265664
	2037346270560 [label=AddBackward0]
	2037346270368 -> 2037346270560
	2037346270368 [label=CudnnBatchNormBackward0]
	2037346270416 -> 2037346270368
	2037346270416 [label=ConvolutionBackward0]
	2037346270032 -> 2037346270416
	2037346270032 [label=ReluBackward0]
	2037346269792 -> 2037346270032
	2037346269792 [label=CudnnBatchNormBackward0]
	2037346269744 -> 2037346269792
	2037346269744 [label=ConvolutionBackward0]
	2037346269408 -> 2037346269744
	2037346269408 [label=ReluBackward0]
	2037346269456 -> 2037346269408
	2037346269456 [label=CudnnBatchNormBackward0]
	2037346269360 -> 2037346269456
	2037346269360 [label=ConvolutionBackward0]
	2037346270512 -> 2037346269360
	2037346270512 [label=ReluBackward0]
	2037346268976 -> 2037346270512
	2037346268976 [label=AddBackward0]
	2037346268880 -> 2037346268976
	2037346268880 [label=CudnnBatchNormBackward0]
	2037346268448 -> 2037346268880
	2037346268448 [label=ConvolutionBackward0]
	2037346268304 -> 2037346268448
	2037346268304 [label=ReluBackward0]
	2037346268112 -> 2037346268304
	2037346268112 [label=CudnnBatchNormBackward0]
	2037346267776 -> 2037346268112
	2037346267776 [label=ConvolutionBackward0]
	2037346267920 -> 2037346267776
	2037346267920 [label=ReluBackward0]
	2037346267488 -> 2037346267920
	2037346267488 [label=CudnnBatchNormBackward0]
	2037346267392 -> 2037346267488
	2037346267392 [label=ConvolutionBackward0]
	2037346267200 -> 2037346267392
	2037346267200 [label=ReluBackward0]
	2037346266816 -> 2037346267200
	2037346266816 [label=AddBackward0]
	2037346267056 -> 2037346266816
	2037346267056 [label=CudnnBatchNormBackward0]
	2037346266720 -> 2037346267056
	2037346266720 [label=ConvolutionBackward0]
	2037346266432 -> 2037346266720
	2037346266432 [label=ReluBackward0]
	2037346266480 -> 2037346266432
	2037346266480 [label=CudnnBatchNormBackward0]
	2037346266192 -> 2037346266480
	2037346266192 [label=ConvolutionBackward0]
	2037346266096 -> 2037346266192
	2037346266096 [label=ReluBackward0]
	2037346265760 -> 2037346266096
	2037346265760 [label=CudnnBatchNormBackward0]
	2037346265568 -> 2037346265760
	2037346265568 [label=ConvolutionBackward0]
	2037346266912 -> 2037346265568
	2037346266912 [label=ReluBackward0]
	2037346265280 -> 2037346266912
	2037346265280 [label=AddBackward0]
	2037346265088 -> 2037346265280
	2037346265088 [label=CudnnBatchNormBackward0]
	2037346265136 -> 2037346265088
	2037346265136 [label=ConvolutionBackward0]
	2037346264752 -> 2037346265136
	2037346264752 [label=ReluBackward0]
	2037346264512 -> 2037346264752
	2037346264512 [label=CudnnBatchNormBackward0]
	2037346264464 -> 2037346264512
	2037346264464 [label=ConvolutionBackward0]
	2037346264128 -> 2037346264464
	2037346264128 [label=ReluBackward0]
	2037346264176 -> 2037346264128
	2037346264176 [label=CudnnBatchNormBackward0]
	2037346264080 -> 2037346264176
	2037346264080 [label=ConvolutionBackward0]
	2037346265232 -> 2037346264080
	2037346265232 [label=ReluBackward0]
	2037346263696 -> 2037346265232
	2037346263696 [label=AddBackward0]
	2037346263600 -> 2037346263696
	2037346263600 [label=CudnnBatchNormBackward0]
	2037346263168 -> 2037346263600
	2037346263168 [label=ConvolutionBackward0]
	2037346263024 -> 2037346263168
	2037346263024 [label=ReluBackward0]
	2037346262832 -> 2037346263024
	2037346262832 [label=CudnnBatchNormBackward0]
	2037346261488 -> 2037346262832
	2037346261488 [label=ConvolutionBackward0]
	2037346257888 -> 2037346261488
	2037346257888 [label=ReluBackward0]
	2037346257936 -> 2037346257888
	2037346257936 [label=CudnnBatchNormBackward0]
	2037346258320 -> 2037346257936
	2037346258320 [label=ConvolutionBackward0]
	2037346263504 -> 2037346258320
	2037346263504 [label=ReluBackward0]
	2037346257504 -> 2037346263504
	2037346257504 [label=AddBackward0]
	2037346261248 -> 2037346257504
	2037346261248 [label=CudnnBatchNormBackward0]
	2037346257840 -> 2037346261248
	2037346257840 [label=ConvolutionBackward0]
	2037346260960 -> 2037346257840
	2037346260960 [label=ReluBackward0]
	2037346255296 -> 2037346260960
	2037346255296 [label=CudnnBatchNormBackward0]
	2037346260144 -> 2037346255296
	2037346260144 [label=ConvolutionBackward0]
	2037346259952 -> 2037346260144
	2037346259952 [label=ReluBackward0]
	2037346257648 -> 2037346259952
	2037346257648 [label=CudnnBatchNormBackward0]
	2037346260864 -> 2037346257648
	2037346260864 [label=ConvolutionBackward0]
	2037346259184 -> 2037346260864
	2037346259184 [label=ReluBackward0]
	2037346259088 -> 2037346259184
	2037346259088 [label=AddBackward0]
	2037346259760 -> 2037346259088
	2037346259760 [label=CudnnBatchNormBackward0]
	2037346257600 -> 2037346259760
	2037346257600 [label=ConvolutionBackward0]
	2037346261920 -> 2037346257600
	2037346261920 [label=ReluBackward0]
	2037346262688 -> 2037346261920
	2037346262688 [label=CudnnBatchNormBackward0]
	2037346255344 -> 2037346262688
	2037346255344 [label=ConvolutionBackward0]
	2037346259280 -> 2037346255344
	2037346259280 [label=ReluBackward0]
	2037346256784 -> 2037346259280
	2037346256784 [label=CudnnBatchNormBackward0]
	2037346260816 -> 2037346256784
	2037346260816 [label=ConvolutionBackward0]
	2037346255392 -> 2037346260816
	2037346255392 [label=ReluBackward0]
	2037346260480 -> 2037346255392
	2037346260480 [label=AddBackward0]
	2037346260384 -> 2037346260480
	2037346260384 [label=CudnnBatchNormBackward0]
	2037346255968 -> 2037346260384
	2037346255968 [label=ConvolutionBackward0]
	2037346256832 -> 2037346255968
	2037346256832 [label=ReluBackward0]
	2037346257120 -> 2037346256832
	2037346257120 [label=CudnnBatchNormBackward0]
	2037346258944 -> 2037346257120
	2037346258944 [label=ConvolutionBackward0]
	2037459383968 -> 2037346258944
	2037459383968 [label=ReluBackward0]
	2037459384016 -> 2037459383968
	2037459384016 [label=CudnnBatchNormBackward0]
	2037459384880 -> 2037459384016
	2037459384880 [label=ConvolutionBackward0]
	2037459385072 -> 2037459384880
	2037459385072 [label=ReluBackward0]
	2037459386128 -> 2037459385072
	2037459386128 [label=AddBackward0]
	2037459384496 -> 2037459386128
	2037459384496 [label=CudnnBatchNormBackward0]
	2037459384448 -> 2037459384496
	2037459384448 [label=ConvolutionBackward0]
	2037459384640 -> 2037459384448
	2037459384640 [label=ReluBackward0]
	2037459386176 -> 2037459384640
	2037459386176 [label=CudnnBatchNormBackward0]
	2037346329232 -> 2037459386176
	2037346329232 [label=ConvolutionBackward0]
	2037457788336 -> 2037346329232
	2037457788336 [label=ReluBackward0]
	2037457787232 -> 2037457788336
	2037457787232 [label=CudnnBatchNormBackward0]
	2037457786752 -> 2037457787232
	2037457786752 [label=ConvolutionBackward0]
	2037459386032 -> 2037457786752
	2037459386032 [label=ReluBackward0]
	2037457785312 -> 2037459386032
	2037457785312 [label=AddBackward0]
	2037457784832 -> 2037457785312
	2037457784832 [label=CudnnBatchNormBackward0]
	2037457784496 -> 2037457784832
	2037457784496 [label=ConvolutionBackward0]
	2037457783536 -> 2037457784496
	2037457783536 [label=ReluBackward0]
	2037457782432 -> 2037457783536
	2037457782432 [label=CudnnBatchNormBackward0]
	2037457781952 -> 2037457782432
	2037457781952 [label=ConvolutionBackward0]
	2037457780992 -> 2037457781952
	2037457780992 [label=ReluBackward0]
	2037457788720 -> 2037457780992
	2037457788720 [label=CudnnBatchNormBackward0]
	2037457788384 -> 2037457788720
	2037457788384 [label=ConvolutionBackward0]
	2037457785456 -> 2037457788384
	2037457785456 [label=ReluBackward0]
	2037457788240 -> 2037457785456
	2037457788240 [label=AddBackward0]
	2037457787904 -> 2037457788240
	2037457787904 [label=CudnnBatchNormBackward0]
	2037457787952 -> 2037457787904
	2037457787952 [label=ConvolutionBackward0]
	2037457787616 -> 2037457787952
	2037457787616 [label=ReluBackward0]
	2037457787664 -> 2037457787616
	2037457787664 [label=CudnnBatchNormBackward0]
	2037457787568 -> 2037457787664
	2037457787568 [label=ConvolutionBackward0]
	2037457786944 -> 2037457787568
	2037457786944 [label=ReluBackward0]
	2037457786992 -> 2037457786944
	2037457786992 [label=CudnnBatchNormBackward0]
	2037457786848 -> 2037457786992
	2037457786848 [label=ConvolutionBackward0]
	2037457788096 -> 2037457786848
	2037457788096 [label=ReluBackward0]
	2037457786512 -> 2037457788096
	2037457786512 [label=AddBackward0]
	2037457786368 -> 2037457786512
	2037457786368 [label=CudnnBatchNormBackward0]
	2037457785984 -> 2037457786368
	2037457785984 [label=ConvolutionBackward0]
	2037457786128 -> 2037457785984
	2037457786128 [label=ReluBackward0]
	2037457785696 -> 2037457786128
	2037457785696 [label=CudnnBatchNormBackward0]
	2037457785600 -> 2037457785696
	2037457785600 [label=ConvolutionBackward0]
	2037457785408 -> 2037457785600
	2037457785408 [label=ReluBackward0]
	2037457785024 -> 2037457785408
	2037457785024 [label=CudnnBatchNormBackward0]
	2037457785264 -> 2037457785024
	2037457785264 [label=ConvolutionBackward0]
	2037457784880 -> 2037457785264
	2037457784880 [label=ReluBackward0]
	2037457784640 -> 2037457784880
	2037457784640 [label=AddBackward0]
	2037457784592 -> 2037457784640
	2037457784592 [label=CudnnBatchNormBackward0]
	2037457784400 -> 2037457784592
	2037457784400 [label=ConvolutionBackward0]
	2037457784304 -> 2037457784400
	2037457784304 [label=ReluBackward0]
	2037457783968 -> 2037457784304
	2037457783968 [label=CudnnBatchNormBackward0]
	2037457783776 -> 2037457783968
	2037457783776 [label=ConvolutionBackward0]
	2037457783632 -> 2037457783776
	2037457783632 [label=ReluBackward0]
	2037457783440 -> 2037457783632
	2037457783440 [label=CudnnBatchNormBackward0]
	2037457783104 -> 2037457783440
	2037457783104 [label=ConvolutionBackward0]
	2037457784784 -> 2037457783104
	2037457784784 [label=ReluBackward0]
	2037457782960 -> 2037457784784
	2037457782960 [label=AddBackward0]
	2037457782624 -> 2037457782960
	2037457782624 [label=CudnnBatchNormBackward0]
	2037457782672 -> 2037457782624
	2037457782672 [label=ConvolutionBackward0]
	2037457782336 -> 2037457782672
	2037457782336 [label=ReluBackward0]
	2037457782384 -> 2037457782336
	2037457782384 [label=CudnnBatchNormBackward0]
	2037457782288 -> 2037457782384
	2037457782288 [label=ConvolutionBackward0]
	2037457781664 -> 2037457782288
	2037457781664 [label=ReluBackward0]
	2037457781712 -> 2037457781664
	2037457781712 [label=CudnnBatchNormBackward0]
	2037457781568 -> 2037457781712
	2037457781568 [label=ConvolutionBackward0]
	2037457782816 -> 2037457781568
	2037457782816 [label=ReluBackward0]
	2037457781232 -> 2037457782816
	2037457781232 [label=AddBackward0]
	2037457781088 -> 2037457781232
	2037457781088 [label=CudnnBatchNormBackward0]
	2037457780800 -> 2037457781088
	2037457780800 [label=ConvolutionBackward0]
	2037457789248 -> 2037457780800
	2037457789248 [label=ReluBackward0]
	2037457789584 -> 2037457789248
	2037457789584 [label=CudnnBatchNormBackward0]
	2037457789680 -> 2037457789584
	2037457789680 [label=ConvolutionBackward0]
	2037457789872 -> 2037457789680
	2037457789872 [label=ReluBackward0]
	2037457790016 -> 2037457789872
	2037457790016 [label=CudnnBatchNormBackward0]
	2037457790112 -> 2037457790016
	2037457790112 [label=ConvolutionBackward0]
	2037457790304 -> 2037457790112
	2037457790304 [label=MaxPool2DWithIndicesBackward0]
	2037457790448 -> 2037457790304
	2037457790448 [label=ReluBackward0]
	2037457790544 -> 2037457790448
	2037457790544 [label=CudnnBatchNormBackward0]
	2037457790640 -> 2037457790544
	2037457790640 [label=ConvolutionBackward0]
	2037457790832 -> 2037457790640
	2037458424112 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2037458424112 -> 2037457790832
	2037457790832 [label=AccumulateGrad]
	2037457790592 -> 2037457790544
	2037498094192 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2037498094192 -> 2037457790592
	2037457790592 [label=AccumulateGrad]
	2037457790352 -> 2037457790544
	2037498094576 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2037498094576 -> 2037457790352
	2037457790352 [label=AccumulateGrad]
	2037457790256 -> 2037457790112
	2037498094672 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2037498094672 -> 2037457790256
	2037457790256 [label=AccumulateGrad]
	2037457790064 -> 2037457790016
	2037498094768 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2037498094768 -> 2037457790064
	2037457790064 [label=AccumulateGrad]
	2037457789920 -> 2037457790016
	2037498095536 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2037498095536 -> 2037457789920
	2037457789920 [label=AccumulateGrad]
	2037457789824 -> 2037457789680
	2037498095440 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2037498095440 -> 2037457789824
	2037457789824 [label=AccumulateGrad]
	2037457789632 -> 2037457789584
	2037498095344 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2037498095344 -> 2037457789632
	2037457789632 [label=AccumulateGrad]
	2037457789488 -> 2037457789584
	2037498095248 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2037498095248 -> 2037457789488
	2037457789488 [label=AccumulateGrad]
	2037457789392 -> 2037457780800
	2037498094960 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2037498094960 -> 2037457789392
	2037457789392 [label=AccumulateGrad]
	2037457780896 -> 2037457781088
	2037498095920 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2037498095920 -> 2037457780896
	2037457780896 [label=AccumulateGrad]
	2037457781040 -> 2037457781088
	2037498096016 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2037498096016 -> 2037457781040
	2037457781040 [label=AccumulateGrad]
	2037457781328 -> 2037457781232
	2037457781328 [label=CudnnBatchNormBackward0]
	2037457789776 -> 2037457781328
	2037457789776 [label=ConvolutionBackward0]
	2037457790304 -> 2037457789776
	2037457790160 -> 2037457789776
	2037498094096 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2037498094096 -> 2037457790160
	2037457790160 [label=AccumulateGrad]
	2037457780848 -> 2037457781328
	2037498094000 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2037498094000 -> 2037457780848
	2037457780848 [label=AccumulateGrad]
	2037457780944 -> 2037457781328
	2037498093904 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2037498093904 -> 2037457780944
	2037457780944 [label=AccumulateGrad]
	2037457781280 -> 2037457781568
	2037498098704 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2037498098704 -> 2037457781280
	2037457781280 [label=AccumulateGrad]
	2037457781808 -> 2037457781712
	2037498098512 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2037498098512 -> 2037457781808
	2037457781808 [label=AccumulateGrad]
	2037457781760 -> 2037457781712
	2037498096880 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2037498096880 -> 2037457781760
	2037457781760 [label=AccumulateGrad]
	2037457781856 -> 2037457782288
	2037498096112 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2037498096112 -> 2037457781856
	2037457781856 [label=AccumulateGrad]
	2037457782192 -> 2037457782384
	2037498096208 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2037498096208 -> 2037457782192
	2037457782192 [label=AccumulateGrad]
	2037457782144 -> 2037457782384
	2037472536304 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2037472536304 -> 2037457782144
	2037457782144 [label=AccumulateGrad]
	2037457782480 -> 2037457782672
	2037472536688 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2037472536688 -> 2037457782480
	2037457782480 [label=AccumulateGrad]
	2037457782864 -> 2037457782624
	2037472536784 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2037472536784 -> 2037457782864
	2037457782864 [label=AccumulateGrad]
	2037457782720 -> 2037457782624
	2037472536880 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2037472536880 -> 2037457782720
	2037457782720 [label=AccumulateGrad]
	2037457782816 -> 2037457782960
	2037457783248 -> 2037457783104
	2037472537264 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2037472537264 -> 2037457783248
	2037457783248 [label=AccumulateGrad]
	2037457783296 -> 2037457783440
	2037472537360 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2037472537360 -> 2037457783296
	2037457783296 [label=AccumulateGrad]
	2037457783728 -> 2037457783440
	2037472537456 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2037472537456 -> 2037457783728
	2037457783728 [label=AccumulateGrad]
	2037457783824 -> 2037457783776
	2037472537840 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2037472537840 -> 2037457783824
	2037457783824 [label=AccumulateGrad]
	2037457783920 -> 2037457783968
	2037472537936 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2037472537936 -> 2037457783920
	2037457783920 [label=AccumulateGrad]
	2037457784112 -> 2037457783968
	2037472538032 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2037472538032 -> 2037457784112
	2037457784112 [label=AccumulateGrad]
	2037457784160 -> 2037457784400
	2037472538416 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2037472538416 -> 2037457784160
	2037457784160 [label=AccumulateGrad]
	2037457784448 -> 2037457784592
	2037472538512 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2037472538512 -> 2037457784448
	2037457784448 [label=AccumulateGrad]
	2037457784688 -> 2037457784592
	2037472538608 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2037472538608 -> 2037457784688
	2037457784688 [label=AccumulateGrad]
	2037457784784 -> 2037457784640
	2037457784928 -> 2037457785264
	2037472539568 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2037472539568 -> 2037457784928
	2037457784928 [label=AccumulateGrad]
	2037457785120 -> 2037457785024
	2037472539664 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2037472539664 -> 2037457785120
	2037457785120 [label=AccumulateGrad]
	2037457785360 -> 2037457785024
	2037472539760 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2037472539760 -> 2037457785360
	2037457785360 [label=AccumulateGrad]
	2037457785648 -> 2037457785600
	2037472540144 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2037472540144 -> 2037457785648
	2037457785648 [label=AccumulateGrad]
	2037457785504 -> 2037457785696
	2037472540240 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2037472540240 -> 2037457785504
	2037457785504 [label=AccumulateGrad]
	2037457785888 -> 2037457785696
	2037472540336 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2037472540336 -> 2037457785888
	2037457785888 [label=AccumulateGrad]
	2037457786032 -> 2037457785984
	2037472540720 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2037472540720 -> 2037457786032
	2037457786032 [label=AccumulateGrad]
	2037457786176 -> 2037457786368
	2037472540816 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2037472540816 -> 2037457786176
	2037457786176 [label=AccumulateGrad]
	2037457786320 -> 2037457786368
	2037472540912 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2037472540912 -> 2037457786320
	2037457786320 [label=AccumulateGrad]
	2037457786608 -> 2037457786512
	2037457786608 [label=CudnnBatchNormBackward0]
	2037457785552 -> 2037457786608
	2037457785552 [label=ConvolutionBackward0]
	2037457784880 -> 2037457785552
	2037457785072 -> 2037457785552
	2037472538992 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2037472538992 -> 2037457785072
	2037457785072 [label=AccumulateGrad]
	2037457786224 -> 2037457786608
	2037472539088 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2037472539088 -> 2037457786224
	2037457786224 [label=AccumulateGrad]
	2037457786080 -> 2037457786608
	2037472539184 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2037472539184 -> 2037457786080
	2037457786080 [label=AccumulateGrad]
	2037457786560 -> 2037457786848
	2037472541296 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2037472541296 -> 2037457786560
	2037457786560 [label=AccumulateGrad]
	2037457787088 -> 2037457786992
	2037472541392 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2037472541392 -> 2037457787088
	2037457787088 [label=AccumulateGrad]
	2037457787040 -> 2037457786992
	2037472541488 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2037472541488 -> 2037457787040
	2037457787040 [label=AccumulateGrad]
	2037457787136 -> 2037457787568
	2037472541872 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2037472541872 -> 2037457787136
	2037457787136 [label=AccumulateGrad]
	2037457787472 -> 2037457787664
	2037472541968 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2037472541968 -> 2037457787472
	2037457787472 [label=AccumulateGrad]
	2037457787424 -> 2037457787664
	2037472542064 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2037472542064 -> 2037457787424
	2037457787424 [label=AccumulateGrad]
	2037457787760 -> 2037457787952
	2037472542448 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2037472542448 -> 2037457787760
	2037457787760 [label=AccumulateGrad]
	2037457788144 -> 2037457787904
	2037472542544 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2037472542544 -> 2037457788144
	2037457788144 [label=AccumulateGrad]
	2037457788000 -> 2037457787904
	2037472542640 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2037472542640 -> 2037457788000
	2037457788000 [label=AccumulateGrad]
	2037457788096 -> 2037457788240
	2037457788528 -> 2037457788384
	2037458682224 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2037458682224 -> 2037457788528
	2037457788528 [label=AccumulateGrad]
	2037457788576 -> 2037457788720
	2037458682320 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2037458682320 -> 2037457788576
	2037457788576 [label=AccumulateGrad]
	2037457781136 -> 2037457788720
	2037458682416 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2037458682416 -> 2037457781136
	2037457781136 [label=AccumulateGrad]
	2037457781616 -> 2037457781952
	2037458682800 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2037458682800 -> 2037457781616
	2037457781616 [label=AccumulateGrad]
	2037457782576 -> 2037457782432
	2037458682896 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2037458682896 -> 2037457782576
	2037457782576 [label=AccumulateGrad]
	2037457782912 -> 2037457782432
	2037458682992 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2037458682992 -> 2037457782912
	2037457782912 [label=AccumulateGrad]
	2037457783392 -> 2037457784496
	2037458683376 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2037458683376 -> 2037457783392
	2037457783392 [label=AccumulateGrad]
	2037457784352 -> 2037457784832
	2037458683472 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2037458683472 -> 2037457784352
	2037457784352 [label=AccumulateGrad]
	2037457784976 -> 2037457784832
	2037458683568 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2037458683568 -> 2037457784976
	2037457784976 [label=AccumulateGrad]
	2037457785456 -> 2037457785312
	2037457785792 -> 2037457786752
	2037458683952 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2037458683952 -> 2037457785792
	2037457785792 [label=AccumulateGrad]
	2037457787376 -> 2037457787232
	2037458684048 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2037458684048 -> 2037457787376
	2037457787376 [label=AccumulateGrad]
	2037457787712 -> 2037457787232
	2037458684144 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2037458684144 -> 2037457787712
	2037457787712 [label=AccumulateGrad]
	2037457788192 -> 2037346329232
	2037458684528 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2037458684528 -> 2037457788192
	2037457788192 [label=AccumulateGrad]
	2037459385552 -> 2037459386176
	2037458684624 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2037458684624 -> 2037459385552
	2037459385552 [label=AccumulateGrad]
	2037459384064 -> 2037459386176
	2037458684720 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2037458684720 -> 2037459384064
	2037459384064 [label=AccumulateGrad]
	2037459383248 -> 2037459384448
	2037458685104 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2037458685104 -> 2037459383248
	2037459383248 [label=AccumulateGrad]
	2037459386272 -> 2037459384496
	2037458685200 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2037458685200 -> 2037459386272
	2037459386272 [label=AccumulateGrad]
	2037459384784 -> 2037459384496
	2037458685296 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2037458685296 -> 2037459384784
	2037459384784 [label=AccumulateGrad]
	2037459386032 -> 2037459386128
	2037459385744 -> 2037459384880
	2037458686256 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2037458686256 -> 2037459385744
	2037459385744 [label=AccumulateGrad]
	2037459384592 -> 2037459384016
	2037458686352 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2037458686352 -> 2037459384592
	2037459384592 [label=AccumulateGrad]
	2037459385264 -> 2037459384016
	2037458686448 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2037458686448 -> 2037459385264
	2037459385264 [label=AccumulateGrad]
	2037459384832 -> 2037346258944
	2037458686832 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2037458686832 -> 2037459384832
	2037459384832 [label=AccumulateGrad]
	2037459383200 -> 2037346257120
	2037458686928 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2037458686928 -> 2037459383200
	2037459383200 [label=AccumulateGrad]
	2037459385024 -> 2037346257120
	2037458687024 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2037458687024 -> 2037459385024
	2037459385024 [label=AccumulateGrad]
	2037346258752 -> 2037346255968
	2037458687408 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2037458687408 -> 2037346258752
	2037346258752 [label=AccumulateGrad]
	2037346258896 -> 2037346260384
	2037458687504 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2037458687504 -> 2037346258896
	2037346258896 [label=AccumulateGrad]
	2037346261056 -> 2037346260384
	2037458687600 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2037458687600 -> 2037346261056
	2037346261056 [label=AccumulateGrad]
	2037346258656 -> 2037346260480
	2037346258656 [label=CudnnBatchNormBackward0]
	2037346257168 -> 2037346258656
	2037346257168 [label=ConvolutionBackward0]
	2037459385072 -> 2037346257168
	2037459384304 -> 2037346257168
	2037458685680 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2037458685680 -> 2037459384304
	2037459384304 [label=AccumulateGrad]
	2037346259568 -> 2037346258656
	2037458685776 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2037458685776 -> 2037346259568
	2037346259568 [label=AccumulateGrad]
	2037346262400 -> 2037346258656
	2037458685872 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2037458685872 -> 2037346262400
	2037346262400 [label=AccumulateGrad]
	2037346260048 -> 2037346260816
	2037458687984 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2037458687984 -> 2037346260048
	2037346260048 [label=AccumulateGrad]
	2037346258272 -> 2037346256784
	2037458688080 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2037458688080 -> 2037346258272
	2037346258272 [label=AccumulateGrad]
	2037346261152 -> 2037346256784
	2037458688176 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2037458688176 -> 2037346261152
	2037346261152 [label=AccumulateGrad]
	2037346256016 -> 2037346255344
	2037458688560 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2037458688560 -> 2037346256016
	2037346256016 [label=AccumulateGrad]
	2037346259856 -> 2037346262688
	2037458688656 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2037458688656 -> 2037346259856
	2037346259856 [label=AccumulateGrad]
	2037346256496 -> 2037346262688
	2037458688752 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2037458688752 -> 2037346256496
	2037346256496 [label=AccumulateGrad]
	2037346256208 -> 2037346257600
	2037458689136 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2037458689136 -> 2037346256208
	2037346256208 [label=AccumulateGrad]
	2037346262064 -> 2037346259760
	2037458689232 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2037458689232 -> 2037346262064
	2037346262064 [label=AccumulateGrad]
	2037346257024 -> 2037346259760
	2037458689328 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2037458689328 -> 2037346257024
	2037346257024 [label=AccumulateGrad]
	2037346255392 -> 2037346259088
	2037346262544 -> 2037346260864
	2037458689712 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2037458689712 -> 2037346262544
	2037346262544 [label=AccumulateGrad]
	2037346256736 -> 2037346257648
	2037458689808 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2037458689808 -> 2037346256736
	2037346256736 [label=AccumulateGrad]
	2037346260912 -> 2037346257648
	2037458689904 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2037458689904 -> 2037346260912
	2037346260912 [label=AccumulateGrad]
	2037346257456 -> 2037346260144
	2037458690288 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2037458690288 -> 2037346257456
	2037346257456 [label=AccumulateGrad]
	2037346255056 -> 2037346255296
	2037458690384 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2037458690384 -> 2037346255056
	2037346255056 [label=AccumulateGrad]
	2037346261872 -> 2037346255296
	2037458690480 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2037458690480 -> 2037346261872
	2037346261872 [label=AccumulateGrad]
	2037346258704 -> 2037346257840
	2037458690864 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2037458690864 -> 2037346258704
	2037346258704 [label=AccumulateGrad]
	2037346255248 -> 2037346261248
	2037458690960 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2037458690960 -> 2037346255248
	2037346255248 [label=AccumulateGrad]
	2037346262448 -> 2037346261248
	2037458691056 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2037458691056 -> 2037346262448
	2037346262448 [label=AccumulateGrad]
	2037346259184 -> 2037346257504
	2037346262112 -> 2037346258320
	2037458691440 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2037458691440 -> 2037346262112
	2037346262112 [label=AccumulateGrad]
	2037346256544 -> 2037346257936
	2037458691536 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2037458691536 -> 2037346256544
	2037346256544 [label=AccumulateGrad]
	2037346261776 -> 2037346257936
	2037458691632 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2037458691632 -> 2037346261776
	2037346261776 [label=AccumulateGrad]
	2037346262304 -> 2037346261488
	2037458692016 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2037458692016 -> 2037346262304
	2037346262304 [label=AccumulateGrad]
	2037346262160 -> 2037346262832
	2037458692112 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2037458692112 -> 2037346262160
	2037346262160 [label=AccumulateGrad]
	2037346263120 -> 2037346262832
	2037458692208 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2037458692208 -> 2037346263120
	2037346263120 [label=AccumulateGrad]
	2037346263216 -> 2037346263168
	2037458692592 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2037458692592 -> 2037346263216
	2037346263216 [label=AccumulateGrad]
	2037346263312 -> 2037346263600
	2037458692688 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2037458692688 -> 2037346263312
	2037346263312 [label=AccumulateGrad]
	2037346263360 -> 2037346263600
	2037458692784 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2037458692784 -> 2037346263360
	2037346263360 [label=AccumulateGrad]
	2037346263504 -> 2037346263696
	2037346263456 -> 2037346264080
	2037458693168 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2037458693168 -> 2037346263456
	2037346263456 [label=AccumulateGrad]
	2037346263984 -> 2037346264176
	2037458693264 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2037458693264 -> 2037346263984
	2037346263984 [label=AccumulateGrad]
	2037346263936 -> 2037346264176
	2037458693360 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2037458693360 -> 2037346263936
	2037346263936 [label=AccumulateGrad]
	2037346264272 -> 2037346264464
	2037458693744 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2037458693744 -> 2037346264272
	2037346264272 [label=AccumulateGrad]
	2037346264656 -> 2037346264512
	2037458693840 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2037458693840 -> 2037346264656
	2037346264656 [label=AccumulateGrad]
	2037346264608 -> 2037346264512
	2037458693936 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2037458693936 -> 2037346264608
	2037346264608 [label=AccumulateGrad]
	2037346264800 -> 2037346265136
	2037458694320 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2037458694320 -> 2037346264800
	2037346264800 [label=AccumulateGrad]
	2037346264992 -> 2037346265088
	2037458694416 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2037458694416 -> 2037346264992
	2037346264992 [label=AccumulateGrad]
	2037346264896 -> 2037346265088
	2037458694512 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2037458694512 -> 2037346264896
	2037346264896 [label=AccumulateGrad]
	2037346265232 -> 2037346265280
	2037346265424 -> 2037346265568
	2037458694896 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2037458694896 -> 2037346265424
	2037346265424 [label=AccumulateGrad]
	2037346265712 -> 2037346265760
	2037458694992 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2037458694992 -> 2037346265712
	2037346265712 [label=AccumulateGrad]
	2037346265904 -> 2037346265760
	2037458695088 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2037458695088 -> 2037346265904
	2037346265904 [label=AccumulateGrad]
	2037346265952 -> 2037346266192
	2037458695472 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2037458695472 -> 2037346265952
	2037346265952 [label=AccumulateGrad]
	2037346266240 -> 2037346266480
	2037458695568 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2037458695568 -> 2037346266240
	2037346266240 [label=AccumulateGrad]
	2037346266576 -> 2037346266480
	2037458695664 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2037458695664 -> 2037346266576
	2037346266576 [label=AccumulateGrad]
	2037346266336 -> 2037346266720
	2037458696048 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2037458696048 -> 2037346266336
	2037346266336 [label=AccumulateGrad]
	2037346266960 -> 2037346267056
	2037458696144 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2037458696144 -> 2037346266960
	2037346266960 [label=AccumulateGrad]
	2037346266864 -> 2037346267056
	2037458696240 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2037458696240 -> 2037346266864
	2037346266864 [label=AccumulateGrad]
	2037346266912 -> 2037346266816
	2037346267440 -> 2037346267392
	2037458697200 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2037458697200 -> 2037346267440
	2037346267440 [label=AccumulateGrad]
	2037346267296 -> 2037346267488
	2037458697296 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2037458697296 -> 2037346267296
	2037346267296 [label=AccumulateGrad]
	2037346267680 -> 2037346267488
	2037458697392 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2037458697392 -> 2037346267680
	2037346267680 [label=AccumulateGrad]
	2037346267824 -> 2037346267776
	2037458697776 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2037458697776 -> 2037346267824
	2037346267824 [label=AccumulateGrad]
	2037346267968 -> 2037346268112
	2037458697872 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2037458697872 -> 2037346267968
	2037346267968 [label=AccumulateGrad]
	2037346268400 -> 2037346268112
	2037458697968 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2037458697968 -> 2037346268400
	2037346268400 [label=AccumulateGrad]
	2037346268496 -> 2037346268448
	2037458419888 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2037458419888 -> 2037346268496
	2037346268496 [label=AccumulateGrad]
	2037346268592 -> 2037346268880
	2037458419984 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2037458419984 -> 2037346268592
	2037346268592 [label=AccumulateGrad]
	2037346268640 -> 2037346268880
	2037458420080 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2037458420080 -> 2037346268640
	2037346268640 [label=AccumulateGrad]
	2037346268784 -> 2037346268976
	2037346268784 [label=CudnnBatchNormBackward0]
	2037346268016 -> 2037346268784
	2037346268016 [label=ConvolutionBackward0]
	2037346267200 -> 2037346268016
	2037346267536 -> 2037346268016
	2037458696624 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2037458696624 -> 2037346267536
	2037346267536 [label=AccumulateGrad]
	2037346268352 -> 2037346268784
	2037458696720 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2037458696720 -> 2037346268352
	2037346268352 [label=AccumulateGrad]
	2037346268256 -> 2037346268784
	2037458696816 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2037458696816 -> 2037346268256
	2037346268256 [label=AccumulateGrad]
	2037346268736 -> 2037346269360
	2037458420464 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2037458420464 -> 2037346268736
	2037346268736 [label=AccumulateGrad]
	2037346269264 -> 2037346269456
	2037458420560 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2037458420560 -> 2037346269264
	2037346269264 [label=AccumulateGrad]
	2037346269216 -> 2037346269456
	2037458420656 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2037458420656 -> 2037346269216
	2037346269216 [label=AccumulateGrad]
	2037346269552 -> 2037346269744
	2037458421040 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2037458421040 -> 2037346269552
	2037346269552 [label=AccumulateGrad]
	2037346269936 -> 2037346269792
	2037458421136 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2037458421136 -> 2037346269936
	2037346269936 [label=AccumulateGrad]
	2037346269888 -> 2037346269792
	2037458421232 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2037458421232 -> 2037346269888
	2037346269888 [label=AccumulateGrad]
	2037346270080 -> 2037346270416
	2037458421616 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2037458421616 -> 2037346270080
	2037346270080 [label=AccumulateGrad]
	2037346270272 -> 2037346270368
	2037458421712 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2037458421712 -> 2037346270272
	2037346270272 [label=AccumulateGrad]
	2037346270176 -> 2037346270368
	2037458421808 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2037458421808 -> 2037346270176
	2037346270176 [label=AccumulateGrad]
	2037346270512 -> 2037346270560
	2037346270704 -> 2037346270848
	2037458422192 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2037458422192 -> 2037346270704
	2037346270704 [label=AccumulateGrad]
	2037346270992 -> 2037346271040
	2037458422288 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2037458422288 -> 2037346270992
	2037346270992 [label=AccumulateGrad]
	2037346271136 -> 2037346271040
	2037458422384 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2037458422384 -> 2037346271136
	2037346271136 [label=AccumulateGrad]
	2037346262352 -> 2037346262928
	2037458422768 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2037458422768 -> 2037346262352
	2037346262352 [label=AccumulateGrad]
	2037346262784 -> 2037346263408
	2037458422864 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2037458422864 -> 2037346262784
	2037346262784 [label=AccumulateGrad]
	2037346263888 -> 2037346263408
	2037458422960 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2037458422960 -> 2037346263888
	2037346263888 [label=AccumulateGrad]
	2037346264368 -> 2037346264704
	2037458423344 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2037458423344 -> 2037346264368
	2037346264368 [label=AccumulateGrad]
	2037346265328 -> 2037346265808
	2037458423440 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2037458423440 -> 2037346265328
	2037346265328 [label=AccumulateGrad]
	2037346265184 -> 2037346265808
	2037458423536 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2037458423536 -> 2037346265184
	2037346265184 [label=AccumulateGrad]
	2037346265664 -> 2037346266288
	2037346267728 -> 2037346268544
	2037346267728 [label=TBackward0]
	2037346266144 -> 2037346267728
	2037458424208 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2037458424208 -> 2037346266144
	2037346266144 [label=AccumulateGrad]
	2037346268544 -> 2037457899248
}
