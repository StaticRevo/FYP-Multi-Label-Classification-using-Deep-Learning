{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python Libraries\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platform’s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch-related Libraries (Deep Learning)\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Dataset and DataLoader for managing and batching data.\n",
    "import torchvision # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Datasets and transformations for image processing.\n",
    "import torchvision.datasets as datasets  # Datasets for computer vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Make grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations for image preprocessing.\n",
    "import torchsummary # PyTorch model summary for Keras-like model summary.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for handling class imbalance in object detection.\n",
    "from torchmetrics import MeanMetric  # Intersection over Union (IoU) metric for object detection.\n",
    "from torchmetrics.classification import MultilabelF1Score, MultilabelRecall, MultilabelPrecision, MultilabelAccuracy  # Multilabel classification metrics.\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Library for reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation functions.\n",
    "from rasterio.enums import Resampling  # Resampling methods used for resizing raster data.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation and Analysis Libraries\n",
    "import pandas as pd  # Data analysis and manipulation library for DataFrames and CSVs.\n",
    "import numpy as np  # Numpy for array operations and numerical computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics for classification models.\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating static and interactive visualizations.\n",
    "import seaborn as sns  # High-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm  # Progress bar for loops and processes.\n",
    "from PIL import Image  # Image handling, opening, manipulating, and saving.\n",
    "import ast  # Abstract Syntax Trees for parsing Python code.\n",
    "import requests  # HTTP library for sending requests.\n",
    "import zstandard as zstd  # Zstandard compression for fast compression and decompression.\n",
    "from collections import Counter # Counter for counting hashable objects.\n",
    "import certifi  # Certificates for verifying HTTPS requests.\n",
    "import ssl  # Secure Sockets Layer for secure connections.\n",
    "import urllib.request  # URL handling for requests.\n",
    "import kaggle # Kaggle API for downloading datasets.\n",
    "import zipfile # Work with ZIP archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)  # Set seed for reproducibility.\n",
    "main_path = 'D:\\Datasets'\n",
    "dataset_path = os.path.join(main_path, 'eurosat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    num_epochs = 10  # Number of epochs for training.\n",
    "    batch_size = 64  # Batch size for training.\n",
    "    learning_rate = 1e-3  # Learning rate for training.\n",
    "    input_size = 224  # Input size for the model.\n",
    "    resize = 256  # Resize size for the images.\n",
    "    num_classes = 10  # Number of classes in the dataset.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Device for training.\n",
    "    num_workers = 4  # Number of workers for the DataLoader.\n",
    "    optimizer = optim.Adam # Optimizer for training.\n",
    "    channels = 3 # Number of channels in the images.\n",
    "\n",
    "    image_mean = [0.485, 0.456, 0.406]  # Image mean for normalization.\n",
    "    image_std = [0.229, 0.224, 0.225]  # Image standard deviation for normalization.\n",
    "\n",
    "    train_path = os.path.join(dataset_path, 'train.csv')\n",
    "    val_path = os.path.join(dataset_path, 'validation.csv')\n",
    "    test_path = os.path.join(dataset_path, 'test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://madm.dfki.de/files/sentinel/EuroSAT.zip to D:\\Datasets\\eurosat\\EuroSAT.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94280567/94280567 [00:04<00:00, 20527229.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Datasets\\eurosat\\EuroSAT.zip to D:\\Datasets\\eurosat\n"
     ]
    }
   ],
   "source": [
    "# Disable SSL verification (not recommended for production)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Download the EuroSAT dataset\n",
    "euro_sat_dataset = datasets.EuroSAT(root=\"D:\\Datasets\", download=True, transform=transforms)\n",
    "\n",
    "# Get the full path of the dataset\n",
    "dataset_root_path = os.path.abspath(euro_sat_dataset.root)\n",
    "dataset_full_path = os.path.join(dataset_root_path, 'eurosat', '2750')  # '2750' is the subdirectory where the dataset files are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset Class\n",
    "This is a custom PyTorch Dataset class designed to load images and labels (if available) for each set. The PyTorch \"Dataset\" class is essential for efficient and organized data handling in machine learning tasks. It provides a standardized interface to load and preprocess data samples from various sources. Encapsulating the dataset into a single object simplifies data management and enables seamless integration with other PyTorch components like data loaders and models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LightningDataModule class to Load dataset\n",
    "This class inherits from Lightning’s LightningDataModule class and encapsulates the following steps:\n",
    "\n",
    "1) Download the dataset.\n",
    "2) Create train and validation splits.\n",
    "3) Create a Dataset class object for each split with appropriate transformations.\n",
    "4) Create DataLoader objects for each split.\n",
    "\n",
    "The class methods are defined to do the following tasks:\n",
    "1) prepare_data(..): This method is used for data preparation, like downloading and one-time preprocessing with the dataset. When training on a distributed GPU, this will be called from a single GPU.\n",
    "2) setup(...):  When you want to perform data operations on every GPU, this method is apt for it will call from every GPU. For example, perform train/val/test splits.\n",
    "3) train_dataloader(...): This method returns the train dataloader.\n",
    "4) val_dataloader(...): This method returns validation dataloader(s).\n",
    "5) test_dataloader(...):  This method returns test dataloader(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, *, num_classes=10, valid_pct=0.1, resize_to=(384, 384), batch_size=32, num_workers=0, pin_memory=False, shuffle_validation=False,):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.valid_pct = valid_pct\n",
    "        self.resize_to = resize_to\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.shuffle_validation = shuffle_validation\n",
    "\n",
    "        self.train_tfms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(Config.input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(Config.image_mean, Config.image_std)\n",
    "        ])\n",
    "\n",
    "        self.valid_tfms = transforms.Compose([\n",
    "            transforms.Resize(Config.resize),\n",
    "            transforms.CenterCrop(Config.input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(Config.image_mean, Config.image_std)\n",
    "        ])\n",
    "        \n",
    "        self.test_tfms = transforms.Compose([\n",
    "            transforms.Resize(Config.resize),\n",
    "            transforms.CenterCrop(Config.input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(Config.image_mean, Config.image_std)\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not os.path.exists(dataset_full_path):\n",
    "            kaggle.api.dataset_download_files('apollo2506/eurosat-dataset', path=dataset_full_path, unzip=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(self.valid_ds, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dm = EuroSATDataModule(\n",
    "    num_classes=Config.num_classes,\n",
    "    batch_size=32, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Donwload dataset.\n",
    "dm.prepare_data()\n",
    "\n",
    "# Split datset into training, validation set.\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Label</th>\n",
       "      <th>ClassName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16257</td>\n",
       "      <td>AnnualCrop/AnnualCrop_142.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3297</td>\n",
       "      <td>HerbaceousVegetation/HerbaceousVegetation_2835...</td>\n",
       "      <td>2</td>\n",
       "      <td>HerbaceousVegetation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17881</td>\n",
       "      <td>PermanentCrop/PermanentCrop_1073.jpg</td>\n",
       "      <td>6</td>\n",
       "      <td>PermanentCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2223</td>\n",
       "      <td>Industrial/Industrial_453.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4887</td>\n",
       "      <td>HerbaceousVegetation/HerbaceousVegetation_1810...</td>\n",
       "      <td>2</td>\n",
       "      <td>HerbaceousVegetation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Filename  Label  \\\n",
       "0       16257                      AnnualCrop/AnnualCrop_142.jpg      0   \n",
       "1        3297  HerbaceousVegetation/HerbaceousVegetation_2835...      2   \n",
       "2       17881               PermanentCrop/PermanentCrop_1073.jpg      6   \n",
       "3        2223                      Industrial/Industrial_453.jpg      4   \n",
       "4        4887  HerbaceousVegetation/HerbaceousVegetation_1810...      2   \n",
       "\n",
       "              ClassName  \n",
       "0            AnnualCrop  \n",
       "1  HerbaceousVegetation  \n",
       "2         PermanentCrop  \n",
       "3            Industrial  \n",
       "4  HerbaceousVegetation  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(Config.train_path)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassName\n",
      "AnnualCrop              2100\n",
      "HerbaceousVegetation    2100\n",
      "Residential             2100\n",
      "SeaLake                 2100\n",
      "Forest                  2100\n",
      "PermanentCrop           1750\n",
      "Industrial              1750\n",
      "Highway                 1750\n",
      "River                   1750\n",
      "Pasture                 1400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(Config.train_path)\n",
    "category_counts = data_df['ClassName'].value_counts()\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Lightning Module Class for the Model\n",
    "Create a class that contains the following main methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(EuroSATModel, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=Config.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        return super().on_train_epoch_end()\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        return super().on_validation_epoch_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
