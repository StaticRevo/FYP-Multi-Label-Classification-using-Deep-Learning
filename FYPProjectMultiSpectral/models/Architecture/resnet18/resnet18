digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2031118233200 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2031105977472 [label=AddmmBackward0]
	2031105977280 -> 2031105977472
	2031115755664 [label="fc.bias
 (19)" fillcolor=lightblue]
	2031115755664 -> 2031105977280
	2031105977280 [label=AccumulateGrad]
	2031105977184 -> 2031105977472
	2031105977184 [label=ViewBackward0]
	2031105975744 -> 2031105977184
	2031105975744 [label=MeanBackward1]
	2031105976464 -> 2031105975744
	2031105976464 [label=ReluBackward0]
	2031105976224 -> 2031105976464
	2031105976224 [label=AddBackward0]
	2031105976176 -> 2031105976224
	2031105976176 [label=CudnnBatchNormBackward0]
	2031105975792 -> 2031105976176
	2031105975792 [label=ConvolutionBackward0]
	2031105975360 -> 2031105975792
	2031105975360 [label=ReluBackward0]
	2031105975120 -> 2031105975360
	2031105975120 [label=CudnnBatchNormBackward0]
	2031105975312 -> 2031105975120
	2031105975312 [label=ConvolutionBackward0]
	2031105975648 -> 2031105975312
	2031105975648 [label=ReluBackward0]
	2031105973920 -> 2031105975648
	2031105973920 [label=AddBackward0]
	2031105974160 -> 2031105973920
	2031105974160 [label=CudnnBatchNormBackward0]
	2031105974448 -> 2031105974160
	2031105974448 [label=ConvolutionBackward0]
	2031105974496 -> 2031105974448
	2031105974496 [label=ReluBackward0]
	2031105974304 -> 2031105974496
	2031105974304 [label=CudnnBatchNormBackward0]
	2031105974256 -> 2031105974304
	2031105974256 [label=ConvolutionBackward0]
	2031105973584 -> 2031105974256
	2031105973584 [label=ReluBackward0]
	2031105972192 -> 2031105973584
	2031105972192 [label=AddBackward0]
	2031105973056 -> 2031105972192
	2031105973056 [label=CudnnBatchNormBackward0]
	2031105971040 -> 2031105973056
	2031105971040 [label=ConvolutionBackward0]
	2031105972000 -> 2031105971040
	2031105972000 [label=ReluBackward0]
	2031105972816 -> 2031105972000
	2031105972816 [label=CudnnBatchNormBackward0]
	2031105972624 -> 2031105972816
	2031105972624 [label=ConvolutionBackward0]
	2031105973248 -> 2031105972624
	2031105973248 [label=ReluBackward0]
	2031105978048 -> 2031105973248
	2031105978048 [label=AddBackward0]
	2031105973392 -> 2031105978048
	2031105973392 [label=CudnnBatchNormBackward0]
	2031105978768 -> 2031105973392
	2031105978768 [label=ConvolutionBackward0]
	2031105969216 -> 2031105978768
	2031105969216 [label=ReluBackward0]
	2031105978336 -> 2031105969216
	2031105978336 [label=CudnnBatchNormBackward0]
	2031105981600 -> 2031105978336
	2031105981600 [label=ConvolutionBackward0]
	2031105969600 -> 2031105981600
	2031105969600 [label=ReluBackward0]
	2031105970752 -> 2031105969600
	2031105970752 [label=AddBackward0]
	2031105980832 -> 2031105970752
	2031105980832 [label=CudnnBatchNormBackward0]
	2031105973104 -> 2031105980832
	2031105973104 [label=ConvolutionBackward0]
	2031105978672 -> 2031105973104
	2031105978672 [label=ReluBackward0]
	2031105978000 -> 2031105978672
	2031105978000 [label=CudnnBatchNormBackward0]
	2031105977808 -> 2031105978000
	2031105977808 [label=ConvolutionBackward0]
	2031105978912 -> 2031105977808
	2031105978912 [label=ReluBackward0]
	2031105972240 -> 2031105978912
	2031105972240 [label=AddBackward0]
	2031105969888 -> 2031105972240
	2031105969888 [label=CudnnBatchNormBackward0]
	2031105969792 -> 2031105969888
	2031105969792 [label=ConvolutionBackward0]
	2031105971664 -> 2031105969792
	2031105971664 [label=ReluBackward0]
	2031105971376 -> 2031105971664
	2031105971376 [label=CudnnBatchNormBackward0]
	2031105971232 -> 2031105971376
	2031105971232 [label=ConvolutionBackward0]
	2031105970944 -> 2031105971232
	2031105970944 [label=ReluBackward0]
	2031105970848 -> 2031105970944
	2031105970848 [label=AddBackward0]
	2031105970608 -> 2031105970848
	2031105970608 [label=CudnnBatchNormBackward0]
	2031105970368 -> 2031105970608
	2031105970368 [label=ConvolutionBackward0]
	2031105969744 -> 2031105970368
	2031105969744 [label=ReluBackward0]
	2031105969648 -> 2031105969744
	2031105969648 [label=CudnnBatchNormBackward0]
	2031105969456 -> 2031105969648
	2031105969456 [label=ConvolutionBackward0]
	2031105970656 -> 2031105969456
	2031105970656 [label=ReluBackward0]
	2031105968976 -> 2031105970656
	2031105968976 [label=AddBackward0]
	2031105967872 -> 2031105968976
	2031105967872 [label=CudnnBatchNormBackward0]
	2031105968832 -> 2031105967872
	2031105968832 [label=ConvolutionBackward0]
	2031105968640 -> 2031105968832
	2031105968640 [label=ReluBackward0]
	2031105968544 -> 2031105968640
	2031105968544 [label=CudnnBatchNormBackward0]
	2031105968352 -> 2031105968544
	2031105968352 [label=ConvolutionBackward0]
	2031105967824 -> 2031105968352
	2031105967824 [label=MaxPool2DWithIndicesBackward0]
	2031105968016 -> 2031105967824
	2031105968016 [label=ReluBackward0]
	2031105967440 -> 2031105968016
	2031105967440 [label=CudnnBatchNormBackward0]
	2031105967200 -> 2031105967440
	2031105967200 [label=ConvolutionBackward0]
	2031105966816 -> 2031105967200
	2031107189904 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2031107189904 -> 2031105966816
	2031105966816 [label=AccumulateGrad]
	2031105967152 -> 2031105967440
	2031077474576 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2031077474576 -> 2031105967152
	2031105967152 [label=AccumulateGrad]
	2031105968304 -> 2031105967440
	2031077474672 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2031077474672 -> 2031105968304
	2031105968304 [label=AccumulateGrad]
	2031105968064 -> 2031105968352
	2031008317264 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2031008317264 -> 2031105968064
	2031105968064 [label=AccumulateGrad]
	2031105968400 -> 2031105968544
	2031008317360 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2031008317360 -> 2031105968400
	2031105968400 [label=AccumulateGrad]
	2031105968592 -> 2031105968544
	2031107178576 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2031107178576 -> 2031105968592
	2031105968592 [label=AccumulateGrad]
	2031105968736 -> 2031105968832
	2031107178960 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2031107178960 -> 2031105968736
	2031105968736 [label=AccumulateGrad]
	2031105967680 -> 2031105967872
	2031107179056 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2031107179056 -> 2031105967680
	2031105967680 [label=AccumulateGrad]
	2031105967392 -> 2031105967872
	2031107179152 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2031107179152 -> 2031105967392
	2031105967392 [label=AccumulateGrad]
	2031105967824 -> 2031105968976
	2031105969024 -> 2031105969456
	2031107179536 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2031107179536 -> 2031105969024
	2031105969024 [label=AccumulateGrad]
	2031105969504 -> 2031105969648
	2031107179632 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2031107179632 -> 2031105969504
	2031105969504 [label=AccumulateGrad]
	2031105969696 -> 2031105969648
	2031107179728 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2031107179728 -> 2031105969696
	2031105969696 [label=AccumulateGrad]
	2031105970272 -> 2031105970368
	2031107180112 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2031107180112 -> 2031105970272
	2031105970272 [label=AccumulateGrad]
	2031105970560 -> 2031105970608
	2031107180208 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2031107180208 -> 2031105970560
	2031105970560 [label=AccumulateGrad]
	2031105970512 -> 2031105970608
	2031107180304 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2031107180304 -> 2031105970512
	2031105970512 [label=AccumulateGrad]
	2031105970656 -> 2031105970848
	2031105971136 -> 2031105971232
	2031107181264 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2031107181264 -> 2031105971136
	2031105971136 [label=AccumulateGrad]
	2031105971424 -> 2031105971376
	2031107181360 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2031107181360 -> 2031105971424
	2031105971424 [label=AccumulateGrad]
	2031105971520 -> 2031105971376
	2031107181456 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2031107181456 -> 2031105971520
	2031105971520 [label=AccumulateGrad]
	2031105971616 -> 2031105969792
	2031107181840 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2031107181840 -> 2031105971616
	2031105971616 [label=AccumulateGrad]
	2031105971856 -> 2031105969888
	2031107181936 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2031107181936 -> 2031105971856
	2031105971856 [label=AccumulateGrad]
	2031105969840 -> 2031105969888
	2031107182032 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2031107182032 -> 2031105969840
	2031105969840 [label=AccumulateGrad]
	2031105972288 -> 2031105972240
	2031105972288 [label=CudnnBatchNormBackward0]
	2031105971088 -> 2031105972288
	2031105971088 [label=ConvolutionBackward0]
	2031105970944 -> 2031105971088
	2031105970896 -> 2031105971088
	2031107180688 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2031107180688 -> 2031105970896
	2031105970896 [label=AccumulateGrad]
	2031105971712 -> 2031105972288
	2031107180784 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2031107180784 -> 2031105971712
	2031105971712 [label=AccumulateGrad]
	2031105971760 -> 2031105972288
	2031107180880 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2031107180880 -> 2031105971760
	2031105971760 [label=AccumulateGrad]
	2031105980064 -> 2031105977808
	2031107182416 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2031107182416 -> 2031105980064
	2031105980064 [label=AccumulateGrad]
	2031105977952 -> 2031105978000
	2031107182512 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2031107182512 -> 2031105977952
	2031105977952 [label=AccumulateGrad]
	2031105978576 -> 2031105978000
	2031107182608 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2031107182608 -> 2031105978576
	2031105978576 [label=AccumulateGrad]
	2031105978192 -> 2031105973104
	2031107182992 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2031107182992 -> 2031105978192
	2031105978192 [label=AccumulateGrad]
	2031105970416 -> 2031105980832
	2031107183088 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2031107183088 -> 2031105970416
	2031105970416 [label=AccumulateGrad]
	2031105981264 -> 2031105980832
	2031107183184 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2031107183184 -> 2031105981264
	2031105981264 [label=AccumulateGrad]
	2031105978912 -> 2031105970752
	2031105981024 -> 2031105981600
	2031107184144 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2031107184144 -> 2031105981024
	2031105981024 [label=AccumulateGrad]
	2031105979056 -> 2031105978336
	2031107184240 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2031107184240 -> 2031105979056
	2031105979056 [label=AccumulateGrad]
	2031105980160 -> 2031105978336
	2031107184336 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2031107184336 -> 2031105980160
	2031105980160 [label=AccumulateGrad]
	2031105980640 -> 2031105978768
	2031107184720 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2031107184720 -> 2031105980640
	2031105980640 [label=AccumulateGrad]
	2031105972672 -> 2031105973392
	2031107184816 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2031107184816 -> 2031105972672
	2031105972672 [label=AccumulateGrad]
	2031105977376 -> 2031105973392
	2031107184912 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2031107184912 -> 2031105977376
	2031105977376 [label=AccumulateGrad]
	2031105980400 -> 2031105978048
	2031105980400 [label=CudnnBatchNormBackward0]
	2031105979968 -> 2031105980400
	2031105979968 [label=ConvolutionBackward0]
	2031105969600 -> 2031105979968
	2031105967728 -> 2031105979968
	2031107183568 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2031107183568 -> 2031105967728
	2031105967728 [label=AccumulateGrad]
	2031105976704 -> 2031105980400
	2031107183664 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2031107183664 -> 2031105976704
	2031105976704 [label=AccumulateGrad]
	2031105967536 -> 2031105980400
	2031107183760 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2031107183760 -> 2031105967536
	2031105967536 [label=AccumulateGrad]
	2031105972384 -> 2031105972624
	2031107185296 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2031107185296 -> 2031105972384
	2031105972384 [label=AccumulateGrad]
	2031105972864 -> 2031105972816
	2031107185392 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2031107185392 -> 2031105972864
	2031105972864 [label=AccumulateGrad]
	2031105972960 -> 2031105972816
	2031107185488 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2031107185488 -> 2031105972960
	2031105972960 [label=AccumulateGrad]
	2031105972048 -> 2031105971040
	2031107185872 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2031107185872 -> 2031105972048
	2031105972048 [label=AccumulateGrad]
	2031105972768 -> 2031105973056
	2031107185968 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2031107185968 -> 2031105972768
	2031105972768 [label=AccumulateGrad]
	2031105970128 -> 2031105973056
	2031107186064 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2031107186064 -> 2031105970128
	2031105970128 [label=AccumulateGrad]
	2031105973248 -> 2031105972192
	2031105973488 -> 2031105974256
	2031107187024 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2031107187024 -> 2031105973488
	2031105973488 [label=AccumulateGrad]
	2031105974208 -> 2031105974304
	2031107187120 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2031107187120 -> 2031105974208
	2031105974208 [label=AccumulateGrad]
	2031105974544 -> 2031105974304
	2031107187216 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2031107187216 -> 2031105974544
	2031105974544 [label=AccumulateGrad]
	2031105974592 -> 2031105974448
	2031107187600 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2031107187600 -> 2031105974592
	2031105974592 [label=AccumulateGrad]
	2031105973536 -> 2031105974160
	2031107187696 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2031107187696 -> 2031105973536
	2031105973536 [label=AccumulateGrad]
	2031105973776 -> 2031105974160
	2031107187792 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2031107187792 -> 2031105973776
	2031105973776 [label=AccumulateGrad]
	2031105973872 -> 2031105973920
	2031105973872 [label=CudnnBatchNormBackward0]
	2031105973632 -> 2031105973872
	2031105973632 [label=ConvolutionBackward0]
	2031105973584 -> 2031105973632
	2031105973344 -> 2031105973632
	2031107186448 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2031107186448 -> 2031105973344
	2031105973344 [label=AccumulateGrad]
	2031105974640 -> 2031105973872
	2031107186544 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2031107186544 -> 2031105974640
	2031105974640 [label=AccumulateGrad]
	2031105973824 -> 2031105973872
	2031107186640 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2031107186640 -> 2031105973824
	2031105973824 [label=AccumulateGrad]
	2031105975024 -> 2031105975312
	2031107188176 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2031107188176 -> 2031105975024
	2031105975024 [label=AccumulateGrad]
	2031105975072 -> 2031105975120
	2031107188272 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2031107188272 -> 2031105975072
	2031105975072 [label=AccumulateGrad]
	2031105975600 -> 2031105975120
	2031107188368 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2031107188368 -> 2031105975600
	2031105975600 [label=AccumulateGrad]
	2031105975408 -> 2031105975792
	2031107188752 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2031107188752 -> 2031105975408
	2031105975408 [label=AccumulateGrad]
	2031105975936 -> 2031105976176
	2031107188848 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2031107188848 -> 2031105975936
	2031105975936 [label=AccumulateGrad]
	2031105975984 -> 2031105976176
	2031107188944 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2031107188944 -> 2031105975984
	2031105975984 [label=AccumulateGrad]
	2031105975648 -> 2031105976224
	2031105977232 -> 2031105977472
	2031105977232 [label=TBackward0]
	2031105976272 -> 2031105977232
	2031107189616 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2031107189616 -> 2031105976272
	2031105976272 [label=AccumulateGrad]
	2031105977472 -> 2031118233200
}
