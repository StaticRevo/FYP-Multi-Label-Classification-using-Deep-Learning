digraph {
	graph [size="86.7,86.7"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1633129586448 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1633129837808 [label=AddmmBackward0]
	1633129837472 -> 1633129837808
	1633146914480 [label="29.bias
 (19)" fillcolor=lightblue]
	1633146914480 -> 1633129837472
	1633129837472 [label=AccumulateGrad]
	1633129836848 -> 1633129837808
	1633129836848 [label=NativeDropoutBackward0]
	1633129836368 -> 1633129836848
	1633129836368 [label=ViewBackward0]
	1633129836032 -> 1633129836368
	1633129836032 [label=MeanBackward1]
	1633129835552 -> 1633129836032
	1633129835552 [label=MulBackward0]
	1633129835072 -> 1633129835552
	1633129835072 [label=MulBackward0]
	1633129833968 -> 1633129835072
	1633129833968 [label=ReluBackward0]
	1633129833632 -> 1633129833968
	1633129833632 [label=AddBackward0]
	1633129833152 -> 1633129833632
	1633129833152 [label=CudnnBatchNormBackward0]
	1633129832048 -> 1633129833152
	1633129832048 [label=ConvolutionBackward0]
	1633129831088 -> 1633129832048
	1633129831088 [label=ReluBackward0]
	1633129830752 -> 1633129831088
	1633129830752 [label=CudnnBatchNormBackward0]
	1633129830272 -> 1633129830752
	1633129830272 [label=ConvolutionBackward0]
	1633129833008 -> 1633129830272
	1633129833008 [label=ReluBackward0]
	1633129828832 -> 1633129833008
	1633129828832 [label=CudnnBatchNormBackward0]
	1633129828352 -> 1633129828832
	1633129828352 [label=ConvolutionBackward0]
	1633129827392 -> 1633129828352
	1633129827392 [label=ConvolutionBackward0]
	1633129826288 -> 1633129827392
	1633129826288 [label=MulBackward0]
	1633129825952 -> 1633129826288
	1633129825952 [label=ReluBackward0]
	1633129824848 -> 1633129825952
	1633129824848 [label=AddBackward0]
	1633129824368 -> 1633129824848
	1633129824368 [label=CudnnBatchNormBackward0]
	1633129840112 -> 1633129824368
	1633129840112 [label=ConvolutionBackward0]
	1633129839968 -> 1633129840112
	1633129839968 [label=ReluBackward0]
	1633129839776 -> 1633129839968
	1633129839776 [label=CudnnBatchNormBackward0]
	1633129839440 -> 1633129839776
	1633129839440 [label=ConvolutionBackward0]
	1633129824992 -> 1633129839440
	1633129824992 [label=ConvolutionBackward0]
	1633129839296 -> 1633129824992
	1633129839296 [label=CatBackward0]
	1633129839200 -> 1633129839296
	1633129839200 [label=ConvolutionBackward0]
	1633129838816 -> 1633129839200
	1633129838816 [label=ReluBackward0]
	1633129838720 -> 1633129838816
	1633129838720 [label=CudnnBatchNormBackward0]
	1633129838624 -> 1633129838720
	1633129838624 [label=ConvolutionBackward0]
	1633129838000 -> 1633129838624
	1633129838000 [label=ConvolutionBackward0]
	1633129838048 -> 1633129838000
	1633129838048 [label=MulBackward0]
	1633129837856 -> 1633129838048
	1633129837856 [label=ReluBackward0]
	1633129837616 -> 1633129837856
	1633129837616 [label=AddBackward0]
	1633129837568 -> 1633129837616
	1633129837568 [label=CudnnBatchNormBackward0]
	1633129837376 -> 1633129837568
	1633129837376 [label=ConvolutionBackward0]
	1633129837280 -> 1633129837376
	1633129837280 [label=ReluBackward0]
	1633129836944 -> 1633129837280
	1633129836944 [label=CudnnBatchNormBackward0]
	1633129836752 -> 1633129836944
	1633129836752 [label=ConvolutionBackward0]
	1633129837760 -> 1633129836752
	1633129837760 [label=ConvolutionBackward0]
	1633129836464 -> 1633129837760
	1633129836464 [label=CatBackward0]
	1633129836176 -> 1633129836464
	1633129836176 [label=ConvolutionBackward0]
	1633129835984 -> 1633129836176
	1633129835984 [label=ReluBackward0]
	1633129835696 -> 1633129835984
	1633129835696 [label=CudnnBatchNormBackward0]
	1633129835648 -> 1633129835696
	1633129835648 [label=ConvolutionBackward0]
	1633129835312 -> 1633129835648
	1633129835312 [label=ConvolutionBackward0]
	1633129835360 -> 1633129835312
	1633129835360 [label=MulBackward0]
	1633129835024 -> 1633129835360
	1633129835024 [label=MulBackward0]
	1633129834640 -> 1633129835024
	1633129834640 [label=MulBackward0]
	1633129834688 -> 1633129834640
	1633129834688 [label=ReluBackward0]
	1633129834496 -> 1633129834688
	1633129834496 [label=AddBackward0]
	1633129834160 -> 1633129834496
	1633129834160 [label=CudnnBatchNormBackward0]
	1633129834208 -> 1633129834160
	1633129834208 [label=ConvolutionBackward0]
	1633129833872 -> 1633129834208
	1633129833872 [label=ReluBackward0]
	1633129833920 -> 1633129833872
	1633129833920 [label=CudnnBatchNormBackward0]
	1633129833824 -> 1633129833920
	1633129833824 [label=ConvolutionBackward0]
	1633129834352 -> 1633129833824
	1633129834352 [label=ReluBackward0]
	1633129833440 -> 1633129834352
	1633129833440 [label=CudnnBatchNormBackward0]
	1633129833344 -> 1633129833440
	1633129833344 [label=ConvolutionBackward0]
	1633129832720 -> 1633129833344
	1633129832720 [label=ConvolutionBackward0]
	1633129832768 -> 1633129832720
	1633129832768 [label=ReluBackward0]
	1633129832576 -> 1633129832768
	1633129832576 [label=CudnnBatchNormBackward0]
	1633129832240 -> 1633129832576
	1633129832240 [label=ConvolutionBackward0]
	1633129832384 -> 1633129832240
	1633159915440 [label="0.weight
 (16, 12, 1, 1)" fillcolor=lightblue]
	1633159915440 -> 1633129832384
	1633129832384 [label=AccumulateGrad]
	1633129832432 -> 1633129832576
	1633159914672 [label="1.weight
 (16)" fillcolor=lightblue]
	1633159914672 -> 1633129832432
	1633129832432 [label=AccumulateGrad]
	1633129832864 -> 1633129832576
	1633159914384 [label="1.bias
 (16)" fillcolor=lightblue]
	1633159914384 -> 1633129832864
	1633129832864 [label=AccumulateGrad]
	1633129832960 -> 1633129832720
	1633159914864 [label="3.depthwise.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	1633159914864 -> 1633129832960
	1633129832960 [label=AccumulateGrad]
	1633129832912 -> 1633129833344
	1633159914960 [label="3.pointwise.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	1633159914960 -> 1633129832912
	1633129832912 [label=AccumulateGrad]
	1633129833248 -> 1633129833440
	1633159915056 [label="4.weight
 (32)" fillcolor=lightblue]
	1633159915056 -> 1633129833248
	1633129833248 [label=AccumulateGrad]
	1633129833536 -> 1633129833440
	1633159915152 [label="4.bias
 (32)" fillcolor=lightblue]
	1633159915152 -> 1633129833536
	1633129833536 [label=AccumulateGrad]
	1633129833200 -> 1633129833824
	1633159914192 [label="6.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1633159914192 -> 1633129833200
	1633129833200 [label=AccumulateGrad]
	1633129833728 -> 1633129833920
	1633159913424 [label="6.bn1.weight
 (32)" fillcolor=lightblue]
	1633159913424 -> 1633129833728
	1633129833728 [label=AccumulateGrad]
	1633129833680 -> 1633129833920
	1633159913136 [label="6.bn1.bias
 (32)" fillcolor=lightblue]
	1633159913136 -> 1633129833680
	1633129833680 [label=AccumulateGrad]
	1633129834016 -> 1633129834208
	1633159913616 [label="6.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1633159913616 -> 1633129834016
	1633129834016 [label=AccumulateGrad]
	1633129834400 -> 1633129834160
	1633159913712 [label="6.bn2.weight
 (32)" fillcolor=lightblue]
	1633159913712 -> 1633129834400
	1633129834400 [label=AccumulateGrad]
	1633129834256 -> 1633129834160
	1633159913808 [label="6.bn2.bias
 (32)" fillcolor=lightblue]
	1633159913808 -> 1633129834256
	1633129834256 [label=AccumulateGrad]
	1633129834352 -> 1633129834496
	1633129834880 -> 1633129834640
	1633129834880 [label=ViewBackward0]
	1633129834304 -> 1633129834880
	1633129834304 [label=SigmoidBackward0]
	1633129833392 -> 1633129834304
	1633129833392 [label=AddmmBackward0]
	1633129833776 -> 1633129833392
	1633159911792 [label="7.fc2.bias
 (32)" fillcolor=lightblue]
	1633159911792 -> 1633129833776
	1633129833776 [label=AccumulateGrad]
	1633129833584 -> 1633129833392
	1633129833584 [label=ReluBackward0]
	1633129833104 -> 1633129833584
	1633129833104 [label=AddmmBackward0]
	1633129832816 -> 1633129833104
	1633159912944 [label="7.fc1.bias
 (2)" fillcolor=lightblue]
	1633159912944 -> 1633129832816
	1633129832816 [label=AccumulateGrad]
	1633129832288 -> 1633129833104
	1633129832288 [label=MeanBackward1]
	1633129832096 -> 1633129832288
	1633129832096 [label=ViewBackward0]
	1633129834688 -> 1633129832096
	1633129832336 -> 1633129833104
	1633129832336 [label=TBackward0]
	1633129831952 -> 1633129832336
	1633159913040 [label="7.fc1.weight
 (2, 32)" fillcolor=lightblue]
	1633159913040 -> 1633129831952
	1633129831952 [label=AccumulateGrad]
	1633129834544 -> 1633129833392
	1633129834544 [label=TBackward0]
	1633129831760 -> 1633129834544
	1633159912176 [label="7.fc2.weight
 (32, 2)" fillcolor=lightblue]
	1633159912176 -> 1633129831760
	1633129831760 [label=AccumulateGrad]
	1633129834832 -> 1633129835024
	1633129834832 [label=SigmoidBackward0]
	1633129834064 -> 1633129834832
	1633129834064 [label=ConvolutionBackward0]
	1633129832624 -> 1633129834064
	1633129832624 [label=SplitWithSizesBackward0]
	1633129833296 -> 1633129832624
	1633129833296 [label=ReluBackward0]
	1633129832000 -> 1633129833296
	1633129832000 [label=CudnnBatchNormBackward0]
	1633129831904 -> 1633129832000
	1633129831904 [label=ConvolutionBackward0]
	1633129831280 -> 1633129831904
	1633129831280 [label=CatBackward0]
	1633129831328 -> 1633129831280
	1633129831328 [label=AdaptiveAvgPool2DBackward0]
	1633129834640 -> 1633129831328
	1633129831520 -> 1633129831280
	1633129831520 [label=PermuteBackward0]
	1633129831424 -> 1633129831520
	1633129831424 [label=AdaptiveAvgPool2DBackward0]
	1633129834640 -> 1633129831424
	1633129831472 -> 1633129831904
	1633159912656 [label="8.conv1.weight
 (8, 32, 1, 1)" fillcolor=lightblue]
	1633159912656 -> 1633129831472
	1633129831472 [label=AccumulateGrad]
	1633129831808 -> 1633129832000
	1633159910256 [label="8.bn1.weight
 (8)" fillcolor=lightblue]
	1633159910256 -> 1633129831808
	1633129831808 [label=AccumulateGrad]
	1633129832480 -> 1633129832000
	1633159910448 [label="8.bn1.bias
 (8)" fillcolor=lightblue]
	1633159910448 -> 1633129832480
	1633129832480 [label=AccumulateGrad]
	1633129833056 -> 1633129834064
	1633159912560 [label="8.conv_h.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1633159912560 -> 1633129833056
	1633129833056 [label=AccumulateGrad]
	1633129835264 -> 1633129835360
	1633129835264 [label=PermuteBackward0]
	1633129834736 -> 1633129835264
	1633129834736 [label=SigmoidBackward0]
	1633129832144 -> 1633129834736
	1633129832144 [label=ConvolutionBackward0]
	1633129832624 -> 1633129832144
	1633129831616 -> 1633129832144
	1633159912848 [label="8.conv_w.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1633159912848 -> 1633129831616
	1633129831616 [label=AccumulateGrad]
	1633129835216 -> 1633129835312
	1633159912752 [label="9.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	1633159912752 -> 1633129835216
	1633129835216 [label=AccumulateGrad]
	1633129835456 -> 1633129835648
	1633147086768 [label="9.pointwise.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	1633147086768 -> 1633129835456
	1633129835456 [label=AccumulateGrad]
	1633129835840 -> 1633129835696
	1633147086864 [label="10.weight
 (64)" fillcolor=lightblue]
	1633147086864 -> 1633129835840
	1633129835840 [label=AccumulateGrad]
	1633129835792 -> 1633129835696
	1633147086960 [label="10.bias
 (64)" fillcolor=lightblue]
	1633147086960 -> 1633129835792
	1633129835792 [label=AccumulateGrad]
	1633129836224 -> 1633129836176
	1633147087344 [label="12.conv_dil1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1633147087344 -> 1633129836224
	1633129836224 [label=AccumulateGrad]
	1633129836128 -> 1633129836176
	1633147087440 [label="12.conv_dil1.bias
 (64)" fillcolor=lightblue]
	1633147087440 -> 1633129836128
	1633129836128 [label=AccumulateGrad]
	1633129836080 -> 1633129836464
	1633129836080 [label=ConvolutionBackward0]
	1633129835984 -> 1633129836080
	1633129835744 -> 1633129836080
	1633147087248 [label="12.conv_dil2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1633147087248 -> 1633129835744
	1633129835744 [label=AccumulateGrad]
	1633129835600 -> 1633129836080
	1633147087536 [label="12.conv_dil2.bias
 (64)" fillcolor=lightblue]
	1633147087536 -> 1633129835600
	1633129835600 [label=AccumulateGrad]
	1633129836272 -> 1633129836464
	1633129836272 [label=ConvolutionBackward0]
	1633129835984 -> 1633129836272
	1633129834784 -> 1633129836272
	1633147087632 [label="12.conv_dil3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1633147087632 -> 1633129834784
	1633129834784 [label=AccumulateGrad]
	1633129835504 -> 1633129836272
	1633147087728 [label="12.conv_dil3.bias
 (64)" fillcolor=lightblue]
	1633147087728 -> 1633129835504
	1633129835504 [label=AccumulateGrad]
	1633129836704 -> 1633129837760
	1633147087824 [label="12.fuse.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	1633147087824 -> 1633129836704
	1633129836704 [label=AccumulateGrad]
	1633129836656 -> 1633129837760
	1633147087920 [label="12.fuse.bias
 (64)" fillcolor=lightblue]
	1633147087920 -> 1633129836656
	1633129836656 [label=AccumulateGrad]
	1633129836608 -> 1633129836752
	1633147088016 [label="13.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1633147088016 -> 1633129836608
	1633129836608 [label=AccumulateGrad]
	1633129836896 -> 1633129836944
	1633147088112 [label="13.bn1.weight
 (64)" fillcolor=lightblue]
	1633147088112 -> 1633129836896
	1633129836896 [label=AccumulateGrad]
	1633129837088 -> 1633129836944
	1633147088208 [label="13.bn1.bias
 (64)" fillcolor=lightblue]
	1633147088208 -> 1633129837088
	1633129837088 [label=AccumulateGrad]
	1633129837136 -> 1633129837376
	1633147088592 [label="13.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1633147088592 -> 1633129837136
	1633129837136 [label=AccumulateGrad]
	1633129837424 -> 1633129837568
	1633147088688 [label="13.bn2.weight
 (64)" fillcolor=lightblue]
	1633147088688 -> 1633129837424
	1633129837424 [label=AccumulateGrad]
	1633129837664 -> 1633129837568
	1633147088784 [label="13.bn2.bias
 (64)" fillcolor=lightblue]
	1633147088784 -> 1633129837664
	1633129837664 [label=AccumulateGrad]
	1633129837760 -> 1633129837616
	1633129837904 -> 1633129838048
	1633129837904 [label=UnsqueezeBackward0]
	1633129837232 -> 1633129837904
	1633129837232 [label=UnsqueezeBackward0]
	1633129836800 -> 1633129837232
	1633129836800 [label=SigmoidBackward0]
	1633129837184 -> 1633129836800
	1633129837184 [label=SqueezeBackward1]
	1633129836320 -> 1633129837184
	1633129836320 [label=ConvolutionBackward0]
	1633129831664 -> 1633129836320
	1633129831664 [label=UnsqueezeBackward0]
	1633129831856 -> 1633129831664
	1633129831856 [label=SqueezeBackward1]
	1633129830800 -> 1633129831856
	1633129830800 [label=SqueezeBackward1]
	1633129831376 -> 1633129830800
	1633129831376 [label=MeanBackward1]
	1633129837856 -> 1633129831376
	1633129836416 -> 1633129836320
	1633147089168 [label="14.conv.weight
 (1, 1, 3)" fillcolor=lightblue]
	1633147089168 -> 1633129836416
	1633129836416 [label=AccumulateGrad]
	1633129838240 -> 1633129838000
	1633147089264 [label="15.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	1633147089264 -> 1633129838240
	1633129838240 [label=AccumulateGrad]
	1633129838192 -> 1633129838624
	1633147089360 [label="15.pointwise.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1633147089360 -> 1633129838192
	1633129838192 [label=AccumulateGrad]
	1633129838528 -> 1633129838720
	1633147089456 [label="16.weight
 (128)" fillcolor=lightblue]
	1633147089456 -> 1633129838528
	1633129838528 [label=AccumulateGrad]
	1633129838480 -> 1633129838720
	1633147089552 [label="16.bias
 (128)" fillcolor=lightblue]
	1633147089552 -> 1633129838480
	1633129838480 [label=AccumulateGrad]
	1633129838864 -> 1633129839200
	1633147089936 [label="18.conv_dil1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1633147089936 -> 1633129838864
	1633129838864 [label=AccumulateGrad]
	1633129839104 -> 1633129839200
	1633147090032 [label="18.conv_dil1.bias
 (128)" fillcolor=lightblue]
	1633147090032 -> 1633129839104
	1633129839104 [label=AccumulateGrad]
	1633129839056 -> 1633129839296
	1633129839056 [label=ConvolutionBackward0]
	1633129838816 -> 1633129839056
	1633129838384 -> 1633129839056
	1633147090128 [label="18.conv_dil2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1633147090128 -> 1633129838384
	1633129838384 [label=AccumulateGrad]
	1633129838576 -> 1633129839056
	1633147090224 [label="18.conv_dil2.bias
 (128)" fillcolor=lightblue]
	1633147090224 -> 1633129838576
	1633129838576 [label=AccumulateGrad]
	1633129838960 -> 1633129839296
	1633129838960 [label=ConvolutionBackward0]
	1633129838816 -> 1633129838960
	1633129837712 -> 1633129838960
	1633147090320 [label="18.conv_dil3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1633147090320 -> 1633129837712
	1633129837712 [label=AccumulateGrad]
	1633129838336 -> 1633129838960
	1633147090416 [label="18.conv_dil3.bias
 (128)" fillcolor=lightblue]
	1633147090416 -> 1633129838336
	1633129838336 [label=AccumulateGrad]
	1633129839344 -> 1633129824992
	1633147090512 [label="18.fuse.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	1633147090512 -> 1633129839344
	1633129839344 [label=AccumulateGrad]
	1633129839680 -> 1633129824992
	1633147090608 [label="18.fuse.bias
 (128)" fillcolor=lightblue]
	1633147090608 -> 1633129839680
	1633129839680 [label=AccumulateGrad]
	1633129839584 -> 1633129839440
	1633147090704 [label="19.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1633147090704 -> 1633129839584
	1633129839584 [label=AccumulateGrad]
	1633129839632 -> 1633129839776
	1633147090800 [label="19.bn1.weight
 (128)" fillcolor=lightblue]
	1633147090800 -> 1633129839632
	1633129839632 [label=AccumulateGrad]
	1633129840064 -> 1633129839776
	1633147090896 [label="19.bn1.bias
 (128)" fillcolor=lightblue]
	1633147090896 -> 1633129840064
	1633129840064 [label=AccumulateGrad]
	1633129840160 -> 1633129840112
	1633147091280 [label="19.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1633147091280 -> 1633129840160
	1633129840160 [label=AccumulateGrad]
	1633129840592 -> 1633129824368
	1633147091376 [label="19.bn2.weight
 (128)" fillcolor=lightblue]
	1633147091376 -> 1633129840592
	1633129840592 [label=AccumulateGrad]
	1633129824512 -> 1633129824368
	1633147091472 [label="19.bn2.bias
 (128)" fillcolor=lightblue]
	1633147091472 -> 1633129824512
	1633129824512 [label=AccumulateGrad]
	1633129824992 -> 1633129824848
	1633129825808 -> 1633129826288
	1633129825808 [label=SigmoidBackward0]
	1633129839920 -> 1633129825808
	1633129839920 [label=ConvolutionBackward0]
	1633129839488 -> 1633129839920
	1633129839488 [label=NativeDropoutBackward0]
	1633129839008 -> 1633129839488
	1633129839008 [label=ReluBackward0]
	1633129837040 -> 1633129839008
	1633129837040 [label=ConvolutionBackward0]
	1633129838144 -> 1633129837040
	1633129838144 [label=MeanBackward1]
	1633129825952 -> 1633129838144
	1633129838096 -> 1633129837040
	1633147091856 [label="20.fc1.weight
 (8, 128, 1, 1)" fillcolor=lightblue]
	1633147091856 -> 1633129838096
	1633129838096 [label=AccumulateGrad]
	1633129839824 -> 1633129837040
	1633147091952 [label="20.fc1.bias
 (8)" fillcolor=lightblue]
	1633147091952 -> 1633129839824
	1633129839824 [label=AccumulateGrad]
	1633129840016 -> 1633129839920
	1633147092048 [label="20.fc2.weight
 (128, 8, 1, 1)" fillcolor=lightblue]
	1633147092048 -> 1633129840016
	1633129840016 [label=AccumulateGrad]
	1633129825472 -> 1633129839920
	1633147092144 [label="20.fc2.bias
 (128)" fillcolor=lightblue]
	1633147092144 -> 1633129825472
	1633129825472 [label=AccumulateGrad]
	1633129826912 -> 1633129827392
	1633147092240 [label="21.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	1633147092240 -> 1633129826912
	1633129826912 [label=AccumulateGrad]
	1633129827248 -> 1633129828352
	1633147092336 [label="21.pointwise.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1633147092336 -> 1633129827248
	1633129827248 [label=AccumulateGrad]
	1633129828208 -> 1633129828832
	1633147092432 [label="22.weight
 (256)" fillcolor=lightblue]
	1633147092432 -> 1633129828208
	1633129828208 [label=AccumulateGrad]
	1633129829792 -> 1633129828832
	1633147092528 [label="22.bias
 (256)" fillcolor=lightblue]
	1633147092528 -> 1633129829792
	1633129829792 [label=AccumulateGrad]
	1633129829312 -> 1633129830272
	1633147092912 [label="24.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1633147092912 -> 1633129829312
	1633129829312 [label=AccumulateGrad]
	1633129830128 -> 1633129830752
	1633146912848 [label="24.bn1.weight
 (256)" fillcolor=lightblue]
	1633146912848 -> 1633129830128
	1633129830128 [label=AccumulateGrad]
	1633129831232 -> 1633129830752
	1633146912944 [label="24.bn1.bias
 (256)" fillcolor=lightblue]
	1633146912944 -> 1633129831232
	1633129831232 [label=AccumulateGrad]
	1633129831712 -> 1633129832048
	1633146913328 [label="24.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1633146913328 -> 1633129831712
	1633129831712 [label=AccumulateGrad]
	1633129832672 -> 1633129833152
	1633146913424 [label="24.bn2.weight
 (256)" fillcolor=lightblue]
	1633146913424 -> 1633129832672
	1633129832672 [label=AccumulateGrad]
	1633129832528 -> 1633129833152
	1633146913520 [label="24.bn2.bias
 (256)" fillcolor=lightblue]
	1633146913520 -> 1633129832528
	1633129832528 [label=AccumulateGrad]
	1633129833008 -> 1633129833632
	1633129834592 -> 1633129835072
	1633129834592 [label=ViewBackward0]
	1633129832192 -> 1633129834592
	1633129832192 [label=SigmoidBackward0]
	1633129829168 -> 1633129832192
	1633129829168 [label=AddmmBackward0]
	1633129830608 -> 1633129829168
	1633146914192 [label="25.channel_att.fc2.bias
 (256)" fillcolor=lightblue]
	1633146914192 -> 1633129830608
	1633129830608 [label=AccumulateGrad]
	1633129829648 -> 1633129829168
	1633129829648 [label=ReluBackward0]
	1633129827728 -> 1633129829648
	1633129827728 [label=AddmmBackward0]
	1633129826768 -> 1633129827728
	1633146914000 [label="25.channel_att.fc1.bias
 (16)" fillcolor=lightblue]
	1633146914000 -> 1633129826768
	1633129826768 [label=AccumulateGrad]
	1633129839536 -> 1633129827728
	1633129839536 [label=MeanBackward1]
	1633129838672 -> 1633129839536
	1633129838672 [label=ViewBackward0]
	1633129833968 -> 1633129838672
	1633129825328 -> 1633129827728
	1633129825328 [label=TBackward0]
	1633129834976 -> 1633129825328
	1633146913904 [label="25.channel_att.fc1.weight
 (16, 256)" fillcolor=lightblue]
	1633146913904 -> 1633129834976
	1633129834976 [label=AccumulateGrad]
	1633129833488 -> 1633129829168
	1633129833488 [label=TBackward0]
	1633129837520 -> 1633129833488
	1633146914096 [label="25.channel_att.fc2.weight
 (256, 16)" fillcolor=lightblue]
	1633146914096 -> 1633129837520
	1633129837520 [label=AccumulateGrad]
	1633129834928 -> 1633129835552
	1633129834928 [label=SigmoidBackward0]
	1633129831568 -> 1633129834928
	1633129831568 [label=ConvolutionBackward0]
	1633129826432 -> 1633129831568
	1633129826432 [label=CatBackward0]
	1633129828688 -> 1633129826432
	1633129828688 [label=MeanBackward1]
	1633129835072 -> 1633129828688
	1633129835936 -> 1633129826432
	1633129835936 [label=MaxBackward0]
	1633129835072 -> 1633129835936
	1633129827872 -> 1633129831568
	1633146914288 [label="25.spatial_att.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	1633146914288 -> 1633129827872
	1633129827872 [label=AccumulateGrad]
	1633129836992 -> 1633129837808
	1633129836992 [label=TBackward0]
	1633129835408 -> 1633129836992
	1633146914384 [label="29.weight
 (19, 256)" fillcolor=lightblue]
	1633146914384 -> 1633129835408
	1633129835408 [label=AccumulateGrad]
	1633129837808 -> 1633129586448
}
