digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2422377661072 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2422377723120 [label=AddmmBackward0]
	2422377722784 -> 2422377723120
	2422378245936 [label="fc.bias
 (19)" fillcolor=lightblue]
	2422378245936 -> 2422377722784
	2422377722784 [label=AccumulateGrad]
	2422377722160 -> 2422377723120
	2422377722160 [label=ViewBackward0]
	2422377721680 -> 2422377722160
	2422377721680 [label=MeanBackward1]
	2422377721344 -> 2422377721680
	2422377721344 [label=ReluBackward0]
	2422377720864 -> 2422377721344
	2422377720864 [label=AddBackward0]
	2422377720384 -> 2422377720864
	2422377720384 [label=CudnnBatchNormBackward0]
	2422377719280 -> 2422377720384
	2422377719280 [label=ConvolutionBackward0]
	2422377718320 -> 2422377719280
	2422377718320 [label=ReluBackward0]
	2422377717984 -> 2422377718320
	2422377717984 [label=CudnnBatchNormBackward0]
	2422377717504 -> 2422377717984
	2422377717504 [label=ConvolutionBackward0]
	2422377720240 -> 2422377717504
	2422377720240 [label=ReluBackward0]
	2422377716064 -> 2422377720240
	2422377716064 [label=AddBackward0]
	2422377715584 -> 2422377716064
	2422377715584 [label=CudnnBatchNormBackward0]
	2422377714480 -> 2422377715584
	2422377714480 [label=ConvolutionBackward0]
	2422377713520 -> 2422377714480
	2422377713520 [label=ReluBackward0]
	2422377713184 -> 2422377713520
	2422377713184 [label=CudnnBatchNormBackward0]
	2422377712704 -> 2422377713184
	2422377712704 [label=ConvolutionBackward0]
	2422377711744 -> 2422377712704
	2422377711744 [label=ReluBackward0]
	2422377710640 -> 2422377711744
	2422377710640 [label=AddBackward0]
	2422377710160 -> 2422377710640
	2422377710160 [label=CudnnBatchNormBackward0]
	2422377709824 -> 2422377710160
	2422377709824 [label=ConvolutionBackward0]
	2422377725232 -> 2422377709824
	2422377725232 [label=ReluBackward0]
	2422377725280 -> 2422377725232
	2422377725280 [label=CudnnBatchNormBackward0]
	2422377725136 -> 2422377725280
	2422377725136 [label=ConvolutionBackward0]
	2422377710784 -> 2422377725136
	2422377710784 [label=ReluBackward0]
	2422377724800 -> 2422377710784
	2422377724800 [label=AddBackward0]
	2422377724656 -> 2422377724800
	2422377724656 [label=CudnnBatchNormBackward0]
	2422377724272 -> 2422377724656
	2422377724272 [label=ConvolutionBackward0]
	2422377724416 -> 2422377724272
	2422377724416 [label=ReluBackward0]
	2422377723984 -> 2422377724416
	2422377723984 [label=CudnnBatchNormBackward0]
	2422377723888 -> 2422377723984
	2422377723888 [label=ConvolutionBackward0]
	2422377723696 -> 2422377723888
	2422377723696 [label=ReluBackward0]
	2422377723312 -> 2422377723696
	2422377723312 [label=AddBackward0]
	2422377723552 -> 2422377723312
	2422377723552 [label=CudnnBatchNormBackward0]
	2422377723216 -> 2422377723552
	2422377723216 [label=ConvolutionBackward0]
	2422377722928 -> 2422377723216
	2422377722928 [label=ReluBackward0]
	2422377722976 -> 2422377722928
	2422377722976 [label=CudnnBatchNormBackward0]
	2422377722688 -> 2422377722976
	2422377722688 [label=ConvolutionBackward0]
	2422377723408 -> 2422377722688
	2422377723408 [label=ReluBackward0]
	2422377722496 -> 2422377723408
	2422377722496 [label=AddBackward0]
	2422377722208 -> 2422377722496
	2422377722208 [label=CudnnBatchNormBackward0]
	2422377721968 -> 2422377722208
	2422377721968 [label=ConvolutionBackward0]
	2422377721776 -> 2422377721968
	2422377721776 [label=ReluBackward0]
	2422377721392 -> 2422377721776
	2422377721392 [label=CudnnBatchNormBackward0]
	2422377721632 -> 2422377721392
	2422377721632 [label=ConvolutionBackward0]
	2422377721248 -> 2422377721632
	2422377721248 [label=ReluBackward0]
	2422377721008 -> 2422377721248
	2422377721008 [label=AddBackward0]
	2422377720960 -> 2422377721008
	2422377720960 [label=CudnnBatchNormBackward0]
	2422377720768 -> 2422377720960
	2422377720768 [label=ConvolutionBackward0]
	2422377720672 -> 2422377720768
	2422377720672 [label=ReluBackward0]
	2422377720336 -> 2422377720672
	2422377720336 [label=CudnnBatchNormBackward0]
	2422377720144 -> 2422377720336
	2422377720144 [label=ConvolutionBackward0]
	2422377721152 -> 2422377720144
	2422377721152 [label=ReluBackward0]
	2422377719856 -> 2422377721152
	2422377719856 [label=AddBackward0]
	2422377719664 -> 2422377719856
	2422377719664 [label=CudnnBatchNormBackward0]
	2422377719712 -> 2422377719664
	2422377719712 [label=ConvolutionBackward0]
	2422377719328 -> 2422377719712
	2422377719328 [label=ReluBackward0]
	2422377719088 -> 2422377719328
	2422377719088 [label=CudnnBatchNormBackward0]
	2422377719040 -> 2422377719088
	2422377719040 [label=ConvolutionBackward0]
	2422377719808 -> 2422377719040
	2422377719808 [label=MaxPool2DWithIndicesBackward0]
	2422377718608 -> 2422377719808
	2422377718608 [label=ReluBackward0]
	2422377718560 -> 2422377718608
	2422377718560 [label=CudnnBatchNormBackward0]
	2422377718416 -> 2422377718560
	2422377718416 [label=ConvolutionBackward0]
	2422377718128 -> 2422377718416
	2422378245744 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2422378245744 -> 2422377718128
	2422377718128 [label=AccumulateGrad]
	2422377718656 -> 2422377718560
	2422384364528 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2422384364528 -> 2422377718656
	2422377718656 [label=AccumulateGrad]
	2422377718896 -> 2422377718560
	2422384364240 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2422384364240 -> 2422377718896
	2422377718896 [label=AccumulateGrad]
	2422377718704 -> 2422377719040
	2422384364720 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2422384364720 -> 2422377718704
	2422377718704 [label=AccumulateGrad]
	2422377719232 -> 2422377719088
	2422384364816 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2422384364816 -> 2422377719232
	2422377719232 [label=AccumulateGrad]
	2422377719184 -> 2422377719088
	2422384364912 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2422384364912 -> 2422377719184
	2422377719184 [label=AccumulateGrad]
	2422377719376 -> 2422377719712
	2422384364144 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2422384364144 -> 2422377719376
	2422377719376 [label=AccumulateGrad]
	2422377719568 -> 2422377719664
	2422384364048 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2422384364048 -> 2422377719568
	2422377719568 [label=AccumulateGrad]
	2422377719472 -> 2422377719664
	2422384363280 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2422384363280 -> 2422377719472
	2422377719472 [label=AccumulateGrad]
	2422377719808 -> 2422377719856
	2422377720000 -> 2422377720144
	2422384363376 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2422384363376 -> 2422377720000
	2422377720000 [label=AccumulateGrad]
	2422377720288 -> 2422377720336
	2422384363472 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2422384363472 -> 2422377720288
	2422377720288 [label=AccumulateGrad]
	2422377720480 -> 2422377720336
	2422384363568 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2422384363568 -> 2422377720480
	2422377720480 [label=AccumulateGrad]
	2422377720528 -> 2422377720768
	2422384363856 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2422384363856 -> 2422377720528
	2422377720528 [label=AccumulateGrad]
	2422377720816 -> 2422377720960
	2422384362896 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2422384362896 -> 2422377720816
	2422377720816 [label=AccumulateGrad]
	2422377721056 -> 2422377720960
	2422384362800 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2422384362800 -> 2422377721056
	2422377721056 [label=AccumulateGrad]
	2422377721152 -> 2422377721008
	2422377721296 -> 2422377721632
	2422384362704 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2422384362704 -> 2422377721296
	2422377721296 [label=AccumulateGrad]
	2422377721488 -> 2422377721392
	2422384362512 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2422384362512 -> 2422377721488
	2422377721488 [label=AccumulateGrad]
	2422377721728 -> 2422377721392
	2422378237680 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2422378237680 -> 2422377721728
	2422377721728 [label=AccumulateGrad]
	2422377722016 -> 2422377721968
	2422378238064 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2422378238064 -> 2422377722016
	2422377722016 [label=AccumulateGrad]
	2422377721872 -> 2422377722208
	2422378238160 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2422378238160 -> 2422377721872
	2422377721872 [label=AccumulateGrad]
	2422377722064 -> 2422377722208
	2422378238256 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2422378238256 -> 2422377722064
	2422377722064 [label=AccumulateGrad]
	2422377722256 -> 2422377722496
	2422377722256 [label=CudnnBatchNormBackward0]
	2422377721536 -> 2422377722256
	2422377721536 [label=ConvolutionBackward0]
	2422377721248 -> 2422377721536
	2422377721104 -> 2422377721536
	2422384362608 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2422384362608 -> 2422377721104
	2422377721104 [label=AccumulateGrad]
	2422377721920 -> 2422377722256
	2422384362128 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2422384362128 -> 2422377721920
	2422377721920 [label=AccumulateGrad]
	2422377722112 -> 2422377722256
	2422384361648 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2422384361648 -> 2422377722112
	2422377722112 [label=AccumulateGrad]
	2422377722592 -> 2422377722688
	2422378238640 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2422378238640 -> 2422377722592
	2422377722592 [label=AccumulateGrad]
	2422377722736 -> 2422377722976
	2422378238736 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2422378238736 -> 2422377722736
	2422377722736 [label=AccumulateGrad]
	2422377723072 -> 2422377722976
	2422378238832 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2422378238832 -> 2422377723072
	2422377723072 [label=AccumulateGrad]
	2422377722832 -> 2422377723216
	2422378239216 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2422378239216 -> 2422377722832
	2422377722832 [label=AccumulateGrad]
	2422377723456 -> 2422377723552
	2422378239312 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2422378239312 -> 2422377723456
	2422377723456 [label=AccumulateGrad]
	2422377723360 -> 2422377723552
	2422378239408 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2422378239408 -> 2422377723360
	2422377723360 [label=AccumulateGrad]
	2422377723408 -> 2422377723312
	2422377723936 -> 2422377723888
	2422378240368 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2422378240368 -> 2422377723936
	2422377723936 [label=AccumulateGrad]
	2422377723792 -> 2422377723984
	2422378240464 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2422378240464 -> 2422377723792
	2422377723792 [label=AccumulateGrad]
	2422377724176 -> 2422377723984
	2422378240560 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2422378240560 -> 2422377724176
	2422377724176 [label=AccumulateGrad]
	2422377724320 -> 2422377724272
	2422378240944 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2422378240944 -> 2422377724320
	2422377724320 [label=AccumulateGrad]
	2422377724464 -> 2422377724656
	2422378241040 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2422378241040 -> 2422377724464
	2422377724464 [label=AccumulateGrad]
	2422377724608 -> 2422377724656
	2422378241136 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2422378241136 -> 2422377724608
	2422377724608 [label=AccumulateGrad]
	2422377724896 -> 2422377724800
	2422377724896 [label=CudnnBatchNormBackward0]
	2422377723840 -> 2422377724896
	2422377723840 [label=ConvolutionBackward0]
	2422377723696 -> 2422377723840
	2422377723648 -> 2422377723840
	2422378239792 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2422378239792 -> 2422377723648
	2422377723648 [label=AccumulateGrad]
	2422377724512 -> 2422377724896
	2422378239888 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2422378239888 -> 2422377724512
	2422377724512 [label=AccumulateGrad]
	2422377724368 -> 2422377724896
	2422378239984 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2422378239984 -> 2422377724368
	2422377724368 [label=AccumulateGrad]
	2422377724848 -> 2422377725136
	2422378241520 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2422378241520 -> 2422377724848
	2422377724848 [label=AccumulateGrad]
	2422377725376 -> 2422377725280
	2422378241616 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2422378241616 -> 2422377725376
	2422377725376 [label=AccumulateGrad]
	2422377725328 -> 2422377725280
	2422378241712 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2422378241712 -> 2422377725328
	2422377725328 [label=AccumulateGrad]
	2422377725424 -> 2422377709824
	2422378242096 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2422378242096 -> 2422377725424
	2422377725424 [label=AccumulateGrad]
	2422377709680 -> 2422377710160
	2422378242192 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2422378242192 -> 2422377709680
	2422377709680 [label=AccumulateGrad]
	2422377710304 -> 2422377710160
	2422378242288 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2422378242288 -> 2422377710304
	2422377710304 [label=AccumulateGrad]
	2422377710784 -> 2422377710640
	2422377711600 -> 2422377712704
	2422378243248 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2422378243248 -> 2422377711600
	2422377711600 [label=AccumulateGrad]
	2422377712560 -> 2422377713184
	2422378243344 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2422378243344 -> 2422377712560
	2422377712560 [label=AccumulateGrad]
	2422377713664 -> 2422377713184
	2422378243440 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2422378243440 -> 2422377713664
	2422377713664 [label=AccumulateGrad]
	2422377714144 -> 2422377714480
	2422378243824 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2422378243824 -> 2422377714144
	2422377714144 [label=AccumulateGrad]
	2422377715104 -> 2422377715584
	2422378243920 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2422378243920 -> 2422377715104
	2422377715104 [label=AccumulateGrad]
	2422377714960 -> 2422377715584
	2422378244016 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2422378244016 -> 2422377714960
	2422377714960 [label=AccumulateGrad]
	2422377715440 -> 2422377716064
	2422377715440 [label=CudnnBatchNormBackward0]
	2422377712224 -> 2422377715440
	2422377712224 [label=ConvolutionBackward0]
	2422377711744 -> 2422377712224
	2422377711120 -> 2422377712224
	2422378242672 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2422378242672 -> 2422377711120
	2422377711120 [label=AccumulateGrad]
	2422377714000 -> 2422377715440
	2422378242768 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2422378242768 -> 2422377714000
	2422377714000 [label=AccumulateGrad]
	2422377714624 -> 2422377715440
	2422378242864 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2422378242864 -> 2422377714624
	2422377714624 [label=AccumulateGrad]
	2422377716544 -> 2422377717504
	2422378244400 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2422378244400 -> 2422377716544
	2422377716544 [label=AccumulateGrad]
	2422377717360 -> 2422377717984
	2422378244496 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2422378244496 -> 2422377717360
	2422377717360 [label=AccumulateGrad]
	2422377718464 -> 2422377717984
	2422378244592 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2422378244592 -> 2422377718464
	2422377718464 [label=AccumulateGrad]
	2422377718944 -> 2422377719280
	2422378244976 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2422378244976 -> 2422377718944
	2422377718944 [label=AccumulateGrad]
	2422377719904 -> 2422377720384
	2422378245072 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2422378245072 -> 2422377719904
	2422377719904 [label=AccumulateGrad]
	2422377719760 -> 2422377720384
	2422378245168 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2422378245168 -> 2422377719760
	2422377719760 [label=AccumulateGrad]
	2422377720240 -> 2422377720864
	2422377722304 -> 2422377723120
	2422377722304 [label=TBackward0]
	2422377720720 -> 2422377722304
	2422378245840 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2422378245840 -> 2422377720720
	2422377720720 [label=AccumulateGrad]
	2422377723120 -> 2422377661072
}
