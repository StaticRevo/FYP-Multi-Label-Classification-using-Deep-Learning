digraph {
	graph [size="86.7,86.7"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1644291213744 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1644539138144 [label=AddmmBackward0]
	1644539135984 -> 1644539138144
	1644263632784 [label="29.bias
 (19)" fillcolor=lightblue]
	1644263632784 -> 1644539135984
	1644539135984 [label=AccumulateGrad]
	1644539134544 -> 1644539138144
	1644539134544 [label=NativeDropoutBackward0]
	1644539140688 -> 1644539134544
	1644539140688 [label=ViewBackward0]
	1644539139536 -> 1644539140688
	1644539139536 [label=MeanBackward1]
	1644539142176 -> 1644539139536
	1644539142176 [label=MulBackward0]
	1644539138864 -> 1644539142176
	1644539138864 [label=MulBackward0]
	1644539144480 -> 1644539138864
	1644539144480 [label=ReluBackward0]
	1644539146304 -> 1644539144480
	1644539146304 [label=AddBackward0]
	1644539135744 -> 1644539146304
	1644539135744 [label=CudnnBatchNormBackward0]
	1644539143952 -> 1644539135744
	1644539143952 [label=ConvolutionBackward0]
	1644539149280 -> 1644539143952
	1644539149280 [label=ReluBackward0]
	1644539147360 -> 1644539149280
	1644539147360 [label=CudnnBatchNormBackward0]
	1644539148368 -> 1644539147360
	1644539148368 [label=ConvolutionBackward0]
	1644539136704 -> 1644539148368
	1644539136704 [label=ReluBackward0]
	1644539134352 -> 1644539136704
	1644539134352 [label=CudnnBatchNormBackward0]
	1644539140064 -> 1644539134352
	1644539140064 [label=ConvolutionBackward0]
	1644539135456 -> 1644539140064
	1644539135456 [label=ConvolutionBackward0]
	1644539135072 -> 1644539135456
	1644539135072 [label=MulBackward0]
	1644539139392 -> 1644539135072
	1644539139392 [label=ReluBackward0]
	1644539147072 -> 1644539139392
	1644539147072 [label=AddBackward0]
	1644539148224 -> 1644539147072
	1644539148224 [label=CudnnBatchNormBackward0]
	1644539145968 -> 1644539148224
	1644539145968 [label=ConvolutionBackward0]
	1644539149376 -> 1644539145968
	1644539149376 [label=ReluBackward0]
	1644539145776 -> 1644539149376
	1644539145776 [label=CudnnBatchNormBackward0]
	1644539148848 -> 1644539145776
	1644539148848 [label=ConvolutionBackward0]
	1644539147984 -> 1644539148848
	1644539147984 [label=ConvolutionBackward0]
	1644539136368 -> 1644539147984
	1644539136368 [label=CatBackward0]
	1644539137520 -> 1644539136368
	1644539137520 [label=ConvolutionBackward0]
	1644539137808 -> 1644539137520
	1644539137808 [label=ReluBackward0]
	1644539139008 -> 1644539137808
	1644539139008 [label=CudnnBatchNormBackward0]
	1644539137376 -> 1644539139008
	1644539137376 [label=ConvolutionBackward0]
	1644539135408 -> 1644539137376
	1644539135408 [label=ConvolutionBackward0]
	1644539138528 -> 1644539135408
	1644539138528 [label=MulBackward0]
	1644539137616 -> 1644539138528
	1644539137616 [label=ReluBackward0]
	1644539135696 -> 1644539137616
	1644539135696 [label=AddBackward0]
	1644539140640 -> 1644539135696
	1644539140640 [label=CudnnBatchNormBackward0]
	1644539137472 -> 1644539140640
	1644539137472 [label=ConvolutionBackward0]
	1644539139728 -> 1644539137472
	1644539139728 [label=ReluBackward0]
	1644539140496 -> 1644539139728
	1644539140496 [label=CudnnBatchNormBackward0]
	1644539140208 -> 1644539140496
	1644539140208 [label=ConvolutionBackward0]
	1644539136128 -> 1644539140208
	1644539136128 [label=ConvolutionBackward0]
	1644539141792 -> 1644539136128
	1644539141792 [label=CatBackward0]
	1644539141168 -> 1644539141792
	1644539141168 [label=ConvolutionBackward0]
	1644539135840 -> 1644539141168
	1644539135840 [label=ReluBackward0]
	1644539142848 -> 1644539135840
	1644539142848 [label=CudnnBatchNormBackward0]
	1644539142800 -> 1644539142848
	1644539142800 [label=ConvolutionBackward0]
	1644539140160 -> 1644539142800
	1644539140160 [label=ConvolutionBackward0]
	1644539141696 -> 1644539140160
	1644539141696 [label=MulBackward0]
	1644539139968 -> 1644539141696
	1644539139968 [label=MulBackward0]
	1644539136032 -> 1644539139968
	1644539136032 [label=MulBackward0]
	1644539142656 -> 1644539136032
	1644539142656 [label=ReluBackward0]
	1644539144432 -> 1644539142656
	1644539144432 [label=AddBackward0]
	1644539145536 -> 1644539144432
	1644539145536 [label=CudnnBatchNormBackward0]
	1644539143856 -> 1644539145536
	1644539143856 [label=ConvolutionBackward0]
	1644539142368 -> 1644539143856
	1644539142368 [label=ReluBackward0]
	1644539141936 -> 1644539142368
	1644539141936 [label=CudnnBatchNormBackward0]
	1644539142608 -> 1644539141936
	1644539142608 [label=ConvolutionBackward0]
	1644539144816 -> 1644539142608
	1644539144816 [label=ReluBackward0]
	1644539146544 -> 1644539144816
	1644539146544 [label=CudnnBatchNormBackward0]
	1644539147696 -> 1644539146544
	1644539147696 [label=ConvolutionBackward0]
	1644539144720 -> 1644539147696
	1644539144720 [label=ConvolutionBackward0]
	1644539146976 -> 1644539144720
	1644539146976 [label=ReluBackward0]
	1644539148176 -> 1644539146976
	1644539148176 [label=CudnnBatchNormBackward0]
	1644539147024 -> 1644539148176
	1644539147024 [label=ConvolutionBackward0]
	1644539142560 -> 1644539147024
	1644504074000 [label="0.weight
 (16, 12, 1, 1)" fillcolor=lightblue]
	1644504074000 -> 1644539142560
	1644539142560 [label=AccumulateGrad]
	1644539146640 -> 1644539148176
	1644504073232 [label="1.weight
 (16)" fillcolor=lightblue]
	1644504073232 -> 1644539146640
	1644539146640 [label=AccumulateGrad]
	1644539145008 -> 1644539148176
	1644504072944 [label="1.bias
 (16)" fillcolor=lightblue]
	1644504072944 -> 1644539145008
	1644539145008 [label=AccumulateGrad]
	1644539143664 -> 1644539144720
	1644504073424 [label="3.depthwise.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	1644504073424 -> 1644539143664
	1644539143664 [label=AccumulateGrad]
	1644539147648 -> 1644539147696
	1644504073520 [label="3.pointwise.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	1644504073520 -> 1644539147648
	1644539147648 [label=AccumulateGrad]
	1644539147312 -> 1644539146544
	1644504073616 [label="4.weight
 (32)" fillcolor=lightblue]
	1644504073616 -> 1644539147312
	1644539147312 [label=AccumulateGrad]
	1644539144768 -> 1644539146544
	1644504073712 [label="4.bias
 (32)" fillcolor=lightblue]
	1644504073712 -> 1644539144768
	1644539144768 [label=AccumulateGrad]
	1644539142944 -> 1644539142608
	1644504072752 [label="6.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1644504072752 -> 1644539142944
	1644539142944 [label=AccumulateGrad]
	1644539145296 -> 1644539141936
	1644504072272 [label="6.bn1.weight
 (32)" fillcolor=lightblue]
	1644504072272 -> 1644539145296
	1644539145296 [label=AccumulateGrad]
	1644539140304 -> 1644539141936
	1644504072368 [label="6.bn1.bias
 (32)" fillcolor=lightblue]
	1644504072368 -> 1644539140304
	1644539140304 [label=AccumulateGrad]
	1644539142896 -> 1644539143856
	1644414795504 [label="6.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1644414795504 -> 1644539142896
	1644539142896 [label=AccumulateGrad]
	1644539144384 -> 1644539145536
	1644414795216 [label="6.bn2.weight
 (32)" fillcolor=lightblue]
	1644414795216 -> 1644539144384
	1644539144384 [label=AccumulateGrad]
	1644539139872 -> 1644539145536
	1644414795312 [label="6.bn2.bias
 (32)" fillcolor=lightblue]
	1644414795312 -> 1644539139872
	1644539139872 [label=AccumulateGrad]
	1644539144816 -> 1644539144432
	1644539144576 -> 1644539136032
	1644539144576 [label=ViewBackward0]
	1644539135024 -> 1644539144576
	1644539135024 [label=SigmoidBackward0]
	1644539145824 -> 1644539135024
	1644539145824 [label=AddmmBackward0]
	1644539145248 -> 1644539145824
	1644414793872 [label="7.fc2.bias
 (32)" fillcolor=lightblue]
	1644414793872 -> 1644539145248
	1644539145248 [label=AccumulateGrad]
	1644539138096 -> 1644539145824
	1644539138096 [label=ReluBackward0]
	1644539143616 -> 1644539138096
	1644539143616 [label=AddmmBackward0]
	1644539147600 -> 1644539143616
	1644414795024 [label="7.fc1.bias
 (2)" fillcolor=lightblue]
	1644414795024 -> 1644539147600
	1644539147600 [label=AccumulateGrad]
	1644539148416 -> 1644539143616
	1644539148416 [label=MeanBackward1]
	1644539148080 -> 1644539148416
	1644539148080 [label=ViewBackward0]
	1644539142656 -> 1644539148080
	1644539143424 -> 1644539143616
	1644539143424 [label=TBackward0]
	1644539148464 -> 1644539143424
	1644414795120 [label="7.fc1.weight
 (2, 32)" fillcolor=lightblue]
	1644414795120 -> 1644539148464
	1644539148464 [label=AccumulateGrad]
	1644539142080 -> 1644539145824
	1644539142080 [label=TBackward0]
	1644539136896 -> 1644539142080
	1644414794256 [label="7.fc2.weight
 (32, 2)" fillcolor=lightblue]
	1644414794256 -> 1644539136896
	1644539136896 [label=AccumulateGrad]
	1644539144624 -> 1644539139968
	1644539144624 [label=SigmoidBackward0]
	1644539145728 -> 1644539144624
	1644539145728 [label=ConvolutionBackward0]
	1644539142512 -> 1644539145728
	1644539142512 [label=SplitWithSizesBackward0]
	1644539147168 -> 1644539142512
	1644539147168 [label=ReluBackward0]
	1644539144960 -> 1644539147168
	1644539144960 [label=CudnnBatchNormBackward0]
	1644539149136 -> 1644539144960
	1644539149136 [label=ConvolutionBackward0]
	1644539150144 -> 1644539149136
	1644539150144 [label=CatBackward0]
	1644539148512 -> 1644539150144
	1644539148512 [label=AdaptiveAvgPool2DBackward0]
	1644539136032 -> 1644539148512
	1644539150240 -> 1644539150144
	1644539150240 [label=PermuteBackward0]
	1644539148992 -> 1644539150240
	1644539148992 [label=AdaptiveAvgPool2DBackward0]
	1644539136032 -> 1644539148992
	1644539147888 -> 1644539149136
	1644414794736 [label="8.conv1.weight
 (8, 32, 1, 1)" fillcolor=lightblue]
	1644414794736 -> 1644539147888
	1644539147888 [label=AccumulateGrad]
	1644539149664 -> 1644539144960
	1644414792336 [label="8.bn1.weight
 (8)" fillcolor=lightblue]
	1644414792336 -> 1644539149664
	1644539149664 [label=AccumulateGrad]
	1644539148800 -> 1644539144960
	1644414792528 [label="8.bn1.bias
 (8)" fillcolor=lightblue]
	1644414792528 -> 1644539148800
	1644539148800 [label=AccumulateGrad]
	1644539147120 -> 1644539145728
	1644414794640 [label="8.conv_h.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1644414794640 -> 1644539147120
	1644539147120 [label=AccumulateGrad]
	1644539144528 -> 1644539141696
	1644539144528 [label=PermuteBackward0]
	1644539137568 -> 1644539144528
	1644539137568 [label=SigmoidBackward0]
	1644539146448 -> 1644539137568
	1644539146448 [label=ConvolutionBackward0]
	1644539142512 -> 1644539146448
	1644539146496 -> 1644539146448
	1644414794928 [label="8.conv_w.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1644414794928 -> 1644539146496
	1644539146496 [label=AccumulateGrad]
	1644539139248 -> 1644539140160
	1644414794832 [label="9.depthwise.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	1644414794832 -> 1644539139248
	1644539139248 [label=AccumulateGrad]
	1644539139584 -> 1644539142800
	1644263624816 [label="9.pointwise.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	1644263624816 -> 1644539139584
	1644539139584 [label=AccumulateGrad]
	1644539136608 -> 1644539142848
	1644263624912 [label="10.weight
 (64)" fillcolor=lightblue]
	1644263624912 -> 1644539136608
	1644539136608 [label=AccumulateGrad]
	1644539134640 -> 1644539142848
	1644263625008 [label="10.bias
 (64)" fillcolor=lightblue]
	1644263625008 -> 1644539134640
	1644539134640 [label=AccumulateGrad]
	1644539140016 -> 1644539141168
	1644263625392 [label="12.conv_dil1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1644263625392 -> 1644539140016
	1644539140016 [label=AccumulateGrad]
	1644539142464 -> 1644539141168
	1644263625488 [label="12.conv_dil1.bias
 (64)" fillcolor=lightblue]
	1644263625488 -> 1644539142464
	1644539142464 [label=AccumulateGrad]
	1644539142224 -> 1644539141792
	1644539142224 [label=ConvolutionBackward0]
	1644539135840 -> 1644539142224
	1644539143760 -> 1644539142224
	1644263625584 [label="12.conv_dil2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1644263625584 -> 1644539143760
	1644539143760 [label=AccumulateGrad]
	1644539143088 -> 1644539142224
	1644263625680 [label="12.conv_dil2.bias
 (64)" fillcolor=lightblue]
	1644263625680 -> 1644539143088
	1644539143088 [label=AccumulateGrad]
	1644539141600 -> 1644539141792
	1644539141600 [label=ConvolutionBackward0]
	1644539135840 -> 1644539141600
	1644539143904 -> 1644539141600
	1644263625776 [label="12.conv_dil3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1644263625776 -> 1644539143904
	1644539143904 [label=AccumulateGrad]
	1644539143280 -> 1644539141600
	1644263625872 [label="12.conv_dil3.bias
 (64)" fillcolor=lightblue]
	1644263625872 -> 1644539143280
	1644539143280 [label=AccumulateGrad]
	1644539142032 -> 1644539136128
	1644263625968 [label="12.fuse.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	1644263625968 -> 1644539142032
	1644539142032 [label=AccumulateGrad]
	1644539141312 -> 1644539136128
	1644263626064 [label="12.fuse.bias
 (64)" fillcolor=lightblue]
	1644263626064 -> 1644539141312
	1644539141312 [label=AccumulateGrad]
	1644539139776 -> 1644539140208
	1644263626160 [label="13.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1644263626160 -> 1644539139776
	1644539139776 [label=AccumulateGrad]
	1644539136272 -> 1644539140496
	1644263626256 [label="13.bn1.weight
 (64)" fillcolor=lightblue]
	1644263626256 -> 1644539136272
	1644539136272 [label=AccumulateGrad]
	1644539134496 -> 1644539140496
	1644263626352 [label="13.bn1.bias
 (64)" fillcolor=lightblue]
	1644263626352 -> 1644539134496
	1644539134496 [label=AccumulateGrad]
	1644539140832 -> 1644539137472
	1644263626736 [label="13.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1644263626736 -> 1644539140832
	1644539140832 [label=AccumulateGrad]
	1644539139104 -> 1644539140640
	1644263626832 [label="13.bn2.weight
 (64)" fillcolor=lightblue]
	1644263626832 -> 1644539139104
	1644539139104 [label=AccumulateGrad]
	1644539137424 -> 1644539140640
	1644263626928 [label="13.bn2.bias
 (64)" fillcolor=lightblue]
	1644263626928 -> 1644539137424
	1644539137424 [label=AccumulateGrad]
	1644539136128 -> 1644539135696
	1644539139296 -> 1644539138528
	1644539139296 [label=UnsqueezeBackward0]
	1644539139824 -> 1644539139296
	1644539139824 [label=UnsqueezeBackward0]
	1644539141648 -> 1644539139824
	1644539141648 [label=SigmoidBackward0]
	1644539134304 -> 1644539141648
	1644539134304 [label=SqueezeBackward1]
	1644539140592 -> 1644539134304
	1644539140592 [label=ConvolutionBackward0]
	1644539144240 -> 1644539140592
	1644539144240 [label=UnsqueezeBackward0]
	1644539147840 -> 1644539144240
	1644539147840 [label=SqueezeBackward1]
	1644539148608 -> 1644539147840
	1644539148608 [label=SqueezeBackward1]
	1644539147552 -> 1644539148608
	1644539147552 [label=MeanBackward1]
	1644539137616 -> 1644539147552
	1644539141024 -> 1644539140592
	1644263627312 [label="14.conv.weight
 (1, 1, 3)" fillcolor=lightblue]
	1644263627312 -> 1644539141024
	1644539141024 [label=AccumulateGrad]
	1644539136464 -> 1644539135408
	1644263627408 [label="15.depthwise.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	1644263627408 -> 1644539136464
	1644539136464 [label=AccumulateGrad]
	1644539137088 -> 1644539137376
	1644263627504 [label="15.pointwise.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1644263627504 -> 1644539137088
	1644539137088 [label=AccumulateGrad]
	1644539136512 -> 1644539139008
	1644263627600 [label="16.weight
 (128)" fillcolor=lightblue]
	1644263627600 -> 1644539136512
	1644539136512 [label=AccumulateGrad]
	1644539138672 -> 1644539139008
	1644263627696 [label="16.bias
 (128)" fillcolor=lightblue]
	1644263627696 -> 1644539138672
	1644539138672 [label=AccumulateGrad]
	1644539137232 -> 1644539137520
	1644263628080 [label="18.conv_dil1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1644263628080 -> 1644539137232
	1644539137232 [label=AccumulateGrad]
	1644539137040 -> 1644539137520
	1644263628176 [label="18.conv_dil1.bias
 (128)" fillcolor=lightblue]
	1644263628176 -> 1644539137040
	1644539137040 [label=AccumulateGrad]
	1644539137184 -> 1644539136368
	1644539137184 [label=ConvolutionBackward0]
	1644539137808 -> 1644539137184
	1644539134016 -> 1644539137184
	1644263628272 [label="18.conv_dil2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1644263628272 -> 1644539134016
	1644539134016 [label=AccumulateGrad]
	1644539138624 -> 1644539137184
	1644263628368 [label="18.conv_dil2.bias
 (128)" fillcolor=lightblue]
	1644263628368 -> 1644539138624
	1644539138624 [label=AccumulateGrad]
	1644539145152 -> 1644539136368
	1644539145152 [label=ConvolutionBackward0]
	1644539137808 -> 1644539145152
	1644539138336 -> 1644539145152
	1644263628464 [label="18.conv_dil3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1644263628464 -> 1644539138336
	1644539138336 [label=AccumulateGrad]
	1644539137280 -> 1644539145152
	1644263628560 [label="18.conv_dil3.bias
 (128)" fillcolor=lightblue]
	1644263628560 -> 1644539137280
	1644539137280 [label=AccumulateGrad]
	1644539145920 -> 1644539147984
	1644263628656 [label="18.fuse.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	1644263628656 -> 1644539145920
	1644539145920 [label=AccumulateGrad]
	1644539139920 -> 1644539147984
	1644263628752 [label="18.fuse.bias
 (128)" fillcolor=lightblue]
	1644263628752 -> 1644539139920
	1644539139920 [label=AccumulateGrad]
	1644539150048 -> 1644539148848
	1644263628848 [label="19.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1644263628848 -> 1644539150048
	1644539150048 [label=AccumulateGrad]
	1644539144000 -> 1644539145776
	1644263628944 [label="19.bn1.weight
 (128)" fillcolor=lightblue]
	1644263628944 -> 1644539144000
	1644539144000 [label=AccumulateGrad]
	1644539150288 -> 1644539145776
	1644263629040 [label="19.bn1.bias
 (128)" fillcolor=lightblue]
	1644263629040 -> 1644539150288
	1644539150288 [label=AccumulateGrad]
	1644539149712 -> 1644539145968
	1644263629424 [label="19.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1644263629424 -> 1644539149712
	1644539149712 [label=AccumulateGrad]
	1644539147456 -> 1644539148224
	1644263629520 [label="19.bn2.weight
 (128)" fillcolor=lightblue]
	1644263629520 -> 1644539147456
	1644539147456 [label=AccumulateGrad]
	1644539145056 -> 1644539148224
	1644263629616 [label="19.bn2.bias
 (128)" fillcolor=lightblue]
	1644263629616 -> 1644539145056
	1644539145056 [label=AccumulateGrad]
	1644539147984 -> 1644539147072
	1644539135792 -> 1644539135072
	1644539135792 [label=SigmoidBackward0]
	1644539148560 -> 1644539135792
	1644539148560 [label=ConvolutionBackward0]
	1644539147408 -> 1644539148560
	1644539147408 [label=NativeDropoutBackward0]
	1644539137904 -> 1644539147408
	1644539137904 [label=ReluBackward0]
	1644539140736 -> 1644539137904
	1644539140736 [label=ConvolutionBackward0]
	1644539136800 -> 1644539140736
	1644539136800 [label=MeanBackward1]
	1644539139392 -> 1644539136800
	1644539138432 -> 1644539140736
	1644263630000 [label="20.fc1.weight
 (8, 128, 1, 1)" fillcolor=lightblue]
	1644263630000 -> 1644539138432
	1644539138432 [label=AccumulateGrad]
	1644539148752 -> 1644539140736
	1644263630096 [label="20.fc1.bias
 (8)" fillcolor=lightblue]
	1644263630096 -> 1644539148752
	1644539148752 [label=AccumulateGrad]
	1644539150192 -> 1644539148560
	1644263630192 [label="20.fc2.weight
 (128, 8, 1, 1)" fillcolor=lightblue]
	1644263630192 -> 1644539150192
	1644539150192 [label=AccumulateGrad]
	1644539145872 -> 1644539148560
	1644263630288 [label="20.fc2.bias
 (128)" fillcolor=lightblue]
	1644263630288 -> 1644539145872
	1644539145872 [label=AccumulateGrad]
	1644539141072 -> 1644539135456
	1644263630384 [label="21.depthwise.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	1644263630384 -> 1644539141072
	1644539141072 [label=AccumulateGrad]
	1644539134064 -> 1644539140064
	1644263630480 [label="21.pointwise.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1644263630480 -> 1644539134064
	1644539134064 [label=AccumulateGrad]
	1644539135120 -> 1644539134352
	1644263630576 [label="22.weight
 (256)" fillcolor=lightblue]
	1644263630576 -> 1644539135120
	1644539135120 [label=AccumulateGrad]
	1644539144912 -> 1644539134352
	1644263630672 [label="22.bias
 (256)" fillcolor=lightblue]
	1644263630672 -> 1644539144912
	1644539144912 [label=AccumulateGrad]
	1644539138240 -> 1644539148368
	1644263631056 [label="24.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1644263631056 -> 1644539138240
	1644539138240 [label=AccumulateGrad]
	1644539136560 -> 1644539147360
	1644263631152 [label="24.bn1.weight
 (256)" fillcolor=lightblue]
	1644263631152 -> 1644539136560
	1644539136560 [label=AccumulateGrad]
	1644539146016 -> 1644539147360
	1644263631248 [label="24.bn1.bias
 (256)" fillcolor=lightblue]
	1644263631248 -> 1644539146016
	1644539146016 [label=AccumulateGrad]
	1644539147936 -> 1644539143952
	1644263631632 [label="24.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1644263631632 -> 1644539147936
	1644539147936 [label=AccumulateGrad]
	1644539134784 -> 1644539135744
	1644263631728 [label="24.bn2.weight
 (256)" fillcolor=lightblue]
	1644263631728 -> 1644539134784
	1644539134784 [label=AccumulateGrad]
	1644539140976 -> 1644539135744
	1644263631824 [label="24.bn2.bias
 (256)" fillcolor=lightblue]
	1644263631824 -> 1644539140976
	1644539140976 [label=AccumulateGrad]
	1644539136704 -> 1644539146304
	1644539143520 -> 1644539138864
	1644539143520 [label=ViewBackward0]
	1644539146736 -> 1644539143520
	1644539146736 [label=SigmoidBackward0]
	1644539146688 -> 1644539146736
	1644539146688 [label=AddmmBackward0]
	1644539150096 -> 1644539146688
	1644263632496 [label="25.channel_att.fc2.bias
 (256)" fillcolor=lightblue]
	1644263632496 -> 1644539150096
	1644539150096 [label=AccumulateGrad]
	1644539148944 -> 1644539146688
	1644539148944 [label=ReluBackward0]
	1644539134736 -> 1644539148944
	1644539134736 [label=AddmmBackward0]
	1644539147264 -> 1644539134736
	1644263632304 [label="25.channel_att.fc1.bias
 (16)" fillcolor=lightblue]
	1644263632304 -> 1644539147264
	1644539147264 [label=AccumulateGrad]
	1644539149760 -> 1644539134736
	1644539149760 [label=MeanBackward1]
	1644539136752 -> 1644539149760
	1644539136752 [label=ViewBackward0]
	1644539144480 -> 1644539136752
	1644539145680 -> 1644539134736
	1644539145680 [label=TBackward0]
	1644539143040 -> 1644539145680
	1644263632208 [label="25.channel_att.fc1.weight
 (16, 256)" fillcolor=lightblue]
	1644263632208 -> 1644539143040
	1644539143040 [label=AccumulateGrad]
	1644539141456 -> 1644539146688
	1644539141456 [label=TBackward0]
	1644539138816 -> 1644539141456
	1644263632400 [label="25.channel_att.fc2.weight
 (256, 16)" fillcolor=lightblue]
	1644263632400 -> 1644539138816
	1644539138816 [label=AccumulateGrad]
	1644539138768 -> 1644539142176
	1644539138768 [label=SigmoidBackward0]
	1644539146784 -> 1644539138768
	1644539146784 [label=ConvolutionBackward0]
	1644539146880 -> 1644539146784
	1644539146880 [label=CatBackward0]
	1644539135216 -> 1644539146880
	1644539135216 [label=MeanBackward1]
	1644539138864 -> 1644539135216
	1644539135360 -> 1644539146880
	1644539135360 [label=MaxBackward0]
	1644539138864 -> 1644539135360
	1644539141504 -> 1644539146784
	1644263632592 [label="25.spatial_att.conv.weight
 (1, 2, 7, 7)" fillcolor=lightblue]
	1644263632592 -> 1644539141504
	1644539141504 [label=AccumulateGrad]
	1644539138048 -> 1644539138144
	1644539138048 [label=TBackward0]
	1644539136992 -> 1644539138048
	1644263632688 [label="29.weight
 (19, 256)" fillcolor=lightblue]
	1644263632688 -> 1644539136992
	1644539136992 [label=AccumulateGrad]
	1644539138144 -> 1644291213744
}
