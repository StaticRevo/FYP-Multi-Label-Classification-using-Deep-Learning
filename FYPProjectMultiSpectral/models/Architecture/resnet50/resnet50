digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2633578180496 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2633532503376 [label=AddmmBackward0]
	2633532503040 -> 2633532503376
	2633528898096 [label="fc.bias
 (19)" fillcolor=lightblue]
	2633528898096 -> 2633532503040
	2633532503040 [label=AccumulateGrad]
	2633532502416 -> 2633532503376
	2633532502416 [label=ViewBackward0]
	2633532501936 -> 2633532502416
	2633532501936 [label=MeanBackward1]
	2633532501600 -> 2633532501936
	2633532501600 [label=ReluBackward0]
	2633532501120 -> 2633532501600
	2633532501120 [label=AddBackward0]
	2633532500640 -> 2633532501120
	2633532500640 [label=CudnnBatchNormBackward0]
	2633532499536 -> 2633532500640
	2633532499536 [label=ConvolutionBackward0]
	2633532498576 -> 2633532499536
	2633532498576 [label=ReluBackward0]
	2633532493440 -> 2633532498576
	2633532493440 [label=CudnnBatchNormBackward0]
	2633532491088 -> 2633532493440
	2633532491088 [label=ConvolutionBackward0]
	2633532506064 -> 2633532491088
	2633532506064 [label=ReluBackward0]
	2633532505824 -> 2633532506064
	2633532505824 [label=CudnnBatchNormBackward0]
	2633532505488 -> 2633532505824
	2633532505488 [label=ConvolutionBackward0]
	2633532500496 -> 2633532505488
	2633532500496 [label=ReluBackward0]
	2633532505344 -> 2633532500496
	2633532505344 [label=AddBackward0]
	2633532505008 -> 2633532505344
	2633532505008 [label=CudnnBatchNormBackward0]
	2633532505056 -> 2633532505008
	2633532505056 [label=ConvolutionBackward0]
	2633532504720 -> 2633532505056
	2633532504720 [label=ReluBackward0]
	2633532504768 -> 2633532504720
	2633532504768 [label=CudnnBatchNormBackward0]
	2633532504672 -> 2633532504768
	2633532504672 [label=ConvolutionBackward0]
	2633532504048 -> 2633532504672
	2633532504048 [label=ReluBackward0]
	2633532504096 -> 2633532504048
	2633532504096 [label=CudnnBatchNormBackward0]
	2633532503952 -> 2633532504096
	2633532503952 [label=ConvolutionBackward0]
	2633532505200 -> 2633532503952
	2633532505200 [label=ReluBackward0]
	2633532503616 -> 2633532505200
	2633532503616 [label=AddBackward0]
	2633532503472 -> 2633532503616
	2633532503472 [label=CudnnBatchNormBackward0]
	2633532503088 -> 2633532503472
	2633532503088 [label=ConvolutionBackward0]
	2633532503232 -> 2633532503088
	2633532503232 [label=ReluBackward0]
	2633532502800 -> 2633532503232
	2633532502800 [label=CudnnBatchNormBackward0]
	2633532502704 -> 2633532502800
	2633532502704 [label=ConvolutionBackward0]
	2633532502512 -> 2633532502704
	2633532502512 [label=ReluBackward0]
	2633532502128 -> 2633532502512
	2633532502128 [label=CudnnBatchNormBackward0]
	2633532502368 -> 2633532502128
	2633532502368 [label=ConvolutionBackward0]
	2633532501984 -> 2633532502368
	2633532501984 [label=ReluBackward0]
	2633532501744 -> 2633532501984
	2633532501744 [label=AddBackward0]
	2633532501696 -> 2633532501744
	2633532501696 [label=CudnnBatchNormBackward0]
	2633532501504 -> 2633532501696
	2633532501504 [label=ConvolutionBackward0]
	2633532501408 -> 2633532501504
	2633532501408 [label=ReluBackward0]
	2633532501072 -> 2633532501408
	2633532501072 [label=CudnnBatchNormBackward0]
	2633532500880 -> 2633532501072
	2633532500880 [label=ConvolutionBackward0]
	2633532500736 -> 2633532500880
	2633532500736 [label=ReluBackward0]
	2633532500544 -> 2633532500736
	2633532500544 [label=CudnnBatchNormBackward0]
	2633532500208 -> 2633532500544
	2633532500208 [label=ConvolutionBackward0]
	2633532501888 -> 2633532500208
	2633532501888 [label=ReluBackward0]
	2633532500064 -> 2633532501888
	2633532500064 [label=AddBackward0]
	2633532499728 -> 2633532500064
	2633532499728 [label=CudnnBatchNormBackward0]
	2633532499776 -> 2633532499728
	2633532499776 [label=ConvolutionBackward0]
	2633532499440 -> 2633532499776
	2633532499440 [label=ReluBackward0]
	2633532499488 -> 2633532499440
	2633532499488 [label=CudnnBatchNormBackward0]
	2633532499392 -> 2633532499488
	2633532499392 [label=ConvolutionBackward0]
	2633532498768 -> 2633532499392
	2633532498768 [label=ReluBackward0]
	2633532498816 -> 2633532498768
	2633532498816 [label=CudnnBatchNormBackward0]
	2633532498672 -> 2633532498816
	2633532498672 [label=ConvolutionBackward0]
	2633532499920 -> 2633532498672
	2633532499920 [label=ReluBackward0]
	2633532498336 -> 2633532499920
	2633532498336 [label=AddBackward0]
	2633532492144 -> 2633532498336
	2633532492144 [label=CudnnBatchNormBackward0]
	2633532493776 -> 2633532492144
	2633532493776 [label=ConvolutionBackward0]
	2633532489840 -> 2633532493776
	2633532489840 [label=ReluBackward0]
	2633532496992 -> 2633532489840
	2633532496992 [label=CudnnBatchNormBackward0]
	2633532497712 -> 2633532496992
	2633532497712 [label=ConvolutionBackward0]
	2633532494640 -> 2633532497712
	2633532494640 [label=ReluBackward0]
	2633532497328 -> 2633532494640
	2633532497328 [label=CudnnBatchNormBackward0]
	2633532496656 -> 2633532497328
	2633532496656 [label=ConvolutionBackward0]
	2633532498432 -> 2633532496656
	2633532498432 [label=ReluBackward0]
	2633532497136 -> 2633532498432
	2633532497136 [label=AddBackward0]
	2633532495168 -> 2633532497136
	2633532495168 [label=CudnnBatchNormBackward0]
	2633532494112 -> 2633532495168
	2633532494112 [label=ConvolutionBackward0]
	2633532493344 -> 2633532494112
	2633532493344 [label=ReluBackward0]
	2633532498048 -> 2633532493344
	2633532498048 [label=CudnnBatchNormBackward0]
	2633532497568 -> 2633532498048
	2633532497568 [label=ConvolutionBackward0]
	2633532495456 -> 2633532497568
	2633532495456 [label=ReluBackward0]
	2633532496128 -> 2633532495456
	2633532496128 [label=CudnnBatchNormBackward0]
	2633532496320 -> 2633532496128
	2633532496320 [label=ConvolutionBackward0]
	2633532496224 -> 2633532496320
	2633532496224 [label=ReluBackward0]
	2633532490608 -> 2633532496224
	2633532490608 [label=AddBackward0]
	2633532492672 -> 2633532490608
	2633532492672 [label=CudnnBatchNormBackward0]
	2633532492912 -> 2633532492672
	2633532492912 [label=ConvolutionBackward0]
	2633532492336 -> 2633532492912
	2633532492336 [label=ReluBackward0]
	2633532491376 -> 2633532492336
	2633532491376 [label=CudnnBatchNormBackward0]
	2633532494928 -> 2633532491376
	2633532494928 [label=ConvolutionBackward0]
	2633532492768 -> 2633532494928
	2633532492768 [label=ReluBackward0]
	2633532497424 -> 2633532492768
	2633532497424 [label=CudnnBatchNormBackward0]
	2633532491616 -> 2633532497424
	2633532491616 [label=ConvolutionBackward0]
	2633532497616 -> 2633532491616
	2633532497616 [label=ReluBackward0]
	2633532494688 -> 2633532497616
	2633532494688 [label=AddBackward0]
	2633532490368 -> 2633532494688
	2633532490368 [label=CudnnBatchNormBackward0]
	2633532492864 -> 2633532490368
	2633532492864 [label=ConvolutionBackward0]
	2633532494304 -> 2633532492864
	2633532494304 [label=ReluBackward0]
	2633604250736 -> 2633532494304
	2633604250736 [label=CudnnBatchNormBackward0]
	2633604251504 -> 2633604250736
	2633604251504 [label=ConvolutionBackward0]
	2633604250352 -> 2633604251504
	2633604250352 [label=ReluBackward0]
	2633604250448 -> 2633604250352
	2633604250448 [label=CudnnBatchNormBackward0]
	2633604250400 -> 2633604250448
	2633604250400 [label=ConvolutionBackward0]
	2633604250208 -> 2633604250400
	2633604250208 [label=ReluBackward0]
	2633604250880 -> 2633604250208
	2633604250880 [label=AddBackward0]
	2633604251264 -> 2633604250880
	2633604251264 [label=CudnnBatchNormBackward0]
	2633532825776 -> 2633604251264
	2633532825776 [label=ConvolutionBackward0]
	2633578585952 -> 2633532825776
	2633578585952 [label=ReluBackward0]
	2633578584848 -> 2633578585952
	2633578584848 [label=CudnnBatchNormBackward0]
	2633578584368 -> 2633578584848
	2633578584368 [label=ConvolutionBackward0]
	2633578583408 -> 2633578584368
	2633578583408 [label=ReluBackward0]
	2633578583072 -> 2633578583408
	2633578583072 [label=CudnnBatchNormBackward0]
	2633578582592 -> 2633578583072
	2633578582592 [label=ConvolutionBackward0]
	2633604250784 -> 2633578582592
	2633604250784 [label=ReluBackward0]
	2633578581152 -> 2633604250784
	2633578581152 [label=AddBackward0]
	2633578580672 -> 2633578581152
	2633578580672 [label=CudnnBatchNormBackward0]
	2633578579568 -> 2633578580672
	2633578579568 [label=ConvolutionBackward0]
	2633578578608 -> 2633578579568
	2633578578608 [label=ReluBackward0]
	2633578578272 -> 2633578578608
	2633578578272 [label=CudnnBatchNormBackward0]
	2633578586336 -> 2633578578272
	2633578586336 [label=ConvolutionBackward0]
	2633578586240 -> 2633578586336
	2633578586240 [label=ReluBackward0]
	2633578585904 -> 2633578586240
	2633578585904 [label=CudnnBatchNormBackward0]
	2633578585712 -> 2633578585904
	2633578585712 [label=ConvolutionBackward0]
	2633578580528 -> 2633578585712
	2633578580528 [label=ReluBackward0]
	2633578585424 -> 2633578580528
	2633578585424 [label=AddBackward0]
	2633578585232 -> 2633578585424
	2633578585232 [label=CudnnBatchNormBackward0]
	2633578585280 -> 2633578585232
	2633578585280 [label=ConvolutionBackward0]
	2633578584896 -> 2633578585280
	2633578584896 [label=ReluBackward0]
	2633578584656 -> 2633578584896
	2633578584656 [label=CudnnBatchNormBackward0]
	2633578584608 -> 2633578584656
	2633578584608 [label=ConvolutionBackward0]
	2633578584272 -> 2633578584608
	2633578584272 [label=ReluBackward0]
	2633578584320 -> 2633578584272
	2633578584320 [label=CudnnBatchNormBackward0]
	2633578584224 -> 2633578584320
	2633578584224 [label=ConvolutionBackward0]
	2633578585376 -> 2633578584224
	2633578585376 [label=ReluBackward0]
	2633578583840 -> 2633578585376
	2633578583840 [label=AddBackward0]
	2633578583744 -> 2633578583840
	2633578583744 [label=CudnnBatchNormBackward0]
	2633578583312 -> 2633578583744
	2633578583312 [label=ConvolutionBackward0]
	2633578583168 -> 2633578583312
	2633578583168 [label=ReluBackward0]
	2633578582976 -> 2633578583168
	2633578582976 [label=CudnnBatchNormBackward0]
	2633578582640 -> 2633578582976
	2633578582640 [label=ConvolutionBackward0]
	2633578582784 -> 2633578582640
	2633578582784 [label=ReluBackward0]
	2633578582352 -> 2633578582784
	2633578582352 [label=CudnnBatchNormBackward0]
	2633578582256 -> 2633578582352
	2633578582256 [label=ConvolutionBackward0]
	2633578582064 -> 2633578582256
	2633578582064 [label=ReluBackward0]
	2633578581680 -> 2633578582064
	2633578581680 [label=AddBackward0]
	2633578581920 -> 2633578581680
	2633578581920 [label=CudnnBatchNormBackward0]
	2633578581584 -> 2633578581920
	2633578581584 [label=ConvolutionBackward0]
	2633578581296 -> 2633578581584
	2633578581296 [label=ReluBackward0]
	2633578581344 -> 2633578581296
	2633578581344 [label=CudnnBatchNormBackward0]
	2633578581056 -> 2633578581344
	2633578581056 [label=ConvolutionBackward0]
	2633578580960 -> 2633578581056
	2633578580960 [label=ReluBackward0]
	2633578580624 -> 2633578580960
	2633578580624 [label=CudnnBatchNormBackward0]
	2633578580432 -> 2633578580624
	2633578580432 [label=ConvolutionBackward0]
	2633578581776 -> 2633578580432
	2633578581776 [label=ReluBackward0]
	2633578580144 -> 2633578581776
	2633578580144 [label=AddBackward0]
	2633578579952 -> 2633578580144
	2633578579952 [label=CudnnBatchNormBackward0]
	2633578580000 -> 2633578579952
	2633578580000 [label=ConvolutionBackward0]
	2633578579616 -> 2633578580000
	2633578579616 [label=ReluBackward0]
	2633578579376 -> 2633578579616
	2633578579376 [label=CudnnBatchNormBackward0]
	2633578579328 -> 2633578579376
	2633578579328 [label=ConvolutionBackward0]
	2633578578992 -> 2633578579328
	2633578578992 [label=ReluBackward0]
	2633578579040 -> 2633578578992
	2633578579040 [label=CudnnBatchNormBackward0]
	2633578578944 -> 2633578579040
	2633578578944 [label=ConvolutionBackward0]
	2633578580096 -> 2633578578944
	2633578580096 [label=ReluBackward0]
	2633578578560 -> 2633578580096
	2633578578560 [label=AddBackward0]
	2633578578464 -> 2633578578560
	2633578578464 [label=CudnnBatchNormBackward0]
	2633578578032 -> 2633578578464
	2633578578032 [label=ConvolutionBackward0]
	2633578586864 -> 2633578578032
	2633578586864 [label=ReluBackward0]
	2633578587200 -> 2633578586864
	2633578587200 [label=CudnnBatchNormBackward0]
	2633578587296 -> 2633578587200
	2633578587296 [label=ConvolutionBackward0]
	2633578587488 -> 2633578587296
	2633578587488 [label=ReluBackward0]
	2633578587632 -> 2633578587488
	2633578587632 [label=CudnnBatchNormBackward0]
	2633578587728 -> 2633578587632
	2633578587728 [label=ConvolutionBackward0]
	2633578587920 -> 2633578587728
	2633578587920 [label=MaxPool2DWithIndicesBackward0]
	2633578588064 -> 2633578587920
	2633578588064 [label=ReluBackward0]
	2633578588160 -> 2633578588064
	2633578588160 [label=CudnnBatchNormBackward0]
	2633578588256 -> 2633578588160
	2633578588256 [label=ConvolutionBackward0]
	2633578588448 -> 2633578588256
	2633528897904 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2633528897904 -> 2633578588448
	2633578588448 [label=AccumulateGrad]
	2633578588208 -> 2633578588160
	2633537224784 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2633537224784 -> 2633578588208
	2633578588208 [label=AccumulateGrad]
	2633578587968 -> 2633578588160
	2633538076016 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2633538076016 -> 2633578587968
	2633578587968 [label=AccumulateGrad]
	2633578587872 -> 2633578587728
	2633538075632 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2633538075632 -> 2633578587872
	2633578587872 [label=AccumulateGrad]
	2633578587680 -> 2633578587632
	2633538075536 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2633538075536 -> 2633578587680
	2633578587680 [label=AccumulateGrad]
	2633578587536 -> 2633578587632
	2633538074768 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2633538074768 -> 2633578587536
	2633578587536 [label=AccumulateGrad]
	2633578587440 -> 2633578587296
	2633538074864 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2633538074864 -> 2633578587440
	2633578587440 [label=AccumulateGrad]
	2633578587248 -> 2633578587200
	2633538074960 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2633538074960 -> 2633578587248
	2633578587248 [label=AccumulateGrad]
	2633578587104 -> 2633578587200
	2633538075056 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2633538075056 -> 2633578587104
	2633578587104 [label=AccumulateGrad]
	2633578587008 -> 2633578578032
	2633538075344 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2633538075344 -> 2633578587008
	2633578587008 [label=AccumulateGrad]
	2633578578176 -> 2633578578464
	2633538074384 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2633538074384 -> 2633578578176
	2633578578176 [label=AccumulateGrad]
	2633578578224 -> 2633578578464
	2633538074288 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2633538074288 -> 2633578578224
	2633578578224 [label=AccumulateGrad]
	2633578578368 -> 2633578578560
	2633578578368 [label=CudnnBatchNormBackward0]
	2633578587392 -> 2633578578368
	2633578587392 [label=ConvolutionBackward0]
	2633578587920 -> 2633578587392
	2633578587776 -> 2633578587392
	2633538076112 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2633538076112 -> 2633578587776
	2633578587776 [label=AccumulateGrad]
	2633578577984 -> 2633578578368
	2633538076208 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2633538076208 -> 2633578577984
	2633578577984 [label=AccumulateGrad]
	2633578578080 -> 2633578578368
	2633538076304 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2633538076304 -> 2633578578080
	2633578578080 [label=AccumulateGrad]
	2633578578320 -> 2633578578944
	2633538074192 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2633538074192 -> 2633578578320
	2633578578320 [label=AccumulateGrad]
	2633578578848 -> 2633578579040
	2633538071792 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2633538071792 -> 2633578578848
	2633578578848 [label=AccumulateGrad]
	2633578578800 -> 2633578579040
	2633538073712 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2633538073712 -> 2633578578800
	2633578578800 [label=AccumulateGrad]
	2633578579136 -> 2633578579328
	2633538074096 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2633538074096 -> 2633578579136
	2633578579136 [label=AccumulateGrad]
	2633578579520 -> 2633578579376
	2633538074000 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2633538074000 -> 2633578579520
	2633578579520 [label=AccumulateGrad]
	2633578579472 -> 2633578579376
	2633529329520 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2633529329520 -> 2633578579472
	2633578579472 [label=AccumulateGrad]
	2633578579664 -> 2633578580000
	2633529329904 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2633529329904 -> 2633578579664
	2633578579664 [label=AccumulateGrad]
	2633578579856 -> 2633578579952
	2633529330000 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2633529330000 -> 2633578579856
	2633578579856 [label=AccumulateGrad]
	2633578579760 -> 2633578579952
	2633529330096 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2633529330096 -> 2633578579760
	2633578579760 [label=AccumulateGrad]
	2633578580096 -> 2633578580144
	2633578580288 -> 2633578580432
	2633529330480 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2633529330480 -> 2633578580288
	2633578580288 [label=AccumulateGrad]
	2633578580576 -> 2633578580624
	2633529330576 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2633529330576 -> 2633578580576
	2633578580576 [label=AccumulateGrad]
	2633578580768 -> 2633578580624
	2633529330672 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2633529330672 -> 2633578580768
	2633578580768 [label=AccumulateGrad]
	2633578580816 -> 2633578581056
	2633529331056 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2633529331056 -> 2633578580816
	2633578580816 [label=AccumulateGrad]
	2633578581104 -> 2633578581344
	2633529331152 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2633529331152 -> 2633578581104
	2633578581104 [label=AccumulateGrad]
	2633578581440 -> 2633578581344
	2633529331248 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2633529331248 -> 2633578581440
	2633578581440 [label=AccumulateGrad]
	2633578581200 -> 2633578581584
	2633529331632 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2633529331632 -> 2633578581200
	2633578581200 [label=AccumulateGrad]
	2633578581824 -> 2633578581920
	2633529331728 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2633529331728 -> 2633578581824
	2633578581824 [label=AccumulateGrad]
	2633578581728 -> 2633578581920
	2633529331824 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2633529331824 -> 2633578581728
	2633578581728 [label=AccumulateGrad]
	2633578581776 -> 2633578581680
	2633578582304 -> 2633578582256
	2633529332784 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2633529332784 -> 2633578582304
	2633578582304 [label=AccumulateGrad]
	2633578582160 -> 2633578582352
	2633529332880 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2633529332880 -> 2633578582160
	2633578582160 [label=AccumulateGrad]
	2633578582544 -> 2633578582352
	2633529332976 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2633529332976 -> 2633578582544
	2633578582544 [label=AccumulateGrad]
	2633578582688 -> 2633578582640
	2633529333360 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2633529333360 -> 2633578582688
	2633578582688 [label=AccumulateGrad]
	2633578582832 -> 2633578582976
	2633529333456 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2633529333456 -> 2633578582832
	2633578582832 [label=AccumulateGrad]
	2633578583264 -> 2633578582976
	2633529333552 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2633529333552 -> 2633578583264
	2633578583264 [label=AccumulateGrad]
	2633578583360 -> 2633578583312
	2633529333936 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2633529333936 -> 2633578583360
	2633578583360 [label=AccumulateGrad]
	2633578583456 -> 2633578583744
	2633529334032 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2633529334032 -> 2633578583456
	2633578583456 [label=AccumulateGrad]
	2633578583504 -> 2633578583744
	2633529334128 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2633529334128 -> 2633578583504
	2633578583504 [label=AccumulateGrad]
	2633578583648 -> 2633578583840
	2633578583648 [label=CudnnBatchNormBackward0]
	2633578582880 -> 2633578583648
	2633578582880 [label=ConvolutionBackward0]
	2633578582064 -> 2633578582880
	2633578582400 -> 2633578582880
	2633529332208 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2633529332208 -> 2633578582400
	2633578582400 [label=AccumulateGrad]
	2633578583216 -> 2633578583648
	2633529332304 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2633529332304 -> 2633578583216
	2633578583216 [label=AccumulateGrad]
	2633578583120 -> 2633578583648
	2633529332400 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2633529332400 -> 2633578583120
	2633578583120 [label=AccumulateGrad]
	2633578583600 -> 2633578584224
	2633529334512 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2633529334512 -> 2633578583600
	2633578583600 [label=AccumulateGrad]
	2633578584128 -> 2633578584320
	2633529334608 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2633529334608 -> 2633578584128
	2633578584128 [label=AccumulateGrad]
	2633578584080 -> 2633578584320
	2633529334704 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2633529334704 -> 2633578584080
	2633578584080 [label=AccumulateGrad]
	2633578584416 -> 2633578584608
	2633529335088 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2633529335088 -> 2633578584416
	2633578584416 [label=AccumulateGrad]
	2633578584800 -> 2633578584656
	2633529335184 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2633529335184 -> 2633578584800
	2633578584800 [label=AccumulateGrad]
	2633578584752 -> 2633578584656
	2633529335280 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2633529335280 -> 2633578584752
	2633578584752 [label=AccumulateGrad]
	2633578584944 -> 2633578585280
	2633529335664 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2633529335664 -> 2633578584944
	2633578584944 [label=AccumulateGrad]
	2633578585136 -> 2633578585232
	2633529335760 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2633529335760 -> 2633578585136
	2633578585136 [label=AccumulateGrad]
	2633578585040 -> 2633578585232
	2633529335856 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2633529335856 -> 2633578585040
	2633578585040 [label=AccumulateGrad]
	2633578585376 -> 2633578585424
	2633578585568 -> 2633578585712
	2633529336240 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2633529336240 -> 2633578585568
	2633578585568 [label=AccumulateGrad]
	2633578585856 -> 2633578585904
	2633529336336 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2633529336336 -> 2633578585856
	2633578585856 [label=AccumulateGrad]
	2633578586048 -> 2633578585904
	2633529336432 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2633529336432 -> 2633578586048
	2633578586048 [label=AccumulateGrad]
	2633578586096 -> 2633578586336
	2633529336816 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2633529336816 -> 2633578586096
	2633578586096 [label=AccumulateGrad]
	2633578586384 -> 2633578578272
	2633529336912 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2633529336912 -> 2633578586384
	2633578586384 [label=AccumulateGrad]
	2633578578752 -> 2633578578272
	2633529337008 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2633529337008 -> 2633578578752
	2633578578752 [label=AccumulateGrad]
	2633578579232 -> 2633578579568
	2633529337392 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2633529337392 -> 2633578579232
	2633578579232 [label=AccumulateGrad]
	2633578580192 -> 2633578580672
	2633529337488 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2633529337488 -> 2633578580192
	2633578580192 [label=AccumulateGrad]
	2633578580048 -> 2633578580672
	2633529337584 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2633529337584 -> 2633578580048
	2633578580048 [label=AccumulateGrad]
	2633578580528 -> 2633578581152
	2633578581632 -> 2633578582592
	2633529337968 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2633529337968 -> 2633578581632
	2633578581632 [label=AccumulateGrad]
	2633578582448 -> 2633578583072
	2633529338064 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2633529338064 -> 2633578582448
	2633578582448 [label=AccumulateGrad]
	2633578583552 -> 2633578583072
	2633529338160 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2633529338160 -> 2633578583552
	2633578583552 [label=AccumulateGrad]
	2633578584032 -> 2633578584368
	2633529338544 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2633529338544 -> 2633578584032
	2633578584032 [label=AccumulateGrad]
	2633578584992 -> 2633578584848
	2633529338640 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2633529338640 -> 2633578584992
	2633578584992 [label=AccumulateGrad]
	2633578585328 -> 2633578584848
	2633529338736 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2633529338736 -> 2633578585328
	2633578585328 [label=AccumulateGrad]
	2633578585808 -> 2633532825776
	2633529339120 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2633529339120 -> 2633578585808
	2633578585808 [label=AccumulateGrad]
	2633604249200 -> 2633604251264
	2633529339216 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2633529339216 -> 2633604249200
	2633604249200 [label=AccumulateGrad]
	2633604251072 -> 2633604251264
	2633529339312 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2633529339312 -> 2633604251072
	2633604251072 [label=AccumulateGrad]
	2633604250784 -> 2633604250880
	2633604251552 -> 2633604250400
	2633529340272 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2633529340272 -> 2633604251552
	2633604251552 [label=AccumulateGrad]
	2633604250064 -> 2633604250448
	2633529340368 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2633529340368 -> 2633604250064
	2633604250064 [label=AccumulateGrad]
	2633604250016 -> 2633604250448
	2633529340464 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2633529340464 -> 2633604250016
	2633604250016 [label=AccumulateGrad]
	2633604250928 -> 2633604251504
	2633529340848 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2633529340848 -> 2633604250928
	2633604250928 [label=AccumulateGrad]
	2633604250304 -> 2633604250736
	2633529340944 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2633529340944 -> 2633604250304
	2633604250304 [label=AccumulateGrad]
	2633604249632 -> 2633604250736
	2633529341040 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2633529341040 -> 2633604249632
	2633604249632 [label=AccumulateGrad]
	2633532495552 -> 2633532492864
	2633529341424 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2633529341424 -> 2633532495552
	2633532495552 [label=AccumulateGrad]
	2633532498192 -> 2633532490368
	2633529341520 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2633529341520 -> 2633532498192
	2633532498192 [label=AccumulateGrad]
	2633532494592 -> 2633532490368
	2633529341616 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2633529341616 -> 2633532494592
	2633532494592 [label=AccumulateGrad]
	2633532494160 -> 2633532494688
	2633532494160 [label=CudnnBatchNormBackward0]
	2633532491280 -> 2633532494160
	2633532491280 [label=ConvolutionBackward0]
	2633604250208 -> 2633532491280
	2633604251120 -> 2633532491280
	2633529339696 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2633529339696 -> 2633604251120
	2633604251120 [label=AccumulateGrad]
	2633604250832 -> 2633532494160
	2633529339792 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2633529339792 -> 2633604250832
	2633604250832 [label=AccumulateGrad]
	2633604250544 -> 2633532494160
	2633529339888 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2633529339888 -> 2633604250544
	2633604250544 [label=AccumulateGrad]
	2633532491472 -> 2633532491616
	2633529342000 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2633529342000 -> 2633532491472
	2633532491472 [label=AccumulateGrad]
	2633532495696 -> 2633532497424
	2633529342096 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2633529342096 -> 2633532495696
	2633532495696 [label=AccumulateGrad]
	2633532491040 -> 2633532497424
	2633529342192 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2633529342192 -> 2633532491040
	2633532491040 [label=AccumulateGrad]
	2633532496080 -> 2633532494928
	2633529342576 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2633529342576 -> 2633532496080
	2633532496080 [label=AccumulateGrad]
	2633532493920 -> 2633532491376
	2633529342672 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2633529342672 -> 2633532493920
	2633532493920 [label=AccumulateGrad]
	2633532489984 -> 2633532491376
	2633529342768 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2633529342768 -> 2633532489984
	2633532489984 [label=AccumulateGrad]
	2633532491904 -> 2633532492912
	2633529343152 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2633529343152 -> 2633532491904
	2633532491904 [label=AccumulateGrad]
	2633532496848 -> 2633532492672
	2633529343248 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2633529343248 -> 2633532496848
	2633532496848 [label=AccumulateGrad]
	2633532491664 -> 2633532492672
	2633529343344 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2633529343344 -> 2633532491664
	2633532491664 [label=AccumulateGrad]
	2633532497616 -> 2633532490608
	2633532497040 -> 2633532496320
	2633529343728 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2633529343728 -> 2633532497040
	2633532497040 [label=AccumulateGrad]
	2633532495024 -> 2633532496128
	2633529343824 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2633529343824 -> 2633532495024
	2633532495024 [label=AccumulateGrad]
	2633532492192 -> 2633532496128
	2633529343920 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2633529343920 -> 2633532492192
	2633532492192 [label=AccumulateGrad]
	2633532493008 -> 2633532497568
	2633528885616 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2633528885616 -> 2633532493008
	2633532493008 [label=AccumulateGrad]
	2633532494880 -> 2633532498048
	2633528885712 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2633528885712 -> 2633532494880
	2633532494880 [label=AccumulateGrad]
	2633532491184 -> 2633532498048
	2633528885808 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2633528885808 -> 2633532491184
	2633532491184 [label=AccumulateGrad]
	2633532496896 -> 2633532494112
	2633528886192 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2633528886192 -> 2633532496896
	2633532496896 [label=AccumulateGrad]
	2633532497760 -> 2633532495168
	2633528886288 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2633528886288 -> 2633532497760
	2633532497760 [label=AccumulateGrad]
	2633532497952 -> 2633532495168
	2633528886384 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2633528886384 -> 2633532497952
	2633532497952 [label=AccumulateGrad]
	2633532496224 -> 2633532497136
	2633532496368 -> 2633532496656
	2633528886768 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2633528886768 -> 2633532496368
	2633532496368 [label=AccumulateGrad]
	2633532495072 -> 2633532497328
	2633528886864 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2633528886864 -> 2633532495072
	2633532495072 [label=AccumulateGrad]
	2633532494400 -> 2633532497328
	2633528886960 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2633528886960 -> 2633532494400
	2633532494400 [label=AccumulateGrad]
	2633532495984 -> 2633532497712
	2633528887344 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2633528887344 -> 2633532495984
	2633532495984 [label=AccumulateGrad]
	2633532495504 -> 2633532496992
	2633528887440 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2633528887440 -> 2633532495504
	2633532495504 [label=AccumulateGrad]
	2633532492432 -> 2633532496992
	2633528887536 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2633528887536 -> 2633532492432
	2633532492432 [label=AccumulateGrad]
	2633532490848 -> 2633532493776
	2633528887920 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2633528887920 -> 2633532490848
	2633532490848 [label=AccumulateGrad]
	2633532495744 -> 2633532492144
	2633528888016 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2633528888016 -> 2633532495744
	2633532495744 [label=AccumulateGrad]
	2633532495888 -> 2633532492144
	2633528888112 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2633528888112 -> 2633532495888
	2633532495888 [label=AccumulateGrad]
	2633532498432 -> 2633532498336
	2633532498384 -> 2633532498672
	2633528888496 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2633528888496 -> 2633532498384
	2633532498384 [label=AccumulateGrad]
	2633532498912 -> 2633532498816
	2633528888592 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2633528888592 -> 2633532498912
	2633532498912 [label=AccumulateGrad]
	2633532498864 -> 2633532498816
	2633528888688 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2633528888688 -> 2633532498864
	2633532498864 [label=AccumulateGrad]
	2633532498960 -> 2633532499392
	2633528889072 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2633528889072 -> 2633532498960
	2633532498960 [label=AccumulateGrad]
	2633532499296 -> 2633532499488
	2633528889168 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2633528889168 -> 2633532499296
	2633532499296 [label=AccumulateGrad]
	2633532499248 -> 2633532499488
	2633528889264 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2633528889264 -> 2633532499248
	2633532499248 [label=AccumulateGrad]
	2633532499584 -> 2633532499776
	2633528889648 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2633528889648 -> 2633532499584
	2633532499584 [label=AccumulateGrad]
	2633532499968 -> 2633532499728
	2633528889744 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2633528889744 -> 2633532499968
	2633532499968 [label=AccumulateGrad]
	2633532499824 -> 2633532499728
	2633528889840 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2633528889840 -> 2633532499824
	2633532499824 [label=AccumulateGrad]
	2633532499920 -> 2633532500064
	2633532500352 -> 2633532500208
	2633528890224 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2633528890224 -> 2633532500352
	2633532500352 [label=AccumulateGrad]
	2633532500400 -> 2633532500544
	2633528890320 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2633528890320 -> 2633532500400
	2633532500400 [label=AccumulateGrad]
	2633532500832 -> 2633532500544
	2633528890416 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2633528890416 -> 2633532500832
	2633532500832 [label=AccumulateGrad]
	2633532500928 -> 2633532500880
	2633528890800 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2633528890800 -> 2633532500928
	2633532500928 [label=AccumulateGrad]
	2633532501024 -> 2633532501072
	2633528890896 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2633528890896 -> 2633532501024
	2633532501024 [label=AccumulateGrad]
	2633532501216 -> 2633532501072
	2633528890992 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2633528890992 -> 2633532501216
	2633532501216 [label=AccumulateGrad]
	2633532501264 -> 2633532501504
	2633528891376 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2633528891376 -> 2633532501264
	2633532501264 [label=AccumulateGrad]
	2633532501552 -> 2633532501696
	2633528891472 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2633528891472 -> 2633532501552
	2633532501552 [label=AccumulateGrad]
	2633532501792 -> 2633532501696
	2633528891568 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2633528891568 -> 2633532501792
	2633532501792 [label=AccumulateGrad]
	2633532501888 -> 2633532501744
	2633532502032 -> 2633532502368
	2633528892528 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2633528892528 -> 2633532502032
	2633532502032 [label=AccumulateGrad]
	2633532502224 -> 2633532502128
	2633528892624 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2633528892624 -> 2633532502224
	2633532502224 [label=AccumulateGrad]
	2633532502464 -> 2633532502128
	2633528892720 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2633528892720 -> 2633532502464
	2633532502464 [label=AccumulateGrad]
	2633532502752 -> 2633532502704
	2633528893104 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2633528893104 -> 2633532502752
	2633532502752 [label=AccumulateGrad]
	2633532502608 -> 2633532502800
	2633528893200 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2633528893200 -> 2633532502608
	2633532502608 [label=AccumulateGrad]
	2633532502992 -> 2633532502800
	2633528893296 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2633528893296 -> 2633532502992
	2633532502992 [label=AccumulateGrad]
	2633532503136 -> 2633532503088
	2633528893680 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2633528893680 -> 2633532503136
	2633532503136 [label=AccumulateGrad]
	2633532503280 -> 2633532503472
	2633528893776 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2633528893776 -> 2633532503280
	2633532503280 [label=AccumulateGrad]
	2633532503424 -> 2633532503472
	2633528893872 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2633528893872 -> 2633532503424
	2633532503424 [label=AccumulateGrad]
	2633532503712 -> 2633532503616
	2633532503712 [label=CudnnBatchNormBackward0]
	2633532502656 -> 2633532503712
	2633532502656 [label=ConvolutionBackward0]
	2633532501984 -> 2633532502656
	2633532502176 -> 2633532502656
	2633528891952 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2633528891952 -> 2633532502176
	2633532502176 [label=AccumulateGrad]
	2633532503328 -> 2633532503712
	2633528892048 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2633528892048 -> 2633532503328
	2633532503328 [label=AccumulateGrad]
	2633532503184 -> 2633532503712
	2633528892144 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2633528892144 -> 2633532503184
	2633532503184 [label=AccumulateGrad]
	2633532503664 -> 2633532503952
	2633528894256 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2633528894256 -> 2633532503664
	2633532503664 [label=AccumulateGrad]
	2633532504192 -> 2633532504096
	2633528894352 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2633528894352 -> 2633532504192
	2633532504192 [label=AccumulateGrad]
	2633532504144 -> 2633532504096
	2633528894448 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2633528894448 -> 2633532504144
	2633532504144 [label=AccumulateGrad]
	2633532504240 -> 2633532504672
	2633528894832 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2633528894832 -> 2633532504240
	2633532504240 [label=AccumulateGrad]
	2633532504576 -> 2633532504768
	2633528894928 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2633528894928 -> 2633532504576
	2633532504576 [label=AccumulateGrad]
	2633532504528 -> 2633532504768
	2633528895024 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2633528895024 -> 2633532504528
	2633532504528 [label=AccumulateGrad]
	2633532504864 -> 2633532505056
	2633528895408 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2633528895408 -> 2633532504864
	2633532504864 [label=AccumulateGrad]
	2633532505248 -> 2633532505008
	2633528895504 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2633528895504 -> 2633532505248
	2633532505248 [label=AccumulateGrad]
	2633532505104 -> 2633532505008
	2633528895600 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2633528895600 -> 2633532505104
	2633532505104 [label=AccumulateGrad]
	2633532505200 -> 2633532505344
	2633532505632 -> 2633532505488
	2633528895984 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2633528895984 -> 2633532505632
	2633532505632 [label=AccumulateGrad]
	2633532505680 -> 2633532505824
	2633528896080 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2633528896080 -> 2633532505680
	2633532505680 [label=AccumulateGrad]
	2633532506016 -> 2633532505824
	2633528896176 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2633528896176 -> 2633532506016
	2633532506016 [label=AccumulateGrad]
	2633532505968 -> 2633532491088
	2633528896560 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2633528896560 -> 2633532505968
	2633532505968 [label=AccumulateGrad]
	2633532493680 -> 2633532493440
	2633528896656 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2633528896656 -> 2633532493680
	2633532493680 [label=AccumulateGrad]
	2633532498720 -> 2633532493440
	2633528896752 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2633528896752 -> 2633532498720
	2633532498720 [label=AccumulateGrad]
	2633532499200 -> 2633532499536
	2633528897136 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2633528897136 -> 2633532499200
	2633532499200 [label=AccumulateGrad]
	2633532500160 -> 2633532500640
	2633528897232 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2633528897232 -> 2633532500160
	2633532500160 [label=AccumulateGrad]
	2633532500016 -> 2633532500640
	2633528897328 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2633528897328 -> 2633532500016
	2633532500016 [label=AccumulateGrad]
	2633532500496 -> 2633532501120
	2633532502560 -> 2633532503376
	2633532502560 [label=TBackward0]
	2633532500976 -> 2633532502560
	2633528898000 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2633528898000 -> 2633532500976
	2633532500976 [label=AccumulateGrad]
	2633532503376 -> 2633578180496
}
