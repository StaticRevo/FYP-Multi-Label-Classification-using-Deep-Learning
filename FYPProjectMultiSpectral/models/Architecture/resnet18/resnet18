digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2161269836208 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2161115657696 [label=AddmmBackward0]
	2161115657360 -> 2161115657696
	2161116182288 [label="fc.bias
 (19)" fillcolor=lightblue]
	2161116182288 -> 2161115657360
	2161115657360 [label=AccumulateGrad]
	2161115656736 -> 2161115657696
	2161115656736 [label=ViewBackward0]
	2161115656256 -> 2161115656736
	2161115656256 [label=MeanBackward1]
	2161115655920 -> 2161115656256
	2161115655920 [label=ReluBackward0]
	2161115655440 -> 2161115655920
	2161115655440 [label=AddBackward0]
	2161115654960 -> 2161115655440
	2161115654960 [label=CudnnBatchNormBackward0]
	2161115653856 -> 2161115654960
	2161115653856 [label=ConvolutionBackward0]
	2161115652896 -> 2161115653856
	2161115652896 [label=ReluBackward0]
	2161115652560 -> 2161115652896
	2161115652560 [label=CudnnBatchNormBackward0]
	2161115652080 -> 2161115652560
	2161115652080 [label=ConvolutionBackward0]
	2161115654816 -> 2161115652080
	2161115654816 [label=ReluBackward0]
	2161115650640 -> 2161115654816
	2161115650640 [label=AddBackward0]
	2161115650160 -> 2161115650640
	2161115650160 [label=CudnnBatchNormBackward0]
	2161115649056 -> 2161115650160
	2161115649056 [label=ConvolutionBackward0]
	2161115648096 -> 2161115649056
	2161115648096 [label=ReluBackward0]
	2161115647760 -> 2161115648096
	2161115647760 [label=CudnnBatchNormBackward0]
	2161115647280 -> 2161115647760
	2161115647280 [label=ConvolutionBackward0]
	2161115646320 -> 2161115647280
	2161115646320 [label=ReluBackward0]
	2161115645216 -> 2161115646320
	2161115645216 [label=AddBackward0]
	2161115644736 -> 2161115645216
	2161115644736 [label=CudnnBatchNormBackward0]
	2161115644400 -> 2161115644736
	2161115644400 [label=ConvolutionBackward0]
	2161115659712 -> 2161115644400
	2161115659712 [label=ReluBackward0]
	2161115659328 -> 2161115659712
	2161115659328 [label=CudnnBatchNormBackward0]
	2161115659568 -> 2161115659328
	2161115659568 [label=ConvolutionBackward0]
	2161115645360 -> 2161115659568
	2161115645360 [label=ReluBackward0]
	2161115658848 -> 2161115645360
	2161115658848 [label=AddBackward0]
	2161115659088 -> 2161115658848
	2161115659088 [label=CudnnBatchNormBackward0]
	2161115658752 -> 2161115659088
	2161115658752 [label=ConvolutionBackward0]
	2161115658464 -> 2161115658752
	2161115658464 [label=ReluBackward0]
	2161115658512 -> 2161115658464
	2161115658512 [label=CudnnBatchNormBackward0]
	2161115658224 -> 2161115658512
	2161115658224 [label=ConvolutionBackward0]
	2161115658128 -> 2161115658224
	2161115658128 [label=ReluBackward0]
	2161115657792 -> 2161115658128
	2161115657792 [label=AddBackward0]
	2161115657600 -> 2161115657792
	2161115657600 [label=CudnnBatchNormBackward0]
	2161115657648 -> 2161115657600
	2161115657648 [label=ConvolutionBackward0]
	2161115657264 -> 2161115657648
	2161115657264 [label=ReluBackward0]
	2161115657024 -> 2161115657264
	2161115657024 [label=CudnnBatchNormBackward0]
	2161115656976 -> 2161115657024
	2161115656976 [label=ConvolutionBackward0]
	2161115657744 -> 2161115656976
	2161115657744 [label=ReluBackward0]
	2161115656544 -> 2161115657744
	2161115656544 [label=AddBackward0]
	2161115656496 -> 2161115656544
	2161115656496 [label=CudnnBatchNormBackward0]
	2161115656304 -> 2161115656496
	2161115656304 [label=ConvolutionBackward0]
	2161115656208 -> 2161115656304
	2161115656208 [label=ReluBackward0]
	2161115655872 -> 2161115656208
	2161115655872 [label=CudnnBatchNormBackward0]
	2161115655680 -> 2161115655872
	2161115655680 [label=ConvolutionBackward0]
	2161115655536 -> 2161115655680
	2161115655536 [label=ReluBackward0]
	2161115655344 -> 2161115655536
	2161115655344 [label=AddBackward0]
	2161115655008 -> 2161115655344
	2161115655008 [label=CudnnBatchNormBackward0]
	2161115655056 -> 2161115655008
	2161115655056 [label=ConvolutionBackward0]
	2161115654720 -> 2161115655056
	2161115654720 [label=ReluBackward0]
	2161115654768 -> 2161115654720
	2161115654768 [label=CudnnBatchNormBackward0]
	2161115654672 -> 2161115654768
	2161115654672 [label=ConvolutionBackward0]
	2161115655200 -> 2161115654672
	2161115655200 [label=ReluBackward0]
	2161115654288 -> 2161115655200
	2161115654288 [label=AddBackward0]
	2161115654192 -> 2161115654288
	2161115654192 [label=CudnnBatchNormBackward0]
	2161115653760 -> 2161115654192
	2161115653760 [label=ConvolutionBackward0]
	2161115653616 -> 2161115653760
	2161115653616 [label=ReluBackward0]
	2161115653424 -> 2161115653616
	2161115653424 [label=CudnnBatchNormBackward0]
	2161115653088 -> 2161115653424
	2161115653088 [label=ConvolutionBackward0]
	2161115654096 -> 2161115653088
	2161115654096 [label=MaxPool2DWithIndicesBackward0]
	2161115652944 -> 2161115654096
	2161115652944 [label=ReluBackward0]
	2161115652608 -> 2161115652944
	2161115652608 [label=CudnnBatchNormBackward0]
	2161115652848 -> 2161115652608
	2161115652848 [label=ConvolutionBackward0]
	2161115652464 -> 2161115652848
	2161116182096 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2161116182096 -> 2161115652464
	2161115652464 [label=AccumulateGrad]
	2161115652704 -> 2161115652608
	2161272656944 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2161272656944 -> 2161115652704
	2161115652704 [label=AccumulateGrad]
	2161115653328 -> 2161115652608
	2161272656656 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2161272656656 -> 2161115653328
	2161115653328 [label=AccumulateGrad]
	2161115653232 -> 2161115653088
	2161272657040 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2161272657040 -> 2161115653232
	2161115653232 [label=AccumulateGrad]
	2161115653280 -> 2161115653424
	2161272657136 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2161272657136 -> 2161115653280
	2161115653280 [label=AccumulateGrad]
	2161115653712 -> 2161115653424
	2161272657232 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2161272657232 -> 2161115653712
	2161115653712 [label=AccumulateGrad]
	2161115653808 -> 2161115653760
	2161272657520 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2161272657520 -> 2161115653808
	2161115653808 [label=AccumulateGrad]
	2161115653904 -> 2161115654192
	2161272656560 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2161272656560 -> 2161115653904
	2161115653904 [label=AccumulateGrad]
	2161115653952 -> 2161115654192
	2161272656464 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2161272656464 -> 2161115653952
	2161115653952 [label=AccumulateGrad]
	2161115654096 -> 2161115654288
	2161115654048 -> 2161115654672
	2161272655600 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2161272655600 -> 2161115654048
	2161115654048 [label=AccumulateGrad]
	2161115654576 -> 2161115654768
	2161272655792 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2161272655792 -> 2161115654576
	2161115654576 [label=AccumulateGrad]
	2161115654528 -> 2161115654768
	2161272655888 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2161272655888 -> 2161115654528
	2161115654528 [label=AccumulateGrad]
	2161115654864 -> 2161115655056
	2161272656368 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2161272656368 -> 2161115654864
	2161115654864 [label=AccumulateGrad]
	2161115655248 -> 2161115655008
	2161272656272 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2161272656272 -> 2161115655248
	2161115655248 [label=AccumulateGrad]
	2161115655104 -> 2161115655008
	2161272655312 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2161272655312 -> 2161115655104
	2161115655104 [label=AccumulateGrad]
	2161115655200 -> 2161115655344
	2161115655728 -> 2161115655680
	2161272654832 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2161272654832 -> 2161115655728
	2161115655728 [label=AccumulateGrad]
	2161115655824 -> 2161115655872
	2161272655120 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2161272655120 -> 2161115655824
	2161115655824 [label=AccumulateGrad]
	2161115656016 -> 2161115655872
	2161272655024 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2161272655024 -> 2161115656016
	2161115656016 [label=AccumulateGrad]
	2161115656064 -> 2161115656304
	2161116174416 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2161116174416 -> 2161115656064
	2161115656064 [label=AccumulateGrad]
	2161115656352 -> 2161115656496
	2161116174512 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2161116174512 -> 2161115656352
	2161115656352 [label=AccumulateGrad]
	2161115656592 -> 2161115656496
	2161116174608 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2161116174608 -> 2161115656592
	2161115656592 [label=AccumulateGrad]
	2161115656688 -> 2161115656544
	2161115656688 [label=CudnnBatchNormBackward0]
	2161115655584 -> 2161115656688
	2161115655584 [label=ConvolutionBackward0]
	2161115655536 -> 2161115655584
	2161115655632 -> 2161115655584
	2161272654928 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2161272654928 -> 2161115655632
	2161115655632 [label=AccumulateGrad]
	2161115655968 -> 2161115656688
	2161272652528 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2161272652528 -> 2161115655968
	2161115655968 [label=AccumulateGrad]
	2161115656160 -> 2161115656688
	2161272652720 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2161272652720 -> 2161115656160
	2161115656160 [label=AccumulateGrad]
	2161115656640 -> 2161115656976
	2161116174992 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2161116174992 -> 2161115656640
	2161115656640 [label=AccumulateGrad]
	2161115657168 -> 2161115657024
	2161116175088 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2161116175088 -> 2161115657168
	2161115657168 [label=AccumulateGrad]
	2161115657120 -> 2161115657024
	2161116175184 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2161116175184 -> 2161115657120
	2161115657120 [label=AccumulateGrad]
	2161115657312 -> 2161115657648
	2161116175568 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2161116175568 -> 2161115657312
	2161115657312 [label=AccumulateGrad]
	2161115657504 -> 2161115657600
	2161116175664 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2161116175664 -> 2161115657504
	2161115657504 [label=AccumulateGrad]
	2161115657408 -> 2161115657600
	2161116175760 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2161116175760 -> 2161115657408
	2161115657408 [label=AccumulateGrad]
	2161115657744 -> 2161115657792
	2161115657984 -> 2161115658224
	2161116176720 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2161116176720 -> 2161115657984
	2161115657984 [label=AccumulateGrad]
	2161115658272 -> 2161115658512
	2161116176816 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2161116176816 -> 2161115658272
	2161115658272 [label=AccumulateGrad]
	2161115658608 -> 2161115658512
	2161116176912 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2161116176912 -> 2161115658608
	2161115658608 [label=AccumulateGrad]
	2161115658368 -> 2161115658752
	2161116177296 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2161116177296 -> 2161115658368
	2161115658368 [label=AccumulateGrad]
	2161115658992 -> 2161115659088
	2161116177392 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2161116177392 -> 2161115658992
	2161115658992 [label=AccumulateGrad]
	2161115658896 -> 2161115659088
	2161116177488 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2161116177488 -> 2161115658896
	2161115658896 [label=AccumulateGrad]
	2161115658944 -> 2161115658848
	2161115658944 [label=CudnnBatchNormBackward0]
	2161115657888 -> 2161115658944
	2161115657888 [label=ConvolutionBackward0]
	2161115658128 -> 2161115657888
	2161115657936 -> 2161115657888
	2161116176144 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2161116176144 -> 2161115657936
	2161115657936 [label=AccumulateGrad]
	2161115658560 -> 2161115658944
	2161116176240 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2161116176240 -> 2161115658560
	2161115658560 [label=AccumulateGrad]
	2161115658704 -> 2161115658944
	2161116176336 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2161116176336 -> 2161115658704
	2161115658704 [label=AccumulateGrad]
	2161115659184 -> 2161115659568
	2161116177872 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2161116177872 -> 2161115659184
	2161115659184 [label=AccumulateGrad]
	2161115659424 -> 2161115659328
	2161116177968 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2161116177968 -> 2161115659424
	2161115659424 [label=AccumulateGrad]
	2161115659664 -> 2161115659328
	2161116178064 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2161116178064 -> 2161115659664
	2161115659664 [label=AccumulateGrad]
	2161115659952 -> 2161115644400
	2161116178448 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2161116178448 -> 2161115659952
	2161115659952 [label=AccumulateGrad]
	2161115644256 -> 2161115644736
	2161116178544 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2161116178544 -> 2161115644256
	2161115644256 [label=AccumulateGrad]
	2161115644880 -> 2161115644736
	2161116178640 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2161116178640 -> 2161115644880
	2161115644880 [label=AccumulateGrad]
	2161115645360 -> 2161115645216
	2161115646176 -> 2161115647280
	2161116179600 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2161116179600 -> 2161115646176
	2161115646176 [label=AccumulateGrad]
	2161115647136 -> 2161115647760
	2161116179696 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2161116179696 -> 2161115647136
	2161115647136 [label=AccumulateGrad]
	2161115648240 -> 2161115647760
	2161116179792 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2161116179792 -> 2161115648240
	2161115648240 [label=AccumulateGrad]
	2161115648720 -> 2161115649056
	2161116180176 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2161116180176 -> 2161115648720
	2161115648720 [label=AccumulateGrad]
	2161115649680 -> 2161115650160
	2161116180272 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2161116180272 -> 2161115649680
	2161115649680 [label=AccumulateGrad]
	2161115649536 -> 2161115650160
	2161116180368 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2161116180368 -> 2161115649536
	2161115649536 [label=AccumulateGrad]
	2161115650016 -> 2161115650640
	2161115650016 [label=CudnnBatchNormBackward0]
	2161115646800 -> 2161115650016
	2161115646800 [label=ConvolutionBackward0]
	2161115646320 -> 2161115646800
	2161115645696 -> 2161115646800
	2161116179024 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2161116179024 -> 2161115645696
	2161115645696 [label=AccumulateGrad]
	2161115648576 -> 2161115650016
	2161116179120 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2161116179120 -> 2161115648576
	2161115648576 [label=AccumulateGrad]
	2161115649200 -> 2161115650016
	2161116179216 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2161116179216 -> 2161115649200
	2161115649200 [label=AccumulateGrad]
	2161115651120 -> 2161115652080
	2161116180752 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2161116180752 -> 2161115651120
	2161115651120 [label=AccumulateGrad]
	2161115651936 -> 2161115652560
	2161116180848 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2161116180848 -> 2161115651936
	2161115651936 [label=AccumulateGrad]
	2161115653040 -> 2161115652560
	2161116180944 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2161116180944 -> 2161115653040
	2161115653040 [label=AccumulateGrad]
	2161115653520 -> 2161115653856
	2161116181328 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2161116181328 -> 2161115653520
	2161115653520 [label=AccumulateGrad]
	2161115654480 -> 2161115654960
	2161116181424 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2161116181424 -> 2161115654480
	2161115654480 [label=AccumulateGrad]
	2161115654336 -> 2161115654960
	2161116181520 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2161116181520 -> 2161115654336
	2161115654336 [label=AccumulateGrad]
	2161115654816 -> 2161115655440
	2161115656880 -> 2161115657696
	2161115656880 [label=TBackward0]
	2161115655296 -> 2161115656880
	2161116182192 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2161116182192 -> 2161115655296
	2161115655296 [label=AccumulateGrad]
	2161115657696 -> 2161269836208
}
