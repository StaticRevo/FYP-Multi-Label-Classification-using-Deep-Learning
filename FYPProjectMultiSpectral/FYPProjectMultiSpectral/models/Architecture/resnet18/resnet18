digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1591391112560 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1593230653088 [label=AddmmBackward0]
	1593230651984 -> 1593230653088
	1591342496272 [label="fc.bias
 (19)" fillcolor=lightblue]
	1591342496272 -> 1593230651984
	1593230651984 [label=AccumulateGrad]
	1593230652128 -> 1593230653088
	1593230652128 [label=ViewBackward0]
	1593230651648 -> 1593230652128
	1593230651648 [label=MeanBackward1]
	1593230650544 -> 1593230651648
	1593230650544 [label=ReluBackward0]
	1593230650064 -> 1593230650544
	1593230650064 [label=AddBackward0]
	1593230649584 -> 1593230650064
	1593230649584 [label=CudnnBatchNormBackward0]
	1593230649248 -> 1593230649584
	1593230649248 [label=ConvolutionBackward0]
	1593230648288 -> 1593230649248
	1593230648288 [label=ReluBackward0]
	1593230647184 -> 1593230648288
	1593230647184 [label=CudnnBatchNormBackward0]
	1593230646704 -> 1593230647184
	1593230646704 [label=ConvolutionBackward0]
	1593230650208 -> 1593230646704
	1593230650208 [label=ReluBackward0]
	1593230645264 -> 1593230650208
	1593230645264 [label=AddBackward0]
	1593230644784 -> 1593230645264
	1593230644784 [label=CudnnBatchNormBackward0]
	1593230644448 -> 1593230644784
	1593230644448 [label=ConvolutionBackward0]
	1593230643488 -> 1593230644448
	1593230643488 [label=ReluBackward0]
	1593230639312 -> 1593230643488
	1593230639312 [label=CudnnBatchNormBackward0]
	1593230641808 -> 1593230639312
	1593230641808 [label=ConvolutionBackward0]
	1593230655440 -> 1593230641808
	1593230655440 [label=ReluBackward0]
	1593230654960 -> 1593230655440
	1593230654960 [label=AddBackward0]
	1593230654768 -> 1593230654960
	1593230654768 [label=CudnnBatchNormBackward0]
	1593230654816 -> 1593230654768
	1593230654816 [label=ConvolutionBackward0]
	1593230654432 -> 1593230654816
	1593230654432 [label=ReluBackward0]
	1593230654192 -> 1593230654432
	1593230654192 [label=CudnnBatchNormBackward0]
	1593230654144 -> 1593230654192
	1593230654144 [label=ConvolutionBackward0]
	1593230654912 -> 1593230654144
	1593230654912 [label=ReluBackward0]
	1593230653712 -> 1593230654912
	1593230653712 [label=AddBackward0]
	1593230653664 -> 1593230653712
	1593230653664 [label=CudnnBatchNormBackward0]
	1593230653472 -> 1593230653664
	1593230653472 [label=ConvolutionBackward0]
	1593230653376 -> 1593230653472
	1593230653376 [label=ReluBackward0]
	1593230653040 -> 1593230653376
	1593230653040 [label=CudnnBatchNormBackward0]
	1593230652848 -> 1593230653040
	1593230652848 [label=ConvolutionBackward0]
	1593230652704 -> 1593230652848
	1593230652704 [label=ReluBackward0]
	1593230652512 -> 1593230652704
	1593230652512 [label=AddBackward0]
	1593230652176 -> 1593230652512
	1593230652176 [label=CudnnBatchNormBackward0]
	1593230652224 -> 1593230652176
	1593230652224 [label=ConvolutionBackward0]
	1593230651888 -> 1593230652224
	1593230651888 [label=ReluBackward0]
	1593230651936 -> 1593230651888
	1593230651936 [label=CudnnBatchNormBackward0]
	1593230651840 -> 1593230651936
	1593230651840 [label=ConvolutionBackward0]
	1593230652368 -> 1593230651840
	1593230652368 [label=ReluBackward0]
	1593230651456 -> 1593230652368
	1593230651456 [label=AddBackward0]
	1593230651360 -> 1593230651456
	1593230651360 [label=CudnnBatchNormBackward0]
	1593230650928 -> 1593230651360
	1593230650928 [label=ConvolutionBackward0]
	1593230650784 -> 1593230650928
	1593230650784 [label=ReluBackward0]
	1593230650592 -> 1593230650784
	1593230650592 [label=CudnnBatchNormBackward0]
	1593230650256 -> 1593230650592
	1593230650256 [label=ConvolutionBackward0]
	1593230650400 -> 1593230650256
	1593230650400 [label=ReluBackward0]
	1593230649968 -> 1593230650400
	1593230649968 [label=AddBackward0]
	1593230649872 -> 1593230649968
	1593230649872 [label=CudnnBatchNormBackward0]
	1593230649920 -> 1593230649872
	1593230649920 [label=ConvolutionBackward0]
	1593230649296 -> 1593230649920
	1593230649296 [label=ReluBackward0]
	1593230649344 -> 1593230649296
	1593230649344 [label=CudnnBatchNormBackward0]
	1593230649200 -> 1593230649344
	1593230649200 [label=ConvolutionBackward0]
	1593230649776 -> 1593230649200
	1593230649776 [label=ReluBackward0]
	1593230648864 -> 1593230649776
	1593230648864 [label=AddBackward0]
	1593230648720 -> 1593230648864
	1593230648720 [label=CudnnBatchNormBackward0]
	1593230648336 -> 1593230648720
	1593230648336 [label=ConvolutionBackward0]
	1593230648480 -> 1593230648336
	1593230648480 [label=ReluBackward0]
	1593230648048 -> 1593230648480
	1593230648048 [label=CudnnBatchNormBackward0]
	1593230647952 -> 1593230648048
	1593230647952 [label=ConvolutionBackward0]
	1593230648960 -> 1593230647952
	1593230648960 [label=MaxPool2DWithIndicesBackward0]
	1593230647568 -> 1593230648960
	1593230647568 [label=ReluBackward0]
	1593230647472 -> 1593230647568
	1593230647472 [label=CudnnBatchNormBackward0]
	1593230647424 -> 1593230647472
	1593230647424 [label=ConvolutionBackward0]
	1593230647088 -> 1593230647424
	1591342496080 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1591342496080 -> 1593230647088
	1593230647088 [label=AccumulateGrad]
	1593230647616 -> 1593230647472
	1591342553744 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1591342553744 -> 1593230647616
	1593230647616 [label=AccumulateGrad]
	1593230647904 -> 1593230647472
	1591342552976 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1591342552976 -> 1593230647904
	1593230647904 [label=AccumulateGrad]
	1593230647760 -> 1593230647952
	1591342552880 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1591342552880 -> 1593230647760
	1593230647760 [label=AccumulateGrad]
	1593230647856 -> 1593230648048
	1591342553072 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1591342553072 -> 1593230647856
	1593230647856 [label=AccumulateGrad]
	1593230648240 -> 1593230648048
	1591342553168 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1591342553168 -> 1593230648240
	1593230648240 [label=AccumulateGrad]
	1593230648384 -> 1593230648336
	1591342553648 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1591342553648 -> 1593230648384
	1593230648384 [label=AccumulateGrad]
	1593230648528 -> 1593230648720
	1591342553552 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1591342553552 -> 1593230648528
	1593230648528 [label=AccumulateGrad]
	1593230648672 -> 1593230648720
	1591342552592 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1591342552592 -> 1593230648672
	1593230648672 [label=AccumulateGrad]
	1593230648960 -> 1593230648864
	1593230648912 -> 1593230649200
	1591342551536 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1591342551536 -> 1593230648912
	1593230648912 [label=AccumulateGrad]
	1593230649440 -> 1593230649344
	1591342551632 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1591342551632 -> 1593230649440
	1593230649440 [label=AccumulateGrad]
	1593230649392 -> 1593230649344
	1591342551824 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1591342551824 -> 1593230649392
	1593230649392 [label=AccumulateGrad]
	1593230649488 -> 1593230649920
	1591342552208 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1591342552208 -> 1593230649488
	1593230649488 [label=AccumulateGrad]
	1593230649824 -> 1593230649872
	1591342552400 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1591342552400 -> 1593230649824
	1593230649824 [label=AccumulateGrad]
	1593230650016 -> 1593230649872
	1591342552304 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1591342552304 -> 1593230650016
	1593230650016 [label=AccumulateGrad]
	1593230649776 -> 1593230649968
	1593230650304 -> 1593230650256
	1591342550768 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1591342550768 -> 1593230650304
	1593230650304 [label=AccumulateGrad]
	1593230650448 -> 1593230650592
	1591342550864 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1591342550864 -> 1593230650448
	1593230650448 [label=AccumulateGrad]
	1593230650880 -> 1593230650592
	1591342551152 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1591342551152 -> 1593230650880
	1593230650880 [label=AccumulateGrad]
	1593230650976 -> 1593230650928
	1591342488400 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1591342488400 -> 1593230650976
	1593230650976 [label=AccumulateGrad]
	1593230651072 -> 1593230651360
	1591342488496 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1591342488496 -> 1593230651072
	1593230651072 [label=AccumulateGrad]
	1593230651120 -> 1593230651360
	1591342488592 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1591342488592 -> 1593230651120
	1593230651120 [label=AccumulateGrad]
	1593230651264 -> 1593230651456
	1593230651264 [label=CudnnBatchNormBackward0]
	1593230650496 -> 1593230651264
	1593230650496 [label=ConvolutionBackward0]
	1593230650400 -> 1593230650496
	1593230650160 -> 1593230650496
	1591342550096 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1591342550096 -> 1593230650160
	1593230650160 [label=AccumulateGrad]
	1593230650832 -> 1593230651264
	1591342550960 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1591342550960 -> 1593230650832
	1593230650832 [label=AccumulateGrad]
	1593230650736 -> 1593230651264
	1591342548560 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1591342548560 -> 1593230650736
	1593230650736 [label=AccumulateGrad]
	1593230651216 -> 1593230651840
	1591342488976 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1591342488976 -> 1593230651216
	1593230651216 [label=AccumulateGrad]
	1593230651744 -> 1593230651936
	1591342489072 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1591342489072 -> 1593230651744
	1593230651744 [label=AccumulateGrad]
	1593230651696 -> 1593230651936
	1591342489168 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1591342489168 -> 1593230651696
	1593230651696 [label=AccumulateGrad]
	1593230652032 -> 1593230652224
	1591342489552 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1591342489552 -> 1593230652032
	1593230652032 [label=AccumulateGrad]
	1593230652416 -> 1593230652176
	1591342489648 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1591342489648 -> 1593230652416
	1593230652416 [label=AccumulateGrad]
	1593230652272 -> 1593230652176
	1591342489744 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1591342489744 -> 1593230652272
	1593230652272 [label=AccumulateGrad]
	1593230652368 -> 1593230652512
	1593230652896 -> 1593230652848
	1591342490704 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1591342490704 -> 1593230652896
	1593230652896 [label=AccumulateGrad]
	1593230652992 -> 1593230653040
	1591342490800 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1591342490800 -> 1593230652992
	1593230652992 [label=AccumulateGrad]
	1593230653184 -> 1593230653040
	1591342490896 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1591342490896 -> 1593230653184
	1593230653184 [label=AccumulateGrad]
	1593230653232 -> 1593230653472
	1591342491280 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1591342491280 -> 1593230653232
	1593230653232 [label=AccumulateGrad]
	1593230653520 -> 1593230653664
	1591342491376 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1591342491376 -> 1593230653520
	1593230653520 [label=AccumulateGrad]
	1593230653760 -> 1593230653664
	1591342491472 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1591342491472 -> 1593230653760
	1593230653760 [label=AccumulateGrad]
	1593230653856 -> 1593230653712
	1593230653856 [label=CudnnBatchNormBackward0]
	1593230652752 -> 1593230653856
	1593230652752 [label=ConvolutionBackward0]
	1593230652704 -> 1593230652752
	1593230652800 -> 1593230652752
	1591342490128 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1591342490128 -> 1593230652800
	1593230652800 [label=AccumulateGrad]
	1593230653136 -> 1593230653856
	1591342490224 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1591342490224 -> 1593230653136
	1593230653136 [label=AccumulateGrad]
	1593230653328 -> 1593230653856
	1591342490320 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1591342490320 -> 1593230653328
	1593230653328 [label=AccumulateGrad]
	1593230653808 -> 1593230654144
	1591342491856 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1591342491856 -> 1593230653808
	1593230653808 [label=AccumulateGrad]
	1593230654336 -> 1593230654192
	1591342491952 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1591342491952 -> 1593230654336
	1593230654336 [label=AccumulateGrad]
	1593230654288 -> 1593230654192
	1591342492048 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1591342492048 -> 1593230654288
	1593230654288 [label=AccumulateGrad]
	1593230654480 -> 1593230654816
	1591342492432 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1591342492432 -> 1593230654480
	1593230654480 [label=AccumulateGrad]
	1593230654672 -> 1593230654768
	1591342492528 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1591342492528 -> 1593230654672
	1593230654672 [label=AccumulateGrad]
	1593230654576 -> 1593230654768
	1591342492624 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1591342492624 -> 1593230654576
	1593230654576 [label=AccumulateGrad]
	1593230654912 -> 1593230654960
	1593230641520 -> 1593230641808
	1591342493584 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1591342493584 -> 1593230641520
	1593230641520 [label=AccumulateGrad]
	1593230641040 -> 1593230639312
	1591342493680 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1591342493680 -> 1593230641040
	1593230641040 [label=AccumulateGrad]
	1593230642864 -> 1593230639312
	1591342493776 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1591342493776 -> 1593230642864
	1593230642864 [label=AccumulateGrad]
	1593230643344 -> 1593230644448
	1591342494160 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1591342494160 -> 1593230643344
	1593230643344 [label=AccumulateGrad]
	1593230644304 -> 1593230644784
	1591342494256 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1591342494256 -> 1593230644304
	1593230644304 [label=AccumulateGrad]
	1593230644928 -> 1593230644784
	1591342494352 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1591342494352 -> 1593230644928
	1593230644928 [label=AccumulateGrad]
	1593230645408 -> 1593230645264
	1593230645408 [label=CudnnBatchNormBackward0]
	1593230640656 -> 1593230645408
	1593230640656 [label=ConvolutionBackward0]
	1593230655440 -> 1593230640656
	1593230655104 -> 1593230640656
	1591342493008 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1591342493008 -> 1593230655104
	1593230655104 [label=AccumulateGrad]
	1593230643968 -> 1593230645408
	1591342493104 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1591342493104 -> 1593230643968
	1593230643968 [label=AccumulateGrad]
	1593230643824 -> 1593230645408
	1591342493200 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1591342493200 -> 1593230643824
	1593230643824 [label=AccumulateGrad]
	1593230645744 -> 1593230646704
	1591342494736 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1591342494736 -> 1593230645744
	1593230645744 [label=AccumulateGrad]
	1593230647328 -> 1593230647184
	1591342494832 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1591342494832 -> 1593230647328
	1593230647328 [label=AccumulateGrad]
	1593230647664 -> 1593230647184
	1591342494928 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1591342494928 -> 1593230647664
	1593230647664 [label=AccumulateGrad]
	1593230648144 -> 1593230649248
	1591342495312 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1591342495312 -> 1593230648144
	1593230648144 [label=AccumulateGrad]
	1593230649104 -> 1593230649584
	1591342495408 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1591342495408 -> 1593230649104
	1593230649104 [label=AccumulateGrad]
	1593230649728 -> 1593230649584
	1591342495504 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1591342495504 -> 1593230649728
	1593230649728 [label=AccumulateGrad]
	1593230650208 -> 1593230650064
	1593230651504 -> 1593230653088
	1593230651504 [label=TBackward0]
	1593230650688 -> 1593230651504
	1591342496176 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1591342496176 -> 1593230650688
	1593230650688 [label=AccumulateGrad]
	1593230653088 -> 1591391112560
}
