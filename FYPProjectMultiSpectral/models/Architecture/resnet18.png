digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1783810199376 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1783795576848 [label=AddmmBackward0]
	1783795576992 -> 1783795576848
	1783795739024 [label="fc.bias
 (19)" fillcolor=lightblue]
	1783795739024 -> 1783795576992
	1783795576992 [label=AccumulateGrad]
	1783795577040 -> 1783795576848
	1783795577040 [label=ViewBackward0]
	1783795577136 -> 1783795577040
	1783795577136 [label=MeanBackward1]
	1783795577280 -> 1783795577136
	1783795577280 [label=ReluBackward0]
	1783795577376 -> 1783795577280
	1783795577376 [label=AddBackward0]
	1783795577472 -> 1783795577376
	1783795577472 [label=CudnnBatchNormBackward0]
	1783795577616 -> 1783795577472
	1783795577616 [label=ConvolutionBackward0]
	1783795577808 -> 1783795577616
	1783795577808 [label=ReluBackward0]
	1783795577952 -> 1783795577808
	1783795577952 [label=CudnnBatchNormBackward0]
	1783795578048 -> 1783795577952
	1783795578048 [label=ConvolutionBackward0]
	1783795577424 -> 1783795578048
	1783795577424 [label=ReluBackward0]
	1783795578192 -> 1783795577424
	1783795578192 [label=AddBackward0]
	1783795578288 -> 1783795578192
	1783795578288 [label=CudnnBatchNormBackward0]
	1783795578432 -> 1783795578288
	1783795578432 [label=ConvolutionBackward0]
	1783788972512 -> 1783795578432
	1783788972512 [label=ReluBackward0]
	1783788972992 -> 1783788972512
	1783788972992 [label=CudnnBatchNormBackward0]
	1783788973136 -> 1783788972992
	1783788973136 [label=ConvolutionBackward0]
	1783788973328 -> 1783788973136
	1783788973328 [label=ReluBackward0]
	1783788973472 -> 1783788973328
	1783788973472 [label=AddBackward0]
	1783788973568 -> 1783788973472
	1783788973568 [label=CudnnBatchNormBackward0]
	1783788973712 -> 1783788973568
	1783788973712 [label=ConvolutionBackward0]
	1783788973904 -> 1783788973712
	1783788973904 [label=ReluBackward0]
	1783788974048 -> 1783788973904
	1783788974048 [label=CudnnBatchNormBackward0]
	1783788974144 -> 1783788974048
	1783788974144 [label=ConvolutionBackward0]
	1783788973520 -> 1783788974144
	1783788973520 [label=ReluBackward0]
	1783788974432 -> 1783788973520
	1783788974432 [label=AddBackward0]
	1783788974528 -> 1783788974432
	1783788974528 [label=CudnnBatchNormBackward0]
	1783788974672 -> 1783788974528
	1783788974672 [label=ConvolutionBackward0]
	1783788974864 -> 1783788974672
	1783788974864 [label=ReluBackward0]
	1783788975008 -> 1783788974864
	1783788975008 [label=CudnnBatchNormBackward0]
	1783788975104 -> 1783788975008
	1783788975104 [label=ConvolutionBackward0]
	1783788975296 -> 1783788975104
	1783788975296 [label=ReluBackward0]
	1783788975440 -> 1783788975296
	1783788975440 [label=AddBackward0]
	1783788975536 -> 1783788975440
	1783788975536 [label=CudnnBatchNormBackward0]
	1783788975680 -> 1783788975536
	1783788975680 [label=ConvolutionBackward0]
	1783788975872 -> 1783788975680
	1783788975872 [label=ReluBackward0]
	1783788976016 -> 1783788975872
	1783788976016 [label=CudnnBatchNormBackward0]
	1783788976112 -> 1783788976016
	1783788976112 [label=ConvolutionBackward0]
	1783788975488 -> 1783788976112
	1783788975488 [label=ReluBackward0]
	1783788976400 -> 1783788975488
	1783788976400 [label=AddBackward0]
	1783788976496 -> 1783788976400
	1783788976496 [label=CudnnBatchNormBackward0]
	1783788976640 -> 1783788976496
	1783788976640 [label=ConvolutionBackward0]
	1783788976832 -> 1783788976640
	1783788976832 [label=ReluBackward0]
	1783788976976 -> 1783788976832
	1783788976976 [label=CudnnBatchNormBackward0]
	1783788977072 -> 1783788976976
	1783788977072 [label=ConvolutionBackward0]
	1783788977264 -> 1783788977072
	1783788977264 [label=ReluBackward0]
	1783788977408 -> 1783788977264
	1783788977408 [label=AddBackward0]
	1783788977504 -> 1783788977408
	1783788977504 [label=CudnnBatchNormBackward0]
	1783788977648 -> 1783788977504
	1783788977648 [label=ConvolutionBackward0]
	1783788977840 -> 1783788977648
	1783788977840 [label=ReluBackward0]
	1783788977984 -> 1783788977840
	1783788977984 [label=CudnnBatchNormBackward0]
	1783788978080 -> 1783788977984
	1783788978080 [label=ConvolutionBackward0]
	1783788977456 -> 1783788978080
	1783788977456 [label=ReluBackward0]
	1783802281552 -> 1783788977456
	1783802281552 [label=AddBackward0]
	1783802281648 -> 1783802281552
	1783802281648 [label=CudnnBatchNormBackward0]
	1783802281792 -> 1783802281648
	1783802281792 [label=ConvolutionBackward0]
	1783802281936 -> 1783802281792
	1783802281936 [label=ReluBackward0]
	1783792828624 -> 1783802281936
	1783792828624 [label=CudnnBatchNormBackward0]
	1783792828720 -> 1783792828624
	1783792828720 [label=ConvolutionBackward0]
	1783802281600 -> 1783792828720
	1783802281600 [label=MaxPool2DWithIndicesBackward0]
	1783792829008 -> 1783802281600
	1783792829008 [label=ReluBackward0]
	1783792829104 -> 1783792829008
	1783792829104 [label=CudnnBatchNormBackward0]
	1783792829200 -> 1783792829104
	1783792829200 [label=ConvolutionBackward0]
	1783792829392 -> 1783792829200
	1783795738928 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1783795738928 -> 1783792829392
	1783792829392 [label=AccumulateGrad]
	1783792829152 -> 1783792829104
	1783788993424 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1783788993424 -> 1783792829152
	1783792829152 [label=AccumulateGrad]
	1783792828816 -> 1783792829104
	1783788993520 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1783788993520 -> 1783792828816
	1783792828816 [label=AccumulateGrad]
	1783792828912 -> 1783792828720
	1783788993904 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1783788993904 -> 1783792828912
	1783792828912 [label=AccumulateGrad]
	1783792828672 -> 1783792828624
	1783788994000 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1783788994000 -> 1783792828672
	1783792828672 [label=AccumulateGrad]
	1783792828528 -> 1783792828624
	1783788994096 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1783788994096 -> 1783792828528
	1783792828528 [label=AccumulateGrad]
	1783802281888 -> 1783802281792
	1783788994480 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1783788994480 -> 1783802281888
	1783802281888 [label=AccumulateGrad]
	1783802281744 -> 1783802281648
	1783795728464 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1783795728464 -> 1783802281744
	1783802281744 [label=AccumulateGrad]
	1783802281696 -> 1783802281648
	1783795728560 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1783795728560 -> 1783802281696
	1783802281696 [label=AccumulateGrad]
	1783802281600 -> 1783802281552
	1783788972656 -> 1783788978080
	1783795728944 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1783795728944 -> 1783788972656
	1783788972656 [label=AccumulateGrad]
	1783788978032 -> 1783788977984
	1783795729040 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1783795729040 -> 1783788978032
	1783788978032 [label=AccumulateGrad]
	1783788977888 -> 1783788977984
	1783795729136 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1783795729136 -> 1783788977888
	1783788977888 [label=AccumulateGrad]
	1783788977792 -> 1783788977648
	1783795729520 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1783795729520 -> 1783788977792
	1783788977792 [label=AccumulateGrad]
	1783788977600 -> 1783788977504
	1783795729616 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1783795729616 -> 1783788977600
	1783788977600 [label=AccumulateGrad]
	1783788977552 -> 1783788977504
	1783795729712 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1783795729712 -> 1783788977552
	1783788977552 [label=AccumulateGrad]
	1783788977456 -> 1783788977408
	1783788977216 -> 1783788977072
	1783795730672 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1783795730672 -> 1783788977216
	1783788977216 [label=AccumulateGrad]
	1783788977024 -> 1783788976976
	1783795730768 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1783795730768 -> 1783788977024
	1783788977024 [label=AccumulateGrad]
	1783788976880 -> 1783788976976
	1783795730864 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1783795730864 -> 1783788976880
	1783788976880 [label=AccumulateGrad]
	1783788976784 -> 1783788976640
	1783795731248 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1783795731248 -> 1783788976784
	1783788976784 [label=AccumulateGrad]
	1783788976592 -> 1783788976496
	1783795731344 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1783795731344 -> 1783788976592
	1783788976592 [label=AccumulateGrad]
	1783788976544 -> 1783788976496
	1783795731440 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1783795731440 -> 1783788976544
	1783788976544 [label=AccumulateGrad]
	1783788976448 -> 1783788976400
	1783788976448 [label=CudnnBatchNormBackward0]
	1783788977168 -> 1783788976448
	1783788977168 [label=ConvolutionBackward0]
	1783788977264 -> 1783788977168
	1783788977312 -> 1783788977168
	1783795730096 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1783795730096 -> 1783788977312
	1783788977312 [label=AccumulateGrad]
	1783788976736 -> 1783788976448
	1783795730192 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1783795730192 -> 1783788976736
	1783788976736 [label=AccumulateGrad]
	1783788976688 -> 1783788976448
	1783795730288 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1783795730288 -> 1783788976688
	1783788976688 [label=AccumulateGrad]
	1783788976304 -> 1783788976112
	1783795731824 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1783795731824 -> 1783788976304
	1783788976304 [label=AccumulateGrad]
	1783788976064 -> 1783788976016
	1783795731920 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1783795731920 -> 1783788976064
	1783788976064 [label=AccumulateGrad]
	1783788975920 -> 1783788976016
	1783795732016 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1783795732016 -> 1783788975920
	1783788975920 [label=AccumulateGrad]
	1783788975824 -> 1783788975680
	1783795732400 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1783795732400 -> 1783788975824
	1783788975824 [label=AccumulateGrad]
	1783788975632 -> 1783788975536
	1783795732496 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1783795732496 -> 1783788975632
	1783788975632 [label=AccumulateGrad]
	1783788975584 -> 1783788975536
	1783795732592 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1783795732592 -> 1783788975584
	1783788975584 [label=AccumulateGrad]
	1783788975488 -> 1783788975440
	1783788975248 -> 1783788975104
	1783795733552 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1783795733552 -> 1783788975248
	1783788975248 [label=AccumulateGrad]
	1783788975056 -> 1783788975008
	1783795733648 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1783795733648 -> 1783788975056
	1783788975056 [label=AccumulateGrad]
	1783788974912 -> 1783788975008
	1783795733744 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1783795733744 -> 1783788974912
	1783788974912 [label=AccumulateGrad]
	1783788974816 -> 1783788974672
	1783795734128 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1783795734128 -> 1783788974816
	1783788974816 [label=AccumulateGrad]
	1783788974624 -> 1783788974528
	1783795734224 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1783795734224 -> 1783788974624
	1783788974624 [label=AccumulateGrad]
	1783788974576 -> 1783788974528
	1783795734320 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1783795734320 -> 1783788974576
	1783788974576 [label=AccumulateGrad]
	1783788974480 -> 1783788974432
	1783788974480 [label=CudnnBatchNormBackward0]
	1783788975200 -> 1783788974480
	1783788975200 [label=ConvolutionBackward0]
	1783788975296 -> 1783788975200
	1783788975344 -> 1783788975200
	1783795732976 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1783795732976 -> 1783788975344
	1783788975344 [label=AccumulateGrad]
	1783788974768 -> 1783788974480
	1783795733072 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1783795733072 -> 1783788974768
	1783788974768 [label=AccumulateGrad]
	1783788974720 -> 1783788974480
	1783795733168 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1783795733168 -> 1783788974720
	1783788974720 [label=AccumulateGrad]
	1783788974336 -> 1783788974144
	1783795734704 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1783795734704 -> 1783788974336
	1783788974336 [label=AccumulateGrad]
	1783788974096 -> 1783788974048
	1783795734800 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1783795734800 -> 1783788974096
	1783788974096 [label=AccumulateGrad]
	1783788973952 -> 1783788974048
	1783795734896 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1783795734896 -> 1783788973952
	1783788973952 [label=AccumulateGrad]
	1783788973856 -> 1783788973712
	1783795735280 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1783795735280 -> 1783788973856
	1783788973856 [label=AccumulateGrad]
	1783788973664 -> 1783788973568
	1783795735376 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1783795735376 -> 1783788973664
	1783788973664 [label=AccumulateGrad]
	1783788973616 -> 1783788973568
	1783795735472 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1783795735472 -> 1783788973616
	1783788973616 [label=AccumulateGrad]
	1783788973520 -> 1783788973472
	1783788973280 -> 1783788973136
	1783795736432 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1783795736432 -> 1783788973280
	1783788973280 [label=AccumulateGrad]
	1783788973088 -> 1783788972992
	1783795736528 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1783795736528 -> 1783788973088
	1783788973088 [label=AccumulateGrad]
	1783788972800 -> 1783788972992
	1783795736624 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1783795736624 -> 1783788972800
	1783788972800 [label=AccumulateGrad]
	1783788973040 -> 1783795578432
	1783795737008 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1783795737008 -> 1783788973040
	1783788973040 [label=AccumulateGrad]
	1783795578384 -> 1783795578288
	1783795737104 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1783795737104 -> 1783795578384
	1783795578384 [label=AccumulateGrad]
	1783795578336 -> 1783795578288
	1783795737200 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1783795737200 -> 1783795578336
	1783795578336 [label=AccumulateGrad]
	1783795578240 -> 1783795578192
	1783795578240 [label=CudnnBatchNormBackward0]
	1783788973232 -> 1783795578240
	1783788973232 [label=ConvolutionBackward0]
	1783788973328 -> 1783788973232
	1783788973376 -> 1783788973232
	1783795735856 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1783795735856 -> 1783788973376
	1783788973376 [label=AccumulateGrad]
	1783788972272 -> 1783795578240
	1783795735952 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1783795735952 -> 1783788972272
	1783788972272 [label=AccumulateGrad]
	1783788972320 -> 1783795578240
	1783795736048 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1783795736048 -> 1783788972320
	1783788972320 [label=AccumulateGrad]
	1783795576272 -> 1783795578048
	1783795737584 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1783795737584 -> 1783795576272
	1783795576272 [label=AccumulateGrad]
	1783795578000 -> 1783795577952
	1783795737680 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1783795737680 -> 1783795578000
	1783795578000 [label=AccumulateGrad]
	1783795577856 -> 1783795577952
	1783795737776 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1783795737776 -> 1783795577856
	1783795577856 [label=AccumulateGrad]
	1783795577760 -> 1783795577616
	1783795738160 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1783795738160 -> 1783795577760
	1783795577760 [label=AccumulateGrad]
	1783795577568 -> 1783795577472
	1783795738256 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1783795738256 -> 1783795577568
	1783795577568 [label=AccumulateGrad]
	1783795577520 -> 1783795577472
	1783795738352 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1783795738352 -> 1783795577520
	1783795577520 [label=AccumulateGrad]
	1783795577424 -> 1783795577376
	1783795577088 -> 1783795576848
	1783795577088 [label=TBackward0]
	1783795577328 -> 1783795577088
	1783795739120 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1783795739120 -> 1783795577328
	1783795577328 [label=AccumulateGrad]
	1783795576848 -> 1783810199376
}
