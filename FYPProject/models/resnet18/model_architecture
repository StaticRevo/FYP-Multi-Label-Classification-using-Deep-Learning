digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1984408865968 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1984405778160 [label=SigmoidBackward0]
	1984405779312 -> 1984405778160
	1984405779312 [label=AddmmBackward0]
	1984405772928 -> 1984405779312
	1984408676176 [label="model.fc.bias
 (19)" fillcolor=lightblue]
	1984408676176 -> 1984405772928
	1984405772928 [label=AccumulateGrad]
	1984405773888 -> 1984405779312
	1984405773888 [label=ViewBackward0]
	1984405770048 -> 1984405773888
	1984405770048 [label=MeanBackward1]
	1984405778448 -> 1984405770048
	1984405778448 [label=ReluBackward0]
	1984405776288 -> 1984405778448
	1984405776288 [label=AddBackward0]
	1984405771008 -> 1984405776288
	1984405771008 [label=CudnnBatchNormBackward0]
	1984405778112 -> 1984405771008
	1984405778112 [label=ConvolutionBackward0]
	1984405770192 -> 1984405778112
	1984405770192 [label=ReluBackward0]
	1984405766208 -> 1984405770192
	1984405766208 [label=CudnnBatchNormBackward0]
	1984405779552 -> 1984405766208
	1984405779552 [label=ConvolutionBackward0]
	1984405768944 -> 1984405779552
	1984405768944 [label=ReluBackward0]
	1984405779408 -> 1984405768944
	1984405779408 [label=AddBackward0]
	1984405776672 -> 1984405779408
	1984405776672 [label=CudnnBatchNormBackward0]
	1984405771632 -> 1984405776672
	1984405771632 [label=ConvolutionBackward0]
	1984405780464 -> 1984405771632
	1984405780464 [label=ReluBackward0]
	1984405780608 -> 1984405780464
	1984405780608 [label=CudnnBatchNormBackward0]
	1984405780704 -> 1984405780608
	1984405780704 [label=ConvolutionBackward0]
	1984405781232 -> 1984405780704
	1984405781232 [label=ReluBackward0]
	1984405780992 -> 1984405781232
	1984405780992 [label=AddBackward0]
	1984405777824 -> 1984405780992
	1984405777824 [label=CudnnBatchNormBackward0]
	1984405767744 -> 1984405777824
	1984405767744 [label=ConvolutionBackward0]
	1984405781616 -> 1984405767744
	1984405781616 [label=ReluBackward0]
	1984405781856 -> 1984405781616
	1984405781856 [label=CudnnBatchNormBackward0]
	1984405782000 -> 1984405781856
	1984405782000 [label=ConvolutionBackward0]
	1984405767504 -> 1984405782000
	1984405767504 [label=ReluBackward0]
	1984405780848 -> 1984405767504
	1984405780848 [label=AddBackward0]
	1984405773216 -> 1984405780848
	1984405773216 [label=CudnnBatchNormBackward0]
	1984405779696 -> 1984405773216
	1984405779696 [label=ConvolutionBackward0]
	1984405771488 -> 1984405779696
	1984405771488 [label=ReluBackward0]
	1984405781760 -> 1984405771488
	1984405781760 [label=CudnnBatchNormBackward0]
	1984405771968 -> 1984405781760
	1984405771968 [label=ConvolutionBackward0]
	1984405774512 -> 1984405771968
	1984405774512 [label=ReluBackward0]
	1984405774608 -> 1984405774512
	1984405774608 [label=AddBackward0]
	1984405768128 -> 1984405774608
	1984405768128 [label=CudnnBatchNormBackward0]
	1984405777776 -> 1984405768128
	1984405777776 [label=ConvolutionBackward0]
	1984405766448 -> 1984405777776
	1984405766448 [label=ReluBackward0]
	1984405776912 -> 1984405766448
	1984405776912 [label=CudnnBatchNormBackward0]
	1984405777392 -> 1984405776912
	1984405777392 [label=ConvolutionBackward0]
	1984405771440 -> 1984405777392
	1984405771440 [label=ReluBackward0]
	1984405779072 -> 1984405771440
	1984405779072 [label=AddBackward0]
	1984405770576 -> 1984405779072
	1984405770576 [label=CudnnBatchNormBackward0]
	1984405769136 -> 1984405770576
	1984405769136 [label=ConvolutionBackward0]
	1984405775520 -> 1984405769136
	1984405775520 [label=ReluBackward0]
	1984405779024 -> 1984405775520
	1984405779024 [label=CudnnBatchNormBackward0]
	1984405769664 -> 1984405779024
	1984405769664 [label=ConvolutionBackward0]
	1984404724352 -> 1984405769664
	1984404724352 [label=ReluBackward0]
	1984404725696 -> 1984404724352
	1984404725696 [label=AddBackward0]
	1984404732848 -> 1984404725696
	1984404732848 [label=CudnnBatchNormBackward0]
	1984404730064 -> 1984404732848
	1984404730064 [label=ConvolutionBackward0]
	1984404726272 -> 1984404730064
	1984404726272 [label=ReluBackward0]
	1984404732944 -> 1984404726272
	1984404732944 [label=CudnnBatchNormBackward0]
	1984404730496 -> 1984404732944
	1984404730496 [label=ConvolutionBackward0]
	1984404730352 -> 1984404730496
	1984404730352 [label=ReluBackward0]
	1984404724304 -> 1984404730352
	1984404724304 [label=AddBackward0]
	1984404732080 -> 1984404724304
	1984404732080 [label=CudnnBatchNormBackward0]
	1984404728816 -> 1984404732080
	1984404728816 [label=ConvolutionBackward0]
	1984404733136 -> 1984404728816
	1984404733136 [label=ReluBackward0]
	1984404722864 -> 1984404733136
	1984404722864 [label=CudnnBatchNormBackward0]
	1984404730688 -> 1984404722864
	1984404730688 [label=ConvolutionBackward0]
	1984404731936 -> 1984404730688
	1984404731936 [label=MaxPool2DWithIndicesBackward0]
	1984404723680 -> 1984404731936
	1984404723680 [label=ReluBackward0]
	1984404733424 -> 1984404723680
	1984404733424 [label=CudnnBatchNormBackward0]
	1984404731408 -> 1984404733424
	1984404731408 [label=ConvolutionBackward0]
	1984404727904 -> 1984404731408
	1984406589840 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1984406589840 -> 1984404727904
	1984404727904 [label=AccumulateGrad]
	1984404726800 -> 1984404733424
	1984408548944 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	1984408548944 -> 1984404726800
	1984404726800 [label=AccumulateGrad]
	1984404725600 -> 1984404733424
	1984408549712 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	1984408549712 -> 1984404725600
	1984404725600 [label=AccumulateGrad]
	1984404724160 -> 1984404730688
	1984408549616 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1984408549616 -> 1984404724160
	1984404724160 [label=AccumulateGrad]
	1984404726704 -> 1984404722864
	1984408550096 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1984408550096 -> 1984404726704
	1984404726704 [label=AccumulateGrad]
	1984404723200 -> 1984404722864
	1984408550384 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1984408550384 -> 1984404723200
	1984404723200 [label=AccumulateGrad]
	1984404723872 -> 1984404728816
	1984408550768 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1984408550768 -> 1984404723872
	1984404723872 [label=AccumulateGrad]
	1984404728576 -> 1984404732080
	1984408550864 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1984408550864 -> 1984404728576
	1984404728576 [label=AccumulateGrad]
	1984404727952 -> 1984404732080
	1984408550960 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1984408550960 -> 1984404727952
	1984404727952 [label=AccumulateGrad]
	1984404731936 -> 1984404724304
	1984404732896 -> 1984404730496
	1984408551344 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1984408551344 -> 1984404732896
	1984404732896 [label=AccumulateGrad]
	1984404725072 -> 1984404732944
	1984408666192 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1984408666192 -> 1984404725072
	1984404725072 [label=AccumulateGrad]
	1984404733520 -> 1984404732944
	1984408666288 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1984408666288 -> 1984404733520
	1984404733520 [label=AccumulateGrad]
	1984404730592 -> 1984404730064
	1984408666672 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1984408666672 -> 1984404730592
	1984404730592 [label=AccumulateGrad]
	1984404729824 -> 1984404732848
	1984408666768 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1984408666768 -> 1984404729824
	1984404729824 [label=AccumulateGrad]
	1984404725648 -> 1984404732848
	1984408666864 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1984408666864 -> 1984404725648
	1984404725648 [label=AccumulateGrad]
	1984404730352 -> 1984404725696
	1984404724640 -> 1984405769664
	1984408667824 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1984408667824 -> 1984404724640
	1984404724640 [label=AccumulateGrad]
	1984405773072 -> 1984405779024
	1984408667920 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1984408667920 -> 1984405773072
	1984405773072 [label=AccumulateGrad]
	1984405775904 -> 1984405779024
	1984408668016 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1984408668016 -> 1984405775904
	1984405775904 [label=AccumulateGrad]
	1984405773552 -> 1984405769136
	1984408668400 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1984408668400 -> 1984405773552
	1984405773552 [label=AccumulateGrad]
	1984405768560 -> 1984405770576
	1984408668496 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1984408668496 -> 1984405768560
	1984405768560 [label=AccumulateGrad]
	1984405775328 -> 1984405770576
	1984408668592 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1984408668592 -> 1984405775328
	1984405775328 [label=AccumulateGrad]
	1984405769616 -> 1984405779072
	1984405769616 [label=CudnnBatchNormBackward0]
	1984405776192 -> 1984405769616
	1984405776192 [label=ConvolutionBackward0]
	1984404724352 -> 1984405776192
	1984404728672 -> 1984405776192
	1984408667248 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1984408667248 -> 1984404728672
	1984404728672 [label=AccumulateGrad]
	1984405775568 -> 1984405769616
	1984408667344 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1984408667344 -> 1984405775568
	1984405775568 [label=AccumulateGrad]
	1984405775952 -> 1984405769616
	1984408667440 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1984408667440 -> 1984405775952
	1984405775952 [label=AccumulateGrad]
	1984405768512 -> 1984405777392
	1984408668976 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1984408668976 -> 1984405768512
	1984405768512 [label=AccumulateGrad]
	1984405766640 -> 1984405776912
	1984408669072 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1984408669072 -> 1984405766640
	1984405766640 [label=AccumulateGrad]
	1984405771776 -> 1984405776912
	1984408669168 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1984408669168 -> 1984405771776
	1984405771776 [label=AccumulateGrad]
	1984405773024 -> 1984405777776
	1984408669552 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1984408669552 -> 1984405773024
	1984405773024 [label=AccumulateGrad]
	1984405771152 -> 1984405768128
	1984408669648 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1984408669648 -> 1984405771152
	1984405771152 [label=AccumulateGrad]
	1984405766688 -> 1984405768128
	1984408669744 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1984408669744 -> 1984405766688
	1984405766688 [label=AccumulateGrad]
	1984405771440 -> 1984405774608
	1984405769520 -> 1984405771968
	1984408670704 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1984408670704 -> 1984405769520
	1984405769520 [label=AccumulateGrad]
	1984405781136 -> 1984405781760
	1984408670800 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1984408670800 -> 1984405781136
	1984405781136 [label=AccumulateGrad]
	1984405774032 -> 1984405781760
	1984408670896 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1984408670896 -> 1984405774032
	1984405774032 [label=AccumulateGrad]
	1984405779216 -> 1984405779696
	1984408671280 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1984408671280 -> 1984405779216
	1984405779216 [label=AccumulateGrad]
	1984405781472 -> 1984405773216
	1984408671376 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1984408671376 -> 1984405781472
	1984405781472 [label=AccumulateGrad]
	1984405781088 -> 1984405773216
	1984408671472 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1984408671472 -> 1984405781088
	1984405781088 [label=AccumulateGrad]
	1984405780800 -> 1984405780848
	1984405780800 [label=CudnnBatchNormBackward0]
	1984405774800 -> 1984405780800
	1984405774800 [label=ConvolutionBackward0]
	1984405774512 -> 1984405774800
	1984405769712 -> 1984405774800
	1984408670128 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1984408670128 -> 1984405769712
	1984405769712 [label=AccumulateGrad]
	1984405781952 -> 1984405780800
	1984408670224 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1984408670224 -> 1984405781952
	1984405781952 [label=AccumulateGrad]
	1984405781328 -> 1984405780800
	1984408670320 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1984408670320 -> 1984405781328
	1984405781328 [label=AccumulateGrad]
	1984405768608 -> 1984405782000
	1984408671856 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1984408671856 -> 1984405768608
	1984405768608 [label=AccumulateGrad]
	1984405782144 -> 1984405781856
	1984408671952 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1984408671952 -> 1984405782144
	1984405782144 [label=AccumulateGrad]
	1984405782096 -> 1984405781856
	1984408672048 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1984408672048 -> 1984405782096
	1984405782096 [label=AccumulateGrad]
	1984405781184 -> 1984405767744
	1984408672432 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1984408672432 -> 1984405781184
	1984405781184 [label=AccumulateGrad]
	1984405767168 -> 1984405777824
	1984408672528 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1984408672528 -> 1984405767168
	1984405767168 [label=AccumulateGrad]
	1984405767264 -> 1984405777824
	1984408672624 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1984408672624 -> 1984405767264
	1984405767264 [label=AccumulateGrad]
	1984405767504 -> 1984405780992
	1984405780944 -> 1984405780704
	1984408673584 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1984408673584 -> 1984405780944
	1984405780944 [label=AccumulateGrad]
	1984405780656 -> 1984405780608
	1984408673680 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1984408673680 -> 1984405780656
	1984405780656 [label=AccumulateGrad]
	1984405780512 -> 1984405780608
	1984408673776 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1984408673776 -> 1984405780512
	1984405780512 [label=AccumulateGrad]
	1984405780416 -> 1984405771632
	1984408674160 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1984408674160 -> 1984405780416
	1984405780416 [label=AccumulateGrad]
	1984405773840 -> 1984405776672
	1984408674256 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1984408674256 -> 1984405773840
	1984405773840 [label=AccumulateGrad]
	1984405777488 -> 1984405776672
	1984408674352 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1984408674352 -> 1984405777488
	1984405777488 [label=AccumulateGrad]
	1984405769472 -> 1984405779408
	1984405769472 [label=CudnnBatchNormBackward0]
	1984405780896 -> 1984405769472
	1984405780896 [label=ConvolutionBackward0]
	1984405781232 -> 1984405780896
	1984405781376 -> 1984405780896
	1984408673008 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1984408673008 -> 1984405781376
	1984405781376 [label=AccumulateGrad]
	1984405780368 -> 1984405769472
	1984408673104 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1984408673104 -> 1984405780368
	1984405780368 [label=AccumulateGrad]
	1984405779264 -> 1984405769472
	1984408673200 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1984408673200 -> 1984405779264
	1984405779264 [label=AccumulateGrad]
	1984405772400 -> 1984405779552
	1984408674736 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1984408674736 -> 1984405772400
	1984405772400 [label=AccumulateGrad]
	1984405779648 -> 1984405766208
	1984408674832 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1984408674832 -> 1984405779648
	1984405779648 [label=AccumulateGrad]
	1984405767600 -> 1984405766208
	1984408674928 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1984408674928 -> 1984405767600
	1984405767600 [label=AccumulateGrad]
	1984405771104 -> 1984405778112
	1984408675312 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1984408675312 -> 1984405771104
	1984405771104 [label=AccumulateGrad]
	1984405768368 -> 1984405771008
	1984408675408 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1984408675408 -> 1984405768368
	1984405768368 [label=AccumulateGrad]
	1984405777296 -> 1984405771008
	1984408675504 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1984408675504 -> 1984405777296
	1984405777296 [label=AccumulateGrad]
	1984405768944 -> 1984405776288
	1984405767936 -> 1984405779312
	1984405767936 [label=TBackward0]
	1984405769376 -> 1984405767936
	1984408676080 [label="model.fc.weight
 (19, 512)" fillcolor=lightblue]
	1984408676080 -> 1984405769376
	1984405769376 [label=AccumulateGrad]
	1984405778160 -> 1984408865968
}
