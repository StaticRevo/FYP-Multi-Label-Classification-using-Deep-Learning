digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2133209954256 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2133526453872 [label=AddmmBackward0]
	2133526457856 -> 2133526453872
	2133200910960 [label="fc.bias
 (19)" fillcolor=lightblue]
	2133200910960 -> 2133526457856
	2133526457856 [label=AccumulateGrad]
	2133526462368 -> 2133526453872
	2133526462368 [label=ViewBackward0]
	2133526462176 -> 2133526462368
	2133526462176 [label=MeanBackward1]
	2133526462224 -> 2133526462176
	2133526462224 [label=ReluBackward0]
	2133526462128 -> 2133526462224
	2133526462128 [label=AddBackward0]
	2133526461840 -> 2133526462128
	2133526461840 [label=CudnnBatchNormBackward0]
	2133526461600 -> 2133526461840
	2133526461600 [label=ConvolutionBackward0]
	2133526461408 -> 2133526461600
	2133526461408 [label=ReluBackward0]
	2133526461024 -> 2133526461408
	2133526461024 [label=CudnnBatchNormBackward0]
	2133526461264 -> 2133526461024
	2133526461264 [label=ConvolutionBackward0]
	2133526460880 -> 2133526461264
	2133526460880 [label=ReluBackward0]
	2133526458528 -> 2133526460880
	2133526458528 [label=CudnnBatchNormBackward0]
	2133526457328 -> 2133526458528
	2133526457328 [label=ConvolutionBackward0]
	2133526461888 -> 2133526457328
	2133526461888 [label=ReluBackward0]
	2133526454592 -> 2133526461888
	2133526454592 [label=AddBackward0]
	2133526457520 -> 2133526454592
	2133526457520 [label=CudnnBatchNormBackward0]
	2133526457472 -> 2133526457520
	2133526457472 [label=ConvolutionBackward0]
	2133526452912 -> 2133526457472
	2133526452912 [label=ReluBackward0]
	2133526459872 -> 2133526452912
	2133526459872 [label=CudnnBatchNormBackward0]
	2133526454688 -> 2133526459872
	2133526454688 [label=ConvolutionBackward0]
	2133526457616 -> 2133526454688
	2133526457616 [label=ReluBackward0]
	2133526447872 -> 2133526457616
	2133526447872 [label=CudnnBatchNormBackward0]
	2133526457424 -> 2133526447872
	2133526457424 [label=ConvolutionBackward0]
	2133526456464 -> 2133526457424
	2133526456464 [label=ReluBackward0]
	2133526455168 -> 2133526456464
	2133526455168 [label=AddBackward0]
	2133526451088 -> 2133526455168
	2133526451088 [label=CudnnBatchNormBackward0]
	2133526457376 -> 2133526451088
	2133526457376 [label=ConvolutionBackward0]
	2133526451328 -> 2133526457376
	2133526451328 [label=ReluBackward0]
	2133526453392 -> 2133526451328
	2133526453392 [label=CudnnBatchNormBackward0]
	2133526457232 -> 2133526453392
	2133526457232 [label=ConvolutionBackward0]
	2133526454544 -> 2133526457232
	2133526454544 [label=ReluBackward0]
	2133526447968 -> 2133526454544
	2133526447968 [label=CudnnBatchNormBackward0]
	2133526456176 -> 2133526447968
	2133526456176 [label=ConvolutionBackward0]
	2133526449840 -> 2133526456176
	2133526449840 [label=ReluBackward0]
	2133526460352 -> 2133526449840
	2133526460352 [label=AddBackward0]
	2133526448064 -> 2133526460352
	2133526448064 [label=CudnnBatchNormBackward0]
	2133526460112 -> 2133526448064
	2133526460112 [label=ConvolutionBackward0]
	2133526455696 -> 2133526460112
	2133526455696 [label=ReluBackward0]
	2133526446240 -> 2133526455696
	2133526446240 [label=CudnnBatchNormBackward0]
	2133526459968 -> 2133526446240
	2133526459968 [label=ConvolutionBackward0]
	2133526447488 -> 2133526459968
	2133526447488 [label=ReluBackward0]
	2133526460208 -> 2133526447488
	2133526460208 [label=CudnnBatchNormBackward0]
	2133526455408 -> 2133526460208
	2133526455408 [label=ConvolutionBackward0]
	2133526448928 -> 2133526455408
	2133526448928 [label=ReluBackward0]
	2133526460496 -> 2133526448928
	2133526460496 [label=AddBackward0]
	2133526460592 -> 2133526460496
	2133526460592 [label=CudnnBatchNormBackward0]
	2133526084576 -> 2133526460592
	2133526084576 [label=ConvolutionBackward0]
	2133526084048 -> 2133526084576
	2133526084048 [label=ReluBackward0]
	2133526084864 -> 2133526084048
	2133526084864 [label=CudnnBatchNormBackward0]
	2133526071808 -> 2133526084864
	2133526071808 [label=ConvolutionBackward0]
	2133526081696 -> 2133526071808
	2133526081696 [label=ReluBackward0]
	2133526082032 -> 2133526081696
	2133526082032 [label=CudnnBatchNormBackward0]
	2133526071664 -> 2133526082032
	2133526071664 [label=ConvolutionBackward0]
	2133526460400 -> 2133526071664
	2133526460400 [label=ReluBackward0]
	2133526083232 -> 2133526460400
	2133526083232 [label=AddBackward0]
	2133210658272 -> 2133526083232
	2133210658272 [label=CudnnBatchNormBackward0]
	2133210657936 -> 2133210658272
	2133210657936 [label=ConvolutionBackward0]
	2133210656976 -> 2133210657936
	2133210656976 [label=ReluBackward0]
	2133210655872 -> 2133210656976
	2133210655872 [label=CudnnBatchNormBackward0]
	2133210655392 -> 2133210655872
	2133210655392 [label=ConvolutionBackward0]
	2133210654432 -> 2133210655392
	2133210654432 [label=ReluBackward0]
	2133210654096 -> 2133210654432
	2133210654096 [label=CudnnBatchNormBackward0]
	2133210653616 -> 2133210654096
	2133210653616 [label=ConvolutionBackward0]
	2133210658752 -> 2133210653616
	2133210658752 [label=ReluBackward0]
	2133210652176 -> 2133210658752
	2133210652176 [label=AddBackward0]
	2133210651696 -> 2133210652176
	2133210651696 [label=CudnnBatchNormBackward0]
	2133210650592 -> 2133210651696
	2133210650592 [label=ConvolutionBackward0]
	2133210649632 -> 2133210650592
	2133210649632 [label=ReluBackward0]
	2133210649296 -> 2133210649632
	2133210649296 [label=CudnnBatchNormBackward0]
	2133210648816 -> 2133210649296
	2133210648816 [label=ConvolutionBackward0]
	2133210647856 -> 2133210648816
	2133210647856 [label=ReluBackward0]
	2133210646752 -> 2133210647856
	2133210646752 [label=CudnnBatchNormBackward0]
	2133210646272 -> 2133210646752
	2133210646272 [label=ConvolutionBackward0]
	2133210651552 -> 2133210646272
	2133210651552 [label=ReluBackward0]
	2133210644832 -> 2133210651552
	2133210644832 [label=AddBackward0]
	2133210658848 -> 2133210644832
	2133210658848 [label=CudnnBatchNormBackward0]
	2133210658464 -> 2133210658848
	2133210658464 [label=ConvolutionBackward0]
	2133210658608 -> 2133210658464
	2133210658608 [label=ReluBackward0]
	2133210658176 -> 2133210658608
	2133210658176 [label=CudnnBatchNormBackward0]
	2133210658080 -> 2133210658176
	2133210658080 [label=ConvolutionBackward0]
	2133210657888 -> 2133210658080
	2133210657888 [label=ReluBackward0]
	2133210657504 -> 2133210657888
	2133210657504 [label=CudnnBatchNormBackward0]
	2133210657744 -> 2133210657504
	2133210657744 [label=ConvolutionBackward0]
	2133210644976 -> 2133210657744
	2133210644976 [label=ReluBackward0]
	2133210657024 -> 2133210644976
	2133210657024 [label=AddBackward0]
	2133210657264 -> 2133210657024
	2133210657264 [label=CudnnBatchNormBackward0]
	2133210656928 -> 2133210657264
	2133210656928 [label=ConvolutionBackward0]
	2133210656640 -> 2133210656928
	2133210656640 [label=ReluBackward0]
	2133210656688 -> 2133210656640
	2133210656688 [label=CudnnBatchNormBackward0]
	2133210656400 -> 2133210656688
	2133210656400 [label=ConvolutionBackward0]
	2133210656304 -> 2133210656400
	2133210656304 [label=ReluBackward0]
	2133210655968 -> 2133210656304
	2133210655968 [label=CudnnBatchNormBackward0]
	2133210655776 -> 2133210655968
	2133210655776 [label=ConvolutionBackward0]
	2133210655632 -> 2133210655776
	2133210655632 [label=ReluBackward0]
	2133210655440 -> 2133210655632
	2133210655440 [label=AddBackward0]
	2133210655104 -> 2133210655440
	2133210655104 [label=CudnnBatchNormBackward0]
	2133210655152 -> 2133210655104
	2133210655152 [label=ConvolutionBackward0]
	2133210654816 -> 2133210655152
	2133210654816 [label=ReluBackward0]
	2133210654864 -> 2133210654816
	2133210654864 [label=CudnnBatchNormBackward0]
	2133210654768 -> 2133210654864
	2133210654768 [label=ConvolutionBackward0]
	2133210654144 -> 2133210654768
	2133210654144 [label=ReluBackward0]
	2133210654192 -> 2133210654144
	2133210654192 [label=CudnnBatchNormBackward0]
	2133210654048 -> 2133210654192
	2133210654048 [label=ConvolutionBackward0]
	2133210655296 -> 2133210654048
	2133210655296 [label=ReluBackward0]
	2133210653712 -> 2133210655296
	2133210653712 [label=AddBackward0]
	2133210653568 -> 2133210653712
	2133210653568 [label=CudnnBatchNormBackward0]
	2133210653184 -> 2133210653568
	2133210653184 [label=ConvolutionBackward0]
	2133210653328 -> 2133210653184
	2133210653328 [label=ReluBackward0]
	2133210652896 -> 2133210653328
	2133210652896 [label=CudnnBatchNormBackward0]
	2133210652800 -> 2133210652896
	2133210652800 [label=ConvolutionBackward0]
	2133210652608 -> 2133210652800
	2133210652608 [label=ReluBackward0]
	2133210652224 -> 2133210652608
	2133210652224 [label=CudnnBatchNormBackward0]
	2133210652464 -> 2133210652224
	2133210652464 [label=ConvolutionBackward0]
	2133210653808 -> 2133210652464
	2133210653808 [label=ReluBackward0]
	2133210651744 -> 2133210653808
	2133210651744 [label=AddBackward0]
	2133210651984 -> 2133210651744
	2133210651984 [label=CudnnBatchNormBackward0]
	2133210651648 -> 2133210651984
	2133210651648 [label=ConvolutionBackward0]
	2133210651360 -> 2133210651648
	2133210651360 [label=ReluBackward0]
	2133210651408 -> 2133210651360
	2133210651408 [label=CudnnBatchNormBackward0]
	2133210651120 -> 2133210651408
	2133210651120 [label=ConvolutionBackward0]
	2133210651024 -> 2133210651120
	2133210651024 [label=ReluBackward0]
	2133210650688 -> 2133210651024
	2133210650688 [label=CudnnBatchNormBackward0]
	2133210650496 -> 2133210650688
	2133210650496 [label=ConvolutionBackward0]
	2133210651840 -> 2133210650496
	2133210651840 [label=ReluBackward0]
	2133210650208 -> 2133210651840
	2133210650208 [label=AddBackward0]
	2133210650016 -> 2133210650208
	2133210650016 [label=CudnnBatchNormBackward0]
	2133210650064 -> 2133210650016
	2133210650064 [label=ConvolutionBackward0]
	2133210649680 -> 2133210650064
	2133210649680 [label=ReluBackward0]
	2133210649440 -> 2133210649680
	2133210649440 [label=CudnnBatchNormBackward0]
	2133210649392 -> 2133210649440
	2133210649392 [label=ConvolutionBackward0]
	2133210649056 -> 2133210649392
	2133210649056 [label=ReluBackward0]
	2133210649104 -> 2133210649056
	2133210649104 [label=CudnnBatchNormBackward0]
	2133210649008 -> 2133210649104
	2133210649008 [label=ConvolutionBackward0]
	2133210648384 -> 2133210649008
	2133210648384 [label=ReluBackward0]
	2133210648432 -> 2133210648384
	2133210648432 [label=AddBackward0]
	2133210648288 -> 2133210648432
	2133210648288 [label=CudnnBatchNormBackward0]
	2133210647904 -> 2133210648288
	2133210647904 [label=ConvolutionBackward0]
	2133210648048 -> 2133210647904
	2133210648048 [label=ReluBackward0]
	2133210647616 -> 2133210648048
	2133210647616 [label=CudnnBatchNormBackward0]
	2133210647520 -> 2133210647616
	2133210647520 [label=ConvolutionBackward0]
	2133210647328 -> 2133210647520
	2133210647328 [label=ReluBackward0]
	2133210646944 -> 2133210647328
	2133210646944 [label=CudnnBatchNormBackward0]
	2133210647184 -> 2133210646944
	2133210647184 [label=ConvolutionBackward0]
	2133210648528 -> 2133210647184
	2133210648528 [label=ReluBackward0]
	2133210646464 -> 2133210648528
	2133210646464 [label=AddBackward0]
	2133210646704 -> 2133210646464
	2133210646704 [label=CudnnBatchNormBackward0]
	2133210646368 -> 2133210646704
	2133210646368 [label=ConvolutionBackward0]
	2133210646080 -> 2133210646368
	2133210646080 [label=ReluBackward0]
	2133210646128 -> 2133210646080
	2133210646128 [label=CudnnBatchNormBackward0]
	2133210645840 -> 2133210646128
	2133210645840 [label=ConvolutionBackward0]
	2133210645744 -> 2133210645840
	2133210645744 [label=ReluBackward0]
	2133210645408 -> 2133210645744
	2133210645408 [label=CudnnBatchNormBackward0]
	2133210645216 -> 2133210645408
	2133210645216 [label=ConvolutionBackward0]
	2133210646560 -> 2133210645216
	2133210646560 [label=ReluBackward0]
	2133210644928 -> 2133210646560
	2133210644928 [label=AddBackward0]
	2133210644736 -> 2133210644928
	2133210644736 [label=CudnnBatchNormBackward0]
	2133210644784 -> 2133210644736
	2133210644784 [label=ConvolutionBackward0]
	2133210659376 -> 2133210644784
	2133210659376 [label=ReluBackward0]
	2133210659712 -> 2133210659376
	2133210659712 [label=CudnnBatchNormBackward0]
	2133210659808 -> 2133210659712
	2133210659808 [label=ConvolutionBackward0]
	2133210660000 -> 2133210659808
	2133210660000 [label=ReluBackward0]
	2133210660144 -> 2133210660000
	2133210660144 [label=CudnnBatchNormBackward0]
	2133210660240 -> 2133210660144
	2133210660240 [label=ConvolutionBackward0]
	2133210660432 -> 2133210660240
	2133210660432 [label=MaxPool2DWithIndicesBackward0]
	2133210660576 -> 2133210660432
	2133210660576 [label=ReluBackward0]
	2133210660672 -> 2133210660576
	2133210660672 [label=CudnnBatchNormBackward0]
	2133210660768 -> 2133210660672
	2133210660768 [label=ConvolutionBackward0]
	2133210758496 -> 2133210660768
	2133200910768 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2133200910768 -> 2133210758496
	2133210758496 [label=AccumulateGrad]
	2133210660720 -> 2133210660672
	2133513977488 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2133513977488 -> 2133210660720
	2133210660720 [label=AccumulateGrad]
	2133210660480 -> 2133210660672
	2133513977584 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2133513977584 -> 2133210660480
	2133210660480 [label=AccumulateGrad]
	2133210660384 -> 2133210660240
	2133509458736 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2133509458736 -> 2133210660384
	2133210660384 [label=AccumulateGrad]
	2133210660192 -> 2133210660144
	2133509458832 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2133509458832 -> 2133210660192
	2133210660192 [label=AccumulateGrad]
	2133210660048 -> 2133210660144
	2133509459024 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2133509459024 -> 2133210660048
	2133210660048 [label=AccumulateGrad]
	2133210659952 -> 2133210659808
	2133509457104 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2133509457104 -> 2133210659952
	2133210659952 [label=AccumulateGrad]
	2133210659760 -> 2133210659712
	2133509456816 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2133509456816 -> 2133210659760
	2133210659760 [label=AccumulateGrad]
	2133210659616 -> 2133210659712
	2133509456912 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2133509456912 -> 2133210659616
	2133210659616 [label=AccumulateGrad]
	2133210659520 -> 2133210644784
	2133509457392 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2133509457392 -> 2133210659520
	2133210659520 [label=AccumulateGrad]
	2133210644640 -> 2133210644736
	2133509457488 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2133509457488 -> 2133210644640
	2133210644640 [label=AccumulateGrad]
	2133210644544 -> 2133210644736
	2133509457584 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2133509457584 -> 2133210644544
	2133210644544 [label=AccumulateGrad]
	2133210644880 -> 2133210644928
	2133210644880 [label=CudnnBatchNormBackward0]
	2133210659904 -> 2133210644880
	2133210659904 [label=ConvolutionBackward0]
	2133210660432 -> 2133210659904
	2133210660288 -> 2133210659904
	2133509458064 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2133509458064 -> 2133210660288
	2133210660288 [label=AccumulateGrad]
	2133210644688 -> 2133210644880
	2133509458160 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2133509458160 -> 2133210644688
	2133210644688 [label=AccumulateGrad]
	2133210644592 -> 2133210644880
	2133509458256 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2133509458256 -> 2133210644592
	2133210644592 [label=AccumulateGrad]
	2133210645072 -> 2133210645216
	2133509456624 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2133509456624 -> 2133210645072
	2133210645072 [label=AccumulateGrad]
	2133210645360 -> 2133210645408
	2133509456432 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2133509456432 -> 2133210645360
	2133210645360 [label=AccumulateGrad]
	2133210645552 -> 2133210645408
	2133509456048 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2133509456048 -> 2133210645552
	2133210645552 [label=AccumulateGrad]
	2133210645600 -> 2133210645840
	2133509456528 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2133509456528 -> 2133210645600
	2133210645600 [label=AccumulateGrad]
	2133210645888 -> 2133210646128
	2133509456336 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2133509456336 -> 2133210645888
	2133210645888 [label=AccumulateGrad]
	2133210646224 -> 2133210646128
	2133201276848 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2133201276848 -> 2133210646224
	2133210646224 [label=AccumulateGrad]
	2133210645984 -> 2133210646368
	2133201277232 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2133201277232 -> 2133210645984
	2133210645984 [label=AccumulateGrad]
	2133210646608 -> 2133210646704
	2133201277328 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2133201277328 -> 2133210646608
	2133210646608 [label=AccumulateGrad]
	2133210646512 -> 2133210646704
	2133201277424 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2133201277424 -> 2133210646512
	2133210646512 [label=AccumulateGrad]
	2133210646560 -> 2133210646464
	2133210646800 -> 2133210647184
	2133201277808 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2133201277808 -> 2133210646800
	2133210646800 [label=AccumulateGrad]
	2133210647040 -> 2133210646944
	2133201277904 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2133201277904 -> 2133210647040
	2133210647040 [label=AccumulateGrad]
	2133210647280 -> 2133210646944
	2133201278000 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2133201278000 -> 2133210647280
	2133210647280 [label=AccumulateGrad]
	2133210647568 -> 2133210647520
	2133201278384 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2133201278384 -> 2133210647568
	2133210647568 [label=AccumulateGrad]
	2133210647424 -> 2133210647616
	2133201278480 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2133201278480 -> 2133210647424
	2133210647424 [label=AccumulateGrad]
	2133210647808 -> 2133210647616
	2133201278576 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2133201278576 -> 2133210647808
	2133210647808 [label=AccumulateGrad]
	2133210647952 -> 2133210647904
	2133201278960 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2133201278960 -> 2133210647952
	2133210647952 [label=AccumulateGrad]
	2133210648096 -> 2133210648288
	2133201279056 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2133201279056 -> 2133210648096
	2133210648096 [label=AccumulateGrad]
	2133210648240 -> 2133210648288
	2133201279152 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2133201279152 -> 2133210648240
	2133210648240 [label=AccumulateGrad]
	2133210648528 -> 2133210648432
	2133210648576 -> 2133210649008
	2133201280112 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2133201280112 -> 2133210648576
	2133210648576 [label=AccumulateGrad]
	2133210648912 -> 2133210649104
	2133201280208 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2133201280208 -> 2133210648912
	2133210648912 [label=AccumulateGrad]
	2133210648864 -> 2133210649104
	2133201280304 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2133201280304 -> 2133210648864
	2133210648864 [label=AccumulateGrad]
	2133210649200 -> 2133210649392
	2133201280688 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2133201280688 -> 2133210649200
	2133210649200 [label=AccumulateGrad]
	2133210649584 -> 2133210649440
	2133201280784 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2133201280784 -> 2133210649584
	2133210649584 [label=AccumulateGrad]
	2133210649536 -> 2133210649440
	2133201280880 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2133201280880 -> 2133210649536
	2133210649536 [label=AccumulateGrad]
	2133210649728 -> 2133210650064
	2133201281264 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2133201281264 -> 2133210649728
	2133210649728 [label=AccumulateGrad]
	2133210649920 -> 2133210650016
	2133201281360 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2133201281360 -> 2133210649920
	2133210649920 [label=AccumulateGrad]
	2133210649824 -> 2133210650016
	2133201281456 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2133201281456 -> 2133210649824
	2133210649824 [label=AccumulateGrad]
	2133210650160 -> 2133210650208
	2133210650160 [label=CudnnBatchNormBackward0]
	2133210649248 -> 2133210650160
	2133210649248 [label=ConvolutionBackward0]
	2133210648384 -> 2133210649248
	2133210648768 -> 2133210649248
	2133201279536 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2133201279536 -> 2133210648768
	2133210648768 [label=AccumulateGrad]
	2133210649968 -> 2133210650160
	2133201279632 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2133201279632 -> 2133210649968
	2133210649968 [label=AccumulateGrad]
	2133210649872 -> 2133210650160
	2133201279728 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2133201279728 -> 2133210649872
	2133210649872 [label=AccumulateGrad]
	2133210650352 -> 2133210650496
	2133201281840 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2133201281840 -> 2133210650352
	2133210650352 [label=AccumulateGrad]
	2133210650640 -> 2133210650688
	2133201281936 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2133201281936 -> 2133210650640
	2133210650640 [label=AccumulateGrad]
	2133210650832 -> 2133210650688
	2133201282032 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2133201282032 -> 2133210650832
	2133210650832 [label=AccumulateGrad]
	2133210650880 -> 2133210651120
	2133201282416 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2133201282416 -> 2133210650880
	2133210650880 [label=AccumulateGrad]
	2133210651168 -> 2133210651408
	2133201282512 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2133201282512 -> 2133210651168
	2133210651168 [label=AccumulateGrad]
	2133210651504 -> 2133210651408
	2133201282608 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2133201282608 -> 2133210651504
	2133210651504 [label=AccumulateGrad]
	2133210651264 -> 2133210651648
	2133201282992 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2133201282992 -> 2133210651264
	2133210651264 [label=AccumulateGrad]
	2133210651888 -> 2133210651984
	2133201283088 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2133201283088 -> 2133210651888
	2133210651888 [label=AccumulateGrad]
	2133210651792 -> 2133210651984
	2133201283184 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2133201283184 -> 2133210651792
	2133210651792 [label=AccumulateGrad]
	2133210651840 -> 2133210651744
	2133210652080 -> 2133210652464
	2133201283568 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2133201283568 -> 2133210652080
	2133210652080 [label=AccumulateGrad]
	2133210652320 -> 2133210652224
	2133201283664 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2133201283664 -> 2133210652320
	2133210652320 [label=AccumulateGrad]
	2133210652560 -> 2133210652224
	2133201283760 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2133201283760 -> 2133210652560
	2133210652560 [label=AccumulateGrad]
	2133210652848 -> 2133210652800
	2133201284144 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2133201284144 -> 2133210652848
	2133210652848 [label=AccumulateGrad]
	2133210652704 -> 2133210652896
	2133201284240 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2133201284240 -> 2133210652704
	2133210652704 [label=AccumulateGrad]
	2133210653088 -> 2133210652896
	2133201284336 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2133201284336 -> 2133210653088
	2133210653088 [label=AccumulateGrad]
	2133210653232 -> 2133210653184
	2133201284720 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2133201284720 -> 2133210653232
	2133210653232 [label=AccumulateGrad]
	2133210653376 -> 2133210653568
	2133201284816 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2133201284816 -> 2133210653376
	2133210653376 [label=AccumulateGrad]
	2133210653520 -> 2133210653568
	2133201284912 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2133201284912 -> 2133210653520
	2133210653520 [label=AccumulateGrad]
	2133210653808 -> 2133210653712
	2133210653760 -> 2133210654048
	2133201285296 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2133201285296 -> 2133210653760
	2133210653760 [label=AccumulateGrad]
	2133210654288 -> 2133210654192
	2133201285392 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2133201285392 -> 2133210654288
	2133210654288 [label=AccumulateGrad]
	2133210654240 -> 2133210654192
	2133201285488 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2133201285488 -> 2133210654240
	2133210654240 [label=AccumulateGrad]
	2133210654336 -> 2133210654768
	2133201285872 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2133201285872 -> 2133210654336
	2133210654336 [label=AccumulateGrad]
	2133210654672 -> 2133210654864
	2133201285968 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2133201285968 -> 2133210654672
	2133210654672 [label=AccumulateGrad]
	2133210654624 -> 2133210654864
	2133201286064 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2133201286064 -> 2133210654624
	2133210654624 [label=AccumulateGrad]
	2133210654960 -> 2133210655152
	2133201286448 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2133201286448 -> 2133210654960
	2133210654960 [label=AccumulateGrad]
	2133210655344 -> 2133210655104
	2133201286544 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2133201286544 -> 2133210655344
	2133210655344 [label=AccumulateGrad]
	2133210655200 -> 2133210655104
	2133201286640 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2133201286640 -> 2133210655200
	2133210655200 [label=AccumulateGrad]
	2133210655296 -> 2133210655440
	2133210655824 -> 2133210655776
	2133201287600 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2133201287600 -> 2133210655824
	2133210655824 [label=AccumulateGrad]
	2133210655920 -> 2133210655968
	2133201287696 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2133201287696 -> 2133210655920
	2133210655920 [label=AccumulateGrad]
	2133210656112 -> 2133210655968
	2133201287792 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2133201287792 -> 2133210656112
	2133210656112 [label=AccumulateGrad]
	2133210656160 -> 2133210656400
	2133201288176 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2133201288176 -> 2133210656160
	2133210656160 [label=AccumulateGrad]
	2133210656448 -> 2133210656688
	2133201288272 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2133201288272 -> 2133210656448
	2133210656448 [label=AccumulateGrad]
	2133210656784 -> 2133210656688
	2133201288368 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2133201288368 -> 2133210656784
	2133210656784 [label=AccumulateGrad]
	2133210656544 -> 2133210656928
	2133201288752 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2133201288752 -> 2133210656544
	2133210656544 [label=AccumulateGrad]
	2133210657168 -> 2133210657264
	2133201288848 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2133201288848 -> 2133210657168
	2133210657168 [label=AccumulateGrad]
	2133210657072 -> 2133210657264
	2133201288944 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2133201288944 -> 2133210657072
	2133210657072 [label=AccumulateGrad]
	2133210657120 -> 2133210657024
	2133210657120 [label=CudnnBatchNormBackward0]
	2133210656064 -> 2133210657120
	2133210656064 [label=ConvolutionBackward0]
	2133210655632 -> 2133210656064
	2133210655584 -> 2133210656064
	2133201287024 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2133201287024 -> 2133210655584
	2133210655584 [label=AccumulateGrad]
	2133210656736 -> 2133210657120
	2133201287120 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2133201287120 -> 2133210656736
	2133210656736 [label=AccumulateGrad]
	2133210656880 -> 2133210657120
	2133201287216 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2133201287216 -> 2133210656880
	2133210656880 [label=AccumulateGrad]
	2133210657360 -> 2133210657744
	2133200896176 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2133200896176 -> 2133210657360
	2133210657360 [label=AccumulateGrad]
	2133210657600 -> 2133210657504
	2133200896272 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2133200896272 -> 2133210657600
	2133210657600 [label=AccumulateGrad]
	2133210657840 -> 2133210657504
	2133200896368 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2133200896368 -> 2133210657840
	2133210657840 [label=AccumulateGrad]
	2133210658128 -> 2133210658080
	2133200896752 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2133200896752 -> 2133210658128
	2133210658128 [label=AccumulateGrad]
	2133210657984 -> 2133210658176
	2133200896848 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2133200896848 -> 2133210657984
	2133210657984 [label=AccumulateGrad]
	2133210658368 -> 2133210658176
	2133200896944 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2133200896944 -> 2133210658368
	2133210658368 [label=AccumulateGrad]
	2133210658512 -> 2133210658464
	2133200897328 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2133200897328 -> 2133210658512
	2133210658512 [label=AccumulateGrad]
	2133210658656 -> 2133210658848
	2133200897424 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2133200897424 -> 2133210658656
	2133210658656 [label=AccumulateGrad]
	2133210658800 -> 2133210658848
	2133200897520 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2133200897520 -> 2133210658800
	2133210658800 [label=AccumulateGrad]
	2133210644976 -> 2133210644832
	2133210645312 -> 2133210646272
	2133200897904 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2133200897904 -> 2133210645312
	2133210645312 [label=AccumulateGrad]
	2133210646896 -> 2133210646752
	2133200898000 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2133200898000 -> 2133210646896
	2133210646896 [label=AccumulateGrad]
	2133210647232 -> 2133210646752
	2133200898096 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2133200898096 -> 2133210647232
	2133210647232 [label=AccumulateGrad]
	2133210647712 -> 2133210648816
	2133200898480 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2133200898480 -> 2133210647712
	2133210647712 [label=AccumulateGrad]
	2133210648672 -> 2133210649296
	2133200898576 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2133200898576 -> 2133210648672
	2133210648672 [label=AccumulateGrad]
	2133210649776 -> 2133210649296
	2133200898672 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2133200898672 -> 2133210649776
	2133210649776 [label=AccumulateGrad]
	2133210650256 -> 2133210650592
	2133200899056 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2133200899056 -> 2133210650256
	2133210650256 [label=AccumulateGrad]
	2133210651216 -> 2133210651696
	2133200899152 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2133200899152 -> 2133210651216
	2133210651216 [label=AccumulateGrad]
	2133210651072 -> 2133210651696
	2133200899248 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2133200899248 -> 2133210651072
	2133210651072 [label=AccumulateGrad]
	2133210651552 -> 2133210652176
	2133210652656 -> 2133210653616
	2133200899632 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2133200899632 -> 2133210652656
	2133210652656 [label=AccumulateGrad]
	2133210653472 -> 2133210654096
	2133200899728 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2133200899728 -> 2133210653472
	2133210653472 [label=AccumulateGrad]
	2133210654576 -> 2133210654096
	2133200899824 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2133200899824 -> 2133210654576
	2133210654576 [label=AccumulateGrad]
	2133210655056 -> 2133210655392
	2133200900208 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2133200900208 -> 2133210655056
	2133210655056 [label=AccumulateGrad]
	2133210656016 -> 2133210655872
	2133200900304 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2133200900304 -> 2133210656016
	2133210656016 [label=AccumulateGrad]
	2133210656352 -> 2133210655872
	2133200900400 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2133200900400 -> 2133210656352
	2133210656352 [label=AccumulateGrad]
	2133210656832 -> 2133210657936
	2133200900784 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2133200900784 -> 2133210656832
	2133210656832 [label=AccumulateGrad]
	2133210657792 -> 2133210658272
	2133200900880 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2133200900880 -> 2133210657792
	2133210657792 [label=AccumulateGrad]
	2133210658416 -> 2133210658272
	2133200900976 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2133200900976 -> 2133210658416
	2133210658416 [label=AccumulateGrad]
	2133210658752 -> 2133526083232
	2133526080976 -> 2133526071664
	2133200901360 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2133200901360 -> 2133526080976
	2133526080976 [label=AccumulateGrad]
	2133526078672 -> 2133526082032
	2133200901456 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2133200901456 -> 2133526078672
	2133526078672 [label=AccumulateGrad]
	2133526080736 -> 2133526082032
	2133200901552 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2133200901552 -> 2133526080736
	2133526080736 [label=AccumulateGrad]
	2133526074448 -> 2133526071808
	2133200901936 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2133200901936 -> 2133526074448
	2133526074448 [label=AccumulateGrad]
	2133526072096 -> 2133526084864
	2133200902032 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2133200902032 -> 2133526072096
	2133526072096 [label=AccumulateGrad]
	2133526078096 -> 2133526084864
	2133200902128 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2133200902128 -> 2133526078096
	2133526078096 [label=AccumulateGrad]
	2133526084384 -> 2133526084576
	2133200902512 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2133200902512 -> 2133526084384
	2133526084384 [label=AccumulateGrad]
	2133526082848 -> 2133526460592
	2133200902608 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2133200902608 -> 2133526082848
	2133526082848 [label=AccumulateGrad]
	2133526074208 -> 2133526460592
	2133200902704 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2133200902704 -> 2133526074208
	2133526074208 [label=AccumulateGrad]
	2133526460400 -> 2133526460496
	2133526460544 -> 2133526455408
	2133200903088 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2133200903088 -> 2133526460544
	2133526460544 [label=AccumulateGrad]
	2133526454448 -> 2133526460208
	2133200903184 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2133200903184 -> 2133526454448
	2133526454448 [label=AccumulateGrad]
	2133526458912 -> 2133526460208
	2133200903280 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2133200903280 -> 2133526458912
	2133526458912 [label=AccumulateGrad]
	2133526454880 -> 2133526459968
	2133200903664 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2133200903664 -> 2133526454880
	2133526454880 [label=AccumulateGrad]
	2133526458096 -> 2133526446240
	2133200903760 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2133200903760 -> 2133526458096
	2133526458096 [label=AccumulateGrad]
	2133526457760 -> 2133526446240
	2133200903856 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2133200903856 -> 2133526457760
	2133526457760 [label=AccumulateGrad]
	2133526454064 -> 2133526460112
	2133200904240 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2133200904240 -> 2133526454064
	2133526454064 [label=AccumulateGrad]
	2133526450512 -> 2133526448064
	2133200904336 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2133200904336 -> 2133526450512
	2133526450512 [label=AccumulateGrad]
	2133526456752 -> 2133526448064
	2133200904432 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2133200904432 -> 2133526456752
	2133526456752 [label=AccumulateGrad]
	2133526448928 -> 2133526460352
	2133526456080 -> 2133526456176
	2133200905392 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2133200905392 -> 2133526456080
	2133526456080 [label=AccumulateGrad]
	2133526456128 -> 2133526447968
	2133200905488 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2133200905488 -> 2133526456128
	2133526456128 [label=AccumulateGrad]
	2133526451856 -> 2133526447968
	2133200905584 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2133200905584 -> 2133526451856
	2133526451856 [label=AccumulateGrad]
	2133526450896 -> 2133526457232
	2133200905968 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2133200905968 -> 2133526450896
	2133526450896 [label=AccumulateGrad]
	2133526459200 -> 2133526453392
	2133200906064 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2133200906064 -> 2133526459200
	2133526459200 [label=AccumulateGrad]
	2133526456704 -> 2133526453392
	2133200906160 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2133200906160 -> 2133526456704
	2133526456704 [label=AccumulateGrad]
	2133526459440 -> 2133526457376
	2133200906544 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2133200906544 -> 2133526459440
	2133526459440 [label=AccumulateGrad]
	2133526459728 -> 2133526451088
	2133200906640 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2133200906640 -> 2133526459728
	2133526459728 [label=AccumulateGrad]
	2133526454928 -> 2133526451088
	2133200906736 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2133200906736 -> 2133526454928
	2133526454928 [label=AccumulateGrad]
	2133526460256 -> 2133526455168
	2133526460256 [label=CudnnBatchNormBackward0]
	2133526458000 -> 2133526460256
	2133526458000 [label=ConvolutionBackward0]
	2133526449840 -> 2133526458000
	2133526456320 -> 2133526458000
	2133200904816 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2133200904816 -> 2133526456320
	2133526456320 [label=AccumulateGrad]
	2133526447344 -> 2133526460256
	2133200904912 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2133200904912 -> 2133526447344
	2133526447344 [label=AccumulateGrad]
	2133526460016 -> 2133526460256
	2133200905008 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2133200905008 -> 2133526460016
	2133526460016 [label=AccumulateGrad]
	2133526451904 -> 2133526457424
	2133200907120 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2133200907120 -> 2133526451904
	2133526451904 [label=AccumulateGrad]
	2133526459248 -> 2133526447872
	2133200907216 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2133200907216 -> 2133526459248
	2133526459248 [label=AccumulateGrad]
	2133526459152 -> 2133526447872
	2133200907312 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2133200907312 -> 2133526459152
	2133526459152 [label=AccumulateGrad]
	2133526457136 -> 2133526454688
	2133200907696 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2133200907696 -> 2133526457136
	2133526457136 [label=AccumulateGrad]
	2133526446288 -> 2133526459872
	2133200907792 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2133200907792 -> 2133526446288
	2133526446288 [label=AccumulateGrad]
	2133526453776 -> 2133526459872
	2133200907888 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2133200907888 -> 2133526453776
	2133526453776 [label=AccumulateGrad]
	2133526451424 -> 2133526457472
	2133200908272 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2133200908272 -> 2133526451424
	2133526451424 [label=AccumulateGrad]
	2133526456992 -> 2133526457520
	2133200908368 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2133200908368 -> 2133526456992
	2133526456992 [label=AccumulateGrad]
	2133526449072 -> 2133526457520
	2133200908464 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2133200908464 -> 2133526449072
	2133526449072 [label=AccumulateGrad]
	2133526456464 -> 2133526454592
	2133526451184 -> 2133526457328
	2133200908848 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2133200908848 -> 2133526451184
	2133526451184 [label=AccumulateGrad]
	2133526460784 -> 2133526458528
	2133200908944 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2133200908944 -> 2133526460784
	2133526460784 [label=AccumulateGrad]
	2133526455312 -> 2133526458528
	2133200909040 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2133200909040 -> 2133526455312
	2133526455312 [label=AccumulateGrad]
	2133526460928 -> 2133526461264
	2133200909424 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2133200909424 -> 2133526460928
	2133526460928 [label=AccumulateGrad]
	2133526461120 -> 2133526461024
	2133200909520 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2133200909520 -> 2133526461120
	2133526461120 [label=AccumulateGrad]
	2133526461360 -> 2133526461024
	2133200909616 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2133200909616 -> 2133526461360
	2133526461360 [label=AccumulateGrad]
	2133526461648 -> 2133526461600
	2133200910000 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2133200910000 -> 2133526461648
	2133526461648 [label=AccumulateGrad]
	2133526461504 -> 2133526461840
	2133200910096 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2133200910096 -> 2133526461504
	2133526461504 [label=AccumulateGrad]
	2133526461696 -> 2133526461840
	2133200910192 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2133200910192 -> 2133526461696
	2133526461696 [label=AccumulateGrad]
	2133526461888 -> 2133526462128
	2133526462320 -> 2133526453872
	2133526462320 [label=TBackward0]
	2133526462032 -> 2133526462320
	2133200910864 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2133200910864 -> 2133526462032
	2133526462032 [label=AccumulateGrad]
	2133526453872 -> 2133209954256
}
