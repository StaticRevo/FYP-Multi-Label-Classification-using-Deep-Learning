digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1929265055888 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1929513470224 [label=AddmmBackward0]
	1929513470512 -> 1929513470224
	1929236951984 [label="fc.bias
 (19)" fillcolor=lightblue]
	1929236951984 -> 1929513470512
	1929513470512 [label=AccumulateGrad]
	1929513468304 -> 1929513470224
	1929513468304 [label=ViewBackward0]
	1929513470320 -> 1929513468304
	1929513470320 [label=MeanBackward1]
	1929513473824 -> 1929513470320
	1929513473824 [label=ReluBackward0]
	1929513476896 -> 1929513473824
	1929513476896 [label=AddBackward0]
	1929513469648 -> 1929513476896
	1929513469648 [label=CudnnBatchNormBackward0]
	1929513468256 -> 1929513469648
	1929513468256 [label=ConvolutionBackward0]
	1929513480688 -> 1929513468256
	1929513480688 [label=ReluBackward0]
	1929513478432 -> 1929513480688
	1929513478432 [label=CudnnBatchNormBackward0]
	1929513480208 -> 1929513478432
	1929513480208 [label=ConvolutionBackward0]
	1929513477184 -> 1929513480208
	1929513477184 [label=ReluBackward0]
	1929513483904 -> 1929513477184
	1929513483904 [label=AddBackward0]
	1929513480400 -> 1929513483904
	1929513480400 [label=CudnnBatchNormBackward0]
	1929513469456 -> 1929513480400
	1929513469456 [label=ConvolutionBackward0]
	1929513474112 -> 1929513469456
	1929513474112 [label=ReluBackward0]
	1929513471088 -> 1929513474112
	1929513471088 [label=CudnnBatchNormBackward0]
	1929513475072 -> 1929513471088
	1929513475072 [label=ConvolutionBackward0]
	1929513469984 -> 1929513475072
	1929513469984 [label=ReluBackward0]
	1929513483328 -> 1929513469984
	1929513483328 [label=AddBackward0]
	1929513483568 -> 1929513483328
	1929513483568 [label=CudnnBatchNormBackward0]
	1929513480880 -> 1929513483568
	1929513480880 [label=ConvolutionBackward0]
	1929513483664 -> 1929513480880
	1929513483664 [label=ReluBackward0]
	1929513484144 -> 1929513483664
	1929513484144 [label=CudnnBatchNormBackward0]
	1929513474688 -> 1929513484144
	1929513474688 [label=ConvolutionBackward0]
	1929513482656 -> 1929513474688
	1929513482656 [label=ReluBackward0]
	1929513483760 -> 1929513482656
	1929513483760 [label=AddBackward0]
	1929513483088 -> 1929513483760
	1929513483088 [label=CudnnBatchNormBackward0]
	1929513475792 -> 1929513483088
	1929513475792 [label=ConvolutionBackward0]
	1929513469936 -> 1929513475792
	1929513469936 [label=ReluBackward0]
	1929513481936 -> 1929513469936
	1929513481936 [label=CudnnBatchNormBackward0]
	1929513478720 -> 1929513481936
	1929513478720 [label=ConvolutionBackward0]
	1929513473728 -> 1929513478720
	1929513473728 [label=ReluBackward0]
	1929513474064 -> 1929513473728
	1929513474064 [label=AddBackward0]
	1929513469072 -> 1929513474064
	1929513469072 [label=CudnnBatchNormBackward0]
	1929513469120 -> 1929513469072
	1929513469120 [label=ConvolutionBackward0]
	1929513469408 -> 1929513469120
	1929513469408 [label=ReluBackward0]
	1929513474592 -> 1929513469408
	1929513474592 [label=CudnnBatchNormBackward0]
	1929513474208 -> 1929513474592
	1929513474208 [label=ConvolutionBackward0]
	1929513473296 -> 1929513474208
	1929513473296 [label=ReluBackward0]
	1929513471280 -> 1929513473296
	1929513471280 [label=AddBackward0]
	1929513470752 -> 1929513471280
	1929513470752 [label=CudnnBatchNormBackward0]
	1929513474016 -> 1929513470752
	1929513474016 [label=ConvolutionBackward0]
	1929513476416 -> 1929513474016
	1929513476416 [label=ReluBackward0]
	1929513469504 -> 1929513476416
	1929513469504 [label=CudnnBatchNormBackward0]
	1929513476752 -> 1929513469504
	1929513476752 [label=ConvolutionBackward0]
	1929513474976 -> 1929513476752
	1929513474976 [label=ReluBackward0]
	1929513472288 -> 1929513474976
	1929513472288 [label=AddBackward0]
	1929513475408 -> 1929513472288
	1929513475408 [label=CudnnBatchNormBackward0]
	1929513471808 -> 1929513475408
	1929513471808 [label=ConvolutionBackward0]
	1929513477520 -> 1929513471808
	1929513477520 [label=ReluBackward0]
	1929513478960 -> 1929513477520
	1929513478960 [label=CudnnBatchNormBackward0]
	1929513468832 -> 1929513478960
	1929513468832 [label=ConvolutionBackward0]
	1929513476080 -> 1929513468832
	1929513476080 [label=ReluBackward0]
	1929513479008 -> 1929513476080
	1929513479008 [label=AddBackward0]
	1929513478192 -> 1929513479008
	1929513478192 [label=CudnnBatchNormBackward0]
	1929513471712 -> 1929513478192
	1929513471712 [label=ConvolutionBackward0]
	1929513480304 -> 1929513471712
	1929513480304 [label=ReluBackward0]
	1929513479584 -> 1929513480304
	1929513479584 [label=CudnnBatchNormBackward0]
	1929513475600 -> 1929513479584
	1929513475600 [label=ConvolutionBackward0]
	1929513476176 -> 1929513475600
	1929513476176 [label=MaxPool2DWithIndicesBackward0]
	1929513479200 -> 1929513476176
	1929513479200 [label=ReluBackward0]
	1929513477904 -> 1929513479200
	1929513477904 [label=CudnnBatchNormBackward0]
	1929513479632 -> 1929513477904
	1929513479632 [label=ConvolutionBackward0]
	1929513481600 -> 1929513479632
	1929236951600 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1929236951600 -> 1929513481600
	1929513481600 [label=AccumulateGrad]
	1929513479104 -> 1929513477904
	1929200630864 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1929200630864 -> 1929513479104
	1929513479104 [label=AccumulateGrad]
	1929513481216 -> 1929513477904
	1929200631728 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1929200631728 -> 1929513481216
	1929513481216 [label=AccumulateGrad]
	1929513480016 -> 1929513475600
	1929200631344 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1929200631344 -> 1929513480016
	1929513480016 [label=AccumulateGrad]
	1929513474640 -> 1929513479584
	1929200631536 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1929200631536 -> 1929513474640
	1929513474640 [label=AccumulateGrad]
	1929513476560 -> 1929513479584
	1929200631632 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1929200631632 -> 1929513476560
	1929513476560 [label=AccumulateGrad]
	1929513474784 -> 1929513471712
	1929313126064 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1929313126064 -> 1929513474784
	1929513474784 [label=AccumulateGrad]
	1929513478576 -> 1929513478192
	1929313126160 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1929313126160 -> 1929513478576
	1929513478576 [label=AccumulateGrad]
	1929513479968 -> 1929513478192
	1929313126256 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1929313126256 -> 1929513479968
	1929513479968 [label=AccumulateGrad]
	1929513476176 -> 1929513479008
	1929513468160 -> 1929513468832
	1929313126736 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1929313126736 -> 1929513468160
	1929513468160 [label=AccumulateGrad]
	1929513478624 -> 1929513478960
	1929313126832 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1929313126832 -> 1929513478624
	1929513478624 [label=AccumulateGrad]
	1929513475264 -> 1929513478960
	1929313127024 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1929313127024 -> 1929513475264
	1929513475264 [label=AccumulateGrad]
	1929513477856 -> 1929513471808
	1929313125104 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1929313125104 -> 1929513477856
	1929513477856 [label=AccumulateGrad]
	1929513476512 -> 1929513475408
	1929313124816 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1929313124816 -> 1929513476512
	1929513476512 [label=AccumulateGrad]
	1929513477088 -> 1929513475408
	1929313124912 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1929313124912 -> 1929513477088
	1929513477088 [label=AccumulateGrad]
	1929513476080 -> 1929513472288
	1929513473344 -> 1929513476752
	1929313124624 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1929313124624 -> 1929513473344
	1929513473344 [label=AccumulateGrad]
	1929513471616 -> 1929513469504
	1929313124528 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1929313124528 -> 1929513471616
	1929513471616 [label=AccumulateGrad]
	1929513475552 -> 1929513469504
	1929313124432 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1929313124432 -> 1929513475552
	1929513475552 [label=AccumulateGrad]
	1929513469600 -> 1929513474016
	1929236944208 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1929236944208 -> 1929513469600
	1929513469600 [label=AccumulateGrad]
	1929513475216 -> 1929513470752
	1929236944304 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1929236944304 -> 1929513475216
	1929513475216 [label=AccumulateGrad]
	1929513471136 -> 1929513470752
	1929236944400 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1929236944400 -> 1929513471136
	1929513471136 [label=AccumulateGrad]
	1929513472912 -> 1929513471280
	1929513472912 [label=CudnnBatchNormBackward0]
	1929513470368 -> 1929513472912
	1929513470368 [label=ConvolutionBackward0]
	1929513474976 -> 1929513470368
	1929513476320 -> 1929513470368
	1929313125392 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1929313125392 -> 1929513476320
	1929513476320 [label=AccumulateGrad]
	1929513471568 -> 1929513472912
	1929313125488 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1929313125488 -> 1929513471568
	1929513471568 [label=AccumulateGrad]
	1929513475936 -> 1929513472912
	1929313125584 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1929313125584 -> 1929513475936
	1929513475936 [label=AccumulateGrad]
	1929513470080 -> 1929513474208
	1929236944784 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1929236944784 -> 1929513470080
	1929513470080 [label=AccumulateGrad]
	1929513469168 -> 1929513474592
	1929236944880 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1929236944880 -> 1929513469168
	1929513469168 [label=AccumulateGrad]
	1929513469840 -> 1929513474592
	1929236944976 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1929236944976 -> 1929513469840
	1929513469840 [label=AccumulateGrad]
	1929513468976 -> 1929513469120
	1929236945360 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1929236945360 -> 1929513468976
	1929513468976 [label=AccumulateGrad]
	1929513472048 -> 1929513469072
	1929236945456 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1929236945456 -> 1929513472048
	1929513472048 [label=AccumulateGrad]
	1929513472576 -> 1929513469072
	1929236945552 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1929236945552 -> 1929513472576
	1929513472576 [label=AccumulateGrad]
	1929513473296 -> 1929513474064
	1929513472960 -> 1929513478720
	1929236946512 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1929236946512 -> 1929513472960
	1929513472960 [label=AccumulateGrad]
	1929513480976 -> 1929513481936
	1929236946608 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1929236946608 -> 1929513480976
	1929513480976 [label=AccumulateGrad]
	1929513472816 -> 1929513481936
	1929236946704 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1929236946704 -> 1929513472816
	1929513472816 [label=AccumulateGrad]
	1929513480832 -> 1929513475792
	1929236947088 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1929236947088 -> 1929513480832
	1929513480832 [label=AccumulateGrad]
	1929513476656 -> 1929513483088
	1929236947184 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1929236947184 -> 1929513476656
	1929513476656 [label=AccumulateGrad]
	1929513482464 -> 1929513483088
	1929236947280 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1929236947280 -> 1929513482464
	1929513482464 [label=AccumulateGrad]
	1929513473488 -> 1929513483760
	1929513473488 [label=CudnnBatchNormBackward0]
	1929513470464 -> 1929513473488
	1929513470464 [label=ConvolutionBackward0]
	1929513473728 -> 1929513470464
	1929513473248 -> 1929513470464
	1929236945936 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1929236945936 -> 1929513473248
	1929513473248 [label=AccumulateGrad]
	1929513482848 -> 1929513473488
	1929236946032 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1929236946032 -> 1929513482848
	1929513482848 [label=AccumulateGrad]
	1929513481552 -> 1929513473488
	1929236946128 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1929236946128 -> 1929513481552
	1929513481552 [label=AccumulateGrad]
	1929513476608 -> 1929513474688
	1929236947664 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1929236947664 -> 1929513476608
	1929513476608 [label=AccumulateGrad]
	1929513484192 -> 1929513484144
	1929236947760 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1929236947760 -> 1929513484192
	1929513484192 [label=AccumulateGrad]
	1929513483856 -> 1929513484144
	1929236947856 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1929236947856 -> 1929513483856
	1929513483856 [label=AccumulateGrad]
	1929513480496 -> 1929513480880
	1929236948240 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1929236948240 -> 1929513480496
	1929513480496 [label=AccumulateGrad]
	1929513483712 -> 1929513483568
	1929236948336 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1929236948336 -> 1929513483712
	1929513483712 [label=AccumulateGrad]
	1929513483952 -> 1929513483568
	1929236948432 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1929236948432 -> 1929513483952
	1929513483952 [label=AccumulateGrad]
	1929513482656 -> 1929513483328
	1929513472240 -> 1929513475072
	1929236949392 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1929236949392 -> 1929513472240
	1929513472240 [label=AccumulateGrad]
	1929513479056 -> 1929513471088
	1929236949488 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1929236949488 -> 1929513479056
	1929513479056 [label=AccumulateGrad]
	1929513471520 -> 1929513471088
	1929236949584 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1929236949584 -> 1929513471520
	1929513471520 [label=AccumulateGrad]
	1929513468112 -> 1929513469456
	1929236949968 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1929236949968 -> 1929513468112
	1929513468112 [label=AccumulateGrad]
	1929513469360 -> 1929513480400
	1929236950064 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1929236950064 -> 1929513469360
	1929513469360 [label=AccumulateGrad]
	1929513481792 -> 1929513480400
	1929236950160 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1929236950160 -> 1929513481792
	1929513481792 [label=AccumulateGrad]
	1929513483280 -> 1929513483904
	1929513483280 [label=CudnnBatchNormBackward0]
	1929513479296 -> 1929513483280
	1929513479296 [label=ConvolutionBackward0]
	1929513469984 -> 1929513479296
	1929513478528 -> 1929513479296
	1929236948816 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1929236948816 -> 1929513478528
	1929513478528 [label=AccumulateGrad]
	1929513468496 -> 1929513483280
	1929236948912 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1929236948912 -> 1929513468496
	1929513468496 [label=AccumulateGrad]
	1929513472528 -> 1929513483280
	1929236949008 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1929236949008 -> 1929513472528
	1929513472528 [label=AccumulateGrad]
	1929513468784 -> 1929513480208
	1929236950544 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1929236950544 -> 1929513468784
	1929513468784 [label=AccumulateGrad]
	1929513482800 -> 1929513478432
	1929236950640 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1929236950640 -> 1929513482800
	1929513482800 [label=AccumulateGrad]
	1929513478000 -> 1929513478432
	1929236950736 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1929236950736 -> 1929513478000
	1929513478000 [label=AccumulateGrad]
	1929513473584 -> 1929513468256
	1929236951120 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1929236951120 -> 1929513473584
	1929513473584 [label=AccumulateGrad]
	1929513478384 -> 1929513469648
	1929236951216 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1929236951216 -> 1929513478384
	1929513478384 [label=AccumulateGrad]
	1929513478864 -> 1929513469648
	1929236951312 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1929236951312 -> 1929513478864
	1929513478864 [label=AccumulateGrad]
	1929513477184 -> 1929513476896
	1929513474400 -> 1929513470224
	1929513474400 [label=TBackward0]
	1929513474448 -> 1929513474400
	1929236951888 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	1929236951888 -> 1929513474448
	1929513474448 [label=AccumulateGrad]
	1929513470224 -> 1929265055888
}
