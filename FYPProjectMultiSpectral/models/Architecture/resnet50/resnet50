digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2305460172080 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2305517499824 [label=AddmmBackward0]
	2305517499488 -> 2305517499824
	2305463498704 [label="fc.bias
 (19)" fillcolor=lightblue]
	2305463498704 -> 2305517499488
	2305517499488 [label=AccumulateGrad]
	2305517498864 -> 2305517499824
	2305517498864 [label=ViewBackward0]
	2305517498384 -> 2305517498864
	2305517498384 [label=MeanBackward1]
	2305517498048 -> 2305517498384
	2305517498048 [label=ReluBackward0]
	2305517497568 -> 2305517498048
	2305517497568 [label=AddBackward0]
	2305517497088 -> 2305517497568
	2305517497088 [label=CudnnBatchNormBackward0]
	2305517495984 -> 2305517497088
	2305517495984 [label=ConvolutionBackward0]
	2305517488352 -> 2305517495984
	2305517488352 [label=ReluBackward0]
	2305517490272 -> 2305517488352
	2305517490272 [label=CudnnBatchNormBackward0]
	2305517493920 -> 2305517490272
	2305517493920 [label=ConvolutionBackward0]
	2305517502128 -> 2305517493920
	2305517502128 [label=ReluBackward0]
	2305517502176 -> 2305517502128
	2305517502176 [label=CudnnBatchNormBackward0]
	2305517502080 -> 2305517502176
	2305517502080 [label=ConvolutionBackward0]
	2305517496944 -> 2305517502080
	2305517496944 [label=ReluBackward0]
	2305517501696 -> 2305517496944
	2305517501696 [label=AddBackward0]
	2305517501600 -> 2305517501696
	2305517501600 [label=CudnnBatchNormBackward0]
	2305517501168 -> 2305517501600
	2305517501168 [label=ConvolutionBackward0]
	2305517501024 -> 2305517501168
	2305517501024 [label=ReluBackward0]
	2305517500832 -> 2305517501024
	2305517500832 [label=CudnnBatchNormBackward0]
	2305517500496 -> 2305517500832
	2305517500496 [label=ConvolutionBackward0]
	2305517500640 -> 2305517500496
	2305517500640 [label=ReluBackward0]
	2305517500208 -> 2305517500640
	2305517500208 [label=CudnnBatchNormBackward0]
	2305517500112 -> 2305517500208
	2305517500112 [label=ConvolutionBackward0]
	2305517501504 -> 2305517500112
	2305517501504 [label=ReluBackward0]
	2305517499728 -> 2305517501504
	2305517499728 [label=AddBackward0]
	2305517499632 -> 2305517499728
	2305517499632 [label=CudnnBatchNormBackward0]
	2305517499680 -> 2305517499632
	2305517499680 [label=ConvolutionBackward0]
	2305517499056 -> 2305517499680
	2305517499056 [label=ReluBackward0]
	2305517499104 -> 2305517499056
	2305517499104 [label=CudnnBatchNormBackward0]
	2305517498960 -> 2305517499104
	2305517498960 [label=ConvolutionBackward0]
	2305517498672 -> 2305517498960
	2305517498672 [label=ReluBackward0]
	2305517498720 -> 2305517498672
	2305517498720 [label=CudnnBatchNormBackward0]
	2305517498432 -> 2305517498720
	2305517498432 [label=ConvolutionBackward0]
	2305517498336 -> 2305517498432
	2305517498336 [label=ReluBackward0]
	2305517498000 -> 2305517498336
	2305517498000 [label=AddBackward0]
	2305517497808 -> 2305517498000
	2305517497808 [label=CudnnBatchNormBackward0]
	2305517497856 -> 2305517497808
	2305517497856 [label=ConvolutionBackward0]
	2305517497472 -> 2305517497856
	2305517497472 [label=ReluBackward0]
	2305517497232 -> 2305517497472
	2305517497232 [label=CudnnBatchNormBackward0]
	2305517497184 -> 2305517497232
	2305517497184 [label=ConvolutionBackward0]
	2305517496848 -> 2305517497184
	2305517496848 [label=ReluBackward0]
	2305517496896 -> 2305517496848
	2305517496896 [label=CudnnBatchNormBackward0]
	2305517496800 -> 2305517496896
	2305517496800 [label=ConvolutionBackward0]
	2305517497952 -> 2305517496800
	2305517497952 [label=ReluBackward0]
	2305517496416 -> 2305517497952
	2305517496416 [label=AddBackward0]
	2305517496320 -> 2305517496416
	2305517496320 [label=CudnnBatchNormBackward0]
	2305517495888 -> 2305517496320
	2305517495888 [label=ConvolutionBackward0]
	2305517495744 -> 2305517495888
	2305517495744 [label=ReluBackward0]
	2305517495552 -> 2305517495744
	2305517495552 [label=CudnnBatchNormBackward0]
	2305517492240 -> 2305517495552
	2305517492240 [label=ConvolutionBackward0]
	2305517491472 -> 2305517492240
	2305517491472 [label=ReluBackward0]
	2305517493440 -> 2305517491472
	2305517493440 [label=CudnnBatchNormBackward0]
	2305517491184 -> 2305517493440
	2305517491184 [label=ConvolutionBackward0]
	2305517496224 -> 2305517491184
	2305517496224 [label=ReluBackward0]
	2305517488976 -> 2305517496224
	2305517488976 [label=AddBackward0]
	2305517491664 -> 2305517488976
	2305517491664 [label=CudnnBatchNormBackward0]
	2305517493776 -> 2305517491664
	2305517493776 [label=ConvolutionBackward0]
	2305517493872 -> 2305517493776
	2305517493872 [label=ReluBackward0]
	2305517488016 -> 2305517493872
	2305517488016 [label=CudnnBatchNormBackward0]
	2305517489408 -> 2305517488016
	2305517489408 [label=ConvolutionBackward0]
	2305517486912 -> 2305517489408
	2305517486912 [label=ReluBackward0]
	2305517494640 -> 2305517486912
	2305517494640 [label=CudnnBatchNormBackward0]
	2305517487392 -> 2305517494640
	2305517487392 [label=ConvolutionBackward0]
	2305517492144 -> 2305517487392
	2305517492144 [label=ReluBackward0]
	2305517492000 -> 2305517492144
	2305517492000 [label=AddBackward0]
	2305517494976 -> 2305517492000
	2305517494976 [label=CudnnBatchNormBackward0]
	2305517494736 -> 2305517494976
	2305517494736 [label=ConvolutionBackward0]
	2305517487584 -> 2305517494736
	2305517487584 [label=ReluBackward0]
	2305517492576 -> 2305517487584
	2305517492576 [label=CudnnBatchNormBackward0]
	2305517486720 -> 2305517492576
	2305517486720 [label=ConvolutionBackward0]
	2305517495360 -> 2305517486720
	2305517495360 [label=ReluBackward0]
	2305517488064 -> 2305517495360
	2305517488064 [label=CudnnBatchNormBackward0]
	2305517492528 -> 2305517488064
	2305517492528 [label=ConvolutionBackward0]
	2305517491856 -> 2305517492528
	2305517491856 [label=ReluBackward0]
	2305517492048 -> 2305517491856
	2305517492048 [label=AddBackward0]
	2305517493008 -> 2305517492048
	2305517493008 [label=CudnnBatchNormBackward0]
	2305517487728 -> 2305517493008
	2305517487728 [label=ConvolutionBackward0]
	2305517493152 -> 2305517487728
	2305517493152 [label=ReluBackward0]
	2305517487200 -> 2305517493152
	2305517487200 [label=CudnnBatchNormBackward0]
	2305517486624 -> 2305517487200
	2305517486624 [label=ConvolutionBackward0]
	2305517487056 -> 2305517486624
	2305517487056 [label=ReluBackward0]
	2305517495312 -> 2305517487056
	2305517495312 [label=CudnnBatchNormBackward0]
	2305517488880 -> 2305517495312
	2305517488880 [label=ConvolutionBackward0]
	2305517495024 -> 2305517488880
	2305517495024 [label=ReluBackward0]
	2305517490032 -> 2305517495024
	2305517490032 [label=AddBackward0]
	2305517491520 -> 2305517490032
	2305517491520 [label=CudnnBatchNormBackward0]
	2305517490896 -> 2305517491520
	2305517490896 [label=ConvolutionBackward0]
	2305514438080 -> 2305517490896
	2305514438080 [label=ReluBackward0]
	2305514438176 -> 2305514438080
	2305514438176 [label=CudnnBatchNormBackward0]
	2305514436928 -> 2305514438176
	2305514436928 [label=ConvolutionBackward0]
	2305514438320 -> 2305514436928
	2305514438320 [label=ReluBackward0]
	2305514437888 -> 2305514438320
	2305514437888 [label=CudnnBatchNormBackward0]
	2305514437840 -> 2305514437888
	2305514437840 [label=ConvolutionBackward0]
	2305460445600 -> 2305514437840
	2305460445600 [label=ReluBackward0]
	2305460444496 -> 2305460445600
	2305460444496 [label=AddBackward0]
	2305460444016 -> 2305460444496
	2305460444016 [label=CudnnBatchNormBackward0]
	2305460443680 -> 2305460444016
	2305460443680 [label=ConvolutionBackward0]
	2305460442720 -> 2305460443680
	2305460442720 [label=ReluBackward0]
	2305460441616 -> 2305460442720
	2305460441616 [label=CudnnBatchNormBackward0]
	2305460441136 -> 2305460441616
	2305460441136 [label=ConvolutionBackward0]
	2305460440176 -> 2305460441136
	2305460440176 [label=ReluBackward0]
	2305460439840 -> 2305460440176
	2305460439840 [label=CudnnBatchNormBackward0]
	2305460439360 -> 2305460439840
	2305460439360 [label=ConvolutionBackward0]
	2305460444640 -> 2305460439360
	2305460444640 [label=ReluBackward0]
	2305460437920 -> 2305460444640
	2305460437920 [label=AddBackward0]
	2305460437440 -> 2305460437920
	2305460437440 [label=CudnnBatchNormBackward0]
	2305460445840 -> 2305460437440
	2305460445840 [label=ConvolutionBackward0]
	2305460445696 -> 2305460445840
	2305460445696 [label=ReluBackward0]
	2305460445504 -> 2305460445696
	2305460445504 [label=CudnnBatchNormBackward0]
	2305460445168 -> 2305460445504
	2305460445168 [label=ConvolutionBackward0]
	2305460445312 -> 2305460445168
	2305460445312 [label=ReluBackward0]
	2305460444880 -> 2305460445312
	2305460444880 [label=CudnnBatchNormBackward0]
	2305460444784 -> 2305460444880
	2305460444784 [label=ConvolutionBackward0]
	2305460437296 -> 2305460444784
	2305460437296 [label=ReluBackward0]
	2305460444400 -> 2305460437296
	2305460444400 [label=AddBackward0]
	2305460444304 -> 2305460444400
	2305460444304 [label=CudnnBatchNormBackward0]
	2305460444352 -> 2305460444304
	2305460444352 [label=ConvolutionBackward0]
	2305460443728 -> 2305460444352
	2305460443728 [label=ReluBackward0]
	2305460443776 -> 2305460443728
	2305460443776 [label=CudnnBatchNormBackward0]
	2305460443632 -> 2305460443776
	2305460443632 [label=ConvolutionBackward0]
	2305460443344 -> 2305460443632
	2305460443344 [label=ReluBackward0]
	2305460443392 -> 2305460443344
	2305460443392 [label=CudnnBatchNormBackward0]
	2305460443104 -> 2305460443392
	2305460443104 [label=ConvolutionBackward0]
	2305460444208 -> 2305460443104
	2305460444208 [label=ReluBackward0]
	2305460442912 -> 2305460444208
	2305460442912 [label=AddBackward0]
	2305460442624 -> 2305460442912
	2305460442624 [label=CudnnBatchNormBackward0]
	2305460442384 -> 2305460442624
	2305460442384 [label=ConvolutionBackward0]
	2305460442192 -> 2305460442384
	2305460442192 [label=ReluBackward0]
	2305460441808 -> 2305460442192
	2305460441808 [label=CudnnBatchNormBackward0]
	2305460442048 -> 2305460441808
	2305460442048 [label=ConvolutionBackward0]
	2305460441664 -> 2305460442048
	2305460441664 [label=ReluBackward0]
	2305460441424 -> 2305460441664
	2305460441424 [label=CudnnBatchNormBackward0]
	2305460441376 -> 2305460441424
	2305460441376 [label=ConvolutionBackward0]
	2305460441040 -> 2305460441376
	2305460441040 [label=ReluBackward0]
	2305460441088 -> 2305460441040
	2305460441088 [label=AddBackward0]
	2305460440992 -> 2305460441088
	2305460440992 [label=CudnnBatchNormBackward0]
	2305460440560 -> 2305460440992
	2305460440560 [label=ConvolutionBackward0]
	2305460440416 -> 2305460440560
	2305460440416 [label=ReluBackward0]
	2305460440224 -> 2305460440416
	2305460440224 [label=CudnnBatchNormBackward0]
	2305460439888 -> 2305460440224
	2305460439888 [label=ConvolutionBackward0]
	2305460440032 -> 2305460439888
	2305460440032 [label=ReluBackward0]
	2305460439600 -> 2305460440032
	2305460439600 [label=CudnnBatchNormBackward0]
	2305460439504 -> 2305460439600
	2305460439504 [label=ConvolutionBackward0]
	2305460440896 -> 2305460439504
	2305460440896 [label=ReluBackward0]
	2305460439120 -> 2305460440896
	2305460439120 [label=AddBackward0]
	2305460439024 -> 2305460439120
	2305460439024 [label=CudnnBatchNormBackward0]
	2305460439072 -> 2305460439024
	2305460439072 [label=ConvolutionBackward0]
	2305460438448 -> 2305460439072
	2305460438448 [label=ReluBackward0]
	2305460438496 -> 2305460438448
	2305460438496 [label=CudnnBatchNormBackward0]
	2305460438352 -> 2305460438496
	2305460438352 [label=ConvolutionBackward0]
	2305460438064 -> 2305460438352
	2305460438064 [label=ReluBackward0]
	2305460438112 -> 2305460438064
	2305460438112 [label=CudnnBatchNormBackward0]
	2305460437824 -> 2305460438112
	2305460437824 [label=ConvolutionBackward0]
	2305460438928 -> 2305460437824
	2305460438928 [label=ReluBackward0]
	2305460437632 -> 2305460438928
	2305460437632 [label=AddBackward0]
	2305460437344 -> 2305460437632
	2305460437344 [label=CudnnBatchNormBackward0]
	2305460437248 -> 2305460437344
	2305460437248 [label=ConvolutionBackward0]
	2305460446560 -> 2305460437248
	2305460446560 [label=ReluBackward0]
	2305460446896 -> 2305460446560
	2305460446896 [label=CudnnBatchNormBackward0]
	2305460446992 -> 2305460446896
	2305460446992 [label=ConvolutionBackward0]
	2305460447184 -> 2305460446992
	2305460447184 [label=ReluBackward0]
	2305460447328 -> 2305460447184
	2305460447328 [label=CudnnBatchNormBackward0]
	2305460447424 -> 2305460447328
	2305460447424 [label=ConvolutionBackward0]
	2305460447616 -> 2305460447424
	2305460447616 [label=MaxPool2DWithIndicesBackward0]
	2305460447760 -> 2305460447616
	2305460447760 [label=ReluBackward0]
	2305460447856 -> 2305460447760
	2305460447856 [label=CudnnBatchNormBackward0]
	2305460447952 -> 2305460447856
	2305460447952 [label=ConvolutionBackward0]
	2305460448144 -> 2305460447952
	2305463498512 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2305463498512 -> 2305460448144
	2305460448144 [label=AccumulateGrad]
	2305460447904 -> 2305460447856
	2305483146064 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2305483146064 -> 2305460447904
	2305460447904 [label=AccumulateGrad]
	2305460447664 -> 2305460447856
	2305483145776 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2305483145776 -> 2305460447664
	2305460447664 [label=AccumulateGrad]
	2305460447568 -> 2305460447424
	2305483145680 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2305483145680 -> 2305460447568
	2305460447568 [label=AccumulateGrad]
	2305460447376 -> 2305460447328
	2305483145584 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2305483145584 -> 2305460447376
	2305460447376 [label=AccumulateGrad]
	2305460447232 -> 2305460447328
	2305483145296 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2305483145296 -> 2305460447232
	2305460447232 [label=AccumulateGrad]
	2305460447136 -> 2305460446992
	2305483439376 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2305483439376 -> 2305460447136
	2305460447136 [label=AccumulateGrad]
	2305460446944 -> 2305460446896
	2305483439472 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2305483439472 -> 2305460446944
	2305460446944 [label=AccumulateGrad]
	2305460446800 -> 2305460446896
	2305483439568 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2305483439568 -> 2305460446800
	2305460446800 [label=AccumulateGrad]
	2305460446704 -> 2305460437248
	2305483440048 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2305483440048 -> 2305460446704
	2305460446704 [label=AccumulateGrad]
	2305460437104 -> 2305460437344
	2305483439280 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2305483439280 -> 2305460437104
	2305460437104 [label=AccumulateGrad]
	2305460437200 -> 2305460437344
	2305483439184 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2305483439184 -> 2305460437200
	2305460437200 [label=AccumulateGrad]
	2305460437392 -> 2305460437632
	2305460437392 [label=CudnnBatchNormBackward0]
	2305460447088 -> 2305460437392
	2305460447088 [label=ConvolutionBackward0]
	2305460447616 -> 2305460447088
	2305460447472 -> 2305460447088
	2305483146256 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2305483146256 -> 2305460447472
	2305460447472 [label=AccumulateGrad]
	2305460437152 -> 2305460437392
	2305483146352 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2305483146352 -> 2305460437152
	2305460437152 [label=AccumulateGrad]
	2305460437056 -> 2305460437392
	2305483146448 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2305483146448 -> 2305460437056
	2305460437056 [label=AccumulateGrad]
	2305460437728 -> 2305460437824
	2305483436496 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2305483436496 -> 2305460437728
	2305460437728 [label=AccumulateGrad]
	2305460437872 -> 2305460438112
	2305483436688 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2305483436688 -> 2305460437872
	2305460437872 [label=AccumulateGrad]
	2305460438208 -> 2305460438112
	2305483438320 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2305483438320 -> 2305460438208
	2305460438208 [label=AccumulateGrad]
	2305460437968 -> 2305460438352
	2305483439088 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2305483439088 -> 2305460437968
	2305460437968 [label=AccumulateGrad]
	2305460438592 -> 2305460438496
	2305483438992 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2305483438992 -> 2305460438592
	2305460438592 [label=AccumulateGrad]
	2305460438544 -> 2305460438496
	2305463569680 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2305463569680 -> 2305460438544
	2305460438544 [label=AccumulateGrad]
	2305460438640 -> 2305460439072
	2305463570064 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2305463570064 -> 2305460438640
	2305460438640 [label=AccumulateGrad]
	2305460438976 -> 2305460439024
	2305463570160 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2305463570160 -> 2305460438976
	2305460438976 [label=AccumulateGrad]
	2305460439168 -> 2305460439024
	2305463570256 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2305463570256 -> 2305460439168
	2305460439168 [label=AccumulateGrad]
	2305460438928 -> 2305460439120
	2305460439312 -> 2305460439504
	2305463570640 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2305463570640 -> 2305460439312
	2305460439312 [label=AccumulateGrad]
	2305460439408 -> 2305460439600
	2305463570736 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2305463570736 -> 2305460439408
	2305460439408 [label=AccumulateGrad]
	2305460439792 -> 2305460439600
	2305463570832 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2305463570832 -> 2305460439792
	2305460439792 [label=AccumulateGrad]
	2305460439936 -> 2305460439888
	2305463571216 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2305463571216 -> 2305460439936
	2305460439936 [label=AccumulateGrad]
	2305460440080 -> 2305460440224
	2305463571312 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2305463571312 -> 2305460440080
	2305460440080 [label=AccumulateGrad]
	2305460440512 -> 2305460440224
	2305463571408 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2305463571408 -> 2305460440512
	2305460440512 [label=AccumulateGrad]
	2305460440608 -> 2305460440560
	2305463571792 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2305463571792 -> 2305460440608
	2305460440608 [label=AccumulateGrad]
	2305460440704 -> 2305460440992
	2305463571888 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2305463571888 -> 2305460440704
	2305460440704 [label=AccumulateGrad]
	2305460440752 -> 2305460440992
	2305463571984 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2305463571984 -> 2305460440752
	2305460440752 [label=AccumulateGrad]
	2305460440896 -> 2305460441088
	2305460441184 -> 2305460441376
	2305463572944 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2305463572944 -> 2305460441184
	2305460441184 [label=AccumulateGrad]
	2305460441568 -> 2305460441424
	2305463573040 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2305463573040 -> 2305460441568
	2305460441568 [label=AccumulateGrad]
	2305460441520 -> 2305460441424
	2305463573136 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2305463573136 -> 2305460441520
	2305460441520 [label=AccumulateGrad]
	2305460441712 -> 2305460442048
	2305463573520 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2305463573520 -> 2305460441712
	2305460441712 [label=AccumulateGrad]
	2305460441904 -> 2305460441808
	2305463573616 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2305463573616 -> 2305460441904
	2305460441904 [label=AccumulateGrad]
	2305460442144 -> 2305460441808
	2305463573712 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2305463573712 -> 2305460442144
	2305460442144 [label=AccumulateGrad]
	2305460442432 -> 2305460442384
	2305463574096 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2305463574096 -> 2305460442432
	2305460442432 [label=AccumulateGrad]
	2305460442288 -> 2305460442624
	2305463574192 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2305463574192 -> 2305460442288
	2305460442288 [label=AccumulateGrad]
	2305460442480 -> 2305460442624
	2305463574288 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2305463574288 -> 2305460442480
	2305460442480 [label=AccumulateGrad]
	2305460442672 -> 2305460442912
	2305460442672 [label=CudnnBatchNormBackward0]
	2305460441952 -> 2305460442672
	2305460441952 [label=ConvolutionBackward0]
	2305460441040 -> 2305460441952
	2305460441472 -> 2305460441952
	2305463572368 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2305463572368 -> 2305460441472
	2305460441472 [label=AccumulateGrad]
	2305460442336 -> 2305460442672
	2305463572464 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2305463572464 -> 2305460442336
	2305460442336 [label=AccumulateGrad]
	2305460442528 -> 2305460442672
	2305463572560 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2305463572560 -> 2305460442528
	2305460442528 [label=AccumulateGrad]
	2305460443008 -> 2305460443104
	2305463574672 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2305463574672 -> 2305460443008
	2305460443008 [label=AccumulateGrad]
	2305460443152 -> 2305460443392
	2305463574768 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2305463574768 -> 2305460443152
	2305460443152 [label=AccumulateGrad]
	2305460443488 -> 2305460443392
	2305463574864 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2305463574864 -> 2305460443488
	2305460443488 [label=AccumulateGrad]
	2305460443248 -> 2305460443632
	2305463575248 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2305463575248 -> 2305460443248
	2305460443248 [label=AccumulateGrad]
	2305460443872 -> 2305460443776
	2305463575344 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2305463575344 -> 2305460443872
	2305460443872 [label=AccumulateGrad]
	2305460443824 -> 2305460443776
	2305463575440 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2305463575440 -> 2305460443824
	2305460443824 [label=AccumulateGrad]
	2305460443920 -> 2305460444352
	2305463575824 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2305463575824 -> 2305460443920
	2305460443920 [label=AccumulateGrad]
	2305460444256 -> 2305460444304
	2305463575920 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2305463575920 -> 2305460444256
	2305460444256 [label=AccumulateGrad]
	2305460444448 -> 2305460444304
	2305463576016 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2305463576016 -> 2305460444448
	2305460444448 [label=AccumulateGrad]
	2305460444208 -> 2305460444400
	2305460444592 -> 2305460444784
	2305463576400 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2305463576400 -> 2305460444592
	2305460444592 [label=AccumulateGrad]
	2305460444688 -> 2305460444880
	2305463576496 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2305463576496 -> 2305460444688
	2305460444688 [label=AccumulateGrad]
	2305460445072 -> 2305460444880
	2305463576592 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2305463576592 -> 2305460445072
	2305460445072 [label=AccumulateGrad]
	2305460445216 -> 2305460445168
	2305463576976 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2305463576976 -> 2305460445216
	2305460445216 [label=AccumulateGrad]
	2305460445360 -> 2305460445504
	2305463577072 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2305463577072 -> 2305460445360
	2305460445360 [label=AccumulateGrad]
	2305460445792 -> 2305460445504
	2305463577168 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2305463577168 -> 2305460445792
	2305460445792 [label=AccumulateGrad]
	2305460445888 -> 2305460445840
	2305463577552 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2305463577552 -> 2305460445888
	2305460445888 [label=AccumulateGrad]
	2305460445984 -> 2305460437440
	2305463577648 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2305463577648 -> 2305460445984
	2305460445984 [label=AccumulateGrad]
	2305460446032 -> 2305460437440
	2305463577744 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2305463577744 -> 2305460446032
	2305460446032 [label=AccumulateGrad]
	2305460437296 -> 2305460437920
	2305460438400 -> 2305460439360
	2305463578128 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2305463578128 -> 2305460438400
	2305460438400 [label=AccumulateGrad]
	2305460439216 -> 2305460439840
	2305463578224 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2305463578224 -> 2305460439216
	2305460439216 [label=AccumulateGrad]
	2305460440320 -> 2305460439840
	2305463578320 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2305463578320 -> 2305460440320
	2305460440320 [label=AccumulateGrad]
	2305460440800 -> 2305460441136
	2305463578704 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2305463578704 -> 2305460440800
	2305460440800 [label=AccumulateGrad]
	2305460441760 -> 2305460441616
	2305463578800 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2305463578800 -> 2305460441760
	2305460441760 [label=AccumulateGrad]
	2305460442096 -> 2305460441616
	2305463578896 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2305463578896 -> 2305460442096
	2305460442096 [label=AccumulateGrad]
	2305460442576 -> 2305460443680
	2305463579280 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2305463579280 -> 2305460442576
	2305460442576 [label=AccumulateGrad]
	2305460443536 -> 2305460444016
	2305463579376 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2305463579376 -> 2305460443536
	2305460443536 [label=AccumulateGrad]
	2305460444160 -> 2305460444016
	2305463579472 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2305463579472 -> 2305460444160
	2305460444160 [label=AccumulateGrad]
	2305460444640 -> 2305460444496
	2305460445456 -> 2305514437840
	2305463580432 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2305463580432 -> 2305460445456
	2305460445456 [label=AccumulateGrad]
	2305514438512 -> 2305514437888
	2305463580528 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2305463580528 -> 2305514438512
	2305514438512 [label=AccumulateGrad]
	2305514438416 -> 2305514437888
	2305463580624 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2305463580624 -> 2305514438416
	2305514438416 [label=AccumulateGrad]
	2305514437648 -> 2305514436928
	2305463581008 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2305463581008 -> 2305514437648
	2305514437648 [label=AccumulateGrad]
	2305514437696 -> 2305514438176
	2305463581104 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2305463581104 -> 2305514437696
	2305514437696 [label=AccumulateGrad]
	2305514437744 -> 2305514438176
	2305463581200 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2305463581200 -> 2305514437744
	2305514437744 [label=AccumulateGrad]
	2305514437792 -> 2305517490896
	2305463581584 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2305463581584 -> 2305514437792
	2305514437792 [label=AccumulateGrad]
	2305517489072 -> 2305517491520
	2305463581680 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2305463581680 -> 2305517489072
	2305517489072 [label=AccumulateGrad]
	2305517489936 -> 2305517491520
	2305463581776 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2305463581776 -> 2305517489936
	2305517489936 [label=AccumulateGrad]
	2305517486384 -> 2305517490032
	2305517486384 [label=CudnnBatchNormBackward0]
	2305517486768 -> 2305517486384
	2305517486768 [label=ConvolutionBackward0]
	2305460445600 -> 2305517486768
	2305514437312 -> 2305517486768
	2305463579856 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2305463579856 -> 2305514437312
	2305514437312 [label=AccumulateGrad]
	2305514438464 -> 2305517486384
	2305463579952 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2305463579952 -> 2305514438464
	2305514438464 [label=AccumulateGrad]
	2305514437024 -> 2305517486384
	2305463580048 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2305463580048 -> 2305514437024
	2305514437024 [label=AccumulateGrad]
	2305517492480 -> 2305517488880
	2305463582160 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2305463582160 -> 2305517492480
	2305517492480 [label=AccumulateGrad]
	2305517489696 -> 2305517495312
	2305463582256 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2305463582256 -> 2305517489696
	2305517489696 [label=AccumulateGrad]
	2305517489456 -> 2305517495312
	2305463582352 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2305463582352 -> 2305517489456
	2305517489456 [label=AccumulateGrad]
	2305517493488 -> 2305517486624
	2305463484496 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2305463484496 -> 2305517493488
	2305517493488 [label=AccumulateGrad]
	2305517491328 -> 2305517487200
	2305463484592 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2305463484592 -> 2305517491328
	2305517491328 [label=AccumulateGrad]
	2305517491280 -> 2305517487200
	2305463484688 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2305463484688 -> 2305517491280
	2305517491280 [label=AccumulateGrad]
	2305517488160 -> 2305517487728
	2305463485072 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2305463485072 -> 2305517488160
	2305517488160 [label=AccumulateGrad]
	2305517489984 -> 2305517493008
	2305463485168 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2305463485168 -> 2305517489984
	2305517489984 [label=AccumulateGrad]
	2305517494016 -> 2305517493008
	2305463485264 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2305463485264 -> 2305517494016
	2305517494016 [label=AccumulateGrad]
	2305517495024 -> 2305517492048
	2305517493200 -> 2305517492528
	2305463485648 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2305463485648 -> 2305517493200
	2305517493200 [label=AccumulateGrad]
	2305517493680 -> 2305517488064
	2305463485744 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2305463485744 -> 2305517493680
	2305517493680 [label=AccumulateGrad]
	2305517492384 -> 2305517488064
	2305463485840 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2305463485840 -> 2305517492384
	2305517492384 [label=AccumulateGrad]
	2305517495072 -> 2305517486720
	2305463486224 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2305463486224 -> 2305517495072
	2305517495072 [label=AccumulateGrad]
	2305517492192 -> 2305517492576
	2305463486320 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2305463486320 -> 2305517492192
	2305517492192 [label=AccumulateGrad]
	2305517490800 -> 2305517492576
	2305463486416 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2305463486416 -> 2305517490800
	2305517490800 [label=AccumulateGrad]
	2305517492288 -> 2305517494736
	2305463486800 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2305463486800 -> 2305517492288
	2305517492288 [label=AccumulateGrad]
	2305517491424 -> 2305517494976
	2305463486896 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2305463486896 -> 2305517491424
	2305517491424 [label=AccumulateGrad]
	2305517492912 -> 2305517494976
	2305463486992 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2305463486992 -> 2305517492912
	2305517492912 [label=AccumulateGrad]
	2305517491856 -> 2305517492000
	2305517494928 -> 2305517487392
	2305463487376 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2305463487376 -> 2305517494928
	2305517494928 [label=AccumulateGrad]
	2305517491568 -> 2305517494640
	2305463487472 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2305463487472 -> 2305517491568
	2305517491568 [label=AccumulateGrad]
	2305517487296 -> 2305517494640
	2305463487568 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2305463487568 -> 2305517487296
	2305517487296 [label=AccumulateGrad]
	2305517487632 -> 2305517489408
	2305463487952 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2305463487952 -> 2305517487632
	2305517487632 [label=AccumulateGrad]
	2305517488256 -> 2305517488016
	2305463488048 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2305463488048 -> 2305517488256
	2305517488256 [label=AccumulateGrad]
	2305517486288 -> 2305517488016
	2305463488144 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2305463488144 -> 2305517486288
	2305517486288 [label=AccumulateGrad]
	2305517492768 -> 2305517493776
	2305463488528 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2305463488528 -> 2305517492768
	2305517492768 [label=AccumulateGrad]
	2305517492624 -> 2305517491664
	2305463488624 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2305463488624 -> 2305517492624
	2305517492624 [label=AccumulateGrad]
	2305517487248 -> 2305517491664
	2305463488720 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2305463488720 -> 2305517487248
	2305517487248 [label=AccumulateGrad]
	2305517492144 -> 2305517488976
	2305517487968 -> 2305517491184
	2305463489104 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2305463489104 -> 2305517487968
	2305517487968 [label=AccumulateGrad]
	2305517493296 -> 2305517493440
	2305463489200 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2305463489200 -> 2305517493296
	2305517493296 [label=AccumulateGrad]
	2305517494160 -> 2305517493440
	2305463489296 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2305463489296 -> 2305517494160
	2305517494160 [label=AccumulateGrad]
	2305517494880 -> 2305517492240
	2305463489680 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2305463489680 -> 2305517494880
	2305517494880 [label=AccumulateGrad]
	2305517494784 -> 2305517495552
	2305463489776 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2305463489776 -> 2305517494784
	2305517494784 [label=AccumulateGrad]
	2305517495840 -> 2305517495552
	2305463489872 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2305463489872 -> 2305517495840
	2305517495840 [label=AccumulateGrad]
	2305517495936 -> 2305517495888
	2305463490256 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2305463490256 -> 2305517495936
	2305517495936 [label=AccumulateGrad]
	2305517496032 -> 2305517496320
	2305463490352 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2305463490352 -> 2305517496032
	2305517496032 [label=AccumulateGrad]
	2305517496080 -> 2305517496320
	2305463490448 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2305463490448 -> 2305517496080
	2305517496080 [label=AccumulateGrad]
	2305517496224 -> 2305517496416
	2305517496176 -> 2305517496800
	2305463490832 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2305463490832 -> 2305517496176
	2305517496176 [label=AccumulateGrad]
	2305517496704 -> 2305517496896
	2305463490928 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2305463490928 -> 2305517496704
	2305517496704 [label=AccumulateGrad]
	2305517496656 -> 2305517496896
	2305463491024 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2305463491024 -> 2305517496656
	2305517496656 [label=AccumulateGrad]
	2305517496992 -> 2305517497184
	2305463491408 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2305463491408 -> 2305517496992
	2305517496992 [label=AccumulateGrad]
	2305517497376 -> 2305517497232
	2305463491504 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2305463491504 -> 2305517497376
	2305517497376 [label=AccumulateGrad]
	2305517497328 -> 2305517497232
	2305463491600 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2305463491600 -> 2305517497328
	2305517497328 [label=AccumulateGrad]
	2305517497520 -> 2305517497856
	2305463491984 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2305463491984 -> 2305517497520
	2305517497520 [label=AccumulateGrad]
	2305517497712 -> 2305517497808
	2305463492080 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2305463492080 -> 2305517497712
	2305517497712 [label=AccumulateGrad]
	2305517497616 -> 2305517497808
	2305463492176 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2305463492176 -> 2305517497616
	2305517497616 [label=AccumulateGrad]
	2305517497952 -> 2305517498000
	2305517498192 -> 2305517498432
	2305463493136 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2305463493136 -> 2305517498192
	2305517498192 [label=AccumulateGrad]
	2305517498480 -> 2305517498720
	2305463493232 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2305463493232 -> 2305517498480
	2305517498480 [label=AccumulateGrad]
	2305517498816 -> 2305517498720
	2305463493328 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2305463493328 -> 2305517498816
	2305517498816 [label=AccumulateGrad]
	2305517498576 -> 2305517498960
	2305463493712 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2305463493712 -> 2305517498576
	2305517498576 [label=AccumulateGrad]
	2305517499200 -> 2305517499104
	2305463493808 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2305463493808 -> 2305517499200
	2305517499200 [label=AccumulateGrad]
	2305517499152 -> 2305517499104
	2305463493904 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2305463493904 -> 2305517499152
	2305517499152 [label=AccumulateGrad]
	2305517499248 -> 2305517499680
	2305463494288 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2305463494288 -> 2305517499248
	2305517499248 [label=AccumulateGrad]
	2305517499584 -> 2305517499632
	2305463494384 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2305463494384 -> 2305517499584
	2305517499584 [label=AccumulateGrad]
	2305517499776 -> 2305517499632
	2305463494480 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2305463494480 -> 2305517499776
	2305517499776 [label=AccumulateGrad]
	2305517499536 -> 2305517499728
	2305517499536 [label=CudnnBatchNormBackward0]
	2305517498768 -> 2305517499536
	2305517498768 [label=ConvolutionBackward0]
	2305517498336 -> 2305517498768
	2305517498288 -> 2305517498768
	2305463492560 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2305463492560 -> 2305517498288
	2305517498288 [label=AccumulateGrad]
	2305517499392 -> 2305517499536
	2305463492656 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2305463492656 -> 2305517499392
	2305517499392 [label=AccumulateGrad]
	2305517499440 -> 2305517499536
	2305463492752 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2305463492752 -> 2305517499440
	2305517499440 [label=AccumulateGrad]
	2305517499920 -> 2305517500112
	2305463494864 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2305463494864 -> 2305517499920
	2305517499920 [label=AccumulateGrad]
	2305517500016 -> 2305517500208
	2305463494960 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2305463494960 -> 2305517500016
	2305517500016 [label=AccumulateGrad]
	2305517500400 -> 2305517500208
	2305463495056 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2305463495056 -> 2305517500400
	2305517500400 [label=AccumulateGrad]
	2305517500544 -> 2305517500496
	2305463495440 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2305463495440 -> 2305517500544
	2305517500544 [label=AccumulateGrad]
	2305517500688 -> 2305517500832
	2305463495536 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2305463495536 -> 2305517500688
	2305517500688 [label=AccumulateGrad]
	2305517501120 -> 2305517500832
	2305463495632 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2305463495632 -> 2305517501120
	2305517501120 [label=AccumulateGrad]
	2305517501216 -> 2305517501168
	2305463496016 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2305463496016 -> 2305517501216
	2305517501216 [label=AccumulateGrad]
	2305517501312 -> 2305517501600
	2305463496112 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2305463496112 -> 2305517501312
	2305517501312 [label=AccumulateGrad]
	2305517501360 -> 2305517501600
	2305463496208 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2305463496208 -> 2305517501360
	2305517501360 [label=AccumulateGrad]
	2305517501504 -> 2305517501696
	2305517501456 -> 2305517502080
	2305463496592 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2305463496592 -> 2305517501456
	2305517501456 [label=AccumulateGrad]
	2305517501984 -> 2305517502176
	2305463496688 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2305463496688 -> 2305517501984
	2305517501984 [label=AccumulateGrad]
	2305517501936 -> 2305517502176
	2305463496784 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2305463496784 -> 2305517501936
	2305517501936 [label=AccumulateGrad]
	2305517502272 -> 2305517493920
	2305463497168 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2305463497168 -> 2305517502272
	2305517502272 [label=AccumulateGrad]
	2305517493968 -> 2305517490272
	2305463497264 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2305463497264 -> 2305517493968
	2305517493968 [label=AccumulateGrad]
	2305517487920 -> 2305517490272
	2305463497360 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2305463497360 -> 2305517487920
	2305517487920 [label=AccumulateGrad]
	2305517495648 -> 2305517495984
	2305463497744 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2305463497744 -> 2305517495648
	2305517495648 [label=AccumulateGrad]
	2305517496608 -> 2305517497088
	2305463497840 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2305463497840 -> 2305517496608
	2305517496608 [label=AccumulateGrad]
	2305517496464 -> 2305517497088
	2305463497936 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2305463497936 -> 2305517496464
	2305517496464 [label=AccumulateGrad]
	2305517496944 -> 2305517497568
	2305517499008 -> 2305517499824
	2305517499008 [label=TBackward0]
	2305517497424 -> 2305517499008
	2305463498608 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	2305463498608 -> 2305517497424
	2305517497424 [label=AccumulateGrad]
	2305517499824 -> 2305460172080
}
