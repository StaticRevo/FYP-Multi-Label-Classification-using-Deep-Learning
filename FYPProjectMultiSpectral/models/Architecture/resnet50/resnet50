digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1926170162768 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	1926102988256 [label=AddmmBackward0]
	1926102987920 -> 1926102988256
	1926179682544 [label="fc.bias
 (19)" fillcolor=lightblue]
	1926179682544 -> 1926102987920
	1926102987920 [label=AccumulateGrad]
	1926102987296 -> 1926102988256
	1926102987296 [label=ViewBackward0]
	1926102986816 -> 1926102987296
	1926102986816 [label=MeanBackward1]
	1926102986480 -> 1926102986816
	1926102986480 [label=ReluBackward0]
	1926102986000 -> 1926102986480
	1926102986000 [label=AddBackward0]
	1926102985520 -> 1926102986000
	1926102985520 [label=CudnnBatchNormBackward0]
	1926102984416 -> 1926102985520
	1926102984416 [label=ConvolutionBackward0]
	1926102983456 -> 1926102984416
	1926102983456 [label=ReluBackward0]
	1926102983120 -> 1926102983456
	1926102983120 [label=CudnnBatchNormBackward0]
	1926102977648 -> 1926102983120
	1926102977648 [label=ConvolutionBackward0]
	1926102975104 -> 1926102977648
	1926102975104 [label=ReluBackward0]
	1926102990560 -> 1926102975104
	1926102990560 [label=CudnnBatchNormBackward0]
	1926102990464 -> 1926102990560
	1926102990464 [label=ConvolutionBackward0]
	1926102985376 -> 1926102990464
	1926102985376 [label=ReluBackward0]
	1926102990080 -> 1926102985376
	1926102990080 [label=AddBackward0]
	1926102989984 -> 1926102990080
	1926102989984 [label=CudnnBatchNormBackward0]
	1926102990032 -> 1926102989984
	1926102990032 [label=ConvolutionBackward0]
	1926102989408 -> 1926102990032
	1926102989408 [label=ReluBackward0]
	1926102989456 -> 1926102989408
	1926102989456 [label=CudnnBatchNormBackward0]
	1926102989312 -> 1926102989456
	1926102989312 [label=ConvolutionBackward0]
	1926102989024 -> 1926102989312
	1926102989024 [label=ReluBackward0]
	1926102989072 -> 1926102989024
	1926102989072 [label=CudnnBatchNormBackward0]
	1926102988784 -> 1926102989072
	1926102988784 [label=ConvolutionBackward0]
	1926102989888 -> 1926102988784
	1926102989888 [label=ReluBackward0]
	1926102988592 -> 1926102989888
	1926102988592 [label=AddBackward0]
	1926102988304 -> 1926102988592
	1926102988304 [label=CudnnBatchNormBackward0]
	1926102988064 -> 1926102988304
	1926102988064 [label=ConvolutionBackward0]
	1926102987872 -> 1926102988064
	1926102987872 [label=ReluBackward0]
	1926102987488 -> 1926102987872
	1926102987488 [label=CudnnBatchNormBackward0]
	1926102987728 -> 1926102987488
	1926102987728 [label=ConvolutionBackward0]
	1926102987344 -> 1926102987728
	1926102987344 [label=ReluBackward0]
	1926102987104 -> 1926102987344
	1926102987104 [label=CudnnBatchNormBackward0]
	1926102987056 -> 1926102987104
	1926102987056 [label=ConvolutionBackward0]
	1926102986720 -> 1926102987056
	1926102986720 [label=ReluBackward0]
	1926102986768 -> 1926102986720
	1926102986768 [label=AddBackward0]
	1926102986672 -> 1926102986768
	1926102986672 [label=CudnnBatchNormBackward0]
	1926102986240 -> 1926102986672
	1926102986240 [label=ConvolutionBackward0]
	1926102986096 -> 1926102986240
	1926102986096 [label=ReluBackward0]
	1926102985904 -> 1926102986096
	1926102985904 [label=CudnnBatchNormBackward0]
	1926102985568 -> 1926102985904
	1926102985568 [label=ConvolutionBackward0]
	1926102985712 -> 1926102985568
	1926102985712 [label=ReluBackward0]
	1926102985280 -> 1926102985712
	1926102985280 [label=CudnnBatchNormBackward0]
	1926102985184 -> 1926102985280
	1926102985184 [label=ConvolutionBackward0]
	1926102986576 -> 1926102985184
	1926102986576 [label=ReluBackward0]
	1926102984800 -> 1926102986576
	1926102984800 [label=AddBackward0]
	1926102984704 -> 1926102984800
	1926102984704 [label=CudnnBatchNormBackward0]
	1926102984752 -> 1926102984704
	1926102984752 [label=ConvolutionBackward0]
	1926102984128 -> 1926102984752
	1926102984128 [label=ReluBackward0]
	1926102984176 -> 1926102984128
	1926102984176 [label=CudnnBatchNormBackward0]
	1926102984032 -> 1926102984176
	1926102984032 [label=ConvolutionBackward0]
	1926102983744 -> 1926102984032
	1926102983744 [label=ReluBackward0]
	1926102983792 -> 1926102983744
	1926102983792 [label=CudnnBatchNormBackward0]
	1926102983504 -> 1926102983792
	1926102983504 [label=ConvolutionBackward0]
	1926102984608 -> 1926102983504
	1926102984608 [label=ReluBackward0]
	1926102983312 -> 1926102984608
	1926102983312 [label=AddBackward0]
	1926102983024 -> 1926102983312
	1926102983024 [label=CudnnBatchNormBackward0]
	1926102982784 -> 1926102983024
	1926102982784 [label=ConvolutionBackward0]
	1926102980000 -> 1926102982784
	1926102980000 [label=ReluBackward0]
	1926102978800 -> 1926102980000
	1926102978800 [label=CudnnBatchNormBackward0]
	1926102978848 -> 1926102978800
	1926102978848 [label=ConvolutionBackward0]
	1926102982544 -> 1926102978848
	1926102982544 [label=ReluBackward0]
	1926102976832 -> 1926102982544
	1926102976832 [label=CudnnBatchNormBackward0]
	1926102975296 -> 1926102976832
	1926102975296 [label=ConvolutionBackward0]
	1926102983072 -> 1926102975296
	1926102983072 [label=ReluBackward0]
	1926102981296 -> 1926102983072
	1926102981296 [label=AddBackward0]
	1926102977984 -> 1926102981296
	1926102977984 [label=CudnnBatchNormBackward0]
	1926102975440 -> 1926102977984
	1926102975440 [label=ConvolutionBackward0]
	1926102976400 -> 1926102975440
	1926102976400 [label=ReluBackward0]
	1926102982448 -> 1926102976400
	1926102982448 [label=CudnnBatchNormBackward0]
	1926102977696 -> 1926102982448
	1926102977696 [label=ConvolutionBackward0]
	1926102974720 -> 1926102977696
	1926102974720 [label=ReluBackward0]
	1926102976688 -> 1926102974720
	1926102976688 [label=CudnnBatchNormBackward0]
	1926102974960 -> 1926102976688
	1926102974960 [label=ConvolutionBackward0]
	1926102980768 -> 1926102974960
	1926102980768 [label=ReluBackward0]
	1926102982112 -> 1926102980768
	1926102982112 [label=AddBackward0]
	1926102980432 -> 1926102982112
	1926102980432 [label=CudnnBatchNormBackward0]
	1926102979568 -> 1926102980432
	1926102979568 [label=ConvolutionBackward0]
	1926102981008 -> 1926102979568
	1926102981008 [label=ReluBackward0]
	1926102974816 -> 1926102981008
	1926102974816 [label=CudnnBatchNormBackward0]
	1926102981920 -> 1926102974816
	1926102981920 [label=ConvolutionBackward0]
	1926102977408 -> 1926102981920
	1926102977408 [label=ReluBackward0]
	1926102982496 -> 1926102977408
	1926102982496 [label=CudnnBatchNormBackward0]
	1926102975248 -> 1926102982496
	1926102975248 [label=ConvolutionBackward0]
	1926102982304 -> 1926102975248
	1926102982304 [label=ReluBackward0]
	1926102981152 -> 1926102982304
	1926102981152 [label=AddBackward0]
	1926102981728 -> 1926102981152
	1926102981728 [label=CudnnBatchNormBackward0]
	1926102980144 -> 1926102981728
	1926102980144 [label=ConvolutionBackward0]
	1926102979136 -> 1926102980144
	1926102979136 [label=ReluBackward0]
	1926102978272 -> 1926102979136
	1926102978272 [label=CudnnBatchNormBackward0]
	1926102978080 -> 1926102978272
	1926102978080 [label=ConvolutionBackward0]
	1926102980864 -> 1926102978080
	1926102980864 [label=ReluBackward0]
	1926102979472 -> 1926102980864
	1926102979472 [label=CudnnBatchNormBackward0]
	1926102981776 -> 1926102979472
	1926102981776 [label=ConvolutionBackward0]
	1926103102992 -> 1926102981776
	1926103102992 [label=ReluBackward0]
	1926103091040 -> 1926103102992
	1926103091040 [label=AddBackward0]
	1926103099200 -> 1926103091040
	1926103099200 [label=CudnnBatchNormBackward0]
	1926103099680 -> 1926103099200
	1926103099680 [label=ConvolutionBackward0]
	1926103103568 -> 1926103099680
	1926103103568 [label=ReluBackward0]
	1926125435056 -> 1926103103568
	1926125435056 [label=CudnnBatchNormBackward0]
	1926103103904 -> 1926125435056
	1926103103904 [label=ConvolutionBackward0]
	1926170467376 -> 1926103103904
	1926170467376 [label=ReluBackward0]
	1926170466272 -> 1926170467376
	1926170466272 [label=CudnnBatchNormBackward0]
	1926170465792 -> 1926170466272
	1926170465792 [label=ConvolutionBackward0]
	1926103104912 -> 1926170465792
	1926103104912 [label=ReluBackward0]
	1926170464352 -> 1926103104912
	1926170464352 [label=AddBackward0]
	1926170463872 -> 1926170464352
	1926170463872 [label=CudnnBatchNormBackward0]
	1926170463536 -> 1926170463872
	1926170463536 [label=ConvolutionBackward0]
	1926170462576 -> 1926170463536
	1926170462576 [label=ReluBackward0]
	1926170461472 -> 1926170462576
	1926170461472 [label=CudnnBatchNormBackward0]
	1926170460992 -> 1926170461472
	1926170460992 [label=ConvolutionBackward0]
	1926170468288 -> 1926170460992
	1926170468288 [label=ReluBackward0]
	1926170467904 -> 1926170468288
	1926170467904 [label=CudnnBatchNormBackward0]
	1926170468144 -> 1926170467904
	1926170468144 [label=ConvolutionBackward0]
	1926170464496 -> 1926170468144
	1926170464496 [label=ReluBackward0]
	1926170467424 -> 1926170464496
	1926170467424 [label=AddBackward0]
	1926170467664 -> 1926170467424
	1926170467664 [label=CudnnBatchNormBackward0]
	1926170467328 -> 1926170467664
	1926170467328 [label=ConvolutionBackward0]
	1926170467040 -> 1926170467328
	1926170467040 [label=ReluBackward0]
	1926170467088 -> 1926170467040
	1926170467088 [label=CudnnBatchNormBackward0]
	1926170466800 -> 1926170467088
	1926170466800 [label=ConvolutionBackward0]
	1926170466704 -> 1926170466800
	1926170466704 [label=ReluBackward0]
	1926170466368 -> 1926170466704
	1926170466368 [label=CudnnBatchNormBackward0]
	1926170466176 -> 1926170466368
	1926170466176 [label=ConvolutionBackward0]
	1926170467520 -> 1926170466176
	1926170467520 [label=ReluBackward0]
	1926170465888 -> 1926170467520
	1926170465888 [label=AddBackward0]
	1926170465696 -> 1926170465888
	1926170465696 [label=CudnnBatchNormBackward0]
	1926170465744 -> 1926170465696
	1926170465744 [label=ConvolutionBackward0]
	1926170465360 -> 1926170465744
	1926170465360 [label=ReluBackward0]
	1926170465120 -> 1926170465360
	1926170465120 [label=CudnnBatchNormBackward0]
	1926170465072 -> 1926170465120
	1926170465072 [label=ConvolutionBackward0]
	1926170464736 -> 1926170465072
	1926170464736 [label=ReluBackward0]
	1926170464784 -> 1926170464736
	1926170464784 [label=CudnnBatchNormBackward0]
	1926170464688 -> 1926170464784
	1926170464688 [label=ConvolutionBackward0]
	1926170464064 -> 1926170464688
	1926170464064 [label=ReluBackward0]
	1926170464112 -> 1926170464064
	1926170464112 [label=AddBackward0]
	1926170463968 -> 1926170464112
	1926170463968 [label=CudnnBatchNormBackward0]
	1926170463584 -> 1926170463968
	1926170463584 [label=ConvolutionBackward0]
	1926170463728 -> 1926170463584
	1926170463728 [label=ReluBackward0]
	1926170463296 -> 1926170463728
	1926170463296 [label=CudnnBatchNormBackward0]
	1926170463200 -> 1926170463296
	1926170463200 [label=ConvolutionBackward0]
	1926170463008 -> 1926170463200
	1926170463008 [label=ReluBackward0]
	1926170462624 -> 1926170463008
	1926170462624 [label=CudnnBatchNormBackward0]
	1926170462864 -> 1926170462624
	1926170462864 [label=ConvolutionBackward0]
	1926170464208 -> 1926170462864
	1926170464208 [label=ReluBackward0]
	1926170462144 -> 1926170464208
	1926170462144 [label=AddBackward0]
	1926170462384 -> 1926170462144
	1926170462384 [label=CudnnBatchNormBackward0]
	1926170462048 -> 1926170462384
	1926170462048 [label=ConvolutionBackward0]
	1926170461760 -> 1926170462048
	1926170461760 [label=ReluBackward0]
	1926170461808 -> 1926170461760
	1926170461808 [label=CudnnBatchNormBackward0]
	1926170461520 -> 1926170461808
	1926170461520 [label=ConvolutionBackward0]
	1926170461424 -> 1926170461520
	1926170461424 [label=ReluBackward0]
	1926170461088 -> 1926170461424
	1926170461088 [label=CudnnBatchNormBackward0]
	1926170460896 -> 1926170461088
	1926170460896 [label=ConvolutionBackward0]
	1926170462240 -> 1926170460896
	1926170462240 [label=ReluBackward0]
	1926170460608 -> 1926170462240
	1926170460608 [label=AddBackward0]
	1926170460416 -> 1926170460608
	1926170460416 [label=CudnnBatchNormBackward0]
	1926170460464 -> 1926170460416
	1926170460464 [label=ConvolutionBackward0]
	1926170468768 -> 1926170460464
	1926170468768 [label=ReluBackward0]
	1926170469104 -> 1926170468768
	1926170469104 [label=CudnnBatchNormBackward0]
	1926170469200 -> 1926170469104
	1926170469200 [label=ConvolutionBackward0]
	1926170469392 -> 1926170469200
	1926170469392 [label=ReluBackward0]
	1926170469536 -> 1926170469392
	1926170469536 [label=CudnnBatchNormBackward0]
	1926170469632 -> 1926170469536
	1926170469632 [label=ConvolutionBackward0]
	1926170469824 -> 1926170469632
	1926170469824 [label=MaxPool2DWithIndicesBackward0]
	1926170469968 -> 1926170469824
	1926170469968 [label=ReluBackward0]
	1926170470064 -> 1926170469968
	1926170470064 [label=CudnnBatchNormBackward0]
	1926170470160 -> 1926170470064
	1926170470160 [label=ConvolutionBackward0]
	1926170470352 -> 1926170470160
	1926179682352 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	1926179682352 -> 1926170470352
	1926170470352 [label=AccumulateGrad]
	1926170470112 -> 1926170470064
	1926098996016 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1926098996016 -> 1926170470112
	1926170470112 [label=AccumulateGrad]
	1926170469872 -> 1926170470064
	1926098995728 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1926098995728 -> 1926170469872
	1926170469872 [label=AccumulateGrad]
	1926170469776 -> 1926170469632
	1926098995632 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1926098995632 -> 1926170469776
	1926170469776 [label=AccumulateGrad]
	1926170469584 -> 1926170469536
	1926098995536 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1926098995536 -> 1926170469584
	1926170469584 [label=AccumulateGrad]
	1926170469440 -> 1926170469536
	1926098994768 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1926098994768 -> 1926170469440
	1926170469440 [label=AccumulateGrad]
	1926170469344 -> 1926170469200
	1926098994864 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1926098994864 -> 1926170469344
	1926170469344 [label=AccumulateGrad]
	1926170469152 -> 1926170469104
	1926098994960 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1926098994960 -> 1926170469152
	1926170469152 [label=AccumulateGrad]
	1926170469008 -> 1926170469104
	1926098995056 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1926098995056 -> 1926170469008
	1926170469008 [label=AccumulateGrad]
	1926170468912 -> 1926170460464
	1926098995344 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1926098995344 -> 1926170468912
	1926170468912 [label=AccumulateGrad]
	1926170460320 -> 1926170460416
	1926098994384 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1926098994384 -> 1926170460320
	1926170460320 [label=AccumulateGrad]
	1926170460224 -> 1926170460416
	1926098994288 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1926098994288 -> 1926170460224
	1926170460224 [label=AccumulateGrad]
	1926170460560 -> 1926170460608
	1926170460560 [label=CudnnBatchNormBackward0]
	1926170469296 -> 1926170460560
	1926170469296 [label=ConvolutionBackward0]
	1926170469824 -> 1926170469296
	1926170469680 -> 1926170469296
	1926098996208 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1926098996208 -> 1926170469680
	1926170469680 [label=AccumulateGrad]
	1926170460368 -> 1926170460560
	1926098996304 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1926098996304 -> 1926170460368
	1926170460368 [label=AccumulateGrad]
	1926170460272 -> 1926170460560
	1926098996400 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1926098996400 -> 1926170460272
	1926170460272 [label=AccumulateGrad]
	1926170460752 -> 1926170460896
	1926098994096 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1926098994096 -> 1926170460752
	1926170460752 [label=AccumulateGrad]
	1926170461040 -> 1926170461088
	1926098993520 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1926098993520 -> 1926170461040
	1926170461040 [label=AccumulateGrad]
	1926170461232 -> 1926170461088
	1926098993712 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1926098993712 -> 1926170461232
	1926170461232 [label=AccumulateGrad]
	1926170461280 -> 1926170461520
	1926098994192 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1926098994192 -> 1926170461280
	1926170461280 [label=AccumulateGrad]
	1926170461568 -> 1926170461808
	1926098994000 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1926098994000 -> 1926170461568
	1926170461568 [label=AccumulateGrad]
	1926170461904 -> 1926170461808
	1926179737136 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1926179737136 -> 1926170461904
	1926170461904 [label=AccumulateGrad]
	1926170461664 -> 1926170462048
	1926179737520 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1926179737520 -> 1926170461664
	1926170461664 [label=AccumulateGrad]
	1926170462288 -> 1926170462384
	1926179737616 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1926179737616 -> 1926170462288
	1926170462288 [label=AccumulateGrad]
	1926170462192 -> 1926170462384
	1926179737712 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1926179737712 -> 1926170462192
	1926170462192 [label=AccumulateGrad]
	1926170462240 -> 1926170462144
	1926170462480 -> 1926170462864
	1926179738096 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1926179738096 -> 1926170462480
	1926170462480 [label=AccumulateGrad]
	1926170462720 -> 1926170462624
	1926179738192 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1926179738192 -> 1926170462720
	1926170462720 [label=AccumulateGrad]
	1926170462960 -> 1926170462624
	1926179738288 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1926179738288 -> 1926170462960
	1926170462960 [label=AccumulateGrad]
	1926170463248 -> 1926170463200
	1926179738672 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1926179738672 -> 1926170463248
	1926170463248 [label=AccumulateGrad]
	1926170463104 -> 1926170463296
	1926179738768 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1926179738768 -> 1926170463104
	1926170463104 [label=AccumulateGrad]
	1926170463488 -> 1926170463296
	1926179738864 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1926179738864 -> 1926170463488
	1926170463488 [label=AccumulateGrad]
	1926170463632 -> 1926170463584
	1926179739248 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1926179739248 -> 1926170463632
	1926170463632 [label=AccumulateGrad]
	1926170463776 -> 1926170463968
	1926179739344 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1926179739344 -> 1926170463776
	1926170463776 [label=AccumulateGrad]
	1926170463920 -> 1926170463968
	1926179739440 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1926179739440 -> 1926170463920
	1926170463920 [label=AccumulateGrad]
	1926170464208 -> 1926170464112
	1926170464256 -> 1926170464688
	1926179740400 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1926179740400 -> 1926170464256
	1926170464256 [label=AccumulateGrad]
	1926170464592 -> 1926170464784
	1926179740496 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1926179740496 -> 1926170464592
	1926170464592 [label=AccumulateGrad]
	1926170464544 -> 1926170464784
	1926179740592 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1926179740592 -> 1926170464544
	1926170464544 [label=AccumulateGrad]
	1926170464880 -> 1926170465072
	1926179740976 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1926179740976 -> 1926170464880
	1926170464880 [label=AccumulateGrad]
	1926170465264 -> 1926170465120
	1926179741072 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1926179741072 -> 1926170465264
	1926170465264 [label=AccumulateGrad]
	1926170465216 -> 1926170465120
	1926179741168 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1926179741168 -> 1926170465216
	1926170465216 [label=AccumulateGrad]
	1926170465408 -> 1926170465744
	1926179741552 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1926179741552 -> 1926170465408
	1926170465408 [label=AccumulateGrad]
	1926170465600 -> 1926170465696
	1926179741648 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1926179741648 -> 1926170465600
	1926170465600 [label=AccumulateGrad]
	1926170465504 -> 1926170465696
	1926179741744 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1926179741744 -> 1926170465504
	1926170465504 [label=AccumulateGrad]
	1926170465840 -> 1926170465888
	1926170465840 [label=CudnnBatchNormBackward0]
	1926170464928 -> 1926170465840
	1926170464928 [label=ConvolutionBackward0]
	1926170464064 -> 1926170464928
	1926170464448 -> 1926170464928
	1926179739824 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1926179739824 -> 1926170464448
	1926170464448 [label=AccumulateGrad]
	1926170465648 -> 1926170465840
	1926179739920 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1926179739920 -> 1926170465648
	1926170465648 [label=AccumulateGrad]
	1926170465552 -> 1926170465840
	1926179740016 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1926179740016 -> 1926170465552
	1926170465552 [label=AccumulateGrad]
	1926170466032 -> 1926170466176
	1926179742128 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1926179742128 -> 1926170466032
	1926170466032 [label=AccumulateGrad]
	1926170466320 -> 1926170466368
	1926179742224 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1926179742224 -> 1926170466320
	1926170466320 [label=AccumulateGrad]
	1926170466512 -> 1926170466368
	1926179742320 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1926179742320 -> 1926170466512
	1926170466512 [label=AccumulateGrad]
	1926170466560 -> 1926170466800
	1926179742704 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1926179742704 -> 1926170466560
	1926170466560 [label=AccumulateGrad]
	1926170466848 -> 1926170467088
	1926179742800 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1926179742800 -> 1926170466848
	1926170466848 [label=AccumulateGrad]
	1926170467184 -> 1926170467088
	1926179742896 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1926179742896 -> 1926170467184
	1926170467184 [label=AccumulateGrad]
	1926170466944 -> 1926170467328
	1926179743280 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1926179743280 -> 1926170466944
	1926170466944 [label=AccumulateGrad]
	1926170467568 -> 1926170467664
	1926179743376 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1926179743376 -> 1926170467568
	1926170467568 [label=AccumulateGrad]
	1926170467472 -> 1926170467664
	1926179743472 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1926179743472 -> 1926170467472
	1926170467472 [label=AccumulateGrad]
	1926170467520 -> 1926170467424
	1926170467760 -> 1926170468144
	1926179743856 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1926179743856 -> 1926170467760
	1926170467760 [label=AccumulateGrad]
	1926170468000 -> 1926170467904
	1926179743952 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1926179743952 -> 1926170468000
	1926170468000 [label=AccumulateGrad]
	1926170468240 -> 1926170467904
	1926179744048 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1926179744048 -> 1926170468240
	1926170468240 [label=AccumulateGrad]
	1926170460656 -> 1926170460992
	1926179744432 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1926179744432 -> 1926170460656
	1926170460656 [label=AccumulateGrad]
	1926170461616 -> 1926170461472
	1926179744528 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1926179744528 -> 1926170461616
	1926170461616 [label=AccumulateGrad]
	1926170461952 -> 1926170461472
	1926179744624 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1926179744624 -> 1926170461952
	1926170461952 [label=AccumulateGrad]
	1926170462432 -> 1926170463536
	1926179745008 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1926179745008 -> 1926170462432
	1926170462432 [label=AccumulateGrad]
	1926170463392 -> 1926170463872
	1926179745104 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1926179745104 -> 1926170463392
	1926170463392 [label=AccumulateGrad]
	1926170464016 -> 1926170463872
	1926179745200 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1926179745200 -> 1926170464016
	1926170464016 [label=AccumulateGrad]
	1926170464496 -> 1926170464352
	1926170464832 -> 1926170465792
	1926179745584 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1926179745584 -> 1926170464832
	1926170464832 [label=AccumulateGrad]
	1926170466416 -> 1926170466272
	1926179745680 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1926179745680 -> 1926170466416
	1926170466416 [label=AccumulateGrad]
	1926170466752 -> 1926170466272
	1926179745776 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1926179745776 -> 1926170466752
	1926170466752 [label=AccumulateGrad]
	1926170467232 -> 1926103103904
	1926179746160 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1926179746160 -> 1926170467232
	1926170467232 [label=AccumulateGrad]
	1926170468192 -> 1926125435056
	1926179746256 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1926179746256 -> 1926170468192
	1926170468192 [label=AccumulateGrad]
	1926170468336 -> 1926125435056
	1926179746352 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1926179746352 -> 1926170468336
	1926170468336 [label=AccumulateGrad]
	1926103104432 -> 1926103099680
	1926179746736 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1926179746736 -> 1926103104432
	1926103104432 [label=AccumulateGrad]
	1926103105344 -> 1926103099200
	1926179746832 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1926179746832 -> 1926103105344
	1926103105344 [label=AccumulateGrad]
	1926103104192 -> 1926103099200
	1926179746928 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1926179746928 -> 1926103104192
	1926103104192 [label=AccumulateGrad]
	1926103104912 -> 1926103091040
	1926103101792 -> 1926102981776
	1926179747888 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1926179747888 -> 1926103101792
	1926103101792 [label=AccumulateGrad]
	1926102981248 -> 1926102979472
	1926179747984 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1926179747984 -> 1926102981248
	1926102981248 [label=AccumulateGrad]
	1926102977312 -> 1926102979472
	1926179748080 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1926179748080 -> 1926102977312
	1926102977312 [label=AccumulateGrad]
	1926102980720 -> 1926102978080
	1926179748464 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1926179748464 -> 1926102980720
	1926102980720 [label=AccumulateGrad]
	1926102981632 -> 1926102978272
	1926179748560 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1926179748560 -> 1926102981632
	1926102981632 [label=AccumulateGrad]
	1926102982160 -> 1926102978272
	1926179748656 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1926179748656 -> 1926102982160
	1926102982160 [label=AccumulateGrad]
	1926102978896 -> 1926102980144
	1926179749040 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1926179749040 -> 1926102978896
	1926102978896 [label=AccumulateGrad]
	1926102980384 -> 1926102981728
	1926179749136 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1926179749136 -> 1926102980384
	1926102980384 [label=AccumulateGrad]
	1926102979040 -> 1926102981728
	1926179749232 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1926179749232 -> 1926102979040
	1926102979040 [label=AccumulateGrad]
	1926102976640 -> 1926102981152
	1926102976640 [label=CudnnBatchNormBackward0]
	1926102976544 -> 1926102976640
	1926102976544 [label=ConvolutionBackward0]
	1926103102992 -> 1926102976544
	1926102977552 -> 1926102976544
	1926179747312 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1926179747312 -> 1926102977552
	1926102977552 [label=AccumulateGrad]
	1926102978512 -> 1926102976640
	1926179747408 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1926179747408 -> 1926102978512
	1926102978512 [label=AccumulateGrad]
	1926102982064 -> 1926102976640
	1926179747504 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1926179747504 -> 1926102982064
	1926102982064 [label=AccumulateGrad]
	1926102979760 -> 1926102975248
	1926179749616 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1926179749616 -> 1926102979760
	1926102979760 [label=AccumulateGrad]
	1926102981536 -> 1926102982496
	1926179749712 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1926179749712 -> 1926102981536
	1926102981536 [label=AccumulateGrad]
	1926102981824 -> 1926102982496
	1926179749808 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1926179749808 -> 1926102981824
	1926102981824 [label=AccumulateGrad]
	1926102975680 -> 1926102981920
	1926179668336 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1926179668336 -> 1926102975680
	1926102975680 [label=AccumulateGrad]
	1926102982256 -> 1926102974816
	1926179668432 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1926179668432 -> 1926102982256
	1926102982256 [label=AccumulateGrad]
	1926102975872 -> 1926102974816
	1926179668528 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1926179668528 -> 1926102975872
	1926102975872 [label=AccumulateGrad]
	1926102980624 -> 1926102979568
	1926179668912 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1926179668912 -> 1926102980624
	1926102980624 [label=AccumulateGrad]
	1926102981200 -> 1926102980432
	1926179669008 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1926179669008 -> 1926102981200
	1926102981200 [label=AccumulateGrad]
	1926102974768 -> 1926102980432
	1926179669104 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1926179669104 -> 1926102974768
	1926102974768 [label=AccumulateGrad]
	1926102982304 -> 1926102982112
	1926102981584 -> 1926102974960
	1926179669488 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1926179669488 -> 1926102981584
	1926102981584 [label=AccumulateGrad]
	1926102980096 -> 1926102976688
	1926179669584 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1926179669584 -> 1926102980096
	1926102980096 [label=AccumulateGrad]
	1926102974576 -> 1926102976688
	1926179669680 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1926179669680 -> 1926102974576
	1926102974576 [label=AccumulateGrad]
	1926102976880 -> 1926102977696
	1926179670064 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1926179670064 -> 1926102976880
	1926102976880 [label=AccumulateGrad]
	1926102978320 -> 1926102982448
	1926179670160 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1926179670160 -> 1926102978320
	1926102978320 [label=AccumulateGrad]
	1926102980336 -> 1926102982448
	1926179670256 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1926179670256 -> 1926102980336
	1926102980336 [label=AccumulateGrad]
	1926102978416 -> 1926102975440
	1926179670640 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1926179670640 -> 1926102978416
	1926102978416 [label=AccumulateGrad]
	1926102976592 -> 1926102977984
	1926179670736 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1926179670736 -> 1926102976592
	1926102976592 [label=AccumulateGrad]
	1926102982208 -> 1926102977984
	1926179670832 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1926179670832 -> 1926102982208
	1926102982208 [label=AccumulateGrad]
	1926102980768 -> 1926102981296
	1926102976304 -> 1926102975296
	1926179671216 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1926179671216 -> 1926102976304
	1926102976304 [label=AccumulateGrad]
	1926102979856 -> 1926102976832
	1926179671312 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1926179671312 -> 1926102979856
	1926102979856 [label=AccumulateGrad]
	1926102974672 -> 1926102976832
	1926179671408 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1926179671408 -> 1926102974672
	1926102974672 [label=AccumulateGrad]
	1926102981968 -> 1926102978848
	1926179671792 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1926179671792 -> 1926102981968
	1926102981968 [label=AccumulateGrad]
	1926102976256 -> 1926102978800
	1926179671888 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1926179671888 -> 1926102976256
	1926102976256 [label=AccumulateGrad]
	1926102980960 -> 1926102978800
	1926179671984 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1926179671984 -> 1926102980960
	1926102980960 [label=AccumulateGrad]
	1926102982832 -> 1926102982784
	1926179672368 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1926179672368 -> 1926102982832
	1926102982832 [label=AccumulateGrad]
	1926102980576 -> 1926102983024
	1926179672464 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1926179672464 -> 1926102980576
	1926102980576 [label=AccumulateGrad]
	1926102982880 -> 1926102983024
	1926179672560 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1926179672560 -> 1926102982880
	1926102982880 [label=AccumulateGrad]
	1926102983072 -> 1926102983312
	1926102983408 -> 1926102983504
	1926179672944 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1926179672944 -> 1926102983408
	1926102983408 [label=AccumulateGrad]
	1926102983552 -> 1926102983792
	1926179673040 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1926179673040 -> 1926102983552
	1926102983552 [label=AccumulateGrad]
	1926102983888 -> 1926102983792
	1926179673136 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1926179673136 -> 1926102983888
	1926102983888 [label=AccumulateGrad]
	1926102983648 -> 1926102984032
	1926179673520 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1926179673520 -> 1926102983648
	1926102983648 [label=AccumulateGrad]
	1926102984272 -> 1926102984176
	1926179673616 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1926179673616 -> 1926102984272
	1926102984272 [label=AccumulateGrad]
	1926102984224 -> 1926102984176
	1926179673712 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1926179673712 -> 1926102984224
	1926102984224 [label=AccumulateGrad]
	1926102984320 -> 1926102984752
	1926179674096 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1926179674096 -> 1926102984320
	1926102984320 [label=AccumulateGrad]
	1926102984656 -> 1926102984704
	1926179674192 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1926179674192 -> 1926102984656
	1926102984656 [label=AccumulateGrad]
	1926102984848 -> 1926102984704
	1926179674288 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1926179674288 -> 1926102984848
	1926102984848 [label=AccumulateGrad]
	1926102984608 -> 1926102984800
	1926102984992 -> 1926102985184
	1926179674672 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1926179674672 -> 1926102984992
	1926102984992 [label=AccumulateGrad]
	1926102985088 -> 1926102985280
	1926179674768 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1926179674768 -> 1926102985088
	1926102985088 [label=AccumulateGrad]
	1926102985472 -> 1926102985280
	1926179674864 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1926179674864 -> 1926102985472
	1926102985472 [label=AccumulateGrad]
	1926102985616 -> 1926102985568
	1926179675248 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1926179675248 -> 1926102985616
	1926102985616 [label=AccumulateGrad]
	1926102985760 -> 1926102985904
	1926179675344 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1926179675344 -> 1926102985760
	1926102985760 [label=AccumulateGrad]
	1926102986192 -> 1926102985904
	1926179675440 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1926179675440 -> 1926102986192
	1926102986192 [label=AccumulateGrad]
	1926102986288 -> 1926102986240
	1926179675824 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1926179675824 -> 1926102986288
	1926102986288 [label=AccumulateGrad]
	1926102986384 -> 1926102986672
	1926179675920 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1926179675920 -> 1926102986384
	1926102986384 [label=AccumulateGrad]
	1926102986432 -> 1926102986672
	1926179676016 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1926179676016 -> 1926102986432
	1926102986432 [label=AccumulateGrad]
	1926102986576 -> 1926102986768
	1926102986864 -> 1926102987056
	1926179676976 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1926179676976 -> 1926102986864
	1926102986864 [label=AccumulateGrad]
	1926102987248 -> 1926102987104
	1926179677072 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1926179677072 -> 1926102987248
	1926102987248 [label=AccumulateGrad]
	1926102987200 -> 1926102987104
	1926179677168 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1926179677168 -> 1926102987200
	1926102987200 [label=AccumulateGrad]
	1926102987392 -> 1926102987728
	1926179677552 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1926179677552 -> 1926102987392
	1926102987392 [label=AccumulateGrad]
	1926102987584 -> 1926102987488
	1926179677648 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1926179677648 -> 1926102987584
	1926102987584 [label=AccumulateGrad]
	1926102987824 -> 1926102987488
	1926179677744 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1926179677744 -> 1926102987824
	1926102987824 [label=AccumulateGrad]
	1926102988112 -> 1926102988064
	1926179678128 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1926179678128 -> 1926102988112
	1926102988112 [label=AccumulateGrad]
	1926102987968 -> 1926102988304
	1926179678224 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1926179678224 -> 1926102987968
	1926102987968 [label=AccumulateGrad]
	1926102988160 -> 1926102988304
	1926179678320 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1926179678320 -> 1926102988160
	1926102988160 [label=AccumulateGrad]
	1926102988352 -> 1926102988592
	1926102988352 [label=CudnnBatchNormBackward0]
	1926102987632 -> 1926102988352
	1926102987632 [label=ConvolutionBackward0]
	1926102986720 -> 1926102987632
	1926102987152 -> 1926102987632
	1926179676400 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1926179676400 -> 1926102987152
	1926102987152 [label=AccumulateGrad]
	1926102988016 -> 1926102988352
	1926179676496 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1926179676496 -> 1926102988016
	1926102988016 [label=AccumulateGrad]
	1926102988208 -> 1926102988352
	1926179676592 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1926179676592 -> 1926102988208
	1926102988208 [label=AccumulateGrad]
	1926102988688 -> 1926102988784
	1926179678704 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1926179678704 -> 1926102988688
	1926102988688 [label=AccumulateGrad]
	1926102988832 -> 1926102989072
	1926179678800 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1926179678800 -> 1926102988832
	1926102988832 [label=AccumulateGrad]
	1926102989168 -> 1926102989072
	1926179678896 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1926179678896 -> 1926102989168
	1926102989168 [label=AccumulateGrad]
	1926102988928 -> 1926102989312
	1926179679280 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1926179679280 -> 1926102988928
	1926102988928 [label=AccumulateGrad]
	1926102989552 -> 1926102989456
	1926179679376 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1926179679376 -> 1926102989552
	1926102989552 [label=AccumulateGrad]
	1926102989504 -> 1926102989456
	1926179679472 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1926179679472 -> 1926102989504
	1926102989504 [label=AccumulateGrad]
	1926102989600 -> 1926102990032
	1926179679856 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1926179679856 -> 1926102989600
	1926102989600 [label=AccumulateGrad]
	1926102989936 -> 1926102989984
	1926179679952 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1926179679952 -> 1926102989936
	1926102989936 [label=AccumulateGrad]
	1926102990128 -> 1926102989984
	1926179680048 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1926179680048 -> 1926102990128
	1926102990128 [label=AccumulateGrad]
	1926102989888 -> 1926102990080
	1926102990272 -> 1926102990464
	1926179680432 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1926179680432 -> 1926102990272
	1926102990272 [label=AccumulateGrad]
	1926102990368 -> 1926102990560
	1926179680528 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1926179680528 -> 1926102990368
	1926102990368 [label=AccumulateGrad]
	1926102990752 -> 1926102990560
	1926179680624 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1926179680624 -> 1926102990752
	1926102990752 [label=AccumulateGrad]
	1926102979808 -> 1926102977648
	1926179681008 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1926179681008 -> 1926102979808
	1926102979808 [label=AccumulateGrad]
	1926102982688 -> 1926102983120
	1926179681104 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1926179681104 -> 1926102982688
	1926102982688 [label=AccumulateGrad]
	1926102983600 -> 1926102983120
	1926179681200 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1926179681200 -> 1926102983600
	1926102983600 [label=AccumulateGrad]
	1926102984080 -> 1926102984416
	1926179681584 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1926179681584 -> 1926102984080
	1926102984080 [label=AccumulateGrad]
	1926102985040 -> 1926102985520
	1926179681680 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1926179681680 -> 1926102985040
	1926102985040 [label=AccumulateGrad]
	1926102984896 -> 1926102985520
	1926179681776 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1926179681776 -> 1926102984896
	1926102984896 [label=AccumulateGrad]
	1926102985376 -> 1926102986000
	1926102987440 -> 1926102988256
	1926102987440 [label=TBackward0]
	1926102985856 -> 1926102987440
	1926179682448 [label="fc.weight
 (19, 2048)" fillcolor=lightblue]
	1926179682448 -> 1926102985856
	1926102985856 [label=AccumulateGrad]
	1926102988256 -> 1926170162768
}
