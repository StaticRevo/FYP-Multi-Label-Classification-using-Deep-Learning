{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python Libraries\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platform’s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch-related Libraries (Deep Learning)\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "from torch.nn import functional as F  # Functional module for defining functions and loss functions.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Dataset and DataLoader for managing and batching data.\n",
    "import torchvision # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Datasets and transformations for image processing.\n",
    "import torchvision.datasets as datasets  # Datasets for computer vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Make grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations for image preprocessing.\n",
    "from torchsummary import summary # PyTorch model summary for Keras-like model summary.\n",
    "import torchsummary\n",
    "import torchmetrics\n",
    "from torchviz import make_dot  # PyTorch model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for handling class imbalance in object detection.\n",
    "from torchmetrics import MeanMetric, Accuracy  # Intersection over Union (IoU) metric for object detection.\n",
    "from torchmetrics.classification import MultilabelF1Score, MultilabelRecall, MultilabelPrecision, MultilabelAccuracy  # Multilabel classification metrics.\n",
    "\n",
    "import pytorch_lightning as pl  # PyTorch Lightning for high-level training loops.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor # Callbacks for model checkpointing and learning rate monitoring.\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Logger for TensorBoard visualization.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Library for reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation functions.\n",
    "from rasterio.enums import Resampling  # Resampling methods used for resizing raster data.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation and Analysis Libraries\n",
    "import pandas as pd  # Data analysis and manipulation library for DataFrames and CSVs.\n",
    "import numpy as np  # Numpy for array operations and numerical computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics for classification models.\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating static and interactive visualizations.\n",
    "import seaborn as sns  # High-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm  # Progress bar for loops and processes.\n",
    "from PIL import Image  # Image handling, opening, manipulating, and saving.\n",
    "import ast  # Abstract Syntax Trees for parsing Python code.\n",
    "import requests  # HTTP library for sending requests.\n",
    "import zstandard as zstd  # Zstandard compression for fast compression and decompression.\n",
    "from collections import Counter # Counter for counting hashable objects.\n",
    "import certifi  # Certificates for verifying HTTPS requests.\n",
    "import ssl  # Secure Sockets Layer for secure connections.\n",
    "import urllib.request  # URL handling for requests.\n",
    "import kaggle # Kaggle API for downloading datasets.\n",
    "import zipfile # Work with ZIP archives.\n",
    "\n",
    "from torchvision.datasets import MNIST, EuroSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "# Setting a seed ensures that the results are consistent and reproducible each time the code is run.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Check if GPU is enabled\n",
    "# PyTorch allows for the use of GPU to speed up training. Here we check if a GPU is available and set the device accordingly.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    # If a GPU is available, print the name of the GPU\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dataset_path = r'C:\\Users\\isaac\\eurosat_small'\n",
    "    imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataset(Dataset):\n",
    "    # Initialize the EuroSAT dataset class\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset  # Store the dataset\n",
    "        self.transform = transform  # Store the optional transformation function\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]  # Get the image and label from the dataset at the specified index\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transformation if provided\n",
    "\n",
    "        return image, label  # Return the image and label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)  # Return the length of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataModule1(pl.LightningDataModule):\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "    # transforms for images\n",
    "    transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                   transforms.Normalize(Config.imagenet_mean, Config.imagenet_std)])\n",
    "    \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    # load the full dataset\n",
    "    full_dataset = EuroSAT(os.getcwd(), download=True, transform=transform)\n",
    "    \n",
    "    print(full_dataset)\n",
    "\n",
    "    class_names = full_dataset.classes\n",
    "    print(class_names)\n",
    "  \n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    self.eurosat_train, self.eurosat_test = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "    print(f\"Train size: {len(self.eurosat_train)}\")\n",
    "    print(f\"Test size: {len(self.eurosat_test)}\")\n",
    "    \n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.eurosat_train, batch_size=64, num_workers=4, pin_memory=True, shuffle=True)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.eurosat_test, batch_size=64, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataModule2(pl.LightningDataModule):\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "    transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                   transforms.Normalize(Config.imagenet_mean, Config.imagenet_std)])\n",
    "      \n",
    "    full_dataset = datasets.ImageFolder(Config.dataset_path, transform=transform) \n",
    "    print(full_dataset)\n",
    "\n",
    "\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    self.eurosat_train, self.eurosat_test = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.eurosat_train, batch_size=64, num_workers=4, pin_memory=True, shuffle=True)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.eurosat_test, batch_size=64, num_workers=4, pin_memory=True)\n",
    "  \n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.eurosat_test, batch_size=64, num_workers=4, pin_memory=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataModule3(pl.LightningDataModule):\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    full_dataset = datasets.ImageFolder(Config.dataset_path) \n",
    "    print(full_dataset)\n",
    "\n",
    "    # Extract and print the class names from the dataset\n",
    "    self.class_labels = full_dataset.classes\n",
    "\n",
    "    # Split the dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "    # Apply transforms to the specific splits\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    test_dataset.dataset.transform = test_transform\n",
    "\n",
    "    # Assign datasets to module attributes\n",
    "    self.eurosat_train = train_dataset\n",
    "    self.eurosat_test = test_dataset\n",
    "\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.eurosat_train, batch_size=64, num_workers=4, pin_memory=True, shuffle=True)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.eurosat_test, batch_size=64, num_workers=4, pin_memory=True)\n",
    "  \n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.eurosat_test, batch_size=64, num_workers=4, pin_memory=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # EuroSAT images are (3, 64, 64) (channels, width, height) \n",
    "        # Flattened image size for fully connected layer input = 3 * 64 * 64 = 12288\n",
    "        self.layer_1 = nn.Linear(3 * 64 * 64, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 10)  # EuroSAT has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "\n",
    "        # Flatten the image tensor (b, 3, 64, 64) -> (b, 3*64*64)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # Apply layers\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        \n",
    "        # Output as log softmax for stable training\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained ResNet18 model\n",
    "class EuroSATModel2(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(EuroSATModel2, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        # Modify the final layer to match the 10 classes in EuroSAT\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 10)\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "\n",
    "        torchsummary.summary(self.model, (3, 224, 224))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.train_acc(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.val_acc(logits, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.test_acc(logits, y)\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n",
      "{0: 'AnnualCrop', 1: 'Forest', 2: 'HerbaceousVegetation', 3: 'Highway', 4: 'Industrial', 5: 'Pasture', 6: 'PermanentCrop', 7: 'Residential', 8: 'River', 9: 'SeaLake'}\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the data module\n",
    "data_module = EuroSATDataModule3()\n",
    "\n",
    "# Call setup to initialize the datasets\n",
    "data_module.setup()\n",
    "\n",
    "# Access class_labels and create a dictionary\n",
    "class_dict = {i: label for i, label in enumerate(data_module.class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 11,181,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 106.01\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | train_acc | MulticlassAccuracy | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "71        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8842e438bd584e71a11b253b758d2d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712c84d1d96d4e24b88fe3e0aaff89d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c7ad323320446ebfe833e1f2dc3d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db1d6cd23044c84a1fded3de7d12a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f1331fbf564bdf863b3ce1c456f74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bfed7b9bbc4e9b9db59de8b6666cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fee4ffca174f17a7b948d468773eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4fd1579f734ea9be8a889972997374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1c3a055a0345c9bd1a307a1d566b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb796ee7f194929a9c16a939ad1d31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fa446fde4c480caf012f9e1ea13de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389679e0e55643438561c1cf8fa2913b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "data_module = EuroSATDataModule3()\n",
    "data_module.setup()\n",
    "model = EuroSATModel2()\n",
    "\n",
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=r'C:\\Users\\isaac\\FYPCodeLatest\\model_checkpoints',\n",
    "    max_epochs=10,\n",
    "    logger=logger,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b7cb0eaa344cbf9eff22ff9a06880e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9750000238418579\n",
      "        test_loss           0.09977135807275772\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.09977135807275772, 'test_acc': 0.9750000238418579}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n",
      "Predicted Class: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3053a_row9_col0, #T_3053a_row9_col1, #T_3053a_row9_col2 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3053a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3053a_level0_col0\" class=\"col_heading level0 col0\" >Class Index</th>\n",
       "      <th id=\"T_3053a_level0_col1\" class=\"col_heading level0 col1\" >Class Label</th>\n",
       "      <th id=\"T_3053a_level0_col2\" class=\"col_heading level0 col2\" >Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3053a_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_3053a_row0_col1\" class=\"data row0 col1\" >AnnualCrop</td>\n",
       "      <td id=\"T_3053a_row0_col2\" class=\"data row0 col2\" >0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3053a_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_3053a_row1_col1\" class=\"data row1 col1\" >Forest</td>\n",
       "      <td id=\"T_3053a_row1_col2\" class=\"data row1 col2\" >0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3053a_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_3053a_row2_col1\" class=\"data row2 col1\" >HerbaceousVegetation</td>\n",
       "      <td id=\"T_3053a_row2_col2\" class=\"data row2 col2\" >0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3053a_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_3053a_row3_col1\" class=\"data row3 col1\" >Highway</td>\n",
       "      <td id=\"T_3053a_row3_col2\" class=\"data row3 col2\" >0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3053a_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_3053a_row4_col1\" class=\"data row4 col1\" >Industrial</td>\n",
       "      <td id=\"T_3053a_row4_col2\" class=\"data row4 col2\" >0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3053a_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_3053a_row5_col1\" class=\"data row5 col1\" >Pasture</td>\n",
       "      <td id=\"T_3053a_row5_col2\" class=\"data row5 col2\" >0.000259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3053a_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_3053a_row6_col1\" class=\"data row6 col1\" >PermanentCrop</td>\n",
       "      <td id=\"T_3053a_row6_col2\" class=\"data row6 col2\" >0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3053a_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_3053a_row7_col1\" class=\"data row7 col1\" >Residential</td>\n",
       "      <td id=\"T_3053a_row7_col2\" class=\"data row7 col2\" >0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3053a_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_3053a_row8_col1\" class=\"data row8 col1\" >River</td>\n",
       "      <td id=\"T_3053a_row8_col2\" class=\"data row8 col2\" >0.001932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3053a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3053a_row9_col0\" class=\"data row9 col0\" >9</td>\n",
       "      <td id=\"T_3053a_row9_col1\" class=\"data row9 col1\" >SeaLake</td>\n",
       "      <td id=\"T_3053a_row9_col2\" class=\"data row9 col2\" >0.996520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2226c0f8ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_image(model, image_path, transform):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Apply the test transformation\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Send image to the correct device (GPU if available)\n",
    "    image = image.to(model.device)\n",
    "    \n",
    "    # Set the model to evaluation mode and disable gradient calculations\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        # Get the predicted class (highest logit)\n",
    "        _, predicted_class = torch.max(logits, 1)\n",
    "        # Calculate probabilities\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    return predicted_class.item(), probabilities.squeeze().cpu().numpy()  # Return the predicted class and probabilities\n",
    "\n",
    "# Define the test transformation used during validation and testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create an instance of the data module\n",
    "data_module = EuroSATDataModule3()\n",
    "\n",
    "# Call setup to initialize the datasets\n",
    "data_module.setup()\n",
    "\n",
    "# Access class_labels and create a dictionary\n",
    "class_dict = {i: label for i, label in enumerate(data_module.class_labels)}\n",
    "\n",
    "# Test the model with an image\n",
    "image_path = r\"C:\\Users\\isaac\\eurosat_small\\SeaLake\\SeaLake_10.jpg\"  # Replace with the path to your image\n",
    "predicted_class, probabilities = predict_image(model, image_path, test_transform)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "\n",
    "# Create a DataFrame with the class labels and probabilities\n",
    "df = pd.DataFrame(list(class_dict.items()), columns=['Class Index', 'Class Label'])\n",
    "df['Probability'] = probabilities\n",
    "\n",
    "# Highlight the predicted class\n",
    "def highlight_predicted(s):\n",
    "    return ['background-color: yellow' if s['Class Index'] == predicted_class else '' for _ in s]\n",
    "\n",
    "# Apply the highlight function\n",
    "df = df.style.apply(highlight_predicted, axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
