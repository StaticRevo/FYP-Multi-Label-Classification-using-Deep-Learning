{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python Libraries\n",
    "import os  # Operating system interactions, such as reading and writing files.\n",
    "import shutil  # High-level file operations like copying and moving files.\n",
    "import random  # Random number generation for various tasks.\n",
    "import textwrap  # Formatting text into paragraphs of a specified width.\n",
    "import warnings  # Warning control context manager.\n",
    "import zipfile  # Work with ZIP archives.\n",
    "import platform  # Access to underlying platform’s identifying data.\n",
    "import itertools  # Functions creating iterators for efficient looping.\n",
    "from dataclasses import dataclass  # Class decorator for adding special methods to classes.\n",
    "\n",
    "# PyTorch-related Libraries (Deep Learning)\n",
    "import torch  # Core PyTorch library for tensor computations.\n",
    "import torch.nn as nn  # Neural network module for defining layers and architectures.\n",
    "from torch.nn import functional as F  # Functional module for defining functions and loss functions.\n",
    "import torch.optim as optim  # Optimizer module for training models (SGD, Adam, etc.).\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split  # Dataset and DataLoader for managing and batching data.\n",
    "import torchvision # PyTorch's computer vision library.\n",
    "from torchvision import datasets, transforms  # Datasets and transformations for image processing.\n",
    "import torchvision.datasets as datasets  # Datasets for computer vision tasks.\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing.\n",
    "from torchvision.utils import make_grid  # Make grid for displaying images.\n",
    "import torchvision.models as models  # Pretrained models for transfer learning.\n",
    "from torchvision.datasets import MNIST, EuroSAT  # MNIST and EuroSAT datasets for image classification.\n",
    "import torchvision.transforms.functional as TF  # Functional transformations for image preprocessing.\n",
    "from torchsummary import summary # PyTorch model summary for Keras-like model summary.\n",
    "import torchsummary\n",
    "import torchmetrics\n",
    "from torchviz import make_dot  # PyTorch model visualization.\n",
    "from torchvision.ops import sigmoid_focal_loss  # Focal loss for handling class imbalance in object detection.\n",
    "from torchmetrics import MeanMetric, Accuracy  # Intersection over Union (IoU) metric for object detection.\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassRecall, MulticlassPrecision, MulticlassAccuracy  # Multilabel classification metrics.\n",
    "from torchvision.models import ResNet18_Weights  # ResNet-18 model with pretrained weights.\n",
    "from torchcam.methods import GradCAM\n",
    "from torchcam.utils import overlay_mask  # Overlay mask on image for visualization.\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl  # PyTorch Lightning for high-level training loops.\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, Callback # Callbacks for model checkpointing and learning rate monitoring.\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Logger for TensorBoard visualization.\n",
    "\n",
    "# Geospatial Data Processing Libraries\n",
    "import rasterio  # Library for reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Reprojection and transformation functions.\n",
    "from rasterio.enums import Resampling  # Resampling methods used for resizing raster data.\n",
    "from rasterio.plot import show  # Visualization of raster data.\n",
    "\n",
    "# Data Manipulation and Analysis Libraries\n",
    "import pandas as pd  # Data analysis and manipulation library for DataFrames and CSVs.\n",
    "import numpy as np  # Numpy for array operations and numerical computations.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score  # Evaluation metrics for classification models.\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating static and interactive visualizations.\n",
    "import seaborn as sns  # High-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm  # Progress bar for loops and processes.\n",
    "from PIL import Image  # Image handling, opening, manipulating, and saving.\n",
    "import ast  # Abstract Syntax Trees for parsing Python code.\n",
    "import requests  # HTTP library for sending requests.\n",
    "import zstandard as zstd  # Zstandard compression for fast compression and decompression.\n",
    "from collections import Counter # Counter for counting hashable objects.\n",
    "import certifi  # Certificates for verifying HTTPS requests.\n",
    "import ssl  # Secure Sockets Layer for secure connections.\n",
    "import urllib.request  # URL handling for requests.\n",
    "import kaggle # Kaggle API for downloading datasets.\n",
    "import zipfile # Work with ZIP archives.\n",
    "from IPython.display import Image # Display images in Jupyter notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "# Setting a seed ensures that the results are consistent and reproducible each time the code is run.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Check if GPU is enabled\n",
    "# PyTorch allows for the use of GPU to speed up training. Here we check if a GPU is available and set the device accordingly.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    # If a GPU is available, print the name of the GPU\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dataset_path = r'C:\\Users\\isaac\\eurosat_small'\n",
    "    imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    batch_size = 64\n",
    "    num_workers = 4\n",
    "    num_classes = 10\n",
    "    model_name = 'resnet18'\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "    ])\n",
    "    \n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataset(Dataset):\n",
    "    # Initialize the EuroSAT dataset class\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset  # Store the dataset\n",
    "        self.transform = transform  # Store the optional transformation function\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]  # Get the image and label from the dataset at the specified index\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transformation if provided\n",
    "\n",
    "        return image, label  # Return the image and label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)  # Return the length of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataModule(pl.LightningDataModule):\n",
    "  def setup(self, stage=None):\n",
    "    self.train_transform = Config.train_transforms\n",
    "    self.test_transform = Config.val_transforms\n",
    "    \n",
    "    full_dataset = datasets.ImageFolder(Config.dataset_path) \n",
    "    print(full_dataset)\n",
    "\n",
    "    # Extract and print the class names from the dataset\n",
    "    self.class_labels = full_dataset.classes\n",
    "\n",
    "    # Split the dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "    # Apply transforms to the specific splits\n",
    "    train_dataset.dataset.transform = self.train_transform\n",
    "    test_dataset.dataset.transform = self.test_transform\n",
    "\n",
    "    # Assign datasets to module attributes\n",
    "    self.eurosat_train = train_dataset\n",
    "    self.eurosat_test = test_dataset\n",
    "\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.eurosat_train, batch_size=Config.batch_size, num_workers=Config.num_workers, pin_memory=True, shuffle=True, persistent_workers=True)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.eurosat_test, batch_size=Config.batch_size, num_workers=Config.num_workers, pin_memory=True, persistent_workers=True)\n",
    "  \n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.eurosat_test, batch_size=Config.batch_size, num_workers=Config.num_workers, pin_memory=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained ResNet18 model\n",
    "class EuroSATModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(EuroSATModel, self).__init__()\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        # Modify the final layer to match the 10 classes in EuroSAT\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, Config.num_classes)\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Accuracy Metrics\n",
    "        self.train_acc = MulticlassAccuracy(num_classes=Config.num_classes)\n",
    "        self.val_acc = MulticlassAccuracy(num_classes=Config.num_classes)\n",
    "        self.test_acc = MulticlassAccuracy(num_classes=Config.num_classes)\n",
    "\n",
    "        # Recall Metrics\n",
    "        self.train_recall = MulticlassRecall(num_classes=Config.num_classes) \n",
    "        self.val_recall = MulticlassRecall(num_classes=Config.num_classes)\n",
    "        self.test_recall = MulticlassRecall(num_classes=Config.num_classes)\n",
    "\n",
    "        # F1 Score Metrics\n",
    "        self.train_f1 = MulticlassF1Score(num_classes=Config.num_classes) \n",
    "        self.val_f1 = MulticlassF1Score(num_classes=Config.num_classes)\n",
    "        self.test_f1 = MulticlassF1Score(num_classes=Config.num_classes)\n",
    "\n",
    "        # Precision Metrics\n",
    "        self.train_precision = MulticlassPrecision(num_classes=Config.num_classes)\n",
    "        self.val_precision = MulticlassPrecision(num_classes=Config.num_classes)\n",
    "        self.test_precision = MulticlassPrecision(num_classes=Config.num_classes)\n",
    "\n",
    "        torchsummary.summary(self.model, (3, 224, 224))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.train_acc(logits, y)\n",
    "        recall = self.train_recall(logits, y)\n",
    "        f1 = self.train_f1(logits, y)\n",
    "        precision = self.train_precision(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_recall', recall, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_f1', f1, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.val_acc(logits, y)\n",
    "        recall = self.val_recall(logits, y)\n",
    "        f1 = self.val_f1(logits, y)\n",
    "        precision = self.val_precision(logits, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_recall', recall, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_precision', precision, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.test_acc(logits, y)\n",
    "        recall = self.test_recall(logits, y)\n",
    "        f1 = self.test_f1(logits, y)\n",
    "        precision = self.test_precision(logits, y)\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_recall', recall, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_precision', precision, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATModelCallback(Callback):\n",
    "    def __init__(self, checkpoint_dir, model_name):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"epoch={epoch}_{self.model_name}.ckpt\")\n",
    "        torch.save(pl_module.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        base_path = os.path.join(self.checkpoint_dir, f\"final_{self.model_name}\")\n",
    "        final_model_path = f\"{base_path}.ckpt\"\n",
    "        counter = 1\n",
    "        \n",
    "        # Check if the path exists and increment the counter until a unique path is found\n",
    "        while os.path.exists(final_model_path):\n",
    "            final_model_path = f\"{base_path}_{counter}.ckpt\"\n",
    "            counter += 1\n",
    "        \n",
    "        torch.save(pl_module.state_dict(), final_model_path)\n",
    "        print(f\"Final model saved at: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 11,181,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 106.01\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name            | Type                | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0  | model           | ResNet              | 11.2 M | train\n",
      "1  | train_acc       | MulticlassAccuracy  | 0      | train\n",
      "2  | val_acc         | MulticlassAccuracy  | 0      | train\n",
      "3  | test_acc        | MulticlassAccuracy  | 0      | train\n",
      "4  | train_recall    | MulticlassRecall    | 0      | train\n",
      "5  | val_recall      | MulticlassRecall    | 0      | train\n",
      "6  | test_recall     | MulticlassRecall    | 0      | train\n",
      "7  | train_f1        | MulticlassF1Score   | 0      | train\n",
      "8  | val_f1          | MulticlassF1Score   | 0      | train\n",
      "9  | test_f1         | MulticlassF1Score   | 0      | train\n",
      "10 | train_precision | MulticlassPrecision | 0      | train\n",
      "11 | val_precision   | MulticlassPrecision | 0      | train\n",
      "12 | test_precision  | MulticlassPrecision | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "80        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1e85405a894cf49b3fcb1813276721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da368ed4e8b4bf98849902045e9135b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e083abf00440d48e0a2a6d1da7547e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c04013c237481e87a371c9bb59faa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabb2db9f8264754a710d897054b9608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ff97f46cce48dc911bbb4eb75052bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de0fbe5f39b40b9a1ebb67795173cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41784d462c134c7e85a3a8a4d31820a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d685eb4edc54b27a2e88d753a321114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de74cce4741f4d7ea3ef58ee4f315a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e845cc4dfdc348f9bc9d22eb874a4478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb80b923d4c74e5da5edae9ff31cc7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved at: C:\\Users\\isaac\\FYPCodeLatest\\model_checkpoints\\final_resnet18_2.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Initialize the EuroSAT data module and model\n",
    "data_module = EuroSATDataModule()\n",
    "data_module.setup()\n",
    "model = EuroSATModel()\n",
    "\n",
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model_resnet18_eurosat_notpretrained\")\n",
    "\n",
    "checkpoint_dir = r'C:\\Users\\isaac\\FYPCodeLatest\\model_checkpoints'\n",
    "model_callback = EuroSATModelCallback(checkpoint_dir, Config.model_name)\n",
    "\n",
    "# Model Training\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    max_epochs=10,\n",
    "    logger=logger,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[model_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 6600), started 4:18:47 ago. (Use '!kill 6600' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir tb_logs/my_model_resnet18_eurosat_notpretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02c214afb66473399cdde07bb520412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.5690656900405884\n",
      "         test_f1            0.5553316473960876\n",
      "        test_loss           1.3024389743804932\n",
      "     test_precision          0.613920271396637\n",
      "       test_recall          0.5690656900405884\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.3024389743804932,\n",
       "  'test_acc': 0.5690656900405884,\n",
       "  'test_recall': 0.5690656900405884,\n",
       "  'test_f1': 0.5553316473960876,\n",
       "  'test_precision': 0.613920271396637}]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, transform):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Apply the test transformation\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Send image to the correct device (GPU if available)\n",
    "    image = image.to(model.device)\n",
    "    \n",
    "    # Set the model to evaluation mode and disable gradient calculations\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        # Get the predicted class (highest logit)\n",
    "        _, predicted_class = torch.max(logits, 1)\n",
    "        # Calculate probabilities\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    return predicted_class.item(), probabilities.squeeze().cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1000\n",
      "    Root location: C:\\Users\\isaac\\eurosat_small\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot register a hook on a tensor that doesn't require gradient",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Test the model with an image\u001b[39;00m\n\u001b[0;32m     14\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124meurosat\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2750\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRiver\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRiver_2.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to your image\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m predicted_class, probabilities \u001b[38;5;241m=\u001b[39m predict_image(model, image_path, test_transform)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with the class labels and probabilities\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[216], line 14\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(model, image_path, transform)\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Get the predicted class (highest logit)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     _, predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(logits, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[207], line 33\u001b[0m, in \u001b[0;36mEuroSATModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torchvision\\models\\resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1616\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1614\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1616\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, result)\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1619\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torchcam\\methods\\gradient.py:49\u001b[0m, in \u001b[0;36m_GradCAM._hook_g\u001b[1;34m(self, module, input, output, idx)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gradient hook\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks_enabled:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_handles\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mregister_hook(partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_grad, idx\u001b[38;5;241m=\u001b[39midx)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\torch\\_tensor.py:561\u001b[0m, in \u001b[0;36mTensor.register_hook\u001b[1;34m(self, hook)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39mregister_hook, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, hook)\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot register a hook on a tensor that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt require gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
     ]
    }
   ],
   "source": [
    "from PIL import Image  # Image handling, opening, manipulating, and saving.\n",
    "test_transform = Config.test_transforms\n",
    "\n",
    "# Create an instance of the data module\n",
    "data_module = EuroSATDataModule()\n",
    "\n",
    "# Call setup to initialize the datasets\n",
    "data_module.setup()\n",
    "\n",
    "# Access class_labels and create a dictionary\n",
    "class_dict = {i: label for i, label in enumerate(data_module.class_labels)}\n",
    "\n",
    "# Test the model with an image\n",
    "image_path = r\"D:\\Datasets\\eurosat\\2750\\River\\River_2.jpg\"  # Replace with the path to your image\n",
    "predicted_class, probabilities = predict_image(model, image_path, test_transform)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "\n",
    "# Create a DataFrame with the class labels and probabilities\n",
    "df = pd.DataFrame(list(class_dict.items()), columns=['Class Index', 'Class Label'])\n",
    "df['Probability'] = probabilities\n",
    "\n",
    "# Highlight the predicted class\n",
    "def highlight_predicted(s):\n",
    "    return ['background-color: yellow' if s['Class Index'] == predicted_class else '' for _ in s]\n",
    "\n",
    "# Apply the highlight function\n",
    "df = df.style.apply(highlight_predicted, axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model from the checkpoint\n",
    "checkpoint_path = r'C:\\Users\\isaac\\FYPCodeLatest\\model_checkpoints\\final_resnet18_1.ckpt'\n",
    "model.eval()\n",
    "\n",
    "# Load an image\n",
    "image_path = r\"D:\\Datasets\\eurosat\\2750\\River\\River_2.jpg\" # Replace with the path to your image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = Config.test_transforms(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Grad-CAM\n",
    "cam_extractor = GradCAM(model.model, target_layer=model.model.layer4)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_tensor)\n",
    "\n",
    "# Get the index corresponding to the maximum logit score\n",
    "pred_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Calculate the GradCAM heatmap\n",
    "activation_map = cam_extractor(pred_label, logits)[0]\n",
    "\n",
    "# Convert tensor to image\n",
    "activation_map = activation_map.squeeze().cpu().numpy()\n",
    "activation_map = np.clip(activation_map, 0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "# Apply colormap and remove alpha channel\n",
    "heatmap = plt.cm.jet(activation_map)[..., :3]  # Convert to RGB only (discard alpha channel)\n",
    "\n",
    "# Convert heatmap to PIL image and ensure it's RGB\n",
    "heatmap = Image.fromarray((heatmap * 255).astype(np.uint8)).convert(\"RGB\").resize(image.size)\n",
    "\n",
    "# Convert images to numpy arrays for overlay\n",
    "image_np = np.array(image)\n",
    "heatmap_np = np.array(heatmap)\n",
    "\n",
    "# Blend images with alpha\n",
    "alpha = 0.5  # Transparency factor\n",
    "result_np = (alpha * image_np + (1 - alpha) * heatmap_np).astype(np.uint8)\n",
    "\n",
    "# Convert result back to PIL for display\n",
    "result = Image.fromarray(result_np)\n",
    "\n",
    "# Display the result\n",
    "plt.imshow(result)\n",
    "plt.title(\"Grad-CAM Visualization\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
