{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c70661e5-8ba8-4acc-9af5-0374651ce0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File handling and manipulation\n",
    "import os  # Provides functions for interacting with the operating system, such as reading and writing files and directories.\n",
    "import shutil  # Offers high-level file operations like copying and moving files.\n",
    "import tarfile  # Library for reading and writing tar archive files.\n",
    "import requests  # Library for making HTTP requests, often used for downloading files.\n",
    "import zstandard as zstd  # Library for Zstandard compression and decompression.\n",
    "from PIL import Image  # Python Imaging Library (PIL) for opening, manipulating, and saving image files.\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd  # Data analysis and manipulation library providing data structures like DataFrames.\n",
    "import numpy as np  # Provides support for large arrays and matrices, along with mathematical functions to operate on these arrays.\n",
    "\n",
    "# Geospatial data processing\n",
    "import rasterio  # Library for reading and writing geospatial raster data.\n",
    "from rasterio.warp import calculate_default_transform, reproject  # Functions for transforming and resizing raster data.\n",
    "from rasterio.enums import Resampling  # Functions for transforming and resizing raster data.\n",
    "from rasterio.plot import show  # Function for visualizing raster data.\n",
    "\n",
    "# Visualization and plotting\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating static, animated, and interactive visualizations in Python.\n",
    "import seaborn as sns  # Data visualization library based on matplotlib, providing a high-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "# Utilities\n",
    "import random  # Provides functions for generating random numbers and performing random operations.\n",
    "from tqdm import tqdm  # Library for displaying progress bars in loops or processes.\n",
    "import textwrap  # Utilities for wrapping and formatting text to fit a specific width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704ad7ff-9e60-4d1b-aadc-7e3105057d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "#metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata.parquet')\n",
    "# Load the metadata for the snow/cloud/shadow data\n",
    "#snow_cloud_shadow_metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata_for_patches_with_snow_cloud_or_shadow.parquet')\n",
    "\n",
    "metadata_csv = pd.read_csv(r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\one_percent_metadata.csv')\n",
    "\n",
    "# Base directories\n",
    "source_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\BigEarthSubset1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99317162-bfa5-4fe6-b515-6cb2c21c758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame contains 480038 rows.\n",
      "\n",
      "Column Names:\n",
      "Index(['patch_id', 'labels', 'split', 'country', 's1_name', 's2v1_name',\n",
      "       'contains_seasonal_snow', 'contains_cloud_or_shadow'],\n",
      "      dtype='object') \n",
      "\n",
      "First few rows of the DataFrame:\n",
      "                                            patch_id  \\\n",
      "0  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "1  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "2  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "3  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "4  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_2...   \n",
      "\n",
      "                                              labels split  country  \\\n",
      "0  [Arable land, Broad-leaved forest, Mixed fores...  test  Austria   \n",
      "1  [Arable land, Broad-leaved forest, Inland wate...  test  Austria   \n",
      "2  [Arable land, Broad-leaved forest, Coniferous ...  test  Austria   \n",
      "3  [Broad-leaved forest, Complex cultivation patt...  test  Austria   \n",
      "4  [Broad-leaved forest, Complex cultivation patt...  test  Austria   \n",
      "\n",
      "                                        s1_name  \\\n",
      "0  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_26_57   \n",
      "1  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_55   \n",
      "2  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_56   \n",
      "3  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_57   \n",
      "4  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_27_58   \n",
      "\n",
      "                          s2v1_name  contains_seasonal_snow  \\\n",
      "0  S2A_MSIL2A_20170613T101031_26_57                   False   \n",
      "1  S2A_MSIL2A_20170613T101031_27_55                   False   \n",
      "2  S2A_MSIL2A_20170613T101031_27_56                   False   \n",
      "3  S2A_MSIL2A_20170613T101031_27_57                   False   \n",
      "4  S2A_MSIL2A_20170613T101031_27_58                   False   \n",
      "\n",
      "   contains_cloud_or_shadow  \n",
      "0                     False  \n",
      "1                     False  \n",
      "2                     False  \n",
      "3                     False  \n",
      "4                     False  \n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the DataFrame\n",
    "num_rows = metadata_df.shape[0]\n",
    "\n",
    "# Print the number of rows in the DataFrame, followed by a blank line\n",
    "print(f\"The DataFrame contains {num_rows} rows.\\n\")\n",
    "\n",
    "# Display the column names in the DataFrame, followed by a blank line\n",
    "print(\"Column Names:\")\n",
    "print(metadata_df.columns, \"\\n\")\n",
    "\n",
    "# Display the first few rows of the DataFrame with a preceding message\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a94c1d0-a005-48f7-97df-474a4f6ba635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels:\n",
      "1. Arable land\n",
      "2. Broad-leaved forest\n",
      "3. Mixed forest\n",
      "4. Pastures\n",
      "5. Inland waters\n",
      "6. Coniferous forest\n",
      "7. Complex cultivation patterns\n",
      "8. Land principally occupied by agriculture, with significant areas of natural vegetation\n",
      "9. Urban fabric\n",
      "10. Industrial or commercial units\n",
      "11. Inland wetlands\n",
      "12. Transitional woodland, shrub\n",
      "13. Natural grassland and sparsely vegetated areas\n",
      "14. Moors, heathland and sclerophyllous vegetation\n",
      "15. Marine waters\n",
      "16. Coastal wetlands\n",
      "17. Permanent crops\n",
      "18. Beaches, dunes, sands\n",
      "19. Agro-forestry areas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique labels from the metadata dataframe\n",
    "unique_labels = metadata_df['labels'].explode().unique()\n",
    "\n",
    "# Print the unique labels in a numbered table format\n",
    "print(\"Unique Labels:\")\n",
    "for i, label in enumerate(unique_labels, start=1):\n",
    "    print(f\"{i}. {label}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14d9067c-cc08-4051-907c-4449e4c6bc45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vector\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 3: Apply the function to each row in the DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m metadata_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_vector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m labels: create_binary_vector(labels, unique_labels))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Fyp311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(labels)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vector\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 3: Apply the function to each row in the DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m metadata_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_vector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m labels: create_binary_vector(labels, unique_labels))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_labels' is not defined"
     ]
    }
   ],
   "source": [
    "def create_binary_vector(labels, categories):\n",
    "    vector = [1 if category in labels else 0 for category in categories]\n",
    "    return vector\n",
    "\n",
    "# Step 3: Apply the function to each row in the DataFrame\n",
    "metadata_df['binary_vector'] = metadata_df['labels'].apply(lambda labels: create_binary_vector(labels, unique_labels))\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "#print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae140aa4-495e-4a68-8170-34428a34dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame contains 69450 rows.\n",
      "\n",
      "Column Names:\n",
      "Index(['patch_id', 'labels', 'split', 'country', 's1_name', 's2v1_name',\n",
      "       'contains_seasonal_snow', 'contains_cloud_or_shadow'],\n",
      "      dtype='object') \n",
      "\n",
      "First few rows of the DataFrame:\n",
      "                                            patch_id  \\\n",
      "0  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "1  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "2  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "3  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_3...   \n",
      "4  S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_4...   \n",
      "\n",
      "                                              labels  split  country  \\\n",
      "0                   [Arable land, Coniferous forest]   test  Austria   \n",
      "1  [Arable land, Coniferous forest, Land principa...   test  Austria   \n",
      "2  [Broad-leaved forest, Coniferous forest, Indus...  train  Austria   \n",
      "3  [Arable land, Broad-leaved forest, Land princi...   test  Austria   \n",
      "4  [Arable land, Broad-leaved forest, Land princi...   test  Austria   \n",
      "\n",
      "                                        s1_name  \\\n",
      "0  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_38_58   \n",
      "1  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_38_59   \n",
      "2  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_38_87   \n",
      "3  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_39_59   \n",
      "4  S1B_IW_GRDH_1SDV_20170612T165809_33UUP_40_59   \n",
      "\n",
      "                          s2v1_name  contains_seasonal_snow  \\\n",
      "0  S2A_MSIL2A_20170613T101031_38_58                   False   \n",
      "1  S2A_MSIL2A_20170613T101031_38_59                   False   \n",
      "2  S2A_MSIL2A_20170613T101031_38_87                   False   \n",
      "3  S2A_MSIL2A_20170613T101031_39_59                   False   \n",
      "4  S2A_MSIL2A_20170613T101031_40_59                   False   \n",
      "\n",
      "   contains_cloud_or_shadow  \n",
      "0                      True  \n",
      "1                      True  \n",
      "2                      True  \n",
      "3                      True  \n",
      "4                      True  \n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the DataFrame\n",
    "num_rows = snow_cloud_shadow_metadata_df.shape[0]\n",
    "\n",
    "# Print the number of rows in the DataFrame, followed by a blank line\n",
    "print(f\"The DataFrame contains {num_rows} rows.\\n\")\n",
    "\n",
    "# Display the column names in the DataFrame, followed by a blank line\n",
    "print(\"Column Names:\")\n",
    "print(snow_cloud_shadow_metadata_df.columns, \"\\n\")\n",
    "\n",
    "# Display the first few rows of the DataFrame with a preceding message\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(snow_cloud_shadow_metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04fbb98-cc39-476f-9bd6-7ccef240fbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 3/3 [05:34<00:00, 111.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total True (contains_cloud_or_shadow): 0\n",
      "Total False (contains_cloud_or_shadow): 14987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the metadata DataFrame\n",
    "snow_cloud_shadow_metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata_for_patches_with_snow_cloud_or_shadow.parquet')\n",
    "\n",
    "# Initialize counters\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "# Base directory path\n",
    "base_dir = r'C:\\Users\\isaac\\Desktop\\BigEarthTests\\BigEarthSubset1'\n",
    "\n",
    "# Iterate through all folders in the base directory\n",
    "for folder_name in tqdm(os.listdir(base_dir), desc=\"Processing folders\"):\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    for patch_name in os.listdir(folder_path):\n",
    "        patch_path = os.path.join(folder_path, patch_name)\n",
    "\n",
    "        # Filter the DataFrame to get the row with the matching patch_id\n",
    "        matching_row = snow_cloud_shadow_metadata_df.loc[metadata_df['patch_id'] == patch_name]\n",
    "\n",
    "         # If a match is found, extract the labels\n",
    "        if not matching_row.empty:\n",
    "            contains_cloud_or_shadow = matching_row['contains_cloud_or_shadow'].values[0]\n",
    "            if contains_cloud_or_shadow:\n",
    "                true_count += 1\n",
    "                shutil.rmtree(patch_path)\n",
    "            else:\n",
    "                false_count += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total True (contains_cloud_or_shadow): {true_count}\")\n",
    "print(f\"Total False (contains_cloud_or_shadow): {false_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bad63e38-42bd-4a9c-a8ae-2bac29b4a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 69450/69450 [03:37<00:00, 320.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total deleted folders: 69450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Base directories\n",
    "source_dir = r'D:\\Datasets\\BigEarthNet-S2\\100%BigEarthNet'\n",
    "\n",
    "# Initialize a counter for deleted folders\n",
    "deleted_folders_count = 0\n",
    "\n",
    "# Loop through each row in the DataFrame with a progress bar\n",
    "for index, row in tqdm(snow_cloud_shadow_metadata_df.iterrows(), total=len(snow_cloud_shadow_metadata_df), desc=\"Processing files\"):\n",
    "    patch_id = row['patch_id']  # Adjust if the column name differs\n",
    "    # Extract the folder name (all but the last part of the patch_id)\n",
    "    folder_name = '_'.join(patch_id.split('_')[:-2])\n",
    "    dest_dir = os.path.join(source_dir, folder_name, patch_id)  # Use os.path.join for better path handling\n",
    "    \n",
    "    # If dest_dir exists, delete it\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)  # Remove the directory and its contents\n",
    "        deleted_folders_count += 1  # Increment the counter\n",
    "\n",
    "# Print the total number of deleted folders\n",
    "print(f\"Total deleted folders: {deleted_folders_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a619b40-3cd2-47af-88a5-3d2b5dd8847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 115/115 [5:06:14<00:00, 159.78s/it] \n"
     ]
    }
   ],
   "source": [
    "# Base directory containing all data\n",
    "base_dir = r'D:\\Datasets\\BigEarthNet-S2\\100%BigEarthNet'\n",
    "# Directories for subsets\n",
    "subsets = {\n",
    "    '50%': r'D:\\Datasets\\BigEarthNet-S2\\50%BigEarthNet',\n",
    "    '10%': r'D:\\Datasets\\BigEarthNet-S2\\10%BigEarthNet',\n",
    "    '1%': r'D:\\Datasets\\BigEarthNet-S2\\1%BigEarthNet'\n",
    "}\n",
    "\n",
    "# Create subset directories if they don't exist\n",
    "for subset_dir in subsets.values():\n",
    "    os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through all folders in the base directory\n",
    "for folder in tqdm(os.listdir(base_dir), desc=\"Processing folders\"):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # Check if the current path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # List all subfolders\n",
    "        subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "        \n",
    "        # Calculate number of subfolders for each subset\n",
    "        num_subfolders = len(subfolders)\n",
    "        num_50_percent = max(1, num_subfolders // 2)  # Ensure at least one folder is copied\n",
    "        num_10_percent = max(1, num_subfolders // 10)\n",
    "        num_1_percent = max(1, num_subfolders // 100)\n",
    "\n",
    "        # Randomly select subfolders for each subset\n",
    "        selected_50 = random.sample(subfolders, num_50_percent)\n",
    "        selected_10 = random.sample(subfolders, num_10_percent)\n",
    "        selected_1 = random.sample(subfolders, num_1_percent)\n",
    "\n",
    "        # Copy selected subfolders to the respective subset directories\n",
    "        for selected in selected_50:\n",
    "            shutil.copytree(os.path.join(folder_path, selected), os.path.join(subsets['50%'], folder, selected))\n",
    "\n",
    "        for selected in selected_10:\n",
    "            shutil.copytree(os.path.join(folder_path, selected), os.path.join(subsets['10%'], folder, selected))\n",
    "\n",
    "        for selected in selected_1:\n",
    "            shutil.copytree(os.path.join(folder_path, selected), os.path.join(subsets['1%'], folder, selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db687ba8-644a-4c45-a706-361872f549f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_subfolders(base_dir, folder):\n",
    "    # Dictionary to hold folder counts\n",
    "    folder_counts = {}\n",
    "    total_subfolders = 0  # Initialize total subfolder counter\n",
    "    \n",
    "    # Iterate through all folders in the base directory\n",
    "    for folder in tqdm(os.listdir(base_dir), desc=\"Processing folders\"):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        \n",
    "        # Check if the current path is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Count subdirectories within this folder\n",
    "            subfolder_count = sum(os.path.isdir(os.path.join(folder_path, subfolder)) for subfolder in os.listdir(folder_path))\n",
    "            folder_counts[folder] = subfolder_count\n",
    "        \n",
    "            # Update total subfolder count\n",
    "            total_subfolders += subfolder_count\n",
    "\n",
    "    # Print total subfolders\n",
    "    return total_subfolders, folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca28a9fc-4c57-47ed-adea-740e5fc35cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 115/115 [02:46<00:00,  1.44s/it]\n",
      "Processing folders: 100%|██████████| 115/115 [01:08<00:00,  1.69it/s]\n",
      "Processing folders: 100%|██████████| 115/115 [00:11<00:00,  9.91it/s]\n",
      "Processing folders: 100%|██████████| 115/115 [00:00<00:00, 518.06it/s]\n"
     ]
    }
   ],
   "source": [
    "full_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\100%BigEarthNet', '100%BigEarthNet')\n",
    "half_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\50%BigEarthNet', '50%BigEarthNet' )\n",
    "tenth_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\10%BigEarthNet', '10%BigEarthNet' )\n",
    "hundredth_subfolder_count, folder = count_subfolders(r'D:\\Datasets\\BigEarthNet-S2\\1%BigEarthNet', '1%BigEarthNet' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "381ba19d-0d14-4074-9d45-a9d7c8b20c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subfolder count in full dataset: 480038\n",
      "\n",
      "Folder: 50%BigEarthNet | Subfolder Count: 239988 | Percentage: 49.99%\n",
      "Folder: 10%BigEarthNet | Subfolder Count: 47948 | Percentage: 9.99%\n",
      "Folder: 1%BigEarthNet | Subfolder Count: 4750 | Percentage: 0.99%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate and display subfolder count and percentage\n",
    "def display_percentage(partial_count, full_count, folder_name):\n",
    "    percentage = (partial_count / full_count) * 100\n",
    "    print(f\"Folder: {folder_name} | Subfolder Count: {partial_count} | Percentage: {percentage:.2f}%\")\n",
    "\n",
    "# Display the counts and percentages for each folder\n",
    "print(f\"Total subfolder count in full dataset: {full_subfolder_count}\\n\")\n",
    "display_percentage(half_subfolder_count, full_subfolder_count, '50%BigEarthNet')\n",
    "display_percentage(tenth_subfolder_count, full_subfolder_count, '10%BigEarthNet')\n",
    "display_percentage(hundredth_subfolder_count, full_subfolder_count, '1%BigEarthNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae9d7b54-a315-44ea-87be-eafbe846569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 4866it [01:32, 52.63it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in each split in the subset:\n",
      "Test: 1165 (24.53%)\n",
      "Train: 2370 (49.89%)\n",
      "Validation: 1215 (25.58%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_splits_in_folders(root_dir, df):\n",
    "    test_count = 0\n",
    "    train_count = 0\n",
    "    val_count = 0\n",
    "\n",
    "    # Get the total number of directories for the progress bar\n",
    "    total_dirs = sum([len(dirnames) for _, dirnames, _ in os.walk(root_dir)])\n",
    "\n",
    "    # Walk through the directories with a progress bar\n",
    "    for dirpath, dirnames, filenames in tqdm(os.walk(root_dir), total=total_dirs, desc=\"Processing directories\"):\n",
    "        for dirname in dirnames:\n",
    "            # Extract patch_id from the folder name\n",
    "            patch_id = dirname\n",
    "\n",
    "            # Fetch the split from the DataFrame\n",
    "            split = df.loc[df['patch_id'] == patch_id, 'split'].values\n",
    "            \n",
    "            # Check if the patch_id exists in the DataFrame\n",
    "            if len(split) > 0:\n",
    "                split = split[0]\n",
    "                if split == 'test':\n",
    "                    test_count += 1\n",
    "                elif split == 'train':\n",
    "                    train_count += 1\n",
    "                elif split == 'validation':\n",
    "                    val_count += 1\n",
    "    \n",
    "    # Calculate total count\n",
    "    total_count = test_count + train_count + val_count\n",
    "\n",
    "    # Calculate percentages\n",
    "    test_percentage = (test_count / total_count) * 100 if total_count > 0 else 0\n",
    "    train_percentage = (train_count / total_count) * 100 if total_count > 0 else 0\n",
    "    val_percentage = (val_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    # Return the counts and percentages\n",
    "    return {\n",
    "        'test': {'count': test_count, 'percentage': test_percentage},\n",
    "        'train': {'count': train_count, 'percentage': train_percentage},\n",
    "        'validation': {'count': val_count, 'percentage': val_percentage}\n",
    "    }\n",
    "\n",
    "metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata.parquet')\n",
    "\n",
    "# Root directory of the subset\n",
    "root_dir = r'D:\\Datasets\\BigEarthNet-S2\\1%BigEarthNet'\n",
    "\n",
    "# Count the splits in the folders\n",
    "split_counts = count_splits_in_folders(root_dir, metadata_df)\n",
    "\n",
    "print(\"Number of samples in each split in the subset:\")\n",
    "for split_type, data in split_counts.items():\n",
    "    print(f\"{split_type.capitalize()}: {data['count']} ({data['percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "560ef2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in each split in the subset:\n",
      "Test: 119825 (24.96%)\n",
      "Train: 237871 (49.55%)\n",
      "Validation: 122342 (25.49%)\n"
     ]
    }
   ],
   "source": [
    "def count_splits(subset_df):\n",
    "    # Initialize counters\n",
    "    test_count = 0\n",
    "    train_count = 0\n",
    "    val_count = 0\n",
    "    \n",
    "    # Iterate through the subset DataFrame\n",
    "    for split in subset_df['split']:\n",
    "        if split == 'test':\n",
    "            test_count += 1\n",
    "        elif split == 'train':\n",
    "            train_count += 1\n",
    "        elif split == 'validation':\n",
    "            val_count += 1\n",
    "    \n",
    "    # Calculate total count\n",
    "    total_count = test_count + train_count + val_count\n",
    "    \n",
    "    # Calculate percentages\n",
    "    test_percentage = (test_count / total_count) * 100 if total_count > 0 else 0\n",
    "    train_percentage = (train_count / total_count) * 100 if total_count > 0 else 0\n",
    "    val_percentage = (val_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    # Return the counts and percentages\n",
    "    return {\n",
    "        'test': {'count': test_count, 'percentage': test_percentage},\n",
    "        'train': {'count': train_count, 'percentage': train_percentage},\n",
    "        'validation': {'count': val_count, 'percentage': val_percentage}\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "metadata_df = pd.read_parquet(r'C:\\Users\\isaac\\Downloads\\metadata.parquet')\n",
    "\n",
    "# Count the splits in the subset\n",
    "split_counts = count_splits(metadata_df)\n",
    "\n",
    "print(\"Number of samples in each split in the subset:\")\n",
    "for split_type, data in split_counts.items():\n",
    "    print(f\"{split_type.capitalize()}: {data['count']} ({data['percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eda82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
