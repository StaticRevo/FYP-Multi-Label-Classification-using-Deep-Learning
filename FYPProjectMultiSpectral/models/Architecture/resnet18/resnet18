digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2436752180528 [label="
 (1, 19)" fillcolor=darkolivegreen1]
	2437013541632 [label=AddmmBackward0]
	2437013539472 -> 2437013541632
	2436725993648 [label="fc.bias
 (19)" fillcolor=lightblue]
	2436725993648 -> 2437013539472
	2437013539472 [label=AccumulateGrad]
	2437013538368 -> 2437013541632
	2437013538368 [label=ViewBackward0]
	2437013543696 -> 2437013538368
	2437013543696 [label=MeanBackward1]
	2437013537264 -> 2437013543696
	2437013537264 [label=ReluBackward0]
	2437013546240 -> 2437013537264
	2437013546240 [label=AddBackward0]
	2437013540240 -> 2437013546240
	2437013540240 [label=CudnnBatchNormBackward0]
	2437013543888 -> 2437013540240
	2437013543888 [label=ConvolutionBackward0]
	2437013547920 -> 2437013543888
	2437013547920 [label=ReluBackward0]
	2437013545568 -> 2437013547920
	2437013545568 [label=CudnnBatchNormBackward0]
	2437013549504 -> 2437013545568
	2437013549504 [label=ConvolutionBackward0]
	2437013546048 -> 2437013549504
	2437013546048 [label=ReluBackward0]
	2437013551664 -> 2437013546048
	2437013551664 [label=AddBackward0]
	2437013550704 -> 2437013551664
	2437013550704 [label=CudnnBatchNormBackward0]
	2437013547872 -> 2437013550704
	2437013547872 [label=ConvolutionBackward0]
	2437013537168 -> 2437013547872
	2437013537168 [label=ReluBackward0]
	2437013542208 -> 2437013537168
	2437013542208 [label=CudnnBatchNormBackward0]
	2437013537648 -> 2437013542208
	2437013537648 [label=ConvolutionBackward0]
	2437013548928 -> 2437013537648
	2437013548928 [label=ReluBackward0]
	2437013545616 -> 2437013548928
	2437013545616 [label=AddBackward0]
	2437013551280 -> 2437013545616
	2437013551280 [label=CudnnBatchNormBackward0]
	2437013549888 -> 2437013551280
	2437013549888 [label=ConvolutionBackward0]
	2437013546624 -> 2437013549888
	2437013546624 [label=ReluBackward0]
	2437013548304 -> 2437013546624
	2437013548304 [label=CudnnBatchNormBackward0]
	2437013549552 -> 2437013548304
	2437013549552 [label=ConvolutionBackward0]
	2437013553104 -> 2437013549552
	2437013553104 [label=ReluBackward0]
	2437013548832 -> 2437013553104
	2437013548832 [label=AddBackward0]
	2437013552960 -> 2437013548832
	2437013552960 [label=CudnnBatchNormBackward0]
	2437013547680 -> 2437013552960
	2437013547680 [label=ConvolutionBackward0]
	2437013541152 -> 2437013547680
	2437013541152 [label=ReluBackward0]
	2437013540864 -> 2437013541152
	2437013540864 [label=CudnnBatchNormBackward0]
	2437013541584 -> 2437013540864
	2437013541584 [label=ConvolutionBackward0]
	2437013537888 -> 2437013541584
	2437013537888 [label=ReluBackward0]
	2437013541008 -> 2437013537888
	2437013541008 [label=AddBackward0]
	2437013538800 -> 2437013541008
	2437013538800 [label=CudnnBatchNormBackward0]
	2437013538992 -> 2437013538800
	2437013538992 [label=ConvolutionBackward0]
	2437013540000 -> 2437013538992
	2437013540000 [label=ReluBackward0]
	2437013542544 -> 2437013540000
	2437013542544 [label=CudnnBatchNormBackward0]
	2437013539568 -> 2437013542544
	2437013539568 [label=ConvolutionBackward0]
	2437013539904 -> 2437013539568
	2437013539904 [label=ReluBackward0]
	2437013542400 -> 2437013539904
	2437013542400 [label=AddBackward0]
	2437013537792 -> 2437013542400
	2437013537792 [label=CudnnBatchNormBackward0]
	2437013544272 -> 2437013537792
	2437013544272 [label=ConvolutionBackward0]
	2437013543984 -> 2437013544272
	2437013543984 [label=ReluBackward0]
	2437013544992 -> 2437013543984
	2437013544992 [label=CudnnBatchNormBackward0]
	2437013540288 -> 2437013544992
	2437013540288 [label=ConvolutionBackward0]
	2437013544896 -> 2437013540288
	2437013544896 [label=ReluBackward0]
	2437013545808 -> 2437013544896
	2437013545808 [label=AddBackward0]
	2437013545280 -> 2437013545808
	2437013545280 [label=CudnnBatchNormBackward0]
	2437013545856 -> 2437013545280
	2437013545856 [label=ConvolutionBackward0]
	2437013539712 -> 2437013545856
	2437013539712 [label=ReluBackward0]
	2437013546480 -> 2437013539712
	2437013546480 [label=CudnnBatchNormBackward0]
	2437013546384 -> 2437013546480
	2437013546384 [label=ConvolutionBackward0]
	2437013545088 -> 2437013546384
	2437013545088 [label=ReluBackward0]
	2437013541392 -> 2437013545088
	2437013541392 [label=AddBackward0]
	2437013546192 -> 2437013541392
	2437013546192 [label=CudnnBatchNormBackward0]
	2437013542976 -> 2437013546192
	2437013542976 [label=ConvolutionBackward0]
	2437013546336 -> 2437013542976
	2437013546336 [label=ReluBackward0]
	2437013540336 -> 2437013546336
	2437013540336 [label=CudnnBatchNormBackward0]
	2437013545328 -> 2437013540336
	2437013545328 [label=ConvolutionBackward0]
	2437013547536 -> 2437013545328
	2437013547536 [label=MaxPool2DWithIndicesBackward0]
	2437013548784 -> 2437013547536
	2437013548784 [label=ReluBackward0]
	2437013550608 -> 2437013548784
	2437013550608 [label=CudnnBatchNormBackward0]
	2437013550080 -> 2437013550608
	2437013550080 [label=ConvolutionBackward0]
	2437013550560 -> 2437013550080
	2436725993456 [label="conv1.weight
 (64, 12, 7, 7)" fillcolor=lightblue]
	2436725993456 -> 2437013550560
	2437013550560 [label=AccumulateGrad]
	2437013548064 -> 2437013550608
	2436860822544 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2436860822544 -> 2437013548064
	2437013548064 [label=AccumulateGrad]
	2437013547056 -> 2437013550608
	2436860822256 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2436860822256 -> 2437013547056
	2437013547056 [label=AccumulateGrad]
	2437013549744 -> 2437013545328
	2436860822736 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2436860822736 -> 2437013549744
	2437013549744 [label=AccumulateGrad]
	2437013549264 -> 2437013540336
	2436860822832 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2436860822832 -> 2437013549264
	2437013549264 [label=AccumulateGrad]
	2437013548208 -> 2437013540336
	2436860822928 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2436860822928 -> 2437013548208
	2437013548208 [label=AccumulateGrad]
	2437013544128 -> 2437013542976
	2436860822160 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2436860822160 -> 2437013544128
	2437013544128 [label=AccumulateGrad]
	2437013540528 -> 2437013546192
	2436860822064 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2436860822064 -> 2437013540528
	2437013540528 [label=AccumulateGrad]
	2437013546768 -> 2437013546192
	2436860821584 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2436860821584 -> 2437013546768
	2437013546768 [label=AccumulateGrad]
	2437013547536 -> 2437013541392
	2437013547776 -> 2437013546384
	2436860821872 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2436860821872 -> 2437013547776
	2437013547776 [label=AccumulateGrad]
	2437013541296 -> 2437013546480
	2436707090160 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2436707090160 -> 2437013541296
	2437013541296 [label=AccumulateGrad]
	2437013547584 -> 2437013546480
	2436707089872 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2436707089872 -> 2437013547584
	2437013547584 [label=AccumulateGrad]
	2437013541488 -> 2437013545856
	2436707090352 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2436707090352 -> 2437013541488
	2437013541488 [label=AccumulateGrad]
	2437013539232 -> 2437013545280
	2436707089776 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2436707089776 -> 2437013539232
	2437013539232 [label=AccumulateGrad]
	2437013537600 -> 2437013545280
	2436707089680 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2436707089680 -> 2437013537600
	2437013537600 [label=AccumulateGrad]
	2437013545088 -> 2437013545808
	2437013539856 -> 2437013540288
	2436707089584 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2436707089584 -> 2437013539856
	2437013539856 [label=AccumulateGrad]
	2437013544176 -> 2437013544992
	2436707089488 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2436707089488 -> 2437013544176
	2437013544176 [label=AccumulateGrad]
	2437013544608 -> 2437013544992
	2436725985392 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2436725985392 -> 2437013544608
	2437013544608 [label=AccumulateGrad]
	2437013543264 -> 2437013544272
	2436725985776 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2436725985776 -> 2437013543264
	2437013543264 [label=AccumulateGrad]
	2437013543072 -> 2437013537792
	2436725985872 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2436725985872 -> 2437013543072
	2437013543072 [label=AccumulateGrad]
	2437013540432 -> 2437013537792
	2436725985968 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2436725985968 -> 2437013540432
	2437013540432 [label=AccumulateGrad]
	2437013542784 -> 2437013542400
	2437013542784 [label=CudnnBatchNormBackward0]
	2437013545664 -> 2437013542784
	2437013545664 [label=ConvolutionBackward0]
	2437013544896 -> 2437013545664
	2437013545424 -> 2437013545664
	2436707086992 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2436707086992 -> 2437013545424
	2437013545424 [label=AccumulateGrad]
	2437013544560 -> 2437013542784
	2436707087184 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2436707087184 -> 2437013544560
	2437013544560 [label=AccumulateGrad]
	2437013544704 -> 2437013542784
	2436707088816 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2436707088816 -> 2437013544704
	2437013544704 [label=AccumulateGrad]
	2437013540816 -> 2437013539568
	2436725986352 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2436725986352 -> 2437013540816
	2437013540816 [label=AccumulateGrad]
	2437013537984 -> 2437013542544
	2436725986448 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2436725986448 -> 2437013537984
	2437013537984 [label=AccumulateGrad]
	2437013539760 -> 2437013542544
	2436725986544 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2436725986544 -> 2437013539760
	2437013539760 [label=AccumulateGrad]
	2437013538656 -> 2437013538992
	2436725986928 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2436725986928 -> 2437013538656
	2437013538656 [label=AccumulateGrad]
	2437013542640 -> 2437013538800
	2436725987024 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2436725987024 -> 2437013542640
	2437013542640 [label=AccumulateGrad]
	2437013537936 -> 2437013538800
	2436725987120 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2436725987120 -> 2437013537936
	2437013537936 [label=AccumulateGrad]
	2437013539904 -> 2437013541008
	2437013540576 -> 2437013541584
	2436725988080 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2436725988080 -> 2437013540576
	2437013540576 [label=AccumulateGrad]
	2437013539952 -> 2437013540864
	2436725988176 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2436725988176 -> 2437013539952
	2437013539952 [label=AccumulateGrad]
	2437013540768 -> 2437013540864
	2436725988272 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2436725988272 -> 2437013540768
	2437013540768 [label=AccumulateGrad]
	2437013536928 -> 2437013547680
	2436725988656 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2436725988656 -> 2437013536928
	2437013536928 [label=AccumulateGrad]
	2437013550368 -> 2437013552960
	2436725988752 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2436725988752 -> 2437013550368
	2437013550368 [label=AccumulateGrad]
	2437013551184 -> 2437013552960
	2436725988848 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2436725988848 -> 2437013551184
	2437013551184 [label=AccumulateGrad]
	2437013549600 -> 2437013548832
	2437013549600 [label=CudnnBatchNormBackward0]
	2437013541920 -> 2437013549600
	2437013541920 [label=ConvolutionBackward0]
	2437013537888 -> 2437013541920
	2437013541968 -> 2437013541920
	2436725987504 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2436725987504 -> 2437013541968
	2437013541968 [label=AccumulateGrad]
	2437013540672 -> 2437013549600
	2436725987600 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2436725987600 -> 2437013540672
	2437013540672 [label=AccumulateGrad]
	2437013540192 -> 2437013549600
	2436725987696 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2436725987696 -> 2437013540192
	2437013540192 [label=AccumulateGrad]
	2437013552480 -> 2437013549552
	2436725989232 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2436725989232 -> 2437013552480
	2437013552480 [label=AccumulateGrad]
	2437013552576 -> 2437013548304
	2436725989328 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2436725989328 -> 2437013552576
	2437013552576 [label=AccumulateGrad]
	2437013548976 -> 2437013548304
	2436725989424 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2436725989424 -> 2437013548976
	2437013548976 [label=AccumulateGrad]
	2437013545712 -> 2437013549888
	2436725989808 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2436725989808 -> 2437013545712
	2437013545712 [label=AccumulateGrad]
	2437013546528 -> 2437013551280
	2436725989904 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2436725989904 -> 2437013546528
	2437013546528 [label=AccumulateGrad]
	2437013549840 -> 2437013551280
	2436725990000 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2436725990000 -> 2437013549840
	2437013549840 [label=AccumulateGrad]
	2437013553104 -> 2437013545616
	2437013543360 -> 2437013537648
	2436725990960 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2436725990960 -> 2437013543360
	2437013543360 [label=AccumulateGrad]
	2437013537456 -> 2437013542208
	2436725991056 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2436725991056 -> 2437013537456
	2437013537456 [label=AccumulateGrad]
	2437013538608 -> 2437013542208
	2436725991152 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2436725991152 -> 2437013538608
	2437013538608 [label=AccumulateGrad]
	2437013542736 -> 2437013547872
	2436725991536 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2436725991536 -> 2437013542736
	2437013542736 [label=AccumulateGrad]
	2437013544416 -> 2437013550704
	2436725991632 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2436725991632 -> 2437013544416
	2437013544416 [label=AccumulateGrad]
	2437013551328 -> 2437013550704
	2436725991728 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2436725991728 -> 2437013551328
	2437013551328 [label=AccumulateGrad]
	2437013552912 -> 2437013551664
	2437013552912 [label=CudnnBatchNormBackward0]
	2437013536880 -> 2437013552912
	2437013536880 [label=ConvolutionBackward0]
	2437013548928 -> 2437013536880
	2437013547104 -> 2437013536880
	2436725990384 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2436725990384 -> 2437013547104
	2437013547104 [label=AccumulateGrad]
	2437013543312 -> 2437013552912
	2436725990480 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2436725990480 -> 2437013543312
	2437013543312 [label=AccumulateGrad]
	2437013546144 -> 2437013552912
	2436725990576 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2436725990576 -> 2437013546144
	2437013546144 [label=AccumulateGrad]
	2437013551232 -> 2437013549504
	2436725992112 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2436725992112 -> 2437013551232
	2437013551232 [label=AccumulateGrad]
	2437013549984 -> 2437013545568
	2436725992208 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2436725992208 -> 2437013549984
	2437013549984 [label=AccumulateGrad]
	2437013549072 -> 2437013545568
	2436725992304 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2436725992304 -> 2437013549072
	2437013549072 [label=AccumulateGrad]
	2437013548592 -> 2437013543888
	2436725992688 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2436725992688 -> 2437013548592
	2437013548592 [label=AccumulateGrad]
	2437013539088 -> 2437013540240
	2436725992784 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2436725992784 -> 2437013539088
	2437013539088 [label=AccumulateGrad]
	2437013544080 -> 2437013540240
	2436725992880 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2436725992880 -> 2437013544080
	2437013544080 [label=AccumulateGrad]
	2437013546048 -> 2437013546240
	2437013542304 -> 2437013541632
	2437013542304 [label=TBackward0]
	2437013545184 -> 2437013542304
	2436725993552 [label="fc.weight
 (19, 512)" fillcolor=lightblue]
	2436725993552 -> 2437013545184
	2437013545184 [label=AccumulateGrad]
	2437013541632 -> 2436752180528
}
